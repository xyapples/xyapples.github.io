<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>LinuxSre云原生</title>
        <link>http://ixuyong.cn</link>
        <description>专注于 Linux 运维、云计算、云原⽣等技术</description>
        <language>zh-CN</language>
        <pubDate>Sun, 18 May 2025 21:42:46 +0800</pubDate>
        <lastBuildDate>Sun, 18 May 2025 21:42:46 +0800</lastBuildDate>
        <category>Docker</category>
        <category>Harbor</category>
        <category>Kubernetes</category>
        <category>Redis</category>
        <category>rsync</category>
        <category>Windows</category>
        <category>MySQL</category>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/626047790.html</guid>
            <title>消费租赁系统微服务应用交付实践</title>
            <link>http://ixuyong.cn/posts/626047790.html</link>
            <category>Kubernetes</category>
            <pubDate>Sun, 18 May 2025 21:42:46 +0800</pubDate>
            <description><![CDATA[ &lt;div class=&#34;hbe hbe-container&#34; id=&#34;hexo-blog-encrypt&#34; data-wpm=&#34;抱歉, 这个密码看着不太对, 请再试试。&#34; data-whm=&#34;抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。&#34;&gt;
  &lt;script id=&#34;hbeData&#34; type=&#34;hbeData&#34; data-hmacdigest=&#34;d691eff019a6b49d8ce1d4f159f97539d9877a88cbb85643a68d5b9f7f14459f&#34;&gt;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&lt;/script&gt;
  &lt;div class=&#34;hbe hbe-content&#34;&gt;
    &lt;div class=&#34;hbe hbe-input hbe-input-xray&#34;&gt;
      &lt;input class=&#34;hbe hbe-input-field hbe-input-field-xray&#34; type=&#34;password&#34; id=&#34;hbePass&#34;&gt;
      &lt;label class=&#34;hbe hbe-input-label hbe-input-label-xray&#34; for=&#34;hbePass&#34;&gt;
        &lt;span class=&#34;hbe hbe-input-label-content hbe-input-label-content-xray&#34;&gt;您好, 这里需要输入密码。&lt;/span&gt;
      &lt;/label&gt;
      &lt;svg class=&#34;hbe hbe-graphic hbe-graphic-xray&#34; width=&#34;300%&#34; height=&#34;100%&#34; viewBox=&#34;0 0 1200 60&#34; preserveAspectRatio=&#34;none&#34;&gt;
        &lt;path d=&#34;M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
        &lt;path d=&#34;M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
      &lt;/svg&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;script data-pjax src=&#34;/lib/hbe.js&#34;&gt;&lt;/script&gt;&lt;link href=&#34;/css/hbe.style.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt; ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/1888662579.html</guid>
            <title>Containerd常用命令</title>
            <link>http://ixuyong.cn/posts/1888662579.html</link>
            <category>Docker</category>
            <pubDate>Wed, 14 May 2025 20:29:07 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;containerd常用命令&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#containerd常用命令&#34;&gt;#&lt;/a&gt; Containerd 常用命令&lt;/h3&gt;
&lt;h4 id=&#34;1-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-安装containerd&#34;&gt;#&lt;/a&gt; 1. 安装 Containerd&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;1.1 配置安装源&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1.2 安装 docker-ce、containerd&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.3 配置 Containerd 所需的模块&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1.4 加载模块&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1.5 配置 Containerd 所需的内核&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1.6 加载内核&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1.7 生成 Containerd 的配置文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1.8 更改 Containerd 的 Cgroup 和 Pause 镜像&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1.9 启动 Containerd，并配置开机自启动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable --now containerd
systemctl status  containerd 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-containerd-配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-containerd-配置镜像加速&#34;&gt;#&lt;/a&gt; 2. Containerd 配置镜像加速&lt;/h4&gt;
&lt;p&gt;打开 /etc/containerd/config.toml 文件，找到 [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors] 部分，添加所需的镜像源配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
[plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors]
  [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
    endpoint = [
      &amp;quot;https://docker.io&amp;quot;,
      &amp;quot;https://6qxc6b6n.mirror.aliyuncs.com&amp;quot;,
      &amp;quot;https://docker.m.daocloud.io&amp;quot;,
      &amp;quot;https://dockerproxy.com/&amp;quot;
    ]
  [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;gcr.io&amp;quot;]
    endpoint = [
      &amp;quot;https://gcr.m.daocloud.io&amp;quot;,
      &amp;quot;https://gcr.nju.edu.cn&amp;quot;,
      &amp;quot;https://gcr.dockerproxy.com&amp;quot;
    ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;重新启动 Containerd&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-containerd常用操作命令实践&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-containerd常用操作命令实践&#34;&gt;#&lt;/a&gt; 3. Containerd 常用操作命令实践&lt;/h4&gt;
&lt;h5 id=&#34;31-查看containerd命名空间&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-查看containerd命名空间&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.1 查看 Containerd 命名空间&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;namespace 来于指定类似于工作空间的隔离区域&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-node02 ~]# ctr namespace ls 
NAME    LABELS 
default        
k8s.io         
moby 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;32-查看containerd镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-查看containerd镜像&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.2 查看 Containerd 镜像&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;因为没有指定 namespace，所以查看的是默认命名空间下的镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ctr images ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看指定命名空间 k8s.io 下的镜像&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io images ls
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-拉取containerd镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-拉取containerd镜像&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.3 拉取 Containerd 镜像&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;拉取指定命名空间 k8s.io 镜像 pause-amd64:3.2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ctr -n k8s.io images pull registry.aliyuncs.com/google_containers/pause-amd64:3.2
 ctr -n k8s.io images pull docker.io/library/nginx:1.21
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;34-删除containerd镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-删除containerd镜像&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.4 删除 containerd 镜像&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io images rm registry.aliyuncs.com/google_containers/pause-amd64:3.2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;35-导出containerd镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#35-导出containerd镜像&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.5 导出 Containerd 镜像&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io images export pause.tar.gz registry.aliyuncs.com/google_containers/pause-amd64:3.2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;36-导入containerd镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#36-导入containerd镜像&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.6 导入 Containerd 镜像&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io image import pause.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;docker save -o 命令导出来的镜像可以用 ctr images import 导出，同理 ctr images export 导出来的镜像也可以有 docker load 还原。&lt;/em&gt;&lt;/p&gt;
&lt;h5 id=&#34;37-标记containerd镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#37-标记containerd镜像&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.7 标记 Containerd 镜像&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io images tag registry.aliyuncs.com/google_containers/pause-amd64:3.2 pause:3.2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;38-运行containerd容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#38-运行containerd容器&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.8 运行 Containerd 容器&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;在后台运行一个 centos 镜像的容器，名称叫做 centos_k8s&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ctr -n k8s.io  run -d  docker.io/library/nginx:1.21 web
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;39-查看运行容器的task&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#39-查看运行容器的task&#34;&gt;#&lt;/a&gt; &lt;strong&gt;3.9 查看运行容器的 task&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io task ls
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;310-启动指定容器task&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#310-启动指定容器task&#34;&gt;#&lt;/a&gt; 3.10 启动指定容器 task&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io task start -d centos_k8s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;311-进入指定容器task&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#311-进入指定容器task&#34;&gt;#&lt;/a&gt; 3.11 进入指定容器 task&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io task exec --exec-id 3118 -t web /bin/bash
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;312-删除指定容器task&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#312-删除指定容器task&#34;&gt;#&lt;/a&gt; 3.12 删除指定容器 task&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io task rm -f web
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;313-停止指定容器task&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#313-停止指定容器task&#34;&gt;#&lt;/a&gt; 3.13 停止指定容器 task&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io task kill --signal 9 centos_k8s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;314-查看容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#314-查看容器&#34;&gt;#&lt;/a&gt; 3.14 查看容器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io c list
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;315-删除容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#315-删除容器&#34;&gt;#&lt;/a&gt; 3.15 删除容器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ctr -n k8s.io c rm centos
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;删除容器以前需要将 task 删除，不然会报以下错误&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-node02 ~]# ctr -n k8s.io c rm web 
ERRO[0000] failed to delete container &amp;quot;web&amp;quot;              error=&amp;quot;cannot delete a non stopped container: &amp;#123;running 0 0001-01-01 00:00:00 +0000 UTC&amp;#125;&amp;quot;
ctr: cannot delete a non stopped container: &amp;#123;running 0 0001-01-01 00:00:00 +0000 UTC&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-docker与containerd常用命令对比&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-docker与containerd常用命令对比&#34;&gt;#&lt;/a&gt; 4. Docker 与 Containerd 常用命令对比&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;th&gt;docker 命令&lt;/th&gt;
&lt;th&gt;containerd 命令&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;查看本地镜像&lt;/td&gt;
&lt;td&gt;docker images&lt;/td&gt;
&lt;td&gt;ctr images ls&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;拉取镜像&lt;/td&gt;
&lt;td&gt;docker pull imagename&lt;/td&gt;
&lt;td&gt;ctr images pull imagename&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;推送镜像&lt;/td&gt;
&lt;td&gt;docker push imagename&lt;/td&gt;
&lt;td&gt;ctr images push imagename&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;给镜像打标签&lt;/td&gt;
&lt;td&gt;docker tag imagename tagname&lt;/td&gt;
&lt;td&gt;ctr images tag imagename tagname&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;导出镜像&lt;/td&gt;
&lt;td&gt;docker save filename imagename&lt;/td&gt;
&lt;td&gt;ctr images export filename imagename&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;导入镜像&lt;/td&gt;
&lt;td&gt;docker load filename&lt;/td&gt;
&lt;td&gt;ctr image import filename&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;运行并创建容器&lt;/td&gt;
&lt;td&gt;docker run [options] imagename commond&lt;/td&gt;
&lt;td&gt;ctr run [options]  imagenamecontainername&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;进入容器&lt;/td&gt;
&lt;td&gt;docker exec [options] names commond&lt;/td&gt;
&lt;td&gt;ctr task exec [options]  names commond&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;查看运行的容器&lt;/td&gt;
&lt;td&gt;docker ps&lt;/td&gt;
&lt;td&gt;ctr task list&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;删除容器&lt;/td&gt;
&lt;td&gt;docker rm [options] names&lt;/td&gt;
&lt;td&gt;1.ctr task rm -f names 2. ctr c rm -f names&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/1490514396.html</guid>
            <title>Redis Cluster集群部署</title>
            <link>http://ixuyong.cn/posts/1490514396.html</link>
            <category>Redis</category>
            <pubDate>Mon, 12 May 2025 21:21:44 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;redis-cluster集群部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#redis-cluster集群部署&#34;&gt;#&lt;/a&gt; Redis Cluster 集群部署&lt;/h3&gt;
&lt;h4 id=&#34;1-环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-环境配置&#34;&gt;#&lt;/a&gt; 1、环境配置&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、Selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;11-配置yum源&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-配置yum源&#34;&gt;#&lt;/a&gt; 1.1 配置 yum 源&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#rocky linux配置
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/rocky-*.repo
yum clean all &amp;amp;&amp;amp; yum makecache
mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-配置文件描述符&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-配置文件描述符&#34;&gt;#&lt;/a&gt; 1.3 配置文件描述符&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-系统内核参数调优&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-系统内核参数调优&#34;&gt;#&lt;/a&gt; 1.4 系统内核参数调优&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# 修改/etc/sysctl.conf文件
cat &amp;gt;&amp;gt; /etc/sysctl.conf &amp;lt;&amp;lt;EOF
vm.max_map_count = 262144
vm.swappiness=1

net.ipv4.tcp_fin_timeout=2
net.ipv4.tcp_tw_reuse=1
#net.ipv4.tcp_tw_recycle=1
net.ipv4.tcp_syncookies=1
net.ipv4.tcp_keepalive_time=600
net.ipv4.ip_local_port_range=4000 65000
net.ipv4.tcp_max_syn_backlog=16384
net.ipv4.route.gc_timeout=100
net.ipv4.tcp_max_tw_buckets= 5000

net.ipv4.tcp_syn_retries=1
net.ipv4.tcp_synack_retries=1
net.core.somaxconn=16384
net.core.netdev_max_backlog=16384
net.ipv4.tcp_max_orphans=16384

# 设置最大内存共享段大小bytes
kernel.shmmax=15461882265
kernel.shmall=3774873
# 修改消息队列长度
kernel.msgmax=65535
kernel.msgmnb=65535
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-修改默认限制内存&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-修改默认限制内存&#34;&gt;#&lt;/a&gt; 1.5 修改默认限制内存&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt;/etc/systemd/system.conf&amp;lt;&amp;lt; EOF
DefaultLimitNOFILE=65536
DefaultLimitNPROC=32000
DefaultLimitMEMLOCK=infinity
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;16-执行命令生效状态&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-执行命令生效状态&#34;&gt;#&lt;/a&gt; 1.6 执行命令生效状态&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;sysctl -p
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;17-安装基础软件包&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#17-安装基础软件包&#34;&gt;#&lt;/a&gt; 1.7 安装基础软件包&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim unzip net-tools telnet tree yum-utils device-mapper-persistent-data \
lvm2 git ntpdate nfs-utils iotop httpd-tools dos2unix lrzsz -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;18-升级系统&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#18-升级系统&#34;&gt;#&lt;/a&gt; 1.8 升级系统&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-redis-cluster部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-redis-cluster部署&#34;&gt;#&lt;/a&gt; 2、Redis cluster 部署&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;192.168.1.135&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;192.168.1.136&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;192.168.1.137&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node1：7001&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node1：7001&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node1：7001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node2：7002&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node2：7002&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node2：7002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node3：7003&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node3：7003&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;node3：7003&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;21-安装包下载&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-安装包下载&#34;&gt;#&lt;/a&gt; 2.1 安装包下载&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;wget https://download.redis.io/releases/redis-7.2.1.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-安装-redis&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-安装-redis&#34;&gt;#&lt;/a&gt; 2.2 安装 redis&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;yum install gcc-c++ -y
mkdir /soft
tar -xzvf redis-7.2.1.tar.gz -C /soft
ln -s /soft/redis-7.2.1 /soft/redis
cd /soft/redis
make
make install prefix=/soft/redis
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-生成集群配置文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-生成集群配置文件&#34;&gt;#&lt;/a&gt; 2.3 生成集群配置文件&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /soft/redis/data/7001
mkdir -p /soft/redis/data/7002
mkdir -p /soft/redis/data/7003
mkdir -p /soft/redis/log
cd /soft/redis
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;redis_7001.conf 配置文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /soft/redis/redis_7001.conf &amp;lt;&amp;lt;EOF
protected-mode yes
port 7001
requirepass admin123
masterauth admin123
cluster-enabled yes
cluster-config-file nodes-7001.conf
cluster-node-timeout 5000
maxmemory 2GB
maxmemory-policy volatile-lru
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize yes
pidfile /soft/redis/data/redis7001.pid
loglevel notice
logfile &amp;quot;/soft/redis/log/redis7001.log&amp;quot;
#databases 16
always-show-logo no
set-proc-title yes
proc-title-template &amp;quot;&amp;#123;title&amp;#125; &amp;#123;listen-addr&amp;#125; &amp;#123;server-mode&amp;#125;&amp;quot;
locale-collate &amp;quot;&amp;quot;
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
rdb-del-sync-files no
dir /soft/redis/data/7001
replica-serve-stale-data yes
replica-read-only yes
repl-diskless-sync yes
repl-diskless-sync-delay 5
repl-diskless-sync-max-replicas 0
repl-diskless-load disabled
repl-disable-tcp-nodelay no
replica-priority 100
acllog-max-len 128
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
replica-lazy-flush no
lazyfree-lazy-user-del no
lazyfree-lazy-user-flush no
oom-score-adj no
oom-score-adj-values 0 200 800
disable-thp yes
appendonly no
appendfilename &amp;quot;appendonly.aof&amp;quot;
appenddirname &amp;quot;appendonlydir&amp;quot;
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble yes
aof-timestamp-enabled no
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events &amp;quot;&amp;quot;
hash-max-listpack-entries 512
hash-max-listpack-value 64
list-max-listpack-size -2
list-compress-depth 0
set-max-intset-entries 512
set-max-listpack-entries 128
set-max-listpack-value 64
zset-max-listpack-entries 128
zset-max-listpack-value 64
hll-sparse-max-bytes 3000
stream-node-max-bytes 4096
stream-node-max-entries 100
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
dynamic-hz yes
aof-rewrite-incremental-fsync yes
rdb-save-incremental-fsync yes
jemalloc-bg-thread yes
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;redis_7002.conf 配置文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /soft/redis/redis_7002.conf &amp;lt;&amp;lt;EOF
protected-mode yes
port 7002
requirepass admin123
masterauth admin123
cluster-enabled yes
cluster-config-file nodes-7002.conf
cluster-node-timeout 5000
maxmemory 2GB
maxmemory-policy  volatile-lru
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize yes
pidfile /soft/redis/data/redis7002.pid
loglevel notice
logfile &amp;quot;/soft/redis/log/redis7002.log&amp;quot;
#databases 16
always-show-logo no
set-proc-title yes
proc-title-template &amp;quot;&amp;#123;title&amp;#125; &amp;#123;listen-addr&amp;#125; &amp;#123;server-mode&amp;#125;&amp;quot;
locale-collate &amp;quot;&amp;quot;
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
rdb-del-sync-files no
dir /soft/redis/data/7002
replica-serve-stale-data yes
replica-read-only yes
repl-diskless-sync yes
repl-diskless-sync-delay 5
repl-diskless-sync-max-replicas 0
repl-diskless-load disabled
repl-disable-tcp-nodelay no
replica-priority 100
acllog-max-len 128
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
replica-lazy-flush no
lazyfree-lazy-user-del no
lazyfree-lazy-user-flush no
oom-score-adj no
oom-score-adj-values 0 200 800
disable-thp yes
appendonly no
appendfilename &amp;quot;appendonly.aof&amp;quot;
appenddirname &amp;quot;appendonlydir&amp;quot;
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble yes
aof-timestamp-enabled no
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events &amp;quot;&amp;quot;
hash-max-listpack-entries 512
hash-max-listpack-value 64
list-max-listpack-size -2
list-compress-depth 0
set-max-intset-entries 512
set-max-listpack-entries 128
set-max-listpack-value 64
zset-max-listpack-entries 128
zset-max-listpack-value 64
hll-sparse-max-bytes 3000
stream-node-max-bytes 4096
stream-node-max-entries 100
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
dynamic-hz yes
aof-rewrite-incremental-fsync yes
rdb-save-incremental-fsync yes
jemalloc-bg-thread yes
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;redis_7003.conf 配置文件&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /soft/redis/redis_7003.conf &amp;lt;&amp;lt;EOF
protected-mode yes
port 7003
requirepass admin123
masterauth admin123
cluster-enabled yes
cluster-config-file nodes-7003.conf
cluster-node-timeout 5000
maxmemory 2GB
maxmemory-policy  volatile-lru
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize yes
pidfile /soft/redis/data/redis7003.pid
loglevel notice
logfile &amp;quot;/soft/redis/log/redis7003.log&amp;quot;
#databases 16
always-show-logo no
set-proc-title yes
proc-title-template &amp;quot;&amp;#123;title&amp;#125; &amp;#123;listen-addr&amp;#125; &amp;#123;server-mode&amp;#125;&amp;quot;
locale-collate &amp;quot;&amp;quot;
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
rdb-del-sync-files no
dir /soft/redis/data/7003
replica-serve-stale-data yes
replica-read-only yes
repl-diskless-sync yes
repl-diskless-sync-delay 5
repl-diskless-sync-max-replicas 0
repl-diskless-load disabled
repl-disable-tcp-nodelay no
replica-priority 100
acllog-max-len 128
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
replica-lazy-flush no
lazyfree-lazy-user-del no
lazyfree-lazy-user-flush no
oom-score-adj no
oom-score-adj-values 0 200 800
disable-thp yes
appendonly no
appendfilename &amp;quot;appendonly.aof&amp;quot;
appenddirname &amp;quot;appendonlydir&amp;quot;
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble yes
aof-timestamp-enabled no
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events &amp;quot;&amp;quot;
hash-max-listpack-entries 512
hash-max-listpack-value 64
list-max-listpack-size -2
list-compress-depth 0
set-max-intset-entries 512
set-max-listpack-entries 128
set-max-listpack-value 64
zset-max-listpack-entries 128
zset-max-listpack-value 64
hll-sparse-max-bytes 3000
stream-node-max-bytes 4096
stream-node-max-entries 100
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
dynamic-hz yes
aof-rewrite-incremental-fsync yes
rdb-save-incremental-fsync yes
jemalloc-bg-thread yes
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-redis开机自启&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-redis开机自启&#34;&gt;#&lt;/a&gt; 2.4 Redis 开机自启&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;redis_7001.service&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; &amp;quot;EOF&amp;quot; &amp;gt; /usr/lib/systemd/system/redis_7001.service
[Unit]
Description=Redis 7001 service
Documentation=https://redis.io/documentation
Wants=network-online.target
After=network-online.target
[Service]
Type=forking
LimitNOFILE=10032
User=root
Group=root
ExecStart=/soft/redis/src/redis-server /soft/redis/redis_7001.conf
PrivateTmp=true
[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;redis_7002.service&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; &amp;quot;EOF&amp;quot; &amp;gt; /usr/lib/systemd/system/redis_7002.service
[Unit]
Description=Redis 7002 service
Documentation=https://redis.io/documentation
Wants=network-online.target
After=network-online.target
[Service]
Type=forking
LimitNOFILE=10032
User=root
Group=root
ExecStart=/soft/redis/src/redis-server /soft/redis/redis_7002.conf
PrivateTmp=true
[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;redis_7003.service&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; &amp;quot;EOF&amp;quot; &amp;gt; /usr/lib/systemd/system/redis_7003.service
[Unit]
Description=Redis 7003 service
Documentation=https://redis.io/documentation
Wants=network-online.target
After=network-online.target
[Service]
Type=forking
LimitNOFILE=10032
User=root
Group=root
ExecStart=/soft/redis/src/redis-server /soft/redis/redis_7003.conf
PrivateTmp=true
[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;设置开机自启&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chown -R root.root /soft/redis
systemctl daemon-reload
systemctl enable redis_7001.service
systemctl enable redis_7002.service
systemctl enable redis_7003.service
systemctl start redis_7001.service
systemctl start redis_7002.service
systemctl start redis_7003.service
systemctl status redis_7001.service
systemctl status redis_7002.service
systemctl status redis_7003.service
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-启动redis集群服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-启动redis集群服务&#34;&gt;#&lt;/a&gt; 2.5 启动 redis 集群服务&lt;/h5&gt;
&lt;p&gt;--cluster-replicas 2 表示为集群中的每个主节点创建 2 个从节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /soft/redis/src
./redis-cli --cluster create \  
192.168.1.135:7001 192.168.1.135:7002 192.168.1.135:7003 \  
192.168.1.136:7001 192.168.1.136:7002 192.168.1.136:7003 \ 
192.168.1.137:7001 192.168.1.137:7002 192.168.1.137:7003 \ 
--cluster-replicas 2 -a admin123 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输入创建集群的命令后会出现以下提示，注意 Can I set the above configuration? (type &#39;yes&#39; to accept): yes，该处请输入 yes&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@qnyp_node01 src]# ./redis-cli --cluster create \
&amp;gt; 192.168.1.135:7001 192.168.1.135:7002 192.168.1.135:7003 \
&amp;gt; 192.168.1.136:7001 192.168.1.136:7002 192.168.1.136:7003 \
&amp;gt; 192.168.1.137:7001 192.168.1.137:7002 192.168.1.137:7003 \
&amp;gt; --cluster-replicas 2 -a admin123
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
&amp;gt;&amp;gt;&amp;gt; Performing hash slots allocation on 9 nodes...
Master[0] -&amp;gt; Slots 0 - 5460
Master[1] -&amp;gt; Slots 5461 - 10922
Master[2] -&amp;gt; Slots 10923 - 16383
Adding replica 192.168.1.136:7002 to 192.168.1.135:7001
Adding replica 192.168.1.137:7002 to 192.168.1.135:7001
Adding replica 192.168.1.135:7003 to 192.168.1.136:7001
Adding replica 192.168.1.137:7003 to 192.168.1.136:7001
Adding replica 192.168.1.136:7003 to 192.168.1.137:7001
Adding replica 192.168.1.135:7002 to 192.168.1.137:7001
M: 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 192.168.1.135:7001
   slots:[0-5460] (5461 slots) master
S: 4508bee0c33784e0d5be25b64e4c7e677cd9d396 192.168.1.135:7002
   replicates f9133541e2175958117753ef4e206ea43a21f07c
S: a0e13083fcc1d6e96398f3bb2ea5581b7a64e05e 192.168.1.135:7003
   replicates 06ea827f8d328d9d776c9643109317b0100727a6
M: 06ea827f8d328d9d776c9643109317b0100727a6 192.168.1.136:7001
   slots:[5461-10922] (5462 slots) master
S: 1d1b9817e39ee8987a3518f62a9b91c3ab666eff 192.168.1.136:7002
   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7
S: a73c099dcc63f5d46a11d0f61c91270ef61290ff 192.168.1.136:7003
   replicates f9133541e2175958117753ef4e206ea43a21f07c
M: f9133541e2175958117753ef4e206ea43a21f07c 192.168.1.137:7001
   slots:[10923-16383] (5461 slots) master
S: 626dc659bb1059ec40039869241f7de88a49cd87 192.168.1.137:7002
   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7
S: 622f73cd06c5658f8d02056925ac708750f12c1a 192.168.1.137:7003
   replicates 06ea827f8d328d9d776c9643109317b0100727a6
Can I set the above configuration? (type &#39;yes&#39; to accept):
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输完 yes 后，会出现如下提示，[OK] All 16384 slots covered. 说明成功啦&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Can I set the above configuration? (type &#39;yes&#39; to accept): yes
&amp;gt;&amp;gt;&amp;gt; Nodes configuration updated
&amp;gt;&amp;gt;&amp;gt; Assign a different config epoch to each node
&amp;gt;&amp;gt;&amp;gt; Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
..
&amp;gt;&amp;gt;&amp;gt; Performing Cluster Check (using node 192.168.1.135:7001)
M: 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 192.168.1.135:7001
   slots:[0-5460] (5461 slots) master
   2 additional replica(s)
M: 06ea827f8d328d9d776c9643109317b0100727a6 192.168.1.136:7001
   slots:[5461-10922] (5462 slots) master
   2 additional replica(s)
S: 622f73cd06c5658f8d02056925ac708750f12c1a 192.168.1.137:7003
   slots: (0 slots) slave
   replicates 06ea827f8d328d9d776c9643109317b0100727a6
S: 1d1b9817e39ee8987a3518f62a9b91c3ab666eff 192.168.1.136:7002
   slots: (0 slots) slave
   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7
S: a73c099dcc63f5d46a11d0f61c91270ef61290ff 192.168.1.136:7003
   slots: (0 slots) slave
   replicates f9133541e2175958117753ef4e206ea43a21f07c
S: a0e13083fcc1d6e96398f3bb2ea5581b7a64e05e 192.168.1.135:7003
   slots: (0 slots) slave
   replicates 06ea827f8d328d9d776c9643109317b0100727a6
M: f9133541e2175958117753ef4e206ea43a21f07c 192.168.1.137:7001
   slots:[10923-16383] (5461 slots) master
   2 additional replica(s)
S: 626dc659bb1059ec40039869241f7de88a49cd87 192.168.1.137:7002
   slots: (0 slots) slave
   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7
S: 4508bee0c33784e0d5be25b64e4c7e677cd9d396 192.168.1.135:7002
   slots: (0 slots) slave
   replicates f9133541e2175958117753ef4e206ea43a21f07c
[OK] All nodes agree about slots configuration.
&amp;gt;&amp;gt;&amp;gt; Check for open slots...
&amp;gt;&amp;gt;&amp;gt; Check slots coverage...
[OK] All 16384 slots covered.
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;26-访问reids集群并验证&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#26-访问reids集群并验证&#34;&gt;#&lt;/a&gt; 2.6 访问 reids 集群并验证&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;cd /data/redis/src
./redis-cli -h 192.168.1.135 -p 7001 -c -a admin123
#列出当前节点的信息：cluster info
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.1.135:7001&amp;gt; cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:9
cluster_size:3
cluster_current_epoch:9
cluster_my_epoch:1
cluster_stats_messages_ping_sent:344
cluster_stats_messages_pong_sent:354
cluster_stats_messages_sent:698
cluster_stats_messages_ping_received:346
cluster_stats_messages_pong_received:344
cluster_stats_messages_meet_received:8
cluster_stats_messages_received:698
total_cluster_links_buffer_limit_exceeded:0
列出集群的节点的信息：cluster nodes
192.168.1.135:7001&amp;gt; cluster nodes
06ea827f8d328d9d776c9643109317b0100727a6 192.168.1.136:7001@17001 master - 0 1747034145581 4 connected 5461-10922
622f73cd06c5658f8d02056925ac708750f12c1a 192.168.1.137:7003@17003 slave 06ea827f8d328d9d776c9643109317b0100727a6 0 1747034145581 4 connected
1d1b9817e39ee8987a3518f62a9b91c3ab666eff 192.168.1.136:7002@17002 slave 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 0 1747034145581 1 connected
a73c099dcc63f5d46a11d0f61c91270ef61290ff 192.168.1.136:7003@17003 slave f9133541e2175958117753ef4e206ea43a21f07c 0 1747034145581 7 connected
a0e13083fcc1d6e96398f3bb2ea5581b7a64e05e 192.168.1.135:7003@17003 slave 06ea827f8d328d9d776c9643109317b0100727a6 0 1747034144077 4 connected
f9133541e2175958117753ef4e206ea43a21f07c 192.168.1.137:7001@17001 master - 0 1747034145079 7 connected 10923-16383
626dc659bb1059ec40039869241f7de88a49cd87 192.168.1.137:7002@17002 slave 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 0 1747034144000 1 connected
928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 192.168.1.135:7001@17001 myself,master - 0 1747034144000 1 connected 0-5460
4508bee0c33784e0d5be25b64e4c7e677cd9d396 192.168.1.135:7002@17002 slave f9133541e2175958117753ef4e206ea43a21f07c 0 1747034144578 7 connected
&lt;/code&gt;&lt;/pre&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/170573601.html</guid>
            <title>K8s服务发布Ingress</title>
            <link>http://ixuyong.cn/posts/170573601.html</link>
            <category>Kubernetes</category>
            <pubDate>Sat, 26 Apr 2025 16:52:06 +0800</pubDate>
            <description><![CDATA[ &lt;h4 id=&#34;1-ingress-nginx-controller-安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-ingress-nginx-controller-安装&#34;&gt;#&lt;/a&gt; 1. Ingress Nginx Controller 安装&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Supported&lt;/th&gt;
&lt;th&gt;Ingress-NGINX version&lt;/th&gt;
&lt;th&gt;k8s supported version&lt;/th&gt;
&lt;th&gt;Alpine Version&lt;/th&gt;
&lt;th&gt;Nginx Version&lt;/th&gt;
&lt;th&gt;Helm Chart Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.12.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.32, 1.31, 1.30, 1.29, 1.28&lt;/td&gt;
&lt;td&gt;3.21.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.12.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.12.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.32, 1.31, 1.30, 1.29, 1.28&lt;/td&gt;
&lt;td&gt;3.21.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.12.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.12.0-beta.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.32, 1.31, 1.30, 1.29, 1.28&lt;/td&gt;
&lt;td&gt;3.20.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.12.0-beta.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.21.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.21.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;🔄&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.6&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.21.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.19.1&lt;/td&gt;
&lt;td&gt;1.25.3&lt;/td&gt;
&lt;td&gt;4.10.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.19.1&lt;/td&gt;
&lt;td&gt;1.25.3&lt;/td&gt;
&lt;td&gt;4.10.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.6&lt;/td&gt;
&lt;td&gt;1.29, 1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.19.0&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.9.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.5&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.9.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.4&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.3&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.1&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.0&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.8.4&lt;/td&gt;
&lt;td&gt;1.27, 1.26, 1.25, 1.24&lt;/td&gt;
&lt;td&gt;3.18.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.7.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.7.1&lt;/td&gt;
&lt;td&gt;1.27, 1.26, 1.25, 1.24&lt;/td&gt;
&lt;td&gt;3.17.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.6.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.6.4&lt;/td&gt;
&lt;td&gt;1.26, 1.25, 1.24, 1.23&lt;/td&gt;
&lt;td&gt;3.17.0&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.5.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.5.1&lt;/td&gt;
&lt;td&gt;1.25, 1.24, 1.23&lt;/td&gt;
&lt;td&gt;3.16.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.4.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.4.0&lt;/td&gt;
&lt;td&gt;1.25, 1.24, 1.23, 1.22&lt;/td&gt;
&lt;td&gt;3.16.2&lt;/td&gt;
&lt;td&gt;1.19.10†&lt;/td&gt;
&lt;td&gt;4.3.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;1.24, 1.23, 1.22, 1.21, 1.20&lt;/td&gt;
&lt;td&gt;3.16.2&lt;/td&gt;
&lt;td&gt;1.19.10†&lt;/td&gt;
&lt;td&gt;4.2.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;11-helm安装ingress-nginx-controller&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-helm安装ingress-nginx-controller&#34;&gt;#&lt;/a&gt; 1.1 Helm 安装 Ingress Nginx Controller&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;安装 Helm&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# wget https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz
# tar xf helm-v3.6.3-linux-amd64.tar.gz
# mv linux-amd64/helm /usr/local/bin/helm
# helm version
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;下载 Ingress Nginx Controller 安装包&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;官方文档：https://github.com/kubernetes/ingress-nginx/tree/helm-chart-4.8.2         #根据自己k8s版本下载
# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
# helm repo update
# helm repo list
# helm pull ingress-nginx/ingress-nginx --version 4.8.2
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;配置 Ingress Nginx Controller&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# tar xf ingress-nginx-4.8.2.tgz
# cd ingress-nginx
# vim values.yaml
...
 16 controller:
 17   name: controller
 18   enableAnnotationValidations: false
 19   image:
 20     ## Keep false as default for now!
 21     chroot: false
 22     registry: registry.cn-hangzhou.aliyuncs.com
 23     image: kubernetes_public/ingress-nginx-controller
 24     ## for backwards compatibility consider setting the full image url via the repository value below
 25     ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml wil    l fail
 26     ## repository:
 27     tag: &amp;quot;v1.9.3&amp;quot;
 28     #digest: sha256:8fd21d59428507671ce0fb47f818b1d859c92d2ad07bb7c947268d433030ba98
...
 42   # -- Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configurat    ion/configmap/
 43   config:
 44     allow-snippet-annotations: true          #开启server snippet的配置
...
 67   dnsPolicy: ClusterFirstWithHostNet
...
 88   hostNetwork: true
...
107   ingressClassResource:
108     # -- Name of the ingressClass
109     name: nginx
110     # -- Is this ingressClass enabled or not
111     enabled: true
112     # -- Is this the default ingressClass for the cluster
113     default: true
...
184   kind: DaemonSet
...
287   nodeSelector:
288     kubernetes.io/os: linux
289     ingress: &amp;quot;true&amp;quot;
...
638       image:
639         registry: registry.cn-hangzhou.aliyuncs.com
640         image: kubernetes_public/kube-webhook-certgen
641         ## for backwards compatibility consider setting the full image url via the repository value below
642         ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml     will fail
643         ## repository:
644         tag: v20231011-8b53cabe0
645         #digest: sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;4. 给需要部署 ingress 的节点上打标签&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl label node k8s-node02 ingress=true
# kubectl label node k8s-node01 ingress=true
# kubectl create ns ingress-nginx
# helm install ingress-nginx -n ingress-nginx .     #安装
# helm upgrade ingress-nginx -n ingress-nginx .     #更新
# kubectl get pods -n ingress-nginx 
NAME                             READY   STATUS    RESTARTS   AGE
ingress-nginx-controller-7nfqn   1/1     Running   0          27s
ingress-nginx-controller-k4p2n   1/1     Running   0          17m
ingress-nginx-controller-kw5jk   1/1     Running   0          24s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-bare-metal安装ingress-nginx-controller&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-bare-metal安装ingress-nginx-controller&#34;&gt;#&lt;/a&gt; 1.2 Bare metal 安装 Ingress Nginx Controller&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;下载 Ingress 部署文件，链接地址：&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters&#34;&gt;https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.1/deploy/static/provider/baremetal/deploy.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;配置 Ingress&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ingress-master]# cat deploy.yaml 
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  name: ingress-nginx
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
  namespace: ingress-nginx
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - pods
  - secrets
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resourceNames:
  - ingress-nginx-leader
  resources:
  - leases
  verbs:
  - get
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
  namespace: ingress-nginx
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - secrets
  verbs:
  - get
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - endpoints
  - nodes
  - pods
  - secrets
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - nodes
  verbs:
  - get
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
rules:
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  verbs:
  - get
  - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: v1
data: null
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - appProtocol: http
    name: http
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  #type: NodePort
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller-admission
  namespace: ingress-nginx
spec:
  ports:
  - appProtocol: https
    name: https-webhook
    port: 443
    targetPort: webhook
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: ClusterIP
---
apiVersion: apps/v1
#kind: Deployment
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.1
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/ingress-nginx-controller-v1.12.1:v1.12.1 
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          readOnlyRootFilesystem: false
          runAsGroup: 82
          runAsNonRoot: true
          runAsUser: 101
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      hostNetwork: true                         # 与节点共享网络名称空间
      #dnsPolicy: ClusterFirst
      dnsPolicy: ClusterFirstWithHostNet        # dns 策略
      nodeSelector:                             # 节点选择器
        kubernetes.io/os: linux
        ingress: &amp;quot;true&amp;quot;
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission-create
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.1
      name: ingress-nginx-admission-create
    spec:
      containers:
      - args:
        - create
        - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
        - --namespace=$(POD_NAMESPACE)
        - --secret-name=ingress-nginx-admission
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 
        imagePullPolicy: IfNotPresent
        name: create
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
          seccompProfile:
            type: RuntimeDefault
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      serviceAccountName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission-patch
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.1
      name: ingress-nginx-admission-patch
    spec:
      containers:
      - args:
        - patch
        - --webhook-name=ingress-nginx-admission
        - --namespace=$(POD_NAMESPACE)
        - --patch-mutating=false
        - --secret-name=ingress-nginx-admission
        - --patch-failure-policy=Fail
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 
        imagePullPolicy: IfNotPresent
        name: patch
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
          seccompProfile:
            type: RuntimeDefault
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      serviceAccountName: ingress-nginx-admission
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: nginx
spec:
  controller: k8s.io/ingress-nginx
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
webhooks:
- admissionReviewVersions:
  - v1
  clientConfig:
    service:
      name: ingress-nginx-controller-admission
      namespace: ingress-nginx
      path: /networking/v1/ingresses
      port: 443
  failurePolicy: Fail
  matchPolicy: Equivalent
  name: validate.nginx.ingress.kubernetes.io
  rules:
  - apiGroups:
    - networking.k8s.io
    apiVersions:
    - v1
    operations:
    - CREATE
    - UPDATE
    resources:
    - ingresses
  sideEffects: None
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;type: ClusterIP                                              #service 类型改为 ClusterIP&lt;/li&gt;
&lt;li&gt;hostNetwork: true                                      # 与节点共享网络名称空间&lt;/li&gt;
&lt;li&gt;dnsPolicy: ClusterFirstWithHostNet        # dns 策略&lt;/li&gt;
&lt;li&gt;nodeSelector:                                             # 节点选择器&lt;/li&gt;
&lt;li&gt;kind: DaemonSet                                        # 资源类型 DaemonSet&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;在指定节点部署 Ingress-Controller&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ingress-master]# kubectl apply -f deploy.yaml -n ingress-nginx

[root@k8s-master01 ingress-master]# kubectl label node k8s-node01 ingress=true
[root@k8s-master01 ingress-master]# kubectl label node k8s-node02 ingress=true
[root@k8s-master01 ingress-master]# kubectl label node k8s-master03 ingress-     #取消节点部署

[root@k8s-master01 ingress-master]# kubectl get pods -n ingress-nginx 
NAME                                   READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-zp6mh   0/1     Completed   0          12m
ingress-nginx-admission-patch-f2bpd    0/1     Completed   0          12m
ingress-nginx-controller-rgtkc         1/1     Running     0          3m59s
ingress-nginx-controller-trmn8         1/1     Running     0          3m59s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-ingress-nginx-入门使用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-ingress-nginx-入门使用&#34;&gt;#&lt;/a&gt; 2. Ingress Nginx 入门使用&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat web-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: test.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-ingress-nginx-域名重定向-redirect&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-ingress-nginx-域名重定向-redirect&#34;&gt;#&lt;/a&gt; 3. Ingress Nginx 域名重定向 Redirect&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat redirect-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: redirect-ingress
  annotations:
    nginx.ingress.kubernetes.io/permanent-redirect: https://www.baidu.com
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: redirect.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-ingress-nginx-前后端分离-rewrite&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-ingress-nginx-前后端分离-rewrite&#34;&gt;#&lt;/a&gt; 4. Ingress Nginx 前后端分离 Rewrite&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat rewrite-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rewrite-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: rewrite.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /api(/|$)(.*)
        pathType: ImplementationSpecif
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-ingress-nginx-错误代码重定向&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-ingress-nginx-错误代码重定向&#34;&gt;#&lt;/a&gt; 5. Ingress Nginx 错误代码重定向&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-ingress-nginx-ssl&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-ingress-nginx-ssl&#34;&gt;#&lt;/a&gt; 6. Ingress Nginx SSL&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.生成证书
# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.cert -subj &amp;quot;/CN=s.hmallleasing.com/O=tls.hmallleasing.com&amp;quot;

2.创建证书
# kubectl create secret tls tls.hmallleasig.com --key tls.key --cert tls.cert

3.ingress配置
# kubectl create secret tls tls.hmallleasig.com --cert=tls.crt --key=tls.key
# cat tls-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;    #禁用https强制跳转
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: tls.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  tls:                  #https
  - hosts:
    - tls.hmallleasing.com
    secretName: &amp;quot;tls.hmallleasig.com&amp;quot;	
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-ingress-nginx-匹配请求头&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-ingress-nginx-匹配请求头&#34;&gt;#&lt;/a&gt; 7. Ingress Nginx 匹配请求头&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.部署移动端应用
# kubectl create deploy phone --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:phone
# kubectl expose deploy phone --port 80
# vim m-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: m-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: m.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: phone
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific

2.部署PC端应用		
# kubectl create deploy laptop --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:laptop	
# kubectl expose deploy laptop --port 80
# vim laptop-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/server-snippet: |
      set $agentflag 0;
          if ($http_user_agent ~* &amp;quot;(Android|iPhone|Windows Phone|UC|Kindle)&amp;quot; )&amp;#123;
              set $agentflag 1;
          &amp;#125;
          if ( $agentflag = 1 ) &amp;#123;
              return 301 http://m.hmallleaing.com;
          &amp;#125;
  name: laptop-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: laptop
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific	
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8ingress-nginx-基本认证&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8ingress-nginx-基本认证&#34;&gt;#&lt;/a&gt; 8.Ingress Nginx 基本认证&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# yum install httpd -y
# htpasswd -c auth superman
# cat auth 
superman:$apr1$AC1pc3dK$RJyWnyDJFNKY6twneGVrA1		

# kubectl create secret generic basic-auth --from-file=auth
# cat basic-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: basic-ingress
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic  # 认证类型
    nginx.ingress.kubernetes.io/auth-secret: basic-auth  # 包含用户和密码的 secret 资源名称
    nginx.ingress.kubernetes.io/auth-realm: &#39;Please User password&#39;  # 要显示的信息
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: basic.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9-ingress-nginx-黑白名单&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-ingress-nginx-黑白名单&#34;&gt;#&lt;/a&gt; 9. Ingress Nginx 黑 / 白名单&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;写法一：
# cat white-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: white-ingress
  annotations:
    nginx.ingress.kubernetes.io/whitelist-source-range: &amp;quot;192.168.40.101&amp;quot;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: white.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific	

写法二：		
[root@k8s-master01 ingress]# cat white-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: white-ingress
  annotations:
    nginx.ingress.kubernetes.io/whitelist-source-range: &amp;quot;192.168.40.0/24&amp;quot;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: white.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific


写法三：
# cat white-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: white-ingress
  annotations:
    nginx.ingress.kubernetes.io/server-snippet: |
      allow 192.168.40.0/24;
      deny all;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: white.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
		

#Master01测试		
# curl -H &amp;quot;Host:white.hmallleasing.com&amp;quot; http://192.168.40.103 -I
HTTP/1.1 200 OK
Date: Sat, 14 Oct 2023 13:12:03 GMT
Content-Type: text/html
Content-Length: 612
Connection: keep-alive
Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT
ETag: &amp;quot;5cb5d3c3-264&amp;quot;
Accept-Ranges: bytes		

#Master02测试
# curl -H &amp;quot;Host:white.hmallleasing.com&amp;quot; http://192.168.40.103 -I
HTTP/1.1 403 Forbidden
Date: Sat, 14 Oct 2023 13:13:34 GMT
Content-Type: text/html
Content-Length: 146
Connection: keep-alive
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;10-ingress-nginx-速率限制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-ingress-nginx-速率限制&#34;&gt;#&lt;/a&gt; 10. Ingress Nginx 速率限制&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ingress]# cat limit-rate-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rate-limit-ingress
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: &amp;quot;50&amp;quot;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: rate-limit.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific

# ab -c 20 -n 1000 http://rate-limit.hmallleasing.com/ |grep request
Complete requests:      1000
Failed requests:        724
Time per request:       10.301 [ms] (mean)
Time per request:       0.515 [ms] (mean, across all concurrent requests)
Percentage of the requests served within a certain time (ms)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11使用-nginx-实现灰度金丝雀发布&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11使用-nginx-实现灰度金丝雀发布&#34;&gt;#&lt;/a&gt; 11. 使用 Nginx 实现灰度 / 金丝雀发布&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.创建 v1 版本
# kubectl create deploy canary-v1 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v1	
# kubectl expose deploy canary-v1 --port 8080
# cat canary-v1-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-v1-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: canary.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: canary-v1
            port:
              number: 8080
        path: /
        pathType: ImplementationSpecific
		
# curl -H &amp;quot;Host:canary.hmallleasing.com&amp;quot; http://192.168.40.103 	

2.创建 v2 版本
# kubectl create deploy canary-v2 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v2
# kubectl expose deploy canary-v2 --port 8080
# cat canary-v2-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-v2-ingress
  annotations:
    nginx.ingress.kubernetes.io/canary: &amp;quot;true&amp;quot;    #启动灰度发布
    nginx.ingress.kubernetes.io/canary-weight: &amp;quot;20&amp;quot;  #基于权重,50%流量调度到这个灰度的版本上
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: canary.hmallleasing.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: canary-v2
            port:
              number: 8080

#测试灰度发布
[root@k8s-master01 ingress]# cat canary.sh 
#!/bin/bash

while true
do
	curl -H &amp;quot;Host:canary.hmallleasing.com&amp;quot; http://192.168.40.103
	sleep 0.5
done
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-kubernetes-dashboard配置证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-kubernetes-dashboard配置证书&#34;&gt;#&lt;/a&gt; 12. kubernetes-dashboard 配置证书&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.创建证书
kubectl create secret tls kubernetes-dashboard-certs --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n kubernetes-dashboard

2.修改kubernetes-dashboard资源清单
kubectl edit deployment -n kubernetes-dashboard kubernetes-dashboard
...
      - args:
        - --auto-generate-certificates=false
        - --tls-key-file=_.hmallleasing.com_key.key
        - --tls-cert-file=_.hmallleasing.com_chain.crt
        - --token-ttl=21600
        - --authentication-mode=basic,token
        - --namespace=kubernetes-dashboard
...

3.创建ingress
#cat dashboard-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/ssl-passthrough: &amp;quot;true&amp;quot;    
    nginx.ingress.kubernetes.io/backend-protocol: &amp;quot;HTTPS&amp;quot;    
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: dashboard.hmallleasing.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443

# kubectl apply -f dashboard-ingress.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;13-入口lb配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-入口lb配置&#34;&gt;#&lt;/a&gt; 13. 入口 LB 配置&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@lb nginx]# cat /etc/nginx/conf.d/ingress.conf 
upstream ingress &amp;#123;
	server 192.168.40.103:80 max_conns=2000 max_fails=2 fail_timeout=5s;
	server 192.168.40.104:80 max_conns=2000 max_fails=2 fail_timeout=5s;
	server 192.168.40.105:80 max_conns=2000 max_fails=2 fail_timeout=5s;
&amp;#125;

server &amp;#123;
    listen 443 ssl;
    server_name test.hmallleasing.com;
    client_max_body_size 1G; 
    ssl_prefer_server_ciphers on;
    ssl_certificate  /etc/nginx/sslkey/*.hmallleasing.com_chain.crt;
    ssl_certificate_key  /etc/nginx/sslkey/*.hmallleasing.com_key.key;

    location / &amp;#123;
        proxy_pass http://ingress;
        include proxy_params;
	    proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
	    proxy_next_upstream_tries 2;
	    proxy_next_upstream_timeout 3s;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name test.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;

[root@lb ~]# mkdir /etc/nginx/sslkey -p


[root@lb ~]# cat proxy_params 
proxy_http_version 1.1;
proxy_set_header Connectin &amp;quot;&amp;quot;;

proxy_set_header Host $http_host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

proxy_connect_timeout 60;
proxy_send_timeout 120;
proxy_read_timeout 120;

proxy_buffering on;
proxy_buffer_size 32k;
proxy_buffers 4 128k;
proxy_temp_file_write_size 10240k;
proxy_max_temp_file_size 10240k;
&lt;/code&gt;&lt;/pre&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3364424907.html</guid>
            <title>阿里云+Github构建镜像仓库</title>
            <link>http://ixuyong.cn/posts/3364424907.html</link>
            <category>Docker</category>
            <pubDate>Sat, 26 Apr 2025 16:20:14 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;阿里云github构建镜像仓库解决-k8sgcrio访问&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#阿里云github构建镜像仓库解决-k8sgcrio访问&#34;&gt;#&lt;/a&gt; 阿里云 + github 构建镜像仓库解决 k8s.gcr.io 访问&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://xn--k8s-xi9d897o.gcr.io/&#34;&gt;由于 k8s.gcr.io/&lt;/a&gt; 镜像仓库位于国外，国内使用 kubeadm 构建 docker 集群时无法访问相应的 docker 镜像。&lt;/p&gt;
&lt;h4 id=&#34;1-登录github创建仓库&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-登录github创建仓库&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.&lt;/strong&gt; 登录 Github 创建仓库&lt;/h4&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/vgZkKBC.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/VnJlhBE.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-创建dockerfile&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-创建dockerfile&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.&lt;/strong&gt; 创建 Dockerfile&lt;/h4&gt;
&lt;p&gt;仓库下面创建一个 Dockerfile，以 ingress-nginx-controller 为例下的 dockerfile 内容如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@manager ~]# mkdir ingress-nginx-controller
[root@manager ~]# cd ingress-nginx-controller/
[root@manager ingress-nginx-controller]# cat Dockerfile 
FROM registry.k8s.io/ingress-nginx/controller:v1.12.1 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-ssh免密登录github&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-ssh免密登录github&#34;&gt;#&lt;/a&gt; 3. SSH 免密登录 GitHub&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@manager ingress-nginx-controller]# ssh-keygen
[root@manager ingress-nginx-controller]# cat ~/.ssh/id_rsa.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进入&lt;em&gt; GitHub&lt;/em&gt; 的个人设置，找到【&lt;em&gt;SSH and GPG keys&lt;/em&gt;】，然后点击新增 SSH Key，进入如下界面，&lt;em&gt;title&lt;/em&gt; 输入你对于当前&lt;em&gt; SSH key&lt;/em&gt; 的备注，下面的&lt;em&gt; key&lt;/em&gt; 就粘贴上一步生成的&lt;em&gt; id_rsa.pub&lt;/em&gt; 内的内容&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/3djSHRS.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/8gVcVu4.png&#34; alt=&#34;5.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;4-推送dockerfile至github&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-推送dockerfile至github&#34;&gt;#&lt;/a&gt; 4. 推送 Dockerfile 至 Github&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@manager ingress-nginx-controller]# yum install git -y
[root@manager ingress-nginx-controller]# git --version
[root@manager ingress-nginx-controller]# git config --global user.email &amp;quot;373370405@qq.com&amp;quot;
[root@manager ingress-nginx-controller]# git config --global color.ui true
[root@manager ingress-nginx-controller]# git init
[root@manager ingress-nginx-controller]# git add .
[root@manager ingress-nginx-controller]# git commit -m &amp;quot;first commit&amp;quot;
[root@manager ingress-nginx-controller]# git branch -M main
[root@manager ingress-nginx-controller]# git remote add origin git@github.com:xyapples/ingress-nginx-controller.git   #添加远程仓库
[root@manager ingress-nginx-controller]# git remote -v
[root@manager ingress-nginx-controller]# git push -u origin main
[root@manager ingress-nginx-controller]# git remote remove origin  #移除远程仓库
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/N3t49eX.png&#34; alt=&#34;6.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-登录阿里云创建镜像仓库&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-登录阿里云创建镜像仓库&#34;&gt;#&lt;/a&gt; &lt;strong&gt;5.&lt;/strong&gt; 登录阿里云创建镜像仓库&lt;/h4&gt;
&lt;p&gt;登录阿里云镜像：&lt;a href=&#34;https://cr.console.aliyun.com/%EF%BC%8C%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%EF%BC%9A&#34;&gt;https://cr.console.aliyun.com/，创建镜像仓库：&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/1zFqa35.png&#34; alt=&#34;7.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/PhzoeeT.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/iYAbB0x.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;6-构建镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-构建镜像&#34;&gt;#&lt;/a&gt; 6. 构建镜像&lt;/h4&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/KD3DI7J.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/WiwNBRK.png&#34; alt=&#34;4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/155pUrE.png&#34; alt=&#34;7.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/155pUrE.png&#34; alt=&#34;7.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/AU1371X.png&#34; alt=&#34;8.png&#34; /&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3030097036.html</guid>
            <title>K8S云原生存储Rook-Ceph</title>
            <link>http://ixuyong.cn/posts/3030097036.html</link>
            <category>Kubernetes</category>
            <pubDate>Thu, 24 Apr 2025 21:43:19 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s云原生存储rook-ceph&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s云原生存储rook-ceph&#34;&gt;#&lt;/a&gt; K8S 云原生存储 Rook-Ceph&lt;/h3&gt;
&lt;h4 id=&#34;1-storageclass动态存储&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-storageclass动态存储&#34;&gt;#&lt;/a&gt; 1. StorageClass 动态存储&lt;/h4&gt;
&lt;p&gt;StorageClass：存储类，由 K8s 管理员创建，用于动态 PV 的管理，可以链接至不同的后端存储，比如 Ceph、GlusterFS 等。之后对存储的请求可以指向 StorageClass，然后 StorageClass 会自动的创建、删除 PV。&lt;/p&gt;
&lt;p&gt;实现方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in-tree: 内置于 K8s 核心代码，对于存储的管理，都需要编写相应的代码。&lt;/li&gt;
&lt;li&gt;out-of-tree：由存储厂商提供一个驱动（CSI 或 Flex Volume），安装到 K8s 集群，然后 StorageClass 只需要配置该驱动即可，驱动器会代替 StorageClass 管理存储。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StorageClass 官网介绍：&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/&#34;&gt;https://kubernetes.io/docs/concepts/storage/storage-classes/&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-云原生存储rook&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-云原生存储rook&#34;&gt;#&lt;/a&gt; 2. 云原生存储 Rook&lt;/h4&gt;
&lt;p&gt;Rook 是一个自我管理的分布式存储编排系统，它本身并不是存储系统，在存储和 k8s 之前搭建了一个桥梁，使存储系统的搭建或者维护变得特别简单，Rook 将分布式存储系统转变为自我管理、自我扩展、自我修复的存储服务。它让一些存储的操作，比如部署、配置、扩容、升级、迁移、灾难恢复、监视和资源管理变得自动化，无需人工处理。并且 Rook 支持 CSI，可以利用 CSI 做一些 PVC 的快照、扩容、克隆等操作。&lt;/p&gt;
&lt;p&gt;Rook 官网介绍：&lt;a href=&#34;https://rook.io/&#34;&gt;https://rook.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/CK4Gn1u.jpeg&#34; alt=&#34;Snipaste_2025-05-07_20-15-59.jpg&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;3-rook-安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-rook-安装&#34;&gt;#&lt;/a&gt; 3. Rook 安装&lt;/h4&gt;
&lt;p&gt;环境准备&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K8s 集群至少五个节点，每个节点的内存不低于 5G，CPU 不低于 2 核&lt;/li&gt;
&lt;li&gt;所有节点时间同步&lt;/li&gt;
&lt;li&gt;至少有三个存储节点，并且每个节点至少有一个裸盘，k8s-master03、k8s-node01、k8s-node02 增加裸盘&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;31-下载-rook-安装文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-下载-rook-安装文件&#34;&gt;#&lt;/a&gt; 3.1 下载 Rook 安装文件&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# git clone --single-branch --branch v1.17.2 https://github.com/rook/rook.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;32-配置更改&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-配置更改&#34;&gt;#&lt;/a&gt; 3.2 配置更改&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd rook/deploy/examples
[root@k8s-master01 ~]# vim operator.yaml
  ROOK_CSI_CEPH_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephcsi:v3.14.0&amp;quot;
  ROOK_CSI_REGISTRAR_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-node-driver-registrar:v2.13.0&amp;quot;
  ROOK_CSI_RESIZER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-resizer:v1.13.1&amp;quot;
  ROOK_CSI_PROVISIONER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-provisioner:v5.1.0&amp;quot;
  ROOK_CSI_SNAPSHOTTER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-snapshotter:v8.2.0&amp;quot;
  ROOK_CSI_ATTACHER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-attacher:v4.8.0&amp;quot;

#ROOK_ENABLE_DISCOVERY_DAEMON 改成 true 即可
ROOK_ENABLE_DISCOVERY_DAEMON: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-部署-rook&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-部署-rook&#34;&gt;#&lt;/a&gt; 3.3 部署 rook&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ceph]# kubectl create -f crds.yaml -f common.yaml -f operator.yaml
[root@k8s-master01 examples]# kubectl get pods -n rook-ceph
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-operator-84ff77778b-7ww2w   1/1     Running   0          91m
rook-discover-6j68f                   1/1     Running   0          82m
rook-discover-9w4kt                   1/1     Running   0          82m
rook-discover-h2zfm                   1/1     Running   0          82m
rook-discover-hsz8b                   1/1     Running   0          19m
rook-discover-rj4t7                   1/1     Running   0          82m
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4创建-ceph-集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4创建-ceph-集群&#34;&gt;#&lt;/a&gt; 4. 创建 Ceph 集群&lt;/h4&gt;
&lt;h5 id=&#34;41-配置更改&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#41-配置更改&#34;&gt;#&lt;/a&gt; 4.1 配置更改&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# vim cluster.yaml
...
    image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephv19.2.2:v19.2.2
...
  skipUpgradeChecks: true     #改为true，跳过升级
....
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # serve the dashboard at the given port.
    # port: 8443
    # serve the dashboard using SSL
    ssl: false          #改为false
...
  storage: # cluster level storage configuration and selection
    useAllNodes: false      #改为false,不使用所有的节点当osd
    useAllDevices: false    #改为false,不使用所有的磁盘当osd
...
    #     deviceFilter: &amp;quot;^sd.&amp;quot;
    nodes:
    - name: &amp;quot;k8s-master03&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;k8s-node01&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;k8s-node02&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意：新版必须采用裸盘，即未格式化的磁盘。其中 k8s-master03、 k8s-node01、  k8s-node02 有新加的一个磁盘，可以通过 lsblk -f 查看新添加的磁盘名称。建议最少三个节点，否则后面的试验可能会出现问题&lt;/p&gt;
&lt;h5 id=&#34;42-创建-ceph-集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#42-创建-ceph-集群&#34;&gt;#&lt;/a&gt; 4.2 创建 Ceph 集群&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# kubectl create -f cluster.yaml
[root@k8s-master01 examples]# kubectl get pods -n rook-ceph
NAME                                                     READY   STATUS      RESTARTS        AGE
csi-cephfsplugin-5nmnl                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-6b6ct                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-8xlnl                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-fh9w5                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-mslst                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-provisioner-59bd447c6d-5zwj2            6/6     Running     0               61s
csi-cephfsplugin-provisioner-59bd447c6d-7t2kg            6/6     Running     2 (20s ago)     61s
csi-rbdplugin-5gvmp                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-dzcs4                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-n82b5                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-provisioner-6856fb8b86-86hw8               6/6     Running     0               19s
csi-rbdplugin-provisioner-6856fb8b86-lj9s4               6/6     Running     0               19s
csi-rbdplugin-vh8j2                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-xfgwr                                      3/3     Running     1 (60m ago)     62m
rook-ceph-crashcollector-k8s-master01-bbc78d496-bzjk8    1/1     Running     0               8m26s
rook-ceph-crashcollector-k8s-master03-765ff964bb-95wmt   1/1     Running     0               28m
rook-ceph-crashcollector-k8s-node01-7cf4c4b6b6-r4n84     1/1     Running     0               20m
rook-ceph-crashcollector-k8s-node02-f887f8cf9-jz2l8      1/1     Running     0               28m
rook-ceph-detect-version-nsrwj                           0/1     Init:0/1    0               3s
rook-ceph-exporter-k8s-master01-5cd4577b79-ckd4m         1/1     Running     0               8m26s
rook-ceph-exporter-k8s-master03-75f4cf6f7-hc9zb          1/1     Running     0               28m
rook-ceph-exporter-k8s-node01-96fc7cf49-d2r24            1/1     Running     0               20m
rook-ceph-exporter-k8s-node02-777b9f555b-7j6cz           1/1     Running     0               27m
rook-ceph-mgr-a-6f46b4b945-q6cjb                         3/3     Running     3 (14m ago)     35m
rook-ceph-mgr-b-5d4cc5465b-8dfh6                         3/3     Running     0               35m
rook-ceph-mon-a-7c7b7555c7-nlhwg                         2/2     Running     2 (6m14s ago)   51m
rook-ceph-mon-c-559bcf95fd-cl62w                         2/2     Running     0               8m27s
rook-ceph-mon-d-7dbc6b8f5c-8264t                         2/2     Running     0               28m
rook-ceph-operator-645478ff5b-jdcrp                      1/1     Running     0               102m
rook-ceph-osd-0-6d9cf78f76-4zhx8                         2/2     Running     0               12m
rook-ceph-osd-1-88c78bbcb-cn48c                          2/2     Running     0               5m15s
rook-ceph-osd-2-b464c9fc6-458hv                          2/2     Running     0               4m29s
rook-ceph-osd-prepare-k8s-master03-pwnrc                 0/1     Completed   0               86s
rook-ceph-osd-prepare-k8s-node01-xxp2j                   0/1     Completed   0               83s
rook-ceph-osd-prepare-k8s-node02-8nz7x                   0/1     Completed   0               78s
rook-discover-jzmkr                                      1/1     Running     0               91m
rook-discover-k7pxt                                      1/1     Running     0               91m
rook-discover-vqjh5                                      1/1     Running     0               91m
rook-discover-wk8jq                                      1/1     Running     0               91m
rook-discover-x8rsn                                      1/1     Running     0               91m

[root@k8s-master01 examples]# kubectl get cephcluster -n rook-ceph
NAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH        EXTERNAL   FSID
rook-ceph   /var/lib/rook     3          63m   Ready   Cluster created successfully   HEALTH_WARN              ca429602-66f4-4a1e-9d5c-a5773a0f594f
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;43-安装-ceph-snapshot-控制器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#43-安装-ceph-snapshot-控制器&#34;&gt;#&lt;/a&gt; 4.3 安装 ceph snapshot 控制器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd /root/k8s-ha-install/
[root@k8s-master01 k8s-ha-install]# git checkout manual-installation-v1.32.x
[root@k8s-master01 k8s-ha-install]# kubectl create -f snapshotter/ -n kube-system
[root@k8s-master01 k8s-ha-install]# kubectl get po -n kube-system -l app=snapshot-controller
NAME                    READY   STATUS    RESTARTS   AGE
snapshot-controller-0   1/1     Running   0          67s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-安装-ceph-客户端工具&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-安装-ceph-客户端工具&#34;&gt;#&lt;/a&gt; 5. 安装 ceph 客户端工具&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# cd /root/rook/deploy/examples/
[root@k8s-master01 examples]# kubectl create -f toolbox.yaml -n rook-ceph
[root@k8s-master01 examples]# kubectl get po -n rook-ceph -l app=rook-ceph-tools
NAME                               READY   STATUS    RESTARTS   AGE
rook-ceph-tools-7b75b967db-sqddk   1/1     Running   0          8s
[root@k8s-master01 examples]# kubectl exec -it rook-ceph-tools-7b75b967db-sqddk -n rook-ceph -- bash
bash-5.1$ ceph status
  cluster:
    id:     87b85368-9487-4967-a4e4-5970d2e0ec94
    health: HEALTH_WARN
            1 mgr modules have recently crashed
 
  services:
    mon: 3 daemons, quorum b,c (age 12s), out of quorum: a
    mgr: a(active, since 7m), standbys: b
    osd: 3 osds: 3 up (since 8m), 3 in (since 3h)
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   82 MiB used, 60 GiB / 60 GiB avail
    pgs: 
	
bash-4.4$  ceph osd status
ID  HOST           USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      
 0  k8s-master03  20.6M  19.9G      0        0       0        0   exists,up  
 1  k8s-node01    20.6M  19.9G      0        0       0        0   exists,up  
 2  k8s-node02    20.6M  19.9G      0        0       0        0   exists,up 

bash-4.4$ ceph df
--- RAW STORAGE ---
CLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED
hdd    60 GiB  60 GiB  62 MiB    62 MiB       0.10
TOTAL  60 GiB  60 GiB  62 MiB    62 MiB       0.10

--- POOLS ---
POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr   1    1  449 KiB        2  1.3 MiB      0     19 GiB
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-ceph-dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-ceph-dashboard&#34;&gt;#&lt;/a&gt; 6. Ceph dashboard&lt;/h4&gt;
&lt;h5 id=&#34;61-暴露服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#61-暴露服务&#34;&gt;#&lt;/a&gt; 6.1 暴露服务&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc -n rook-ceph
NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
rook-ceph-mgr             ClusterIP   10.96.54.15     &amp;lt;none&amp;gt;        9283/TCP            133m
rook-ceph-mgr-dashboard   ClusterIP   10.96.97.117    &amp;lt;none&amp;gt;        7000/TCP            133m        #暴露ingresss也可
rook-ceph-mon-a           ClusterIP   10.96.125.216   &amp;lt;none&amp;gt;        6789/TCP,3300/TCP   170m
rook-ceph-mon-b           ClusterIP   10.96.34.183    &amp;lt;none&amp;gt;        6789/TCP,3300/TCP   133m
rook-ceph-mon-c           ClusterIP   10.96.232.252   &amp;lt;none&amp;gt;        6789/TCP,3300/TCP   133m


[root@k8s-master01 examples]# kubectl create -f dashboard-external-http.yaml           #暴露nodeport
[root@k8s-master01 examples]# kubectl get svc -n rook-ceph rook-ceph-mgr-dashboard-external-http
NAME                                     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
rook-ceph-mgr-dashboard-external-https   NodePort   10.96.11.120   &amp;lt;none&amp;gt;        8443:32611/TCP   45s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;62-配置ingress访问ceph&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#62-配置ingress访问ceph&#34;&gt;#&lt;/a&gt; 6.2 配置 ingress 访问 ceph&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# cat dashboard-ingress-https.yaml 
#
# This example is for Kubernetes running an nginx-ingress
# and an ACME (e.g. Let&#39;s Encrypt) certificate service
#
# The nginx-ingress annotations support the dashboard
# running using HTTPS with a self-signed certificate
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rook-ceph-mgr-dashboard
  namespace: rook-ceph # namespace:cluster
#  annotations:
#    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
#    kubernetes.io/tls-acme: &amp;quot;true&amp;quot;
#    nginx.ingress.kubernetes.io/backend-protocol: &amp;quot;HTTPS&amp;quot;
#    nginx.ingress.kubernetes.io/server-snippet: |
#      proxy_ssl_verify off;

spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
#  tls:
#    - hosts:
#        - rook-ceph.hmallleasing.com
#      secretName: rook-ceph.example.com
  rules:
    - host: rook-ceph.hmallleasing.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: rook-ceph-mgr-dashboard
                port:
                  name: http-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;63-登录&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#63-登录&#34;&gt;#&lt;/a&gt; 6.3 登录&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;http://192.168.40.100:32611
用户名：admin
密码：kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&amp;quot;&amp;#123;[&#39;data&#39;][&#39;password&#39;]&amp;#125;&amp;quot; | base64 --decode &amp;amp;&amp;amp; echo
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-ceph-块存储的使用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-ceph-块存储的使用&#34;&gt;#&lt;/a&gt; 7. Ceph 块存储的使用&lt;/h4&gt;
&lt;p&gt;块存储一般用于一个 Pod 挂载一块存储使用，相当于一个服务器新挂了一个盘，只给一个应用使用。&lt;/p&gt;
&lt;h5 id=&#34;71-创建-storageclass-和-ceph-的存储池&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#71-创建-storageclass-和-ceph-的存储池&#34;&gt;#&lt;/a&gt; 7.1 创建 StorageClass 和 ceph 的存储池&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# kubectl get csidriver
NAME                            ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE
rook-ceph.cephfs.csi.ceph.com   true             false            false             &amp;lt;unset&amp;gt;         false               Persistent   15h       #文件存储csi
rook-ceph.rbd.csi.ceph.com      true             false            false             &amp;lt;unset&amp;gt;         false               Persistent   15h       #块存储csi

[root@k8s-master01 ~]# cd /root/rook/deploy/examples/
[root@k8s-master01 examples]# vim csi/rbd/storageclass.yaml
...
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph # namespace:cluster
spec:
  failureDomain: host
  replicated:
    size: 3                #数据保存几份，测试环境可以将副本数设置成了 2（不能设置为 1），生产环境最少为 3，且要小于等于 osd 的数量
...
allowVolumeExpansion: true     #是否可以扩容
reclaimPolicy: Delete          #pv回收策略

[root@k8s-master01 examples]# kubectl create -f csi/rbd/storageclass.yaml -n rook-ceph

[root@k8s-master01 examples]# kubectl get cephblockpool -n rook-ceph
NAME          PHASE
replicapool   Ready
[root@k8s-master01 examples]# kubectl get sc
NAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage       nfzl.com/nfs                 Delete          Immediate           false                  16h
rook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   37s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;72-挂载测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#72-挂载测试&#34;&gt;#&lt;/a&gt; 7.2 挂载测试&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat ceph-block-pvc.yaml        #创建PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-block-pvc
spec:
  storageClassName: &amp;quot;rook-ceph-block&amp;quot;     # 明确指定使用哪个sc的供应商来创建pv
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi                      # 根据业务实际大小进行资源申请

[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc.yaml 

[root@k8s-master01 ~]# kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
ceph-block-pvc   Bound    pvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            rook-ceph-block   3s

[root@k8s-master01 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS      REASON   AGE
pvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            Delete           Bound    default/ceph-block-pvc   rook-ceph-block
	  
[root@k8s-master01 ~]# cat ceph-block-pvc-pod.yaml    #挂载PVC测试 
apiVersion: v1
kind: Pod
metadata:
  name: ceph-block-pvc-pod
spec:
  containers:
  - name: ceph-block-pvc-pod
    image: nginx
    volumeMounts:
    - name: nginx-page
      mountPath: /usr/share/nginx/html
  volumes:
  - name: nginx-page
    persistentVolumeClaim:      
      claimName: ceph-block-pv

[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-pod.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;73-statefulset-volumeclaimtemplates&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#73-statefulset-volumeclaimtemplates&#34;&gt;#&lt;/a&gt; 7.3 StatefulSet volumeClaimTemplates&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat ceph-block-pvc-sts.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:

  - port: 80
    name: web
      clusterIP: None
      selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # 必须匹配 .spec.template.metadata.labels
  serviceName: &amp;quot;nginx&amp;quot;
  replicas: 3 # 默认值是 1
  template:
    metadata:
      labels:
        app: nginx # 必须匹配 .spec.selector.matchLabels
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
    name: www
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: &amp;quot;rook-ceph-block&amp;quot;
      resources:
        requests:
          storage: 1Gi
    	  
[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-sts.yaml 

[root@k8s-master01 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS       AGE
web-0                                     1/1     Running   0              4m19s
web-1                                     1/1     Running   0              4m10s
web-2                                     1/1     Running   0              2m21s

[root@k8s-master01 ~]# kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
www-web-0   Bound    pvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            rook-ceph-block   4m23s
www-web-1   Bound    pvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            rook-ceph-block   4m14s
www-web-2   Bound    pvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            rook-ceph-block   2m25s

[root@k8s-master01 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS      REASON   AGE
pvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            Delete           Bound    default/www-web-0   rook-ceph-block            4m25s
pvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            Delete           Bound    default/www-web-2   rook-ceph-block            2m27s
pvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            Delete           Bound    default/www-web-1   rook-ceph-block            4m16s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-共享文件系统的使用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-共享文件系统的使用&#34;&gt;#&lt;/a&gt; 8. 共享文件系统的使用&lt;/h4&gt;
&lt;p&gt;共享文件系统一般用于多个 Pod 共享一个存储&lt;/p&gt;
&lt;h5 id=&#34;81-创建共享类型的文件系统&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#81-创建共享类型的文件系统&#34;&gt;#&lt;/a&gt; 8.1 创建共享类型的文件系统&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd /root/rook/deploy/examples/
[root@k8s-master01 examples]# kubectl apply -f filesystem.yaml
[root@k8s-master01 examples]# kubectl get pod -l app=rook-ceph-mds -n rook-ceph
NAME                                    READY   STATUS    RESTARTS   AGE
rook-ceph-mds-myfs-a-7d76cb5988-9nz9p   2/2     Running   0          36s
rook-ceph-mds-myfs-b-76ff7c784c-vs8nm   2/2     Running   0          33s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;82-创建共享类型文件系统的-storageclass&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#82-创建共享类型文件系统的-storageclass&#34;&gt;#&lt;/a&gt; 8.2 创建共享类型文件系统的 StorageClass&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# cd csi/cephfs
[root@k8s-master01 cephfs]# kubectl create -f storageclass.yaml
[root@k8s-master01 cephfs]# kubectl get sc
NAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  17h
rook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   82m
rook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   13s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;83-挂载测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#83-挂载测试&#34;&gt;#&lt;/a&gt; 8.3 挂载测试&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat cephfs-pvc-deploy.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  selector:
    app: nginx
  type: ClusterIP
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-share-pvc
spec:
  storageClassName: rook-cephfs 
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
---
apiVersion: apps/v1
kind: Deployment 
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      containers:
      - name: nginx
        image: nginx 
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
      volumes:
        - name: www
          persistentVolumeClaim:
            claimName: nginx-share-pvc
			
[root@k8s-master01 ~]# kubectl apply -f cephfs-pvc-deploy.yaml
[root@k8s-master01 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS        AGE
cluster-test-84dfc9c68b-5q4ng             1/1     Running   84 (4m2s ago)   16d
nfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (123m ago)    18h
web-6c59f8559-g5xzb                       1/1     Running   0               46s
web-6c59f8559-ns77q                       1/1     Running   0               46s
web-6c59f8559-qxb5f                       1/1     Running   0               46s

[root@k8s-master01 ~]# kubectl get pvc
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    52s

[root@k8s-master01 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             53s

[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash
root@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# echo &amp;quot;hello cephfs&amp;quot; &amp;gt;&amp;gt; index.html

[root@k8s-master01 ~]# kubectl get svc
NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
kubernetes           ClusterIP   10.96.0.1     &amp;lt;none&amp;gt;        443/TCP    16d
mysql-svc-external   ClusterIP   None          &amp;lt;none&amp;gt;        3306/TCP   9d
nginx                ClusterIP   10.96.58.17   &amp;lt;none&amp;gt;        80/TCP     4m34s
[root@k8s-master01 ~]# curl 10.96.58.17
hello cephfs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9pvc-扩容&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9pvc-扩容&#34;&gt;#&lt;/a&gt; 9.PVC 扩容&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get sc
NAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  18h
rook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   104m     #true允许扩容
rook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   22m      #true允许扩容

[root@k8s-master01 ~]# kubectl get pvc
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    13m
[root@k8s-master01 ~]# kubectl edit pvc nginx-share-pvc
...
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi         #更改pvc大小
  storageClassName: rook-cephfs
...

[root@k8s-master01 ~]# kubectl get pvc       #查看PVC是否扩容
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    15m

[root@k8s-master01 ~]# kubectl get pv         #查看PV是否扩容
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             15m

[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash       #进入容器，查看pod是否扩容  
root@web-6c59f8559-g5xzb:/# df -h 
Filesystem                                                                                                                                             Size  Used Avail Use% Mounted on
overlay                                                                                                                                                 17G   13G  4.1G  76% /
tmpfs                                                                                                                                                   64M     0   64M   0% /dev
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/sda3                                                                                                                                               17G   13G  4.1G  76% /etc/hosts
shm                                                                                                                                                     64M     0   64M   0% /dev/shm
10.96.121.140:6789,10.96.131.130:6789,10.96.62.64:6789:/volumes/csi/csi-vol-3b645a11-58f4-475a-9404-5d84964f5291/e4bdf743-eb18-42c8-b04f-41964f76de4f  5.0G     0  5.0G   0% /usr/share/nginx/html
tmpfs                                                                                                                                                  3.8G   12K  3.8G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/asound
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/acpi
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/scsi
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/firmware
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;10-pvc-快照&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-pvc-快照&#34;&gt;#&lt;/a&gt; 10. PVC 快照&lt;/h4&gt;
&lt;h5 id=&#34;101-文件共享类型快照&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#101-文件共享类型快照&#34;&gt;#&lt;/a&gt; 10.1 文件共享类型快照&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd rook/deploy/examples
[root@k8s-master01 examples]# kubectl create -f csi/cephfs/snapshotclass.yaml 

[root@k8s-master01 examples]# kubectl get volumesnapshotclass
NAME                         DRIVER                          DELETIONPOLICY   AGE
csi-cephfsplugin-snapclass   rook-ceph.cephfs.csi.ceph.com   Delete           25s


#拍摄快照	
[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash         #pvc新增数据
root@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# touch &amp;#123;1..10&amp;#125;
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls
1  10  2  3  4	5  6  7  8  9  index.html

[root@k8s-master01 examples]# kubectl get pvc       #查看pvs并对nginx-share-pvc拍摄快照
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h23m

[root@k8s-master01 examples]# cat csi/cephfs/snapshot.yaml         #拍摄快照
---
# 1.17 &amp;lt;= K8s &amp;lt;= v1.19
# apiVersion: snapshot.storage.k8s.io/v1beta1
# K8s &amp;gt;= v1.20
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: cephfs-pvc-snapshot
spec:
  volumeSnapshotClassName: csi-cephfsplugin-snapclass
  source:
    persistentVolumeClaimName: nginx-share-pvc         #基于那个PVC拍摄快照
	
[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/snapshot.yaml
[root@k8s-master01 examples]# kubectl get volumesnapshot
NAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE
cephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   4m6s           4m8s

#删除pvc数据
[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash
root@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls
1  10  2  3  4	5  6  7  8  9  index.html
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# rm -rf &amp;#123;1..10&amp;#125;
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls
index.html

#pvc回滚数据
[root@k8s-master01 examples]# kubectl get volumesnapshot
NAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE
cephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   7m39s          7m41s
	
[root@k8s-master01 examples]# cat csi/cephfs/pvc-restore.yaml 
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc-restore
spec:
  storageClassName: rook-cephfs       #创建pv的storageclass名称相同
  dataSource:
    name: cephfs-pvc-snapshot         #volumesnapshot数据源
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi      #大小等于snapshot大小

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-restore.yaml

[root@k8s-master01 examples]# kubectl get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    54s          
nginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h50m

#挂载PVC测试数据是否恢复
[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: csicephfs-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: cephfs-pvc-restore        #挂载恢复pvc
        readOnly: false

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml
[root@k8s-master01 examples]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS        AGE
cluster-test-84dfc9c68b-5q4ng             1/1     Running   88 (57m ago)    16d
csicephfs-demo-pod                        1/1     Running   0               24s
nfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (6h57m ago)   23h
web-6c59f8559-g5xzb                       1/1     Running   0               4h54m
web-6c59f8559-ns77q                       1/1     Running   0               4h54m
web-6c59f8559-qxb5f                       1/1     Running   0               4h54m
[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash
root@csicephfs-demo-pod:/# ls /var/lib/www/html/               #s删除数据已经恢复
1  10  2  3  4	5  6  7  8  9  index.html
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;102-pvc-克隆&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#102-pvc-克隆&#34;&gt;#&lt;/a&gt; 10.2 PVC 克隆&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# kubectl get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    11m
nginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h1m


[root@k8s-master01 examples]# cat csi/cephfs/pvc-clone.yaml 
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc-clone
spec:
  storageClassName: rook-cephfs      # pvc 的 storageClass 名称
  dataSource:
    name: nginx-share-pvc          #克隆的PVC名称
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi                 #大小等于所克隆的PVC大小

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-clone.yaml

[root@k8s-master01 examples]# kubectl get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cephfs-pvc-clone     Bound    pvc-0a19b65e-cb5e-4379-a7f7-e0783fcf8ddf   5Gi        RWX            rook-cephfs    22s
cephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    15m
nginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h4m

#挂载克隆PVC测试
[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: csicephfs-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: cephfs-pvc-clone      #挂载克隆的pvc
        readOnly: false

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml          
[root@k8s-master01 examples]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS         AGE
cluster-test-84dfc9c68b-5q4ng             1/1     Running   89 (9m54s ago)   16d
csicephfs-demo-pod                        1/1     Running   0                17s
nfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (7h9m ago)     23h
web-6c59f8559-g5xzb                       1/1     Running   0                5h6m
web-6c59f8559-ns77q                       1/1     Running   0                5h6m
web-6c59f8559-qxb5f                       1/1     Running   0                5h6m

[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash
root@csicephfs-demo-pod:/# cat /var/lib/www/html/index.html 
hello cephfs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11-测试数据清理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-测试数据清理&#34;&gt;#&lt;/a&gt; 11. 测试数据清理&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;参考文档：https://rook.io/docs/rook/v1.11/Getting-Started/ceph-teardown/#delete-the-cephcluster-crd
[root@k8s-master01 ~]# kubectl delete deploy web

[root@k8s-master01 ~]# kubectl delete pods csicephfs-demo-pod

[root@k8s-master01 ~]# kubectl delete pvc --all
[root@k8s-master01 ~]# kubectl get pvc
No resources found in default namespace.
[root@k8s-master01 ~]# kubectl get pv
No resources found


[root@k8s-master01 ~]# kubectl get volumesnapshot
NAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE
cephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   61m            61m
[root@k8s-master01 ~]# kubectl delete volumesnapshot cephfs-pvc-snapshot
volumesnapshot.snapshot.storage.k8s.io &amp;quot;cephfs-pvc-snapshot&amp;quot; deleted

kubectl delete -n rook-ceph cephblockpool replicapool
kubectl delete -n rook-ceph cephfilesystem myfs

kubectl delete storageclass rook-ceph-block
kubectl delete storageclass rook-cephfs
kubectl delete -f csi/cephfs/kube-registry.yaml
kubectl delete storageclass csi-cephfs

kubectl -n rook-ceph delete cephcluster rook-ceph

kubectl delete -f operator.yaml
kubectl delete -f common.yaml
kubectl delete -f crds.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3890389502.html</guid>
            <title>K8S持久化存储NFS+StorageClass</title>
            <link>http://ixuyong.cn/posts/3890389502.html</link>
            <category>Kubernetes</category>
            <pubDate>Wed, 23 Apr 2025 20:08:26 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s持久化存储nfsstorageclass&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s持久化存储nfsstorageclass&#34;&gt;#&lt;/a&gt; K8S 持久化存储 NFS+StorageClass&lt;/h3&gt;
&lt;h4 id=&#34;1-搭建nfs服务器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-搭建nfs服务器&#34;&gt;#&lt;/a&gt; 1. 搭建 NFS 服务器&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#所有K8S节点安装nfs-utils
[root@k8s-node02 ~]# yum install nfs-utils -y    

#K8S-node02节点配置nfs服务
[root@k8s-node02 ~]# mkdir /data/nfs -p
[root@k8s-node02 ~]# cat /etc/exports
/data/nfs 192.168.1.0/24(rw,no_root_squash)
[root@k8s-node02 ~]# exportfs -arv   #NFS配置生效 
[root@k8s-node02 ~]# systemctl start nfs-server &amp;amp;&amp;amp; systemctl enable nfs-server &amp;amp;&amp;amp; systemctl status nfs-server
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-创建rbac&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-创建rbac&#34;&gt;#&lt;/a&gt; 2.  创建 RBAC&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-node02 ~]# cat 01-rbac.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;nodes&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;persistentvolumes&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;delete&amp;quot;]
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;persistentvolumeclaims&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;update&amp;quot;]
  - apiGroups: [&amp;quot;storage.k8s.io&amp;quot;]
    resources: [&amp;quot;storageclasses&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;events&amp;quot;]
    verbs: [&amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;patch&amp;quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;endpoints&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;patch&amp;quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
  
  
[root@k8s-master01 ~]# kubectl apply -f 01-rbac.yaml 
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-创建nfs-provisioner&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-创建nfs-provisioner&#34;&gt;#&lt;/a&gt; 3. 创建 nfs-provisioner&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 02-nfs-provisioner.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.cn-hangzhou.aliyuncs.com/old_xu/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME	# nfs-provisioner的名称，后续storageClass要与该名称一致
              value: nfzl.com/nfs
            - name: NFS_SERVER		# NFS服务的IP地址
              value: 192.168.1.75
            - name: NFS_PATH		# NFS服务共享的路径
              value: /data/nfs
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.1.75
            path: /data/nfs

[root@k8s-master01 ~]# kubectl apply -f 02-nfs-provisioner.yaml 
[root@k8s-master01 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-6bcc4587f8-zp8qc   1/1     Running   0          17s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-创建storageclass&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建storageclass&#34;&gt;#&lt;/a&gt; 4. 创建 StorageClass&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 03-storageClass.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage 	# pvc申请时需明确指定的storageClass名称
provisioner: nfzl.com/nfs        # 供应商名称，必须和上面创建的&amp;quot;PROVISIONER_NAME&amp;quot;变量值致
parameters:
  archiveOnDelete: &amp;quot;false&amp;quot;     # 如果值为false，删除PVC后也会删除目录内容, &amp;quot;true&amp;quot;则会对数据进行保留
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-创建pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-创建pvc&#34;&gt;#&lt;/a&gt; 5. 创建 PVC&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 04-nginx-pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sc-pvc-001
spec:
  storageClassName: &amp;quot;nfs-storage&amp;quot;     # 明确指定使用哪个sc的供应商来创建pv
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi                      # 根据业务实际大小进行资源申请
      
[root@k8s-master01 ~]# kubectl apply -f 04-nginx-pvc.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/fgpaP15.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;6-挂载pvc测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-挂载pvc测试&#34;&gt;#&lt;/a&gt; 6. 挂载 PVC 测试&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 05-nginx-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-sc-001
spec:
  containers:
  - name: nginx-sc-001
    image: nginx
    volumeMounts:
    - name: nginx-page
      mountPath: /usr/share/nginx/html
  volumes:
  - name: nginx-page
    persistentVolumeClaim:      
      claimName: sc-pvc-001

[root@k8s-master01 ~]# kubectl apply -f 05-nginx-pod.yaml
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                                      READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
nginx-sc-001                              1/1     Running   0          15s   172.16.85.244   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

[root@k8s-master01 ~]# curl 172.16.85.244
hello world
&lt;/code&gt;&lt;/pre&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/722512536.html</guid>
            <title>K8s细粒度权限控制RBAC</title>
            <link>http://ixuyong.cn/posts/722512536.html</link>
            <category>Kubernetes</category>
            <pubDate>Wed, 23 Apr 2025 20:04:03 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s细粒度权限控制rbac&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s细粒度权限控制rbac&#34;&gt;#&lt;/a&gt; K8s 细粒度权限控制 RBAC&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/KCZPPkv.jpeg&#34; alt=&#34;rbac.jpg&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;1-创建不同权限的clusterrole&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-创建不同权限的clusterrole&#34;&gt;#&lt;/a&gt; 1. 创建不同权限的 clusterrole&lt;/h4&gt;
&lt;h5 id=&#34;11-命令空间只读namespace-readonly&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-命令空间只读namespace-readonly&#34;&gt;#&lt;/a&gt; 1.1 命令空间只读 namespace-readonly&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat namespace-readonly.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: namespace-readonly
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-资源查看resource-readonly&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-资源查看resource-readonly&#34;&gt;#&lt;/a&gt; 1.2 资源查看 resource-readonly&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat resource-readonly.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: resource-readonly
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  - daemonsets
  - deployments
  - deployments/scale
  - replicasets
  - replicasets/scale
  - statefulsets
  - statefulsets/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-pod日志查看&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-pod日志查看&#34;&gt;#&lt;/a&gt; 1.3 pod 日志查看&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat pod-log.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-log
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods
  - pods/log
  verbs:
  - get
  - list
  - watch
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-pod删除&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-pod删除&#34;&gt;#&lt;/a&gt; 1.4 Pod 删除&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat pod-delete.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-delete
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods
  verbs:
  - get
  - list
  - delete
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-pod执行&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-pod执行&#34;&gt;#&lt;/a&gt; 1.5 Pod 执行&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat pod-exec.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-exec
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods
  verbs:
  - get
  - list
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods/exec
  verbs:
  - create
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;16-创建不同权限的clusterrole&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-创建不同权限的clusterrole&#34;&gt;#&lt;/a&gt; 1.6 创建不同权限的 clusterrole&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 rbac]# kubectl apply -f .
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-创建serviceaccount&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-创建serviceaccount&#34;&gt;#&lt;/a&gt; 2. 创建 serviceaccount&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create ns kube-users

# kubectl create sa test -n kube-users   
# kubectl create sa dev -n kube-users    
# kubectl create sa ops -n kube-users    

# kubectl create token test -n kube-users
# kubectl create token dev -n kube-users
# kubectl create token ops -n kube-users
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-创建clusterrolebinding&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-创建clusterrolebinding&#34;&gt;#&lt;/a&gt; 3. 创建 ClusterRoleBinding&lt;/h4&gt;
&lt;h5 id=&#34;31-绑定全局命名空间查看权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-绑定全局命名空间查看权限&#34;&gt;#&lt;/a&gt; 3.1 绑定全局命名空间查看权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat clusterrolebinding-namespace-readonly.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: clusterrolebinding-namespace-readonly 
subjects:
- kind: Group
  name: system:serviceaccounts:kube-users
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: namespace-readonly
  apiGroup: rbac.authorization.k8s.io
  
# kubectl apply -f clusterrolebinding-namespace-readonly.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;32-绑定日志查看权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-绑定日志查看权限&#34;&gt;#&lt;/a&gt; 3.2 绑定日志查看权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-绑定资源查看权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-绑定资源查看权限&#34;&gt;#&lt;/a&gt; 3.3 绑定资源查看权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;34-绑定pod执行权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-绑定pod执行权限&#34;&gt;#&lt;/a&gt; 3.4 绑定 Pod 执行权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;35-绑定pod删除权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#35-绑定pod删除权限&#34;&gt;#&lt;/a&gt; 3.5 绑定 Pod 删除权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/176412055.html</guid>
            <title>K8s准入控制ResourceQuota、LimitRange、QoS服务质量</title>
            <link>http://ixuyong.cn/posts/176412055.html</link>
            <category>Kubernetes</category>
            <pubDate>Wed, 23 Apr 2025 19:55:19 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s准入控制resourcequota-limitrange-qos服务质量&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s准入控制resourcequota-limitrange-qos服务质量&#34;&gt;#&lt;/a&gt; K8s 准入控制 ResourceQuota、LimitRange、QoS 服务质量&lt;/h3&gt;
&lt;h4 id=&#34;1-resourcequota配置解析&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-resourcequota配置解析&#34;&gt;#&lt;/a&gt; 1. ResourceQuota 配置解析&lt;/h4&gt;
&lt;p&gt;ResourceQuotas 实现资源配额，避免过度创建资源，针对 namespace 进行限制。cpu 内存则是根据 pod 配置的 resources 总额进行限制，如果没有配置 resources 参数则无法限制。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ResourceQuota
metadata:
  name: resourcequota-test
  namespace: test
  labels:
    app: resourcequota
spec:
  hard:
    pods: 3
    requests.cpu: 3
    requests.memory: 512Mi
    limits.cpu: 8
    limits.memory: 16Gi
    configmaps: 201
    requests.storage: 40Gi
    persistentvolumeclaims: 20
    replicationcontrollers: 20
    secrets: 20
    services: 50
    services.loadbalancers: &amp;quot;2&amp;quot;
    services.nodeports: &amp;quot;10&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;pods：限制最多启动 Pod 的个数&lt;/li&gt;
&lt;li&gt;requests.cpu：限制最高 CPU 请求数&lt;/li&gt;
&lt;li&gt;requests.memory：限制最高内存的请求数&lt;/li&gt;
&lt;li&gt;limits.cpu：限制最高 CPU 的 limit 上限&lt;/li&gt;
&lt;li&gt;limits.memory：限制最高内存的 limit 上限&lt;/li&gt;
&lt;li&gt;services：限制 services 数量&lt;/li&gt;
&lt;li&gt;services.nodeports：限制 services 中 nodeport 类型 service 数量&lt;/li&gt;
&lt;li&gt;services.loadbalancers：限制 services 中 loadbalancers 类型 service 数量&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-resourcequota配置示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-resourcequota配置示例&#34;&gt;#&lt;/a&gt; 1.1 ResourceQuota 配置示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.限制test命名空间pods数量量为3、configmap数量为2
[root@k8s-master01 resourcequota]# cat rq-test.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resourcequota-test
  namespace: test
  labels:
    app: resourcequota
spec:
  hard:
    pods: 3
#    requests.cpu: 3
#    requests.memory: 512Mi
#    limits.cpu: 8
#    limits.memory: 16Gi
    configmaps: 2
#    requests.storage: 40Gi
#    persistentvolumeclaims: 20
#    replicationcontrollers: 20
#    secrets: 20
#    services: 50
#    services.loadbalancers: &amp;quot;2&amp;quot;
#    services.nodeports: &amp;quot;10&amp;quot;

#2.test命名空间已创建configmap数量为1,限制数量为2
[root@k8s-master01 resourcequota]# kubectl get resourcequota -n test
NAME                 AGE   REQUEST                      LIMIT
resourcequota-test   61s   configmaps: 1/2, pods: 0/3  

#3.test命名空间创建第2个configmap时正常，创建第3个configmap时报错
[root@k8s-master01 resourcequota]# kubectl create cm rq-cm1 -n test --from-literal=key1=value1
[root@k8s-master01 resourcequota]# kubectl create cm rq-cm2 -n test --from-literal=key2=value2
error: failed to create configmap: configmaps &amp;quot;rq-cm2&amp;quot; is forbidden: exceeded quota: resourcequota-test, requested: configmaps=1, used: configmaps=2, limited: configmaps=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-limitrange配置解析&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-limitrange配置解析&#34;&gt;#&lt;/a&gt; 2. LimitRange 配置解析&lt;/h4&gt;
&lt;p&gt;虽然 ResourceQuota 可以实现资源配额，可以限制某个命名空间内存和 CPU，但是如果创建的 Pod 都没有配置 resources 参数则无法限制。如果配置 LimitRange，Pod 没有配置 resources 情况下，创建的 Pod 会根据 LimitRange 配置自动添加 CPU 内存配置，并且可以限制 resources 参数最大配置和最小配置，LimitRange 针对 Pod 进行限制。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #限制CPU内存默认limits配置
      cpu: 1
      memory: 512Mi
    defaultRequest:  #限制CPU内存默认request配置
      cpu: 0.5
      memory: 256Mi
    max:                #限制CPU内存最大配置 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #限制CPU内存最小配置
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #限制pvc大小
    max:
      storage: 2Gi
    min:
      storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;default：默认 limits 配置&lt;/li&gt;
&lt;li&gt;defaultRequest：默认 requests 配置&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;21-配置默认的requests和limits&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-配置默认的requests和limits&#34;&gt;#&lt;/a&gt; 2.1 配置默认的 requests 和 limits&lt;/h5&gt;
&lt;p&gt;Pod 没有配置 resources 情况下，创建的 Pod 会根据 LimitRange 配置自动添加 CPU 内存配置。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.创建LimitRange
[root@k8s-master01 resourcequota]# cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #限制CPU内存默认limits配置
      cpu: 1
      memory: 512Mi
    defaultRequest:  #限制CPU内存默认request配置
      cpu: 0.5
      memory: 256Mi
    max:                #限制CPU内存最大配置 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #限制CPU内存最小配置
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #限制pvc大小
    max:
      storage: 2Gi
    min:
      storage: 1Gi  
      
[root@k8s-master01 resourcequota]# kubectl apply -f limitrange.yaml
[root@k8s-master01 resourcequota]# kubectl get limitrange -n test
NAME                  CREATED AT
cpu-mem-limit-range   2025-04-23T07:55:03Z

#2.创建deployment, 查看是否会根据LimitRange自动添加CPU内存配置
[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-limirange
  labels:
    app: deploy-limirange
  namespace: test
spec:
  selector:
    matchLabels:
      app: deploy-limirange
  replicas: 1
  template:
    metadata:
      labels:
        app: deploy-limirange
    spec:
      restartPolicy: Always
      containers:
        - name: deploy-limirange
          image: nginx
          imagePullPolicy: IfNotPresent

[root@k8s-master01 resourcequota]# kubectl get pod -n test
NAME                                READY   STATUS    RESTARTS   AGE
deploy-limirange-854c9545ff-grpxr   1/1     Running   0          39s
[root@k8s-master01 resourcequota]# kubectl get pod -n test -oyaml
...
  spec:
    containers:
    - image: nginx
      imagePullPolicy: IfNotPresent
      name: deploy-limirange
      resources:
        limits:
          cpu: &amp;quot;1&amp;quot;
          memory: 512Mi
        requests:
          cpu: 500m
          memory: 256Mi
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-限制requests和limits范围&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-限制requests和limits范围&#34;&gt;#&lt;/a&gt; 2.2 限制 requests 和 limits 范围&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.创建LimitRange
[root@k8s-master01 resourcequota]# cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #限制CPU内存默认limits配置
      cpu: 1
      memory: 512Mi
    defaultRequest:  #限制CPU内存默认request配置
      cpu: 0.5
      memory: 256Mi
    max:                #限制CPU内存最大配置 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #限制CPU内存最小配置
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #限制pvc大小
    max:
      storage: 2Gi
    min:
      storage: 1Gi  

#2.创建deployment, CPU内存limits和requests高于/低于LimitRangeCPU内存max、min配置
[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-limirange
  labels:
    app: deploy-limirange
  namespace: test
spec:
  selector:
    matchLabels:
      app: deploy-limirange
  replicas: 1
  template:
    metadata:
      labels:
        app: deploy-limirange
    spec:
      restartPolicy: Always
      containers:
        - name: deploy-limirange
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 8096Mi
              cpu: 5
            requests:
              memory: 64Mi
              cpu: 10m

#3.由于创建deployment, CPU内存limits和requests高于/低于LimitRangeCPU内存max、min配置，pod没有创建
[root@k8s-master01 resourcequota]# kubectl create -f deploy-limitrange.yaml 

[root@k8s-master01 resourcequota]# kubectl get deploy deploy-limirange -n test
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
deploy-limirange   0/1     0            0           2m7s
[root@k8s-master01 resourcequota]# kubectl get pods -n test

[root@k8s-master01 resourcequota]# kubectl describe rs deploy-limirange-54c5d69b4b -n test
Name:           deploy-limirange-54c5d69b4b
Namespace:      test
Selector:       app=deploy-limirange,pod-template-hash=54c5d69b4b
Labels:         app=deploy-limirange
                pod-template-hash=54c5d69b4b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/deploy-limirange
Replicas:       0 current / 1 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=deploy-limirange
           pod-template-hash=54c5d69b4b
  Containers:
   deploy-limirange:
    Image:      nginx
    Port:       &amp;lt;none&amp;gt;
    Host Port:  &amp;lt;none&amp;gt;
    Limits:
      cpu:     5
      memory:  8096Mi
    Requests:
      cpu:         10m
      memory:      64Mi
    Environment:   &amp;lt;none&amp;gt;
    Mounts:        &amp;lt;none&amp;gt;
  Volumes:         &amp;lt;none&amp;gt;
  Node-Selectors:  &amp;lt;none&amp;gt;
  Tolerations:     &amp;lt;none&amp;gt;
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate
Events:
  Type     Reason        Age                 From                   Message
  ----     ------        ----                ----                   -------
  Warning  FailedCreate  3m8s                replicaset-controller  Error creating: pods &amp;quot;deploy-limirange-54c5d69b4b-zxhzk&amp;quot; is forbidden: [minimum cpu usage per Container is 100m, but request is 10m, minimum memory usage per Container is 100Mi, but request is 64Mi, maximum cpu usage per Container is 4, but limit is 5, maximum memory usage per Container is 4Gi, but limit is 8096Mi]
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-限制存储空间大小&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-限制存储空间大小&#34;&gt;#&lt;/a&gt; 2.3 限制存储空间大小&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.创建LimitRange
[root@k8s-master01 resourcequota]# cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #限制CPU内存默认limits配置
      cpu: 1
      memory: 512Mi
    defaultRequest:  #限制CPU内存默认request配置
      cpu: 0.5
      memory: 256Mi
    max:                #限制CPU内存最大配置 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #限制CPU内存最小配置
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #限制pvc大小
    max:
      storage: 2Gi
    min:
      storage: 1Gi  
  
#2.由于创建的pvc大于2G，所以报错  
[root@k8s-master01 ~]# cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sc-pvc-001
spec:
  storageClassName: &amp;quot;nfs-storage&amp;quot;     # 明确指定使用哪个sc的供应商来创建pv
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 3Gi                      # 根据业务实际大小进行资源申请  
[root@k8s-master01 ~]# kubectl create -f pvc.yaml -n test
Error from server (Forbidden): error when creating &amp;quot;pvc.yaml&amp;quot;: persistentvolumeclaims &amp;quot;sc-pvc-001&amp;quot; is forbidden: maximum storage usage per PersistentVolumeClaim is 2Gi, but request is 3Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-服务质量-qos&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-服务质量-qos&#34;&gt;#&lt;/a&gt; 3. 服务质量 QoS&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed：最高服务质量，当宿主机内存不够时，会先 kill 掉 QoS 为 BestEffort 和 Burstable 的 Pod，如果内存还是不够，才会 kill 掉 QoS 为 Guaranteed，该级别 Pod 的资源占用量一般比较明确，即 requests 的 cpu 和 memory 和 limits 的 cpu 和 memory 配置的一致。&lt;/li&gt;
&lt;li&gt;Burstable： 服务质量低于 Guaranteed，当宿主机内存不够时，会先 kill 掉 QoS 为 BestEffort 的 Pod，如果内存还是不够之后就会 kill 掉 QoS 级别为 Burstable 的 Pod，用来保证 QoS 质量为 Guaranteed 的 Pod，该级别 Pod 一般知道最小资源使用量，但是当机器资源充足时，还是想尽可能的使用更多的资源，即 limits 字段的 cpu 和 memory 大于 requests 的 cpu 和 memory 的配置。&lt;/li&gt;
&lt;li&gt;BestEffort：尽力而为，当宿主机内存不够时，首先 kill 的就是该 QoS 的 Pod，用以保证 Burstable 和 Guaranteed 级别的 Pod 正常运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;31-实现qos为guaranteed的pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-实现qos为guaranteed的pod&#34;&gt;#&lt;/a&gt; 3.1 实现 QoS 为 Guaranteed 的 Pod&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pod 中的每个容器必须指定 limits.memory 和 requests.memory，并且两者需要相等；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pod 中的每个容器必须指定 limits.cpu 和 limits.memory，并且两者需要相等。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 1024Mi
              cpu: 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;32-实现qos为burstable的pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-实现qos为burstable的pod&#34;&gt;#&lt;/a&gt; 3.2 实现 QoS 为 Burstable 的 Pod&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pod 不符合 Guaranteed 的配置要求；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pod 中至少有一个容器配置了 requests.cpu 或 requests.memory。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-实现qos为besteffort的pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-实现qos为besteffort的pod&#34;&gt;#&lt;/a&gt; 3.3 实现 QoS 为 BestEffort 的 Pod&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;不设置 resources 参数&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/312010518.html</guid>
            <title>K8s亲和力Affinity</title>
            <link>http://ixuyong.cn/posts/312010518.html</link>
            <category>Kubernetes</category>
            <pubDate>Sun, 20 Apr 2025 17:59:58 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s亲和力affinity&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s亲和力affinity&#34;&gt;#&lt;/a&gt; K8s 亲和力 Affinity&lt;/h3&gt;
&lt;p&gt;Pod 和节点之间的关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;某些 Pod 优先选择有 ssd=true 标签的节点，如果没有在考虑部署到其它节点；&lt;/li&gt;
&lt;li&gt;某些 Pod 需要部署在 ssd=true 和 type=physical 的节点上，但是优先部署在 ssd=true 的节点上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pod 和 Pod 之间的关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同一个应用的 Pod 不同的副本或者同一个项目的应用尽量或必须不部署在同一个节点或者符合某个标签的一类节点上或者不同的区域；&lt;/li&gt;
&lt;li&gt;相互依赖的两个 Pod 尽量或必须部署在同一个节点上或者同一个域内。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;1-affinity分类&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-affinity分类&#34;&gt;#&lt;/a&gt; 1. Affinity 分类&lt;/h4&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/hTd0wmD.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-节点亲和力配置详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-节点亲和力配置详解&#34;&gt;#&lt;/a&gt; 2. 节点亲和力配置详解&lt;/h4&gt;
&lt;h5 id=&#34;21-硬亲和力required&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-硬亲和力required&#34;&gt;#&lt;/a&gt; 2.1 硬亲和力 required&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                      - k8s-node01
                      - k8s-node02
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;requiredDuringSchedulingIgnoredDuringExecution：硬亲和力配置&lt;/li&gt;
&lt;li&gt;nodeSelectorTerms：节点选择器配置，可以配置多个 matchExpressions（满足其一即可）&lt;/li&gt;
&lt;li&gt;matchExpressions：matchExpressions 下可以配置多个 key、values（都需要满足），其中 values 可以配置多个（满足其一即可）&lt;/li&gt;
&lt;li&gt;operator：
&lt;ul&gt;
&lt;li&gt;IN 相当于 key = value 的形式，&lt;strong&gt;NotIn 相当于 key!=value 的形式 (反亲和力)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Exists: 节点存在 label 的 key 为指定的值即可，不能配置 values 字段&lt;/li&gt;
&lt;li&gt;DoesNotExist: 节点不存在 label 的 key 为指定的值即可，不能配置 values 字段&lt;/li&gt;
&lt;li&gt;Gt：大于 value 指定的值&lt;/li&gt;
&lt;li&gt;Lt：小于 value 指定的值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;22-软亲和力preferred&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-软亲和力preferred&#34;&gt;#&lt;/a&gt; 2.2 软亲和力 preferred&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 6
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: ssd
                    operator: In
                    values:
                      - &#39;true&#39;
            - weight: 50
              preference:
                matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                      - k8s-master01
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;preferredDuringSchedulingIgnoredDuringExecution：软亲和力配置&lt;/li&gt;
&lt;li&gt;weight：软亲和力的权重，权重越高优先级越大，范围 1-100&lt;/li&gt;
&lt;li&gt;matchExpressions：matchExpressions 下可以配置多个 key、values（都需要满足），其中 values 可以配置多个（满足其一即可）&lt;/li&gt;
&lt;li&gt;operator：
&lt;ul&gt;
&lt;li&gt;IN 相当于 key = value 的形式，&lt;strong&gt;NotIn 相当于 key!=value 的形式 (反亲和力)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Exists: 节点存在 label 的 key 为指定的值即可，不能配置 values 字段&lt;/li&gt;
&lt;li&gt;DoesNotExist: 节点不存在 label 的 key 为指定的值即可，不能配置 values 字段&lt;/li&gt;
&lt;li&gt;Gt：大于 value 指定的值&lt;/li&gt;
&lt;li&gt;Lt：小于 value 指定的值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-pod亲和力详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-pod亲和力详解&#34;&gt;#&lt;/a&gt; 3. Pod 亲和力详解&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:              
        podAntiAffinity:   #pod硬反亲和力
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx-deploy
            topologyKey: kubernetes.io/hostname
        podAntiAffinity:       #pod软反亲和力
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nginx-deploy
              namespaces:     #和哪个命名空间的Pod进行匹配，为空为当前命名空间
              - default
              topologyKey: kubernetes.io/hostname
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;labelSelector：Pod 选择器配置，可以配置多个&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;matchExpressions：matchExpressions 下可以配置多个 key、values（都需要满足），其中 values 可以配置多个（满足其一即可）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;topologyKey：匹配的拓扑域的 key，也就是节点上 label 的 key，key 和 value 相同的为同一个域，可以用于标注不同的机房和地区&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Namespaces: 和哪个命名空间的 Pod 进行匹配，为空为当前命名空间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;operator：配置和节点亲和力一致，但是没有 Gt 和 Lt&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IN 相当于 key = value 的形式；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exists: 节点存在 label 的 key 为指定的值即可，不能配置 values 字段；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DoesNotExist: 节点不存在 label 的 key 为指定的值即可，不能配置 values 字段&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-节点亲和力配置示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-节点亲和力配置示例&#34;&gt;#&lt;/a&gt; 4. 节点亲和力配置示例&lt;/h4&gt;
&lt;p&gt;Pod 尽量部署在 ssd=true 和 type=physical 的节点上，但是优先部署在 ssd=true 的节点上，不能部署 label 为 gpu=true 的节点。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl label nodes k8s-node01 ssd=true
[root@k8s-master01 ~]# kubectl label nodes k8s-master01 ssd=true
[root@k8s-master01 ~]# kubectl label nodes k8s-master01 gpu=true
[root@k8s-master01 ~]# kubectl label nodes k8s-node02 type=physical

[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
      annotations:
        app: nginx-deploy
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: ssd
                    operator: In
                    values:
                      - &#39;true&#39;
                  - key: gpu
                    operator: NotIn
                    values:
                      - &#39;true&#39;
            - weight: 50
              preference:
                matchExpressions:
                  - key: type
                    operator: In
                    values:
                      - physical
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;


[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
nginx-deploy-7d65fbdf-2b4jr   1/1     Running   0          5s    172.16.85.236   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-jjzwr   1/1     Running   0          5s    172.16.58.251   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-kx5lm   1/1     Running   0          5s    172.16.85.237   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-lrmcg   1/1     Running   0          5s    172.16.85.238   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-n6mlp   1/1     Running   0          5s    172.16.58.250   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-pod亲和力-反亲和力配置示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-pod亲和力-反亲和力配置示例&#34;&gt;#&lt;/a&gt; 5. Pod 亲和力、反亲和力配置示例&lt;/h4&gt;
&lt;h5 id=&#34;51-pod反亲和力required&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-pod反亲和力required&#34;&gt;#&lt;/a&gt; 5.1 Pod 反亲和力 required&lt;/h5&gt;
&lt;p&gt;同一个应用部署在不同的宿主机&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.节点存在污点pod无法调度至该节点
# kubectl describe nodes|grep -i taint
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;

#2.pod反亲和力required
# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - nginx-deploy
              topologyKey: kubernetes.io/hostname
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;

#3.部署deployment
[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-deploy-5787887b6f-4654b   1/1     Running   0          4s    172.16.85.234    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4s    172.16.122.152   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-fdkft   1/1     Running   0          4s    172.16.58.247    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4s    172.16.32.152    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4s    172.16.195.14    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

#4.将副本扩成6个，由于K8s集群只有5个节点，即5个topologyKey（拓扑域），每个域只能有一个副本，所以有一个pod会pending
[root@k8s-master01 ~]# kubectl scale deploy nginx-deploy --replicas=6 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES
nginx-deploy-5787887b6f-4654b   1/1     Running   0          4m44s   172.16.85.234    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4m44s   172.16.122.152   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-fdkft   1/1     Running   0          4m44s   172.16.58.247    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4m44s   172.16.32.152    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4m44s   172.16.195.14    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-sztm7   0/1     Pending   0          9s      &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

[root@k8s-master01 ~]# kubectl describe pods nginx-deploy-5787887b6f-sztm7
...
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  102s  default-scheduler  0/5 nodes are available: 5 node(s) didn&#39;t match pod anti-affinity rules. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;将副本扩成 6 个，有一个会 pending 状态，原因 K8s 集群只有 5 个节点，即 5 个 topologyKey（拓扑域），每个拓扑域只能有一个副本，所以有一个 pod 会 pending。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;topologyKey：匹配的拓扑域的 key，也就是节点上 label 的 key，key 和 value 相同的为同一个域，可以用于标注不同的机房和地区&lt;/strong&gt;。&lt;/p&gt;
&lt;h5 id=&#34;52-pod反亲和力preferred&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-pod反亲和力preferred&#34;&gt;#&lt;/a&gt; 5.2 Pod 反亲和力 preferred&lt;/h5&gt;
&lt;p&gt;同一个应用尽量部署在不同的宿主机&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.节点存在污点pod无法调度至该节点
# kubectl describe nodes|grep -i taint
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;

#2.pod反亲和力preferred
# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 6
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - nginx-deploy
                topologyKey: kubernetes.io/hostname
              weight: 100
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;

#3.部署deployment
[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-deploy-7c47567b79-97qs5   1/1     Running   0          6s    172.16.122.153   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-g49h4   1/1     Running   0          6s    172.16.85.235    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-g5n2s   1/1     Running   0          6s    172.16.58.248    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-g5v5b   1/1     Running   0          6s    172.16.195.15    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-pjwws   1/1     Running   0          6s    172.16.58.249    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-q2hn5   1/1     Running   0          6s    172.16.32.153    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;53-pod亲和力required&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#53-pod亲和力required&#34;&gt;#&lt;/a&gt; 5.3 Pod 亲和力 required&lt;/h5&gt;
&lt;p&gt;同一个应用必须部署在同一个宿主机&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 8
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:              
        podAffinity:   #pod硬亲和力
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx-deploy
            topologyKey: kubernetes.io/hostname
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
      volumes:
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File

[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
nginx-deploy-dbcc4d65c-2sthn   1/1     Running   0          12s   172.16.58.255   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-78nxf   1/1     Running   0          12s   172.16.58.197   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-82ssq   1/1     Running   0          12s   172.16.58.194   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-986cb   1/1     Running   0          12s   172.16.58.254   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-9rnt7   1/1     Running   0          12s   172.16.58.252   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-knm8q   1/1     Running   0          12s   172.16.58.195   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-kx56f   1/1     Running   0          12s   172.16.58.253   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-sqlhf   1/1     Running   0          12s   172.16.58.198   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;54-pod亲和力preferre&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#54-pod亲和力preferre&#34;&gt;#&lt;/a&gt; 5.4 Pod 亲和力 preferre&lt;/h5&gt;
&lt;p&gt;同一个应用尽量部署在同一个宿主机&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 20
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:              
        podAffinity:       #pod软亲和力
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nginx-deploy
              namespaces:     #和哪个命名空间的Pod进行匹配，为空为当前命名空间
              - default
              topologyKey: kubernetes.io/hostname
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
      volumes:
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3254599477.html</guid>
            <title>K8s容忍和污点</title>
            <link>http://ixuyong.cn/posts/3254599477.html</link>
            <category>Kubernetes</category>
            <pubDate>Sun, 20 Apr 2025 15:51:58 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s容忍和污点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s容忍和污点&#34;&gt;#&lt;/a&gt; K8s 容忍和污点&lt;/h3&gt;
&lt;p&gt;Taint 指定服务器上打上污点，让不能容忍这个污点的 Pod 不能部署在打了污点的服务器上。Toleration 是让 Pod 容忍节点上配置的污点，可以让一些需要特殊配置的 Pod 能够调用到具有污点和特殊配置的节点上。&lt;/p&gt;
&lt;h4 id=&#34;1-taint配置解析&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-taint配置解析&#34;&gt;#&lt;/a&gt; 1. Taint 配置解析&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#1.Taint语法
# kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT

#2.创建Taint示例
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule

#3.查看污点
# kubectl describe node k8s-node01 | grep Taints -A 10

#4.删除污点
# kubectl taint nodes k8s-node01 ssd-                   #基于Key删除
# kubectl taint nodes k8s-node01 ssd:PreferNoSchedule-  #基于Key+Effect删除

#5.修改污点（Key和Effect相同）
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule --overwrite
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;EFFECT 排斥等级：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NoSchedule：禁止调度到该节点，已经在该节点上的 Pod 不受影响&lt;/li&gt;
&lt;li&gt;NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后）&lt;/li&gt;
&lt;li&gt;PreferNoSchedule：尽量避免将 Pod 调度到指定的节点上，如果没有更合适的节点，可以部署到该节点&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2toleration配置解析&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2toleration配置解析&#34;&gt;#&lt;/a&gt; 2.Toleration 配置解析&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#1.完全匹配
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;taintValue&amp;quot;
  effect: &amp;quot;NoSchedule
 
#2.不完全匹配 
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Exists&amp;quot;
  effect: &amp;quot;NoSchedule&amp;quot;
  
#3.大范围匹配（不推荐key为内置Taint，会导致节点故障pod无法漂移）
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Exists
  
#4.容忍时间配置
tolerations:
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoExecute&amp;quot;
  tolerationSeconds: 3600
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-taint-toleration配置示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-taint-toleration配置示例&#34;&gt;#&lt;/a&gt; 3. Taint、Toleration 配置示例&lt;/h4&gt;
&lt;p&gt;有一个 K8s 节点是纯 SSD 硬盘的节点，现需要只有一些需要高性能存储的 Pod 才能调度到该节点上。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.给节点打上污点和标签
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule
# kubectl label node k8s-node01 ssd=true

#2.配置Toleration：
# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
      nodeSelector:
        ssd: &#39;true&#39;
      tolerations:
        - key: ssd
          operator: Exists
          effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-k8s内置污点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-k8s内置污点&#34;&gt;#&lt;/a&gt; 4. K8s 内置污点&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/not-ready%EF%BC%9A%E8%8A%82%E7%82%B9%E6%9C%AA%E5%87%86%E5%A4%87%E5%A5%BD%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81Ready%E7%9A%84%E5%80%BC%E4%B8%BAFalse%E3%80%82&#34;&gt;node.kubernetes.io/not-ready：节点未准备好，相当于节点状态 Ready 的值为 False。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/unreachable%EF%BC%9ANode&#34;&gt;node.kubernetes.io/unreachable：Node&lt;/a&gt; Controller 访问不到节点，相当于节点状态 Ready 的值为 Unknown。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/out-of-disk%EF%BC%9A%E8%8A%82%E7%82%B9%E7%A3%81%E7%9B%98%E8%80%97%E5%B0%BD%E3%80%82&#34;&gt;node.kubernetes.io/out-of-disk：节点磁盘耗尽。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/memory-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E5%86%85%E5%AD%98%E5%8E%8B%E5%8A%9B%E3%80%82&#34;&gt;node.kubernetes.io/memory-pressure：节点存在内存压力。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/disk-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E7%A3%81%E7%9B%98%E5%8E%8B%E5%8A%9B%E3%80%82&#34;&gt;node.kubernetes.io/disk-pressure：节点存在磁盘压力。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/network-unavailable%EF%BC%9A%E8%8A%82%E7%82%B9%E7%BD%91%E7%BB%9C%E4%B8%8D%E5%8F%AF%E8%BE%BE%E3%80%82&#34;&gt;node.kubernetes.io/network-unavailable：节点网络不可达。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/unschedulable%EF%BC%9A%E8%8A%82%E7%82%B9%E4%B8%8D%E5%8F%AF%E8%B0%83%E5%BA%A6%E3%80%82&#34;&gt;node.kubernetes.io/unschedulable：节点不可调度。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.cloudprovider.kubernetes.io/uninitialized%EF%BC%9A%E5%A6%82%E6%9E%9CKubelet%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8C%87%E5%AE%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%A4%96%E9%83%A8%E7%9A%84cloudprovider%EF%BC%8C%E5%AE%83%E5%B0%86%E7%BB%99%E5%BD%93%E5%89%8D%E8%8A%82%E7%82%B9%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AATaint%E5%B0%86%E5%85%B6%E6%A0%87%E8%AE%B0%E4%B8%BA%E4%B8%8D%E5%8F%AF%E7%94%A8%E3%80%82%E5%9C%A8cloud-controller-manager%E7%9A%84%E4%B8%80%E4%B8%AAcontroller%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%E5%90%8E%EF%BC%8CKubelet%E5%B0%86%E5%88%A0%E9%99%A4%E8%BF%99%E4%B8%AATaint%E3%80%82&#34;&gt;node.cloudprovider.kubernetes.io/uninitialized：如果 Kubelet 启动时指定了一个外部的 cloudprovider，它将给当前节点添加一个 Taint 将其标记为不可用。在 cloud-controller-manager 的一个 controller 初始化这个节点后，Kubelet 将删除这个 Taint。&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/vO7kURL.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Deployment 创建后 K8s 默认为 Pod 添加容忍，当 Pod 所在的节点宕机，300 秒后 pod 会漂移，默认容忍时间 300 秒。&lt;/p&gt;
&lt;h4 id=&#34;5节点宕机快速恢复业务应用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5节点宕机快速恢复业务应用&#34;&gt;#&lt;/a&gt; 5. 节点宕机快速恢复业务应用&lt;/h4&gt;
&lt;p&gt;节点不健康，180 秒后再驱逐（默认是 300 秒）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
      tolerations:
        - key: node.kubernetes.io/unreachable
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 180
        - key: node.kubernetes.io/not-ready
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 180
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3142072607.html</guid>
            <title>K8s初始化容器、临时容器</title>
            <link>http://ixuyong.cn/posts/3142072607.html</link>
            <category>Kubernetes</category>
            <pubDate>Sat, 19 Apr 2025 21:07:20 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s初始化容器-临时容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s初始化容器-临时容器&#34;&gt;#&lt;/a&gt; K8s 初始化容器、临时容器&lt;/h3&gt;
&lt;h4 id=&#34;1-初始化容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-初始化容器&#34;&gt;#&lt;/a&gt; 1. 初始化容器&lt;/h4&gt;
&lt;h5 id=&#34;1-1-初始化容器的用途&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-1-初始化容器的用途&#34;&gt;#&lt;/a&gt; 1. 1 初始化容器的用途&lt;/h5&gt;
&lt;p&gt;初始化容器主要是在主应用启动之前，做一些初始化的操作，比如创建文件、修改内核参数、等待依赖程序启动或其他需要在主程序启动之前需要做的工作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码；&lt;/li&gt;
&lt;li&gt;Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低；&lt;/li&gt;
&lt;li&gt;Init 容器可以以 root 身份运行，执行一些高权限命令；&lt;/li&gt;
&lt;li&gt;Init 容器相关操作执行完成以后即退出，不会给业务容器带来安全隐患。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;12-初始化容器和poststart区别&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-初始化容器和poststart区别&#34;&gt;#&lt;/a&gt; 1.2 初始化容器和 PostStart 区别&lt;/h5&gt;
&lt;p&gt;PostStart：依赖主应用的环境，而且并不一定先于 Command 运行。&lt;/p&gt;
&lt;p&gt;InitContainer：不依赖主应用的环境，可以有更高的权限和更多的工具，一定会在主应用启动之前完成&lt;/p&gt;
&lt;h5 id=&#34;13-初始化容器和普通容器的区别&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-初始化容器和普通容器的区别&#34;&gt;#&lt;/a&gt; 1.3 初始化容器和普通容器的区别&lt;/h5&gt;
&lt;p&gt;Init 容器与普通的容器非常像，除了如下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一个 Init 容器运行成功后才会运行下一个 Init 容器；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所有的 Init 容器运行成功后才会运行主容器；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止，但是 Pod 对应的 restartPolicy 值为 Never，Kubernetes 不会重新启动 Pod。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Init 容器不支持 lifecycle、livenessProbe、readinessProbe 和 startupProbe&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;14-初始化容器示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-初始化容器示例&#34;&gt;#&lt;/a&gt; 1.4 初始化容器示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat init.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&amp;quot;sh&amp;quot;,&amp;quot;-c&amp;quot;,&amp;quot;echo hello kubernetes&amp;gt;/usr/share/nginx/html/index.html&amp;quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: share-volume
          mountPath: /usr/share/nginx/html
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: share-volume
          mountPath: /usr/share/nginx/html
      volumes:
      - name: share-volume
        emptyDir: &amp;#123;&amp;#125;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File

[root@k8s-master01 ~]# kubectl create -f init.yaml

[root@k8s-master01 ~]# curl 172.16.32.145
hello kubernetes
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-临时容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-临时容器&#34;&gt;#&lt;/a&gt; 2. 临时容器&lt;/h4&gt;
&lt;h5 id=&#34;21-注入临时容器到pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-注入临时容器到pod&#34;&gt;#&lt;/a&gt; 2.1 注入临时容器到 Pod&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods
NAME                            READY   STATUS    RESTARTS      AGE
nginx-deploy-7dd6cd4b44-ktw5k   1/1     Running   1             16h
nginx-deploy-7dd6cd4b44-mjcgq   1/1     Running   1 (28m ago)   16h
nginx-deploy-7dd6cd4b44-wdm6p   1/1     Running   1 (28m ago)   16h

#1.进入容器发现pod没有ps和netstat命令
[root@k8s-master01 ~]# kubectl exec -it nginx-deploy-7dd6cd4b44-ktw5k  -- bash
root@nginx-deploy-7dd6cd4b44-ktw5k:/# ps aux
root@nginx-deploy-7dd6cd4b44-ktw5k:/# netstat -lntp

#2.注入临时容器至该Pod
[root@k8s-master01 ~]# kubectl debug nginx-deploy-7dd6cd4b44-wdm6p -ti --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;21-注入临时容器到节点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-注入临时容器到节点&#34;&gt;#&lt;/a&gt; 2.1 注入临时容器到节点&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;kubectl debug node k8s-node01 -it --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3833778957.html</guid>
            <title>K8s计划任务Job、Cronjob</title>
            <link>http://ixuyong.cn/posts/3833778957.html</link>
            <category>Kubernetes</category>
            <pubDate>Sat, 19 Apr 2025 21:00:21 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s计划任务job-cronjob&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s计划任务job-cronjob&#34;&gt;#&lt;/a&gt; K8s 计划任务 Job、Cronjob&lt;/h3&gt;
&lt;h4 id=&#34;1-job配置参数详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-job配置参数详解&#34;&gt;#&lt;/a&gt; 1. Job 配置参数详解&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    job-name: echo
  name: echo
  namespace: default
spec:
  #suspend: true # 1.21+
  #ttlSecondsAfterFinished: 100
  backoffLimit: 4
  completions: 1
  parallelism: 1
  template:
    spec:
      containers:
      - name: echo
        image: busybox
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - echo &amp;quot;Hello Job&amp;quot;
      restartPolicy: Never
      
[root@k8s-master01 ~]# kubectl get jobs
NAME   STATUS     COMPLETIONS   DURATION   AGE
echo   Complete   1/1           70s        2m5s

[root@k8s-master01 ~]# kubectl get pods
NAME          READY   STATUS      RESTARTS      AGE
echo-564c8    0/1     Completed   0             2m10s

[root@k8s-master01 ~]# kubectl logs echo-564c8
Hello Job
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;backoffLimit:：如果任务执行失败，失败多少次后不再执行&lt;/li&gt;
&lt;li&gt;completions：有多少个 Pod 执行成功，认为任务是成功的，默认为空和 parallelism 数值一样&lt;/li&gt;
&lt;li&gt;parallelism：并行执行任务的数量，如果 parallelism 数值大于 completions 数值，只会创建 completions 的数量；如果 completions 是 4，并发是 3，第一次会创建 3 个 Pod 执行任务，第二次只会创建一个 Pod 执行任务&lt;/li&gt;
&lt;li&gt;ttlSecondsAfterFinished：Job 在执行结束之后（状态为 completed 或 Failed）自动清理。设置为 0 表示执行结束立即删除，不设置则不会清除，需要开启 TTLAfterFinished 特性&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-cronjob配置参数详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-cronjob配置参数详解&#34;&gt;#&lt;/a&gt; 2. CronJob 配置参数详解&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat cronjob.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  concurrencyPolicy: Allow   #允许同时运行多个任务
  failedJobsHistoryLimit: 10  #保留多少失败的任务
  successfulJobsHistoryLimit: 10  #保留多少已完成的任务
  #suspend: true             #如果true则取消周期性执行任务
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command:
            - sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure 
          
[root@k8s-master01 ~]# kubectl get  cj
NAME    SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   &amp;lt;none&amp;gt;     False     0        6s              81s

[root@k8s-master01 ~]# kubectl get  jobs
NAME             STATUS     COMPLETIONS   DURATION   AGE
hello-29084454   Complete   1/1           4s         72s
hello-29084455   Complete   1/1           5s         12s

[root@k8s-master01 ~]# kubectl get  pods
NAME                   READY   STATUS      RESTARTS   AGE
hello-29084454-hwv7p   0/1     Completed   0          78s
hello-29084455-vf99w   0/1     Completed   0          18s

[root@k8s-master01 ~]# kubectl logs -f hello-29084455-vf99w
Sat Apr 19 12:55:02 UTC 2025
Hello from the Kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;apiVersion: batch/v1beta1   #1.21+ batch/v1&lt;/li&gt;
&lt;li&gt;schedule：调度周期，和 Linux 一致，分别是分时日月周。&lt;/li&gt;
&lt;li&gt;restartPolicy：重启策略，和 Pod 一致。&lt;/li&gt;
&lt;li&gt;concurrencyPolicy：并发调度策略。可选参数如下：
&lt;ul&gt;
&lt;li&gt;Allow：允许同时运行多个任务。&lt;/li&gt;
&lt;li&gt;Forbid：不允许并发运行，如果之前的任务尚未完成，新的任务不会被创建。&lt;/li&gt;
&lt;li&gt;Replace：如果之前的任务尚未完成，新的任务会替换的之前的任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;suspend：如果设置为 true，则暂停后续的任务，默认为 false。&lt;/li&gt;
&lt;li&gt;successfulJobsHistoryLimit：保留多少已完成的任务，按需配置。&lt;/li&gt;
&lt;li&gt;failedJobsHistoryLimit：保留多少失败的任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/169153047.html</guid>
            <title>K8s持久化存储</title>
            <link>http://ixuyong.cn/posts/169153047.html</link>
            <category>Kubernetes</category>
            <pubDate>Fri, 18 Apr 2025 22:25:17 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s持久化存储&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s持久化存储&#34;&gt;#&lt;/a&gt; K8s 持久化存储&lt;/h3&gt;
&lt;h4 id=&#34;1-volume&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-volume&#34;&gt;#&lt;/a&gt; 1. Volume&lt;/h4&gt;
&lt;p&gt;Container（容器）中的磁盘文件是短暂的，当容器崩溃时，kubelet 会重新启动容器，Container 会以最干净的状态启动，最初的文件将丢失。另外，当一个 Pod 运行多个 Container 时，各个容器可能需要共享一些文件。Kubernetes Volume 可以解决这两个问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一些需要持久化数据的程序才会用到 Volumes，或者一些需要共享数据的容器需要 volumes。&lt;/li&gt;
&lt;li&gt;日志收集的需求需要在应用程序的容器里面加一个 sidecar，这个容器是一个收集日志的容器，比如 filebeat，它通过 volumes 共享应用程序的日志文件目录。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-emptydir实现数据共享&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-emptydir实现数据共享&#34;&gt;#&lt;/a&gt; 1.1 EmptyDir 实现数据共享&lt;/h5&gt;
&lt;p&gt;和上述 volume 不同的是，如果删除 Pod，emptyDir 卷中的数据也将被删除，一般 emptyDir 卷用于 Pod 中的不同 Container 共享数据。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
        - name: share-volume
          emptyDir: &amp;#123;&amp;#125;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
            - name: share-volume
              mountPath: /opt
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
            - name: share-volume
              mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-volumes-hostpath挂载宿主机路径&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-volumes-hostpath挂载宿主机路径&#34;&gt;#&lt;/a&gt; 1.2 Volumes HostPath 挂载宿主机路径&lt;/h5&gt;
&lt;p&gt;hostPath 卷可将节点上的文件或目录挂载到 Pod 上，用于 Pod 自定义日志输出或访问 Docker 内部的容器等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: share-volume
        emptyDir: &amp;#123;&amp;#125;
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: share-volume
            mountPath: /opt
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
          - name: share-volume
            mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPath 卷常用的 type（类型）如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;type 为空字符串：默认选项，意味着挂载 hostPath 卷之前不会执行任何检查。&lt;/li&gt;
&lt;li&gt;DirectoryOrCreate：如果给定的 path 不存在任何东西，那么将根据需要创建一个权限为 0755 的空目录，和 Kubelet 具有相同的组和权限。&lt;/li&gt;
&lt;li&gt;Directory：目录必须存在于给定的路径下。&lt;/li&gt;
&lt;li&gt;FileOrCreate：如果给定的路径不存储任何内容，则会根据需要创建一个空文件，权限设置为 0644，和 Kubelet 具有相同的组和所有权。&lt;/li&gt;
&lt;li&gt;File：文件，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;Socket：UNIX 套接字，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;CharDevice：字符设备，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;BlockDevice：块设备，必须存在于给定路径中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;13-挂载nfs至容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-挂载nfs至容器&#34;&gt;#&lt;/a&gt; 1.3 挂载 NFS 至容器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.安装nfs
# yum install nfs-utils -y       
# mkdir /data/nfs -p
# vim /etc/exports 
/data 192.168.1.0/24(rw,no_root_squash)
# exportfs -arv   
# systemctl start nfs-server &amp;amp;&amp;amp; systemctl enable nfs-server &amp;amp;&amp;amp; systemctl status nfs-server 

#2.测试客户端挂载
# showmount -e 192.168.1.75
# mount -t nfs 192.168.1.75:/data/nfs /mnt

#3.Deploy挂载NFS
[root@k8s-master01 ~]# cat nginx-deploy-nfs.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
      annotations:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: nfs-volume
        nfs:
          server: 192.168.1.75
          path: /data/nfs
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: nfs-volume
            mountPath: /usr/share/nginx/html
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-pv-pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-pv-pvc&#34;&gt;#&lt;/a&gt; 2. PV、PVC&lt;/h4&gt;
&lt;p&gt;PersistentVolume：简称 PV，是由 Kubernetes 管理员设置的存储，可以配置 Ceph、NFS、GlusterFS 等常用存储配置，相对于 Volume 配置，提供了更多的功能，比如生命周期的管理、大小的限制。PV 分为静态和动态。&lt;/p&gt;
&lt;p&gt;PersistentVolumeClaim：简称 PVC，是对存储 PV 的请求，表示需要什么类型的 PV，需要存储的技术人员只需要配置 PVC 即可使用存储，或者 Volume 配置 PVC 的名称即可。&lt;/p&gt;
&lt;h5 id=&#34;21-pv回收策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-pv回收策略&#34;&gt;#&lt;/a&gt; 2.1 PV 回收策略&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Retain：保留，该策略允许手动回收资源，当删除 PVC 时，PV 仍然存在，PV 被视为已释放，管理员可以手动回收卷。&lt;/li&gt;
&lt;li&gt;Recycle：回收，如果 Volume 插件支持，Recycle 策略会对卷执行 rm -rf 清理该 PV，并使其可用于下一个新的 PVC，但是本策略将来会被弃用，目前只有 NFS 和 HostPath 支持该策略。&lt;/li&gt;
&lt;li&gt;Delete：删除，如果 Volume 插件支持，删除 PVC 时会同时删除 PV，动态卷默认为 Delete，目前支持 Delete 的存储后端包括 AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder 等。&lt;/li&gt;
&lt;li&gt;可以通过 persistentVolumeReclaimPolicy: Recycle 字段配置&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;22-pv访问策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-pv访问策略&#34;&gt;#&lt;/a&gt; 2.2 PV 访问策略&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce：可以被单节点以读写模式挂载，命令行中可以被缩写为 RWO。&lt;/li&gt;
&lt;li&gt;ReadOnlyMany：可以被多个节点以只读模式挂载，命令行中可以被缩写为 ROX。&lt;/li&gt;
&lt;li&gt;ReadWriteMany：可以被多个节点以读写模式挂载，命令行中可以被缩写为 RWX。&lt;/li&gt;
&lt;li&gt;ReadWriteOncePod ：只允许被单个 Pod 访问，需要 K8s 1.22 + 以上版本，并且是 CSI 创建的 PV 才可使用，缩写为 RWOP&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOnce&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadOnlyMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOncePod&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HostPath&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;- (works when Pods are collocated)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;23-存储分类&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-存储分类&#34;&gt;#&lt;/a&gt; 2.3 存储分类&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;文件存储：一些数据可能需要被多个节点使用，比如用户的头像、用户上传的文件等，实现方式：NFS、NAS、FTP、CephFS 等。&lt;/li&gt;
&lt;li&gt;块存储：一些数据只能被一个节点使用，或者是需要将一块裸盘整个挂载使用，比如数据库、Redis 等，实现方式：Ceph、GlusterFS、公有云。&lt;/li&gt;
&lt;li&gt;对象存储：由程序代码直接实现的一种存储方式，云原生应用无状态化常用的实现方式，实现方式：一般是符合 S3 协议的云存储，比如 AWS 的 S3 存储、Minio、七牛云等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;24-pv配置示例nfs&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-pv配置示例nfs&#34;&gt;#&lt;/a&gt; 2.4 PV 配置示例 NFS&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv1
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-slow
  nfs:
    path: /data/pv1
    server: 192.168.1.75
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;capacity：容量配置&lt;/p&gt;
&lt;p&gt;volumeMode：卷的模式，目前支持 Filesystem（文件系统） 和 Block（块），其中 Block 类型需要后端存储支持，默认为文件系统&lt;/p&gt;
&lt;p&gt;accessModes：该 PV 的访问模式&lt;/p&gt;
&lt;p&gt;storageClassName：PV 的类，一个特定类型的 PV 只能绑定到特定类别的 PVC；&lt;/p&gt;
&lt;p&gt;persistentVolumeReclaimPolicy：回收策略&lt;/p&gt;
&lt;p&gt;mountOptions：非必须，新版本中已弃用&lt;/p&gt;
&lt;p&gt;nfs：NFS 服务配置，包括以下两个选项&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;path：NFS 上的共享目录&lt;/li&gt;
&lt;li&gt;server：NFS 的 IP 地址&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;25-pv配置示例hostpath&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-pv配置示例hostpath&#34;&gt;#&lt;/a&gt; 2.5 PV 配置示例 HostPath&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: hostpath
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: hostpath
  hostPath:
    path: &amp;quot;/mnt/data&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPath：hostPath 服务配置&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;path：宿主机路径&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;26-pv的状态&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#26-pv的状态&#34;&gt;#&lt;/a&gt; 2.6 PV 的状态&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Available：可用，没有被 PVC 绑定的空闲资源。&lt;/li&gt;
&lt;li&gt;Bound：已绑定，已经被 PVC 绑定。&lt;/li&gt;
&lt;li&gt;Released：已释放，PVC 被删除，但是资源还未被重新使用。&lt;/li&gt;
&lt;li&gt;Failed：失败，自动回收失败。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;27-pvc绑定pv&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-pvc绑定pv&#34;&gt;#&lt;/a&gt; 2.7 PVC 绑定 PV&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: nfs-slow
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi      
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;PVC 的空间申请大小≤PV 的大小&lt;/li&gt;
&lt;li&gt;PVC 的 StorageClassName 和 PV 的一致&lt;/li&gt;
&lt;li&gt;PVC 的 accessModes 和 PV 的一致&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;28-depoyment挂载pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#28-depoyment挂载pvc&#34;&gt;#&lt;/a&gt; 2.8 Depoyment 挂载 PVC&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      volumes:
      - name: nfs-pvc-storage  #volume名称
        persistentVolumeClaim:
          claimName: nfs-pvc   #PVC名称
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
         - name: nfs-pvc-storage
          mountPath: /usr/share/nginx/html
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;挂载 PVC 的 Pod 一直处于 Pending：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PVC 没有创建成功或 PVC 不存在&lt;/li&gt;
&lt;li&gt;PVC 和 Pod 不在同一个 Namespace&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3992668367.html</guid>
            <title>K8s配置管理Configmap</title>
            <link>http://ixuyong.cn/posts/3992668367.html</link>
            <category>Kubernetes</category>
            <pubDate>Mon, 14 Apr 2025 21:47:47 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s配置管理configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s配置管理configmap&#34;&gt;#&lt;/a&gt; K8s 配置管理 Configmap&lt;/h3&gt;
&lt;h4 id=&#34;1-configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-configmap&#34;&gt;#&lt;/a&gt; 1. Configmap&lt;/h4&gt;
&lt;h5 id=&#34;1-1-基于from-env-file创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-1-基于from-env-file创建configmap&#34;&gt;#&lt;/a&gt; 1. 1 基于 from-env-file 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat cm_env.conf 
podname=nf-flms-system
podip=192.168.1.100
env=prod
nacosaddr=nacos.svc.cluster.local

#kubectl create cm cmenv --from-env-file=./cm_env.conf 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-基于from-literal创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-基于from-literal创建configmap&#34;&gt;#&lt;/a&gt; 1.2 基于 from-literal 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm cmliteral --from-literal=level=INFO --from-literal=passwd=Superman*2023
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-基于from-file创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-基于from-file创建configmap&#34;&gt;#&lt;/a&gt; 1.3 基于 from-file 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat s.hmallleasing.com.conf 
server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    client_max_body_size 1G; 
    location / &amp;#123;
        proxy_pass http://192.168.1.134;
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        proxy_connect_timeout 30;
        proxy_send_timeout 60;
        proxy_read_timeout 60;
        
        proxy_buffering on;
        proxy_buffer_size 32k;
        proxy_buffers 4 128k;
        proxy_temp_file_write_size 10240k;		
        proxy_max_temp_file_size 10240k;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;

# kubectl create cm nginxconfig --from-file=./s.hmallleasing.com.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-deployment挂载configmap示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-deployment挂载configmap示例&#34;&gt;#&lt;/a&gt; 1.4 Deployment 挂载 configmap 示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-重命名挂载的configmaq-key的名称&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-重命名挂载的configmaq-key的名称&#34;&gt;#&lt;/a&gt; 1.5 重命名挂载的 configmaq key 的名称&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
          items:                # 重命名挂载的configmaq key的名称为nginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
 
#查看挂载的configmaq key的名称重命名为nginx.conf
[root@k8s-master01 cm]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-deploy-bc476bc56-flln4   1/1     Running   0          10h
nginx-deploy-bc476bc56-jhsh6   1/1     Running   0          10h
nginx-deploy-bc476bc56-splv9   1/1     Running   0          10h
[root@k8s-master01 cm]# kubectl exec -it nginx-deploy-bc476bc56-flln4 -- bash
root@nginx-deploy-bc476bc56-flln4:/# ls /etc/nginx/conf.d/
nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;16-修改挂载的configmaq-权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-修改挂载的configmaq-权限&#34;&gt;#&lt;/a&gt; 1.6 修改挂载的 configmaq 权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
          items:                # 重命名挂载的configmaq key的名称为nginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
            mode: 0644        # 配置挂载权限，针对单个key生效
          defaultMode: 0666   # 配置挂载权限，针对整个key生效
    
#查看挂载权限
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/nginx.conf 
lrwxrwxrwx 1 root root 17 Apr 16 13:37 /etc/nginx/conf.d/nginx.conf -&amp;gt; ..data/nginx.conf
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/..data/nginx.conf 
-rw-rw-rw- 1 root root 722 Apr 16 13:37 /etc/nginx/conf.d/..data/nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;17-subpath解决挂载覆盖问题&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#17-subpath解决挂载覆盖问题&#34;&gt;#&lt;/a&gt; 1.7 subpath 解决挂载覆盖问题&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.创建configmap
[root@k8s-master01 cm]# cat nginx.conf 

user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events &amp;#123;
    worker_connections  512;
&amp;#125;


http &amp;#123;
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  &#39;$remote_addr - $remote_user [$time_local] &amp;quot;$request&amp;quot; &#39;
                      &#39;$status $body_bytes_sent &amp;quot;$http_referer&amp;quot; &#39;
                      &#39;&amp;quot;$http_user_agent&amp;quot; &amp;quot;$http_x_forwarded_for&amp;quot;&#39;;

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
&amp;#125;

[root@k8s-master01 cm]# kubectl create cm nginx-config --from-file=./nginx.conf

#subpath解决挂载覆盖问题
[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # ①批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # ②自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # ③挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:
        - name: config
          mountPath: &amp;quot;/etc/nginx/nginx.conf&amp;quot;   #只挂在nginx.conf一个文件,不覆盖目录
          subPath: nginx.conf      
      volumes:
      - name: config
        configMap:
          name: nginx-config      # 提供你想要挂载的ConfigMap的名字
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-secret&#34;&gt;#&lt;/a&gt; 2. Secret&lt;/h4&gt;
&lt;h5 id=&#34;21-secret拉取私有仓库镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-secret拉取私有仓库镜像&#34;&gt;#&lt;/a&gt; 2.1 Secret 拉取私有仓库镜像&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret docker-registry harboradmin \
--docker-server=s.hmallleasing.com \
--docker-username=admin \
--docker-password=Superman*2023 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-创建ssl-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-创建ssl-secret&#34;&gt;#&lt;/a&gt; 2.2 创建 ssl Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret tls dev.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-基于命令创建generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-基于命令创建generic-secret&#34;&gt;#&lt;/a&gt; 2.3 基于命令创建 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.通过from-env-file创建
# cat db.conf 
username=xuyong
passwd=Superman*2023

# kubectl create secret generic dbconf --from-env-file=./db.conf

#2.通过from-literal创建
kubectl create secret generic db-user-pass \
    --from-literal=username=admin \
    --from-literal=password=&#39;S!B\*d$zDsb=&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-secret加密-解密&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-secret加密-解密&#34;&gt;#&lt;/a&gt; 2.4 Secret 加密、解密&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;1.加密
# echo -n &amp;quot;Superman*2023&amp;quot; | base64
U3VwZXJtYW4qMjAyMw==

2.解密
# echo &amp;quot;U3VwZXJtYW4qMjAyMw==&amp;quot; | base64 --decode
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-基于文件创建非加密generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-基于文件创建非加密generic-secret&#34;&gt;#&lt;/a&gt; 2.5 基于文件创建非加密 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get secret dbconf -oyaml
apiVersion: v1
data:
  passwd: U3VwZXJtYW4qMjAyMw==
  username: eHV5b25n
kind: Secret
metadata:
  name: dbconf
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-6-基于yaml创建加密generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-6-基于yaml创建加密generic-secret&#34;&gt;#&lt;/a&gt; 2. 6 基于 yaml 创建加密 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat mysql-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
  namespace: dev
stringData:
  MYSQL_ROOT_PASSWORD: Superman*2023
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;27-deployment挂载secret示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-deployment挂载secret示例&#34;&gt;#&lt;/a&gt; 2.7 Deployment 挂载 Secret 示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - name: MYSQL_ROOT_PASSWORD  
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_ROOT_PASSWORD
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-configmapsecret热更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-configmapsecret热更新&#34;&gt;#&lt;/a&gt; 3. ConfigMap&amp;amp;Secret 热更新&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm nginxconfig --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f -
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/858611107.html</guid>
            <title>K8s服务发布Service</title>
            <link>http://ixuyong.cn/posts/858611107.html</link>
            <category>Kubernetes</category>
            <pubDate>Mon, 14 Apr 2025 19:25:51 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s服务发布service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s服务发布service&#34;&gt;#&lt;/a&gt; K8s 服务发布 Service&lt;/h3&gt;
&lt;h4 id=&#34;1-service类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-service类型&#34;&gt;#&lt;/a&gt; 1. Service 类型&lt;/h4&gt;
&lt;p&gt;Kubernetes Service Type（服务类型）主要包括以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP：在集群内部使用，默认值，只能从集群中访问。&lt;/li&gt;
&lt;li&gt;NodePort：在所有安装了 Kube-Proxy 的节点上打开一个端口，此端口可以代理至后端 Pod，可以通过 NodePort 从集群外部访问集群内的服务，格式为 NodeIP:NodePort。&lt;/li&gt;
&lt;li&gt;LoadBalancer：使用云提供商的负载均衡器公开服务，成本较高。&lt;/li&gt;
&lt;li&gt;ExternalName：通过返回定义的 CNAME 别名，没有设置任何类型的代理，需要 1.7 或更高版本 kube-dns 支持。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-nodeport类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-nodeport类型&#34;&gt;#&lt;/a&gt; 1.1 NodePort 类型&lt;/h5&gt;
&lt;p&gt;如果将 Service 的 type 字段设置为 NodePort，则 Kubernetes 将从 --service-node-port-range 参数指定的范围（默认为 30000-32767）中自动分配端口，也可以手动指定 NodePort，创建该 Service 后，集群每个节点都将暴露一个端口，通过某个宿主机的 IP + 端口即可访问到后端的应用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-clusterip类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-clusterip类型&#34;&gt;#&lt;/a&gt; 1.2 ClusterIP 类型&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-使用service代理k8s外部服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-使用service代理k8s外部服务&#34;&gt;#&lt;/a&gt; 1.3 使用 Service 代理 K8s 外部服务&lt;/h5&gt;
&lt;p&gt;使用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;希望在生产环境中使用某个固定的名称而非 IP 地址访问外部的中间件服务；&lt;/li&gt;
&lt;li&gt;希望 Service 指向另一个 Namespace 中或其他集群中的服务；&lt;/li&gt;
&lt;li&gt;正在将工作负载转移到 Kubernetes 集群，但是一部分服务仍运行在 Kubernetes 集群之外的 backend。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
spec:
  clusterIP: None
  ports:
  - name: mysql
    port: 3306 
    protocol: TCP
    targetPort: 3306
  type: ClusterIP
---
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
subsets:
- addresses:
  - ip: 192.168.40.150
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-externalname-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-externalname-service&#34;&gt;#&lt;/a&gt; 1.4 ExternalName Service&lt;/h5&gt;
&lt;p&gt;ExternalName Service 是 Service 的特例，它没有 Selector，也没有定义任何端口和 Endpoint，它通过返回该外部服务的别名来提供服务。&lt;/p&gt;
&lt;p&gt;比如可以定义一个 Service，后端设置为一个外部域名，这样通过 Service 的名称即可访问到该域名。使用 nslookup 解析以下文件定义的 Service，集群的 DNS &lt;a href=&#34;http://xn--my-uu2cmg2cx7mswf9rko5lsx1a5n3h.database.example.com&#34;&gt;服务将返回一个值为 my.database.example.com&lt;/a&gt; 的 CNAME 记录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-多端口-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-多端口-service&#34;&gt;#&lt;/a&gt; 1.5 多端口 Service&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
    - port: 443
      targetPort: 443
      protocol: TCP
      name: https
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/108692210.html</guid>
            <title>K8s资源调度deployment、statefulset、daemonset</title>
            <link>http://ixuyong.cn/posts/108692210.html</link>
            <category>Kubernetes</category>
            <pubDate>Mon, 14 Apr 2025 19:25:00 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s资源调度deployment-statefulset-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s资源调度deployment-statefulset-daemonset&#34;&gt;#&lt;/a&gt; K8s 资源调度 deployment、statefulset、daemonset&lt;/h3&gt;
&lt;h4 id=&#34;1-无状态应用管理-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-无状态应用管理-deployment&#34;&gt;#&lt;/a&gt; 1. 无状态应用管理 Deployment&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.0
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例解析：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;nginx-deploy：Deployment 的名称；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;replicas： 创建 Pod 的副本数；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;selector：定义 Deployment 如何找到要管理的 Pod，与 template 的 label（标签）对应，apiVersion 为 apps/v1 必须指定该字段；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;template 字段包含以下字段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;app: nginx-deploy 使用 label（标签）标记 Pod；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;spec：表示 Pod 运行一个名字为 nginx 的容器；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image：运行此 Pod 使用的镜像；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Port：容器用于发送和接收流量的端口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;11-更新-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-更新-deployment&#34;&gt;#&lt;/a&gt; 1.1 更新 Deployment&lt;/h5&gt;
&lt;p&gt;假如更新 Nginx Pod 的 image 使用 nginx:latest，并使用 --record 记录当前更改的参数，后期回滚时可以查看到对应的信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新过程为新旧交替更新，首先新建一个 Pod，当 Pod 状态为 Running 时，删除一个旧的 Pod，同时再创建一个新的 Pod。当触发一个更新后，会有新的 ReplicaSet 产生，旧的 ReplicaSet 会被保存，查看此时 ReplicaSet，可以从 AGE 或 READY 看出来新旧 ReplicaSet：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get rs
NAME                      DESIRED   CURRENT   READY   AGE
nginx-deploy-65bfb77869   0         0         0       50s
nginx-deploy-85b94dddb4   3         3         3       8s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过 describe 查看 Deployment 的详细信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl describe deploy nginx-deploy
Name:                   nginx-deploy
Namespace:              default
CreationTimestamp:      Mon, 14 Apr 2025 11:28:03 +0800
Labels:                 app=nginx-deploy
Annotations:            app: nginx-deploy
                        deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
Selector:               app=nginx-deploy
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx-deploy
  Containers:
   nginx-deploy:
    Image:         nginx:latest
    Port:          &amp;lt;none&amp;gt;
    Host Port:     &amp;lt;none&amp;gt;
    Environment:   &amp;lt;none&amp;gt;
    Mounts:        &amp;lt;none&amp;gt;
  Volumes:         &amp;lt;none&amp;gt;
  Node-Selectors:  &amp;lt;none&amp;gt;
  Tolerations:     &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  nginx-deploy-65bfb77869 (0/0 replicas created)
NewReplicaSet:   nginx-deploy-85b94dddb4 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  71s   deployment-controller  Scaled up replica set nginx-deploy-65bfb77869 from 0 to 3
  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 0 to 1
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 3 to 2
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 1 to 2
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 2 to 1
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 2 to 3
  Normal  ScalingReplicaSet  26s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 1 to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 describe 中可以看出，第一次创建时，它创建了一个名为 nginx-deploy-65bfb77869 的 ReplicaSet，并直接将其扩展为 3 个副本。更新部署时，它创建了一个新的 ReplicaSet，命名为 nginx-deploy-85b94dddb4，并将其副本数扩展为 1，然后将旧的 ReplicaSet 缩小为 2，这样至少可以有 2 个 Pod 可用，最多创建了 4 个 Pod。以此类推，使用相同的滚动更新策略向上和向下扩展新旧 ReplicaSet，最终新的 ReplicaSet 可以拥有 3 个副本，并将旧的 ReplicaSet 缩小为 0。&lt;/p&gt;
&lt;h5 id=&#34;12-回滚-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-回滚-deployment&#34;&gt;#&lt;/a&gt; 1.2 回滚 Deployment&lt;/h5&gt;
&lt;p&gt;当更新了版本不稳定或配置不合理时，可以对其进行回滚操作，假设我们又进行了几次更新（此处以更新镜像版本触发更新，更改配置效果类似）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record
# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用 kubectl rollout history 查看更新历史：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
3         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 Deployment 某次更新的详细信息，使用 --revision 指定某次更新版本号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy --revision=4
deployment.apps/nginx-deploy with revision #4
Pod Template:
  Labels:	app=nginx-deploy
	pod-template-hash=65b576b795
  Annotations:	kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
  Containers:
   nginx-deploy:
    Image:	nginx:1.21.2
    Port:	&amp;lt;none&amp;gt;
    Host Port:	&amp;lt;none&amp;gt;
    Environment:	&amp;lt;none&amp;gt;
    Mounts:	&amp;lt;none&amp;gt;
  Volumes:	&amp;lt;none&amp;gt;
  Node-Selectors:	&amp;lt;none&amp;gt;
  Tolerations:	&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果只需要回滚到上一个稳定版本，使用 kubectl rollout undo 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再次查看更新历史，发现 REVISION3 回到了 nginx:1.21.1：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
5         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果要回滚到指定版本，使用 --to-revision 参数：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy --to-revision=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-扩容-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-扩容-deployment&#34;&gt;#&lt;/a&gt; 1.3 扩容 Deployment&lt;/h5&gt;
&lt;p&gt;当公司访问量变大，或者有预期内的活动时，三个 Pod 可能已无法支撑业务时，可以提前对其进行扩展。&lt;/p&gt;
&lt;p&gt;使用 kubectl scale 动态调整 Pod 的副本数，比如增加 Pod 为 5 个：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale deployment nginx-deploy --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 Pod，此时 Pod 已经变成了 5 个：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
nginx-deploy-85b94dddb4-2qrh6   1/1     Running   0          2m9s
nginx-deploy-85b94dddb4-gvkqj   1/1     Running   0          2m10s
nginx-deploy-85b94dddb4-mdfjs   1/1     Running   0          22s
nginx-deploy-85b94dddb4-rhgpr   1/1     Running   0          2m8s
nginx-deploy-85b94dddb4-vwjhl   1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-暂停和恢复-deployment-更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-暂停和恢复-deployment-更新&#34;&gt;#&lt;/a&gt; 1.4 暂停和恢复 Deployment 更新&lt;/h5&gt;
&lt;p&gt;上述演示的均为更改某一处的配置，更改后立即触发更新，大多数情况下可能需要针对一个资源文件更改多处地方，而并不需要多次触发更新，此时可以使用 Deployment 暂停功能，临时禁用更新操作，对 Deployment 进行多次修改后在进行更新。&lt;/p&gt;
&lt;p&gt;使用 kubectl rollout pause 命令即可暂停 Deployment 更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout pause deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后对 Deployment 进行相关更新操作，比如先更新镜像，然后对其资源进行限制（如果使用的是 kubectl edit 命令，可以直接进行多次修改，无需暂停更新，kubectlset 命令一般会集成在 CICD 流水线中）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.3
# kubectl set resources deployment nginx-deploy -c=nginx-deploy --limits=cpu=200m,memory=512Mi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过 rollout history 可以看到没有新的更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#  kubectl rollout history deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行完最后一处配置更改后，使用 kubectl rollout resume 恢复 Deployment 更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout resume deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以查看到恢复更新的 Deployment 创建了一个新的 RS（ReplicaSet 缩写）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get rs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以查看 Deployment 的 image（镜像）已经变为 nginx:1.21.3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-更新-deployment-的注意事项&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-更新-deployment-的注意事项&#34;&gt;#&lt;/a&gt; 1.5 更新 Deployment 的注意事项&lt;/h5&gt;
&lt;p&gt;在默认情况下，revision 保留 10 个旧的 ReplicaSet，其余的将在后台进行垃圾回收，可以在.spec.revisionHistoryLimit 设置保留 ReplicaSet 的个数。当设置为 0 时，不保留历史记录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  namespace: default
  labels:
    app: nginx-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.3
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spec.strategy.type==Recreate，表示重建，先删掉旧的 Pod 再创建新的 Pod；&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: Recreate
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.type==RollingUpdate，表示滚动更新，可以指定 maxUnavailable 和 maxSurge 来控制滚动更新过程；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxUnavailable，指定在回滚更新时最大不可用的 Pod 数量，可选字段，默认为 25%，可以设置为数字或百分比，如果 maxSurge 为 0，则该值不能为 0；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxSurge 可以超过期望值的最大 Pod 数，可选字段，默认为 25%，可以设置成数字或百分比，如果 maxUnavailable 为 0，则该值不能为 0。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-有状态应用管理-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-有状态应用管理-statefulset&#34;&gt;#&lt;/a&gt; 2. 有状态应用管理 StatefulSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: &#39;1&#39;
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 128Mi
      restartPolicy: Always
  serviceName: web
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;kind: Service 定义了一个名字为 web 的 Headless Service，创建的 Service 格式为 nginx-0.web.default.svc.cluster.local，其他的类似，因为没有指定 Namespace（命名空间），所以默认部署在 default；&lt;/li&gt;
&lt;li&gt;kind: StatefulSet 定义了一个名字为 nginx 的 StatefulSet，replicas 表示部署 Pod 的副本数，本实例为 3。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;21-创建-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-创建-statefulset&#34;&gt;#&lt;/a&gt; 2.1 创建 StatefulSet&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
nginx-0   1/1     Running   0          8m51s
nginx-1   1/1     Running   0          8m50s
nginx-2   1/1     Running   0          8m48s
[root@k8s-master01 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   6d1h
web          ClusterIP   None         &amp;lt;none&amp;gt;        80/TCP    9m28s
[root@k8s-master01 ~]# kubectl get sts
NAME    READY   AGE
nginx   3/3     8m58s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-statefulset创建pod流程&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-statefulset创建pod流程&#34;&gt;#&lt;/a&gt; 2.2 StatefulSet 创建 Pod 流程&lt;/h5&gt;
&lt;p&gt;StatefulSet 管理的 Pod 部署和扩展规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于具有 N 个副本的 StatefulSet，将按顺序从 0 到 N-1 开始创建 Pod；&lt;/li&gt;
&lt;li&gt;当删除 Pod 时，将按照 N-1 到 0 的反顺序终止；&lt;/li&gt;
&lt;li&gt;在缩放 Pod 之前，必须保证当前的 Pod 是 Running（运行中）或者 Ready（就绪）；&lt;/li&gt;
&lt;li&gt;在终止 Pod 之前，它所有的继任者必须是完全关闭状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet 的 pod.Spec.TerminationGracePeriodSeconds（终止 Pod 的等待时间）不应该指定为 0，设置为 0 对 StatefulSet 的 Pod 是极其不安全的做法，优雅地删除 StatefulSet 的 Pod 是非常有必要的，而且是安全的，因为它可以确保在 Kubelet 从 APIServer 删除之前，让 Pod 正常关闭。&lt;/p&gt;
&lt;p&gt;当创建上面的 Nginx 实例时，Pod 将按 nginx-0、nginx-1、nginx-2 的顺序部署 3 个 Pod。在 nginx-0 处于 Running 或者 Ready 之前，nginx-1 不会被部署，相同的，nginx-2 在 web-1 未处于 Running 和 Ready 之前也不会被部署。如果在 nginx-1 处于 Running 和 Ready 状态时，nginx-0 变成 Failed 失败）状态，那么 nginx-2 将不会被启动，直到 nginx-0 恢复为 Running 和 Ready 状态。&lt;/p&gt;
&lt;p&gt;如果用户将 StatefulSet 的 replicas 设置为 1，那么 nginx-2 将首先被终止，在完全关闭并删除 nginx-2 之前，不会删除 nginx-1。如果 nginx-2 终止并且完全关闭后，nginx-0 突然失败，那么在 nginx-0 未恢复成 Running 或者 Ready 时，nginx-1 不会被删除。&lt;/p&gt;
&lt;h5 id=&#34;23-tatefulset-扩容和缩容&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-tatefulset-扩容和缩容&#34;&gt;#&lt;/a&gt; 2.3 tatefulSet 扩容和缩容&lt;/h5&gt;
&lt;p&gt;和 Deployment 类似，可以通过更新 replicas 字段扩容 / 缩容 StatefulSet，也可以使用 kubectlscale、kubectl edit 和 kubectl patch 来扩容 / 缩容一个 StatefulSet。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale sts nginx --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-statefulset-更新策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-statefulset-更新策略&#34;&gt;#&lt;/a&gt; 2.4 StatefulSet 更新策略&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;On Delete 策略&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OnDelete 更新策略实现了传统（1.7 版本之前）的行为，它也是默认的更新策略。当我们选择这个更新策略并修改 StatefulSet 的.spec.template 字段时，StatefulSet 控制器不会自动更新 Pod，必须手动删除 Pod 才能使控制器创建新的 Pod。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: OnDelete
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;RollingUpdate 策略&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RollingUpdate（滚动更新）更新策略会自动更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序进行滚动更新。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-分段更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-分段更新&#34;&gt;#&lt;/a&gt; 2.5 分段更新&lt;/h5&gt;
&lt;p&gt;将分区改为 2，此时会自动更新 nginx-2、nginx-3、nginx-4（因为之前更改了更新策略），但是不会更新 nginx-0 和 nginx-1：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 sts 镜像为 nginx:1.21.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image sts nginx nginx=nginx:1.21.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;按照上述方式，可以实现分阶段更新，类似于灰度 / 金丝雀发布。查看最终的结果如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;26-statefulset-挂载动态存储&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#26-statefulset-挂载动态存储&#34;&gt;#&lt;/a&gt; 2.6 StatefulSet 挂载动态存储&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx 
  serviceName: &amp;quot;nginx&amp;quot;
  replicas: 3 1
  template:
    metadata:
      labels:
        app: nginx 
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: &amp;quot;rook-ceph-block&amp;quot;
      resources:
        requests:
          storage: 10Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3守护进程集-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3守护进程集-daemonset&#34;&gt;#&lt;/a&gt; 3. 守护进程集 DaemonSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
        - name: nginx-ds
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时会在每个节点创建一个 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-ds-47dxc   1/1     Running   0          56s   172.16.85.213    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-4m89f   1/1     Running   0          56s   172.16.32.143    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-mtpc2   1/1     Running   0          56s   172.16.195.12    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-t5rxc   1/1     Running   0          56s   172.16.122.142   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-x86kc   1/1     Running   0          56s   172.16.58.222    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;指定节点部署 Pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;      nodeSelector:
        ingress: &#39;true&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新和回滚 DaemonSet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image ds nginx-ds nginx-ds=1.21.0 --record=true
# kubectl rollout undo daemonset &amp;lt;daemonset-name&amp;gt; --to-revision=&amp;lt;revision&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DaemonSet 的更新和回滚与 Deployment 类似，此处不再演示。&lt;/p&gt;
&lt;h4 id=&#34;4-hpa&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-hpa&#34;&gt;#&lt;/a&gt; 4. HPA&lt;/h4&gt;
&lt;p&gt;创建 deployment、service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-hpa-svc
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx-hpa
  type: ClusterIP

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-hpa
  labels:
    app: nginx-hpa
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-hpa
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-hpa
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-hpa
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建 HPA&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl autoscale deployment nginx-hpa --cpu-percent=10 --min=1 --max=10
# kubectl get hpa
NAME        REFERENCE              TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
nginx-hpa   Deployment/nginx-hpa   cpu: 0%/10%   1         10        1          16s

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试自动扩缩容&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;while true; do wget -q -O- http://10.96.18.221 &amp;gt; /dev/null; done
[root@k8s-master01 ~]# kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
nginx-hpa-d8bcbdf7d-4mkxp   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-974q5   1/1     Running   0          6m36s
nginx-hpa-d8bcbdf7d-g6p2h   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-lvvsq   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tgqmr   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tzfbs   1/1     Running   0          21s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/1771242682.html</guid>
            <title>K8s零宕机服务发布-探针</title>
            <link>http://ixuyong.cn/posts/1771242682.html</link>
            <category>Kubernetes</category>
            <pubDate>Mon, 14 Apr 2025 19:23:48 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;k8s零宕机服务发布-探针&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s零宕机服务发布-探针&#34;&gt;#&lt;/a&gt; K8s 零宕机服务发布 - 探针&lt;/h3&gt;
&lt;h4 id=&#34;1-pod状态及-pod-故障排查命令&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-pod状态及-pod-故障排查命令&#34;&gt;#&lt;/a&gt; 1. Pod 状态及 Pod 故障排查命令&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;状态&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pending（挂起）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已被 Kubernetes 系统接收，但仍有一个或多个容器未被创建，可以通过 kubectl describe 查看处于 Pending 状态的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Running（运行中）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经被绑定到一个节点上，并且所有的容器都已经被创建，而且至少有一个是运行状态，或者是正在启动或者重启，可以通过 kubectl logs 查看 Pod 的日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Succeeded（成功）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;所有容器执行成功并终止，并且不会再次重启，可以通过 kubectl logs 查看 Pod 日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Failed（失败）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;所有容器都已终止，并且至少有一个容器以失败的方式终止，也就是说这个容器要么以非零状态退出，要么被系统终止，可以通过 logs 和 describe 查看 Pod 日志和状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Unknown（未知）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;通常是由于通信问题造成的无法获得 Pod 的状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ImagePullBackOff ErrImagePull&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;镜像拉取失败，一般是由于镜像不存在、网络不通或者需要登录认证引起的，可以使用 describe 命令查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CrashLoopBackOff&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器启动失败，可以通过 logs 命令查看具体原因，一般为启动命令不正确，健康检查不通过等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OOMKilled&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器内存溢出，一般是容器的内存 Limit 设置的过小，或者程序本身有内存溢出，可以通过 logs 查看程序启动日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Terminating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 正在被删除，可以通过 describe 查看状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SysctlForbidden&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 自定义了内核配置，但 kubelet 没有添加内核配置或配置的内核参数不支持，可以通过 describe 查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Completed&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器内部主进程退出，一般计划任务执行结束会显示该状态，此时可以通过 logs 查看容器日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ContainerCreating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 正在创建，一般为正在下载镜像，或者有配置不当的地方，可以通过 describe 查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;2-pod镜像拉取策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-pod镜像拉取策略&#34;&gt;#&lt;/a&gt; 2. Pod 镜像拉取策略&lt;/h4&gt;
&lt;p&gt;通过 spec.containers [].imagePullPolicy 参数可以指定镜像的拉取策略，目前支持的策略如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;总是拉取，当镜像 tag 为 latest 时，且 imagePullPolicy 未配置，默认为 Always&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;不管是否存在都不会拉取&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IfNotPresent&lt;/td&gt;
&lt;td&gt;镜像不存在时拉取镜像，如果 tag 为非 latest，且 imagePullPolicy 未配置，默认为 IfNotPresent&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;更改镜像拉取策略为 IfNotPresent：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-pod-重启策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-pod-重启策略&#34;&gt;#&lt;/a&gt; 3. &lt;strong&gt;Pod&lt;/strong&gt; 重启策略&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;默认策略。容器失效时，自动重启该容器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OnFailure&lt;/td&gt;
&lt;td&gt;容器以不为 0 的状态码终止，自动重启该容器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;无论何种状态，都不会重启&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;指定重启策略为 Always ：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-pod的三种探针&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-pod的三种探针&#34;&gt;#&lt;/a&gt; 4. Pod 的三种探针&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;种类&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;startupProbe&lt;/td&gt;
&lt;td&gt;Kubernetes1.16 新加的探测方式，用于判断容器内的应用程序是否已经启动。如果配置了 startupProbe，就会先禁用其他探测，直到它成功为止。如果探测失败，Kubelet 会杀死容器，之后根据重启策略进行处理，如果探测成功，或没有配置 startupProbe，则状态为成功，之后就不再探测。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;livenessProbe&lt;/td&gt;
&lt;td&gt;用于探测容器是否在运行，如果探测失败，kubelet 会 “杀死” 容器并根据重启策略进行相应的处理。如果未指定该探针，将默认为 Success&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;readinessProbe&lt;/td&gt;
&lt;td&gt;一般用于探测容器内的程序是否健康，即判断容器是否为就绪（Ready）状态。如果是，则可以处理请求，反之 Endpoints Controller 将从所有的 Service 的 Endpoints 中删除此容器所在 Pod 的 IP 地址。如果未指定，将默认为 Success&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;5-pod探针的实现方式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-pod探针的实现方式&#34;&gt;#&lt;/a&gt; 5. Pod 探针的实现方式&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;实现方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ExecAction&lt;/td&gt;
&lt;td&gt;在容器内执行一个指定的命令，如果命令返回值为 0，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCPSocketAction&lt;/td&gt;
&lt;td&gt;通过 TCP 连接检查容器指定的端口，如果端口开放，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HTTPGetAction&lt;/td&gt;
&lt;td&gt;对指定的 URL 进行 Get 请求，如果状态码在 200~400 之间，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;6-健康检查配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-健康检查配置&#34;&gt;#&lt;/a&gt; 6. 健康检查配置&lt;/h4&gt;
&lt;p&gt;配置健康检查：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-prestop和-poststart配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-prestop和-poststart配置&#34;&gt;#&lt;/a&gt; 7. PreStop 和 PostStart 配置&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
          lifecycle:
            postStart:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - mkdir /data
            preStop:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - sleep 30
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3071070979.html</guid>
            <title>一键永久激活Window、office教程</title>
            <link>http://ixuyong.cn/posts/3071070979.html</link>
            <category>Windows</category>
            <pubDate>Thu, 10 Apr 2025 21:32:09 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;一键永久激活window-office教程&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#一键永久激活window-office教程&#34;&gt;#&lt;/a&gt; 一键永久激活 Window、office 教程&lt;/h3&gt;
&lt;p&gt;1、按下 Win 键 + R，调出运行对话框，输入 powershell 并回车，启动命令提示符窗口。接着输入以下指令执行激活：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;irm https://get.activated.win | iex
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/ilMT403.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;该脚本包含四个功能：首个命令用于 Windows 系统永久激活，第二个用于 Office 永久激活，第三个将系统有效期延长至 2038 年，第四个则实现每 180 天自动循环激活。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/taJbKQr.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;2. 我们再次使用 Windows 徽标 + R 快捷键打开运行框，输入 slmgr.vbs/xpr 就可以看到系统已经永久激活了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;slmgr.vbs /xpr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/JMWlUpc.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;以上，既然看到这里了，如果觉得不错，随手点个赞、打赏一下吧，⭐～谢谢你看我的文章，我们下次再见。&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/985149017.html</guid>
            <title>二进制高可用安装K8S集群</title>
            <link>http://ixuyong.cn/posts/985149017.html</link>
            <category>Kubernetes</category>
            <pubDate>Thu, 10 Apr 2025 20:58:40 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;二进制高可用安装k8s集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#二进制高可用安装k8s集群&#34;&gt;#&lt;/a&gt; 二进制高可用安装 K8s 集群&lt;/h2&gt;
&lt;h4 id=&#34;1-基本配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-基本配置&#34;&gt;#&lt;/a&gt; 1. 基本配置&lt;/h4&gt;
&lt;h5 id=&#34;11-基本环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-基本环境配置&#34;&gt;#&lt;/a&gt; 1.1 基本环境配置&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP 地址&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master 节点 * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived 虚拟 IP（不占用机器）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker 节点 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* 配置信息 *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;系统版本&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 网段&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service 网段&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改主机名（其它节点按需修改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 hosts，修改 /etc/hosts 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 yum 源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 配置基础源
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;必备工具安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭 swap 分区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ntpdate：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;同步时间并配置上海时区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 limit：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;升级系统：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;下载安装所有的源码文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-内核配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-内核配置&#34;&gt;#&lt;/a&gt; 1.2 内核配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ipvsadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 ipvs 模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 ipvs.conf，并配置开机自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# 加入以下内容
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;内核优化配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;应用配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置完内核后，重启机器，之后查看内核模块是否已自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-高可用组件安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-高可用组件安装&#34;&gt;#&lt;/a&gt; 2. 高可用组件安装&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;通过 yum 安装 HAProxy 和 KeepAlived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 HAProxy，需要注意黄色部分的 IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:8443       #HAProxy监听端口
  bind 127.0.0.1:8443     #HAProxy监听端口
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IP地址
  server k8s-master02	192.168.1.72:6443  check       #API Server IP地址
  server k8s-master03	192.168.1.73:6443  check       #API Server IP地址
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 KeepAlived，需要注意黄色部分的配置。&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #网卡名称
    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #网卡名称
    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #网卡名称
    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置 KeepAlived 健康检查文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置健康检查文件添加执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;启动 haproxy 和 keepalived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;所有节点测试VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld&lt;/li&gt;
&lt;li&gt;所有节点查看 selinux 状态，必须为 disable：getenforce&lt;/li&gt;
&lt;li&gt;master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master 节点查看监听端口：netstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果以上都没有问题，需要确认：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;是否是公有云机器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否是私有云机器（类似 OpenStack）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询&lt;/p&gt;
&lt;h4 id=&#34;3-runtime安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtime安装&#34;&gt;#&lt;/a&gt; 3. Runtime 安装&lt;/h4&gt;
&lt;p&gt;如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。&lt;/p&gt;
&lt;h5 id=&#34;31-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-安装containerd&#34;&gt;#&lt;/a&gt; 3.1 安装 Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置安装源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先配置 Containerd 所需的模块（&lt;mark&gt;所有节点&lt;/mark&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;，配置 Containerd 所需的内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;生成 Containerd 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改 Containerd 的 Cgroup 和 Pause 镜像配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 Containerd，并配置开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 crictl 客户端连接的运行时位置（可选）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-k8s及etcd安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-k8s及etcd安装&#34;&gt;#&lt;/a&gt; 4 . K8S 及 etcd 安装&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 下载 kubernetes 安装包（1.32.3 需要更改为你看到的最新版本）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最新版获取地址：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;以下操作都在 master01 执行&lt;/mark&gt;&lt;/p&gt;
&lt;p&gt;下载 etcd 安装包：&lt;a href=&#34;https://github.com/etcd-io/etcd/releases/&#34;&gt;https://github.com/etcd-io/etcd/releases/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解压 kubernetes 安装文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解压 etcd 安装文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&amp;#123;,ctl&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;版本查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubelet --version
Kubernetes v1.32.3
[root@k8s-master01 ~]# etcdctl version
etcdctl version: 3.5.16
API version: 3.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将组件发送到其他节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;
WorkNodes=&#39;k8s-node01 k8s-node02&#39;
for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done
for NODE in $WorkNodes; do     scp /usr/local/bin/kube&amp;#123;let,-proxy&amp;#125; $NODE:/usr/local/bin/ ; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;切换到 1.32.x 分支（其他版本可以切换到其他分支，.x 即可，不需要更改为具体的小版本）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-生成证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-生成证书&#34;&gt;#&lt;/a&gt; 5 . 生成证书&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;二进制安装最关键步骤，一步错误全盘皆输，一定要注意每个步骤都要是正确的&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 下载生成证书工具（下载不成功可以去百度网盘）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&amp;quot; -O /usr/local/bin/cfssl
wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&amp;quot; -O /usr/local/bin/cfssljson
chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-etcd证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-etcd证书&#34;&gt;#&lt;/a&gt; 5.1 Etcd 证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd 证书目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/etcd/ssl -p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 kubernetes 相关目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/pki
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;生成 etcd 证书&lt;/p&gt;
&lt;p&gt;生成证书的 CSR（证书签名请求文件，配置了一些域名、公司、单位）文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

# 生成etcd CA证书和CA证书的key
cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca


cfssl gencert \
   -ca=/etc/etcd/ssl/etcd-ca.pem \
   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \
   -config=ca-config.json \
   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \
   -profile=kubernetes \
   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd

执行结果
[INFO] generate received request
 	[INFO] received CSR
     [INFO] generating key: rsa-2048
     [INFO] encoded CSR
     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将证书复制到其他 master 节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;

for NODE in $MasterNodes; do
     ssh $NODE &amp;quot;mkdir -p /etc/etcd/ssl&amp;quot;
     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do
       scp /etc/etcd/ssl/$&amp;#123;FILE&amp;#125; $NODE:/etc/etcd/ssl/$&amp;#123;FILE&amp;#125;
     done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;52-k8s组件证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-k8s组件证书&#34;&gt;#&lt;/a&gt; 5.2 K8s 组件证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 生成 kubernetes CA 证书：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;521-apiserver证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#521-apiserver证书&#34;&gt;#&lt;/a&gt; 5.2.1 APIServer 证书&lt;/h6&gt;
&lt;p&gt;注意：10.96.0. 是 k8s service 的网段，如果说需要更改 k8s service 网段，那就需要更改 10.96.0.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;生成 apiserver 的聚合证书：：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca 

cfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;返回结果（忽略警告）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2020/12/11 18:15:28 [INFO] generate received request
2020/12/11 18:15:28 [INFO] received CSR
2020/12/11 18:15:28 [INFO] generating key: rsa-2048

2020/12/11 18:15:28 [INFO] encoded CSR
2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845
2020/12/11 18:15:28 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;522-controllermanager&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#522-controllermanager&#34;&gt;#&lt;/a&gt; 5.2.2 ControllerManager&lt;/h6&gt;
&lt;p&gt;生成 controller-manage 的证书：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager

注意：修改黄色部分的IP地址
# set-cluster：设置一个集群项，

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# 设置一个环境项，一个上下文
kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# set-credentials 设置一个用户项

kubectl config set-credentials system:kube-controller-manager \
     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \
     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig


# 使用某个环境当做默认环境

kubectl config use-context system:kube-controller-manager@kubernetes \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;523-scheduler证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#523-scheduler证书&#34;&gt;#&lt;/a&gt; 5.2.3 Scheduler 证书&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler

注意：修改黄色部分的IP地址

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig


kubectl config set-credentials system:kube-scheduler \
     --client-certificate=/etc/kubernetes/pki/scheduler.pem \
     --client-key=/etc/kubernetes/pki/scheduler-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config set-context system:kube-scheduler@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-scheduler \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config use-context system:kube-scheduler@kubernetes \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;524-生成管理员证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#524-生成管理员证书&#34;&gt;#&lt;/a&gt; 5.2.4 生成管理员证书&lt;/h6&gt;
&lt;p&gt;Kubectl /etc/Kubernetes/admin.conf ~/.kube/config&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin

注意：修改黄色部分的IP

kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig
kubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;525-创建serviceaccount证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#525-创建serviceaccount证书&#34;&gt;#&lt;/a&gt; 5.2.5 创建 ServiceAccount 证书&lt;/h6&gt;
&lt;p&gt;创建一对公钥，用来签发 ServiceAccount 的 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl genrsa -out /etc/kubernetes/pki/sa.key 2048
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;返回结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Generating RSA private key, 2048 bit long modulus (2 primes)
...................................................................................+++++
...............+++++
e is 65537 (0x010001)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;发送证书至其他节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do 
  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do 
    scp /etc/kubernetes/pki/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/pki/$&amp;#123;FILE&amp;#125;;
  done; 
  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do 
    scp /etc/kubernetes/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;;
  done;
done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看证书文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# ls /etc/kubernetes/pki/
admin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem
admin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem
admin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr
[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l
23
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-kubernetes组件配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-kubernetes组件配置&#34;&gt;#&lt;/a&gt; 6. Kubernetes 组件配置&lt;/h4&gt;
&lt;h5 id=&#34;61-ecd配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#61-ecd配置&#34;&gt;#&lt;/a&gt; 6.1 Ecd 配置&lt;/h5&gt;
&lt;p&gt;Etcd 配置大致相同，注意修改每个 Master 节点的 etcd 配置的主机名和 IP 地址&lt;/p&gt;
&lt;h6 id=&#34;611-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#611-master01&#34;&gt;#&lt;/a&gt; 6.1.1 Master01&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master01&#39;     # k8s-master01名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.71:2380&#39;            # k8s-master01 IP
listen-client-urls: &#39;https://192.168.1.71:2379,http://127.0.0.1:2379&#39;   # k8s-master01 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.71:2380&#39;  # k8s-master01 IP
advertise-client-urls: &#39;https://192.168.1.71:2379&#39;        # k8s-master01 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;     # k8s-master01、k8s-master02、k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;612-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#612-master02&#34;&gt;#&lt;/a&gt; 6.1.2 Master02&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml	
name: &#39;k8s-master02&#39;   # k8s-master02名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.72:2380&#39;      # k8s-master02 IP
listen-client-urls: &#39;https://192.168.1.72:2379,http://127.0.0.1:2379&#39;    # k8s-master02 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.72:2380&#39;    # k8s-master02 IP
advertise-client-urls: &#39;https://192.168.1.72:2379&#39;     # k8s-master02 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;             # k8s-master01、k8s-master02、k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;613-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#613-master03&#34;&gt;#&lt;/a&gt; 6.1.3 Master03&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master03&#39;           # k8s-master03名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.73:2380&#39;           # k8s-master03 IP
listen-client-urls: &#39;https://192.168.1.73:2379,http://127.0.0.1:2379&#39;       # k8s-master03 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.73:2380&#39;      # k8s-master03 IP
advertise-client-urls: &#39;https://192.168.1.73:2379&#39;            # k8s-master03 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;                # k8s-master01、k8s-master02、k8s-master03 IP
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;614-启动etcd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#614-启动etcd&#34;&gt;#&lt;/a&gt; 6.1.4 启动 Etcd&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd service 并启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Service
Documentation=https://coreos.com/etcd/docs/latest/
After=network.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml
Restart=on-failure
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd3.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd 的证书目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/kubernetes/pki/etcd
ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/
systemctl daemon-reload
systemctl enable --now etcd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 etcd 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export ETCDCTL_API=3
etcdctl --endpoints=&amp;quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&amp;quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;62-apiserver配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#62-apiserver配置&#34;&gt;#&lt;/a&gt; 6.2 APIServer 配置&lt;/h5&gt;
&lt;h6 id=&#34;621-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#621-master01&#34;&gt;#&lt;/a&gt; 6.2.1 Master01&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.71 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;622-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#622-master02&#34;&gt;#&lt;/a&gt; 6.2.2 Master02&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.72 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;623-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#623-master03&#34;&gt;#&lt;/a&gt; 6.2.3 Master03&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.73 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;624-启动apiserver&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#624-启动apiserver&#34;&gt;#&lt;/a&gt; 6.2.4 启动 apiserver&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;开启 kube-apiserver：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload &amp;amp;&amp;amp; systemctl enable --now kube-apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;检测 kube-server 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl status kube-apiserver

● kube-apiserver.service – Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s ago 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果系统日志有这些提示可以忽略:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to “pick_first”
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;CONNECTING &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;READY &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = “transport is closing”
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;63-controllermanage&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#63-controllermanage&#34;&gt;#&lt;/a&gt; 6.3 ControllerManage&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 kube-controller-manager service（所有 master 节点配置一样）&lt;/p&gt;
&lt;p&gt;注意：本文档使用的 k8s Pod 网段为 172.16.0.0/16，该网段不能和宿主机的网段、k8s Service 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
      --v=2 \
      --root-ca-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \
      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --leader-elect=true \
      --use-service-account-credentials=true \
      --node-monitor-grace-period=40s \
      --node-monitor-period=5s \
      --controllers=*,bootstrapsigner,tokencleaner \
      --allocate-node-cidrs=true \
      --cluster-cidr=172.16.0.0/16 \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
      --node-cidr-mask-size=24
      
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;启动 kube-controller-manager&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager
Created symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service → /usr/lib/systemd/system/kube-controller-manager.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看启动状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl  status kube-controller-manager
● kube-controller-manager.service – Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)
 Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago
     Docs: https://github.com/  ubernetes/  ubernetes
 Main PID: 7518 (kube-controller)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;64-scheduler&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#64-scheduler&#34;&gt;#&lt;/a&gt; 6.4 Scheduler&lt;/h5&gt;
&lt;p&gt;所有 Master 节点配置 kube-scheduler service（所有 master 节点配置一样）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service 
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
      --v=2 \
      --leader-elect=true \
      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动 scheduler：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-scheduler
Created symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service → /usr/lib/systemd/system/kube-scheduler.service.
[root@k8s-master01 pki]# systemctl status kube-scheduler
● kube-scheduler.service - Kubernetes Scheduler
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 5815 (kube-scheduler)
    Tasks: 9
   Memory: 19.8M
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-tls-bootstrapping配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-tls-bootstrapping配置&#34;&gt;#&lt;/a&gt; 7. TLS Bootstrapping 配置&lt;/h4&gt;
&lt;p&gt;只需要在&lt;mark&gt; Master01&lt;/mark&gt; 创建 bootstrap&lt;/p&gt;
&lt;p&gt;注意： 修改黄色部分的 IP 地址&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/bootstrap
kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig

[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以正常查询集群状态，才可以继续往下，否则不行，需要排查 k8s 组件是否有故障（只要有结果即可，如果返回不一样不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok        
scheduler            Healthy   ok        
etcd-0               Healthy   ok
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建 bootstrap 相关资源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml 
secret/bootstrap-token-c8ad9c created
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-node节点配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-node节点配置&#34;&gt;#&lt;/a&gt; 8. Node 节点配置&lt;/h4&gt;
&lt;h5 id=&#34;81-复制证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#81-复制证书&#34;&gt;#&lt;/a&gt; 8.1 复制证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;复制证书至其他节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/kubernetes/

for NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do
     ssh $NODE mkdir -p /etc/kubernetes/pki
     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do
       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;
 done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    
…
bootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;82-kubelet配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#82-kubelet配置&#34;&gt;#&lt;/a&gt; 8.2 Kubelet 配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 Kubelet 配置目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 kubelet service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service

[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kubelet

Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 kubelet service 的配置文件（也可以写到 kubelet.service）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Runtime为Containerd
# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf

[Service]
Environment=&amp;quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&amp;quot;
Environment=&amp;quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&amp;quot;
Environment=&amp;quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&amp;quot;
Environment=&amp;quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=&#39;&#39; &amp;quot;
ExecStart=
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 kubelet 的配置文件&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：如果更改了 k8s 的 service 网段，需要更改 kubelet-conf.yml 的 clusterDNS: 配置，改成 k8s Service 网段的第十个地址，比如 10.96.0.10&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动&lt;mark&gt;所有节点&lt;/mark&gt; kubelet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;如果有很多报错日志，或者有大量看不懂的报错，说明 kubelet 的配置有误，需要检查 kubelet 配置&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Master01 查看集群状态 (Ready 或 NotReady 都正常)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl get node
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;83-kube-proxy配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#83-kube-proxy配置&#34;&gt;#&lt;/a&gt; 8.3 kube-proxy 配置&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;注意，如果不是高可用集群，192.168.1.70:8443 改为 master01 的地址，8443 改为 apiserver 的端口，默认是 6443&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;生成 kube-proxy 的证书，以下操作只在&lt;mark&gt; Master01&lt;/mark&gt; 执行&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/pki
cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config set-credentials system:kube-proxy \
     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \
     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

kubectl config set-context system:kube-proxy@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-proxy \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config use-context system:kube-proxy@kubernetes \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 kubeconfig 发送至其他节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do
     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done

for NODE in k8s-node01 k8s-node02; do
     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;添加 kube-proxy 的配置和 service 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /usr/lib/systemd/system/kube-proxy.service

[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --v=2

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果更改了集群 Pod 的网段，需要更改 kube-proxy.yaml 的 clusterCIDR 为自己的 Pod 网段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/kubernetes/kube-proxy.yaml

apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  acceptContentTypes: &amp;quot;&amp;quot;
  burst: 10
  contentType: application/vnd.kubernetes.protobuf
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
  qps: 5
clusterCIDR: 172.16.0.0/16 
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
hostnameOverride: &amp;quot;&amp;quot;
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
ipvs:
  masqueradeAll: true
  minSyncPeriod: 5s
  scheduler: &amp;quot;rr&amp;quot;
  syncPeriod: 30s
kind: KubeProxyConfiguration
metricsBindAddress: 127.0.0.1:10249
mode: &amp;quot;ipvs&amp;quot;
nodePortAddresses: null
oomScoreAdj: -999
portRange: &amp;quot;&amp;quot;
udpIdleTimeout: 250ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload
[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /usr/lib/systemd/system/kube-proxy.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;9-calico组件的安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-calico组件的安装&#34;&gt;#&lt;/a&gt; 9. Calico 组件的安装&lt;/h4&gt;
&lt;p&gt;以下步骤只在 master01 执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更改 calico 的网段，主要需要将红色部分的网段，改为自己的 Pod 网段&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &amp;quot;s#POD_CIDR#172.16.0.0/16#g&amp;quot; calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;检查网段是自己的 Pod 网段， grep &amp;quot;IPV4POOL_CIDR&amp;quot; calico.yaml  -A 1&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看容器和节点状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 calico]# kubectl get po -n kube-system
NAME                                       READY   STATUS    RESTARTS      AGE
calico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s
calico-node-8fxqp                          1/1     Running   0             85s
calico-node-8nkfl                          1/1     Running   0             86s
calico-node-pmpf4                          1/1     Running   0             86s
calico-node-vnlk7                          1/1     Running   0             86s
calico-node-xpchb                          1/1     Running   0             85s
calico-typha-67c6dc57d6-259t8              1/1     Running   0             86s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;如果容器状态异常可以使用 kubectl describe 或者 kubectl logs 查看容器的日志&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -n kube-system&lt;/li&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;10-安装coredns&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-安装coredns&#34;&gt;#&lt;/a&gt; 10. 安装 CoreDNS&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果更改了 k8s service 的网段需要将 coredns 的 serviceIP 改成 k8s service 网段的第十个 IP&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk &#39;&amp;#123;print $3&amp;#125;&#39;`0
sed -i &amp;quot;s#KUBEDNS_SERVICE_IP#$&amp;#123;COREDNS_SERVICE_IP&amp;#125;#g&amp;quot; CoreDNS/coredns.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装 coredns&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11-metrics部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-metrics部署&#34;&gt;#&lt;/a&gt; 11. Metrics 部署&lt;/h4&gt;
&lt;p&gt;在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。&lt;/p&gt;
&lt;p&gt;以下操作均在&lt;mark&gt; master01 节点&lt;/mark&gt;执行，安装 metrics server:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/metrics-server
kubectl  create -f . 

serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;等待 metrics server 启动然后查看状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl  top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master01   231m         5%     1620Mi          42%       
k8s-master02   274m         6%     1203Mi          31%       
k8s-master03   202m         5%     1251Mi          32%       
k8s-node01     69m          1%     667Mi           17%       
k8s-node02     73m          1%     650Mi           16%
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果有如下报错，可以等待 10 分钟后，再次查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-dashboard部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-dashboard部署&#34;&gt;#&lt;/a&gt; 12. Dashboard 部署&lt;/h4&gt;
&lt;h5 id=&#34;121-安装dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#121-安装dashboard&#34;&gt;#&lt;/a&gt; 12.1 安装 Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;122-登录dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#122-登录dashboard&#34;&gt;#&lt;/a&gt; 12.2 登录 dashboard&lt;/h5&gt;
&lt;p&gt;在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更改 dashboard 的 svc 为 NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看端口号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：&lt;/p&gt;
&lt;p&gt;访问 Dashboard：&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建登录 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;14-containerd配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-containerd配置镜像加速&#34;&gt;#&lt;/a&gt; 14. Containerd 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Containerd：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;15-docker配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-docker配置镜像加速&#34;&gt;#&lt;/a&gt; 15. Docker 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/2628187572.html</guid>
            <title>MySQL运维DBA应用与实践</title>
            <link>http://ixuyong.cn/posts/2628187572.html</link>
            <category>MySQL</category>
            <pubDate>Wed, 09 Apr 2025 22:02:40 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;mysql运维dba应用与实践&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#mysql运维dba应用与实践&#34;&gt;#&lt;/a&gt; MySQL 运维 DBA 应用与实践&lt;/h3&gt;
&lt;h4 id=&#34;1日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1日志&#34;&gt;#&lt;/a&gt; 1. 日志&lt;/h4&gt;
&lt;p&gt;在任何一种数据库中，都会有各种各样的日志，这些日志记录了数据库运行的各个方面。可以帮助数据库管理员追踪数据库曾经发生的一些事情。&lt;/p&gt;
&lt;p&gt;对于 MySQL 数据库，提供了四种不同的日志帮助我们追踪。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;错误日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;二进制日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查询日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;慢查询日志&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-错误日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-错误日志&#34;&gt;#&lt;/a&gt; 1.1 错误日志&lt;/h5&gt;
&lt;p&gt;错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld (MySQL 服务) 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，建议首先查看此日志。&lt;/p&gt;
&lt;p&gt;该日志是默认开启的，默认存放目录 /var/log/，默认的日志文件名为 mysqld.log。查看日志位置；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &#39;%log_error%&#39;;
+---------------------+---------------------+
| Variable_name       | Value               |
+---------------------+---------------------+
| binlog_error_action | ABORT_SERVER        |
| log_error           | /var/log/mysqld.log |
| log_error_verbosity | 3                   |
+---------------------+---------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-二进制日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-二进制日志&#34;&gt;#&lt;/a&gt; 1.2 二进制日志&lt;/h5&gt;
&lt;p&gt;二进制日志 (BINLOG) 记录了所有的 DDL (数据定义语言) 语句和 DML (数据操纵语言) 语句，但不包括数据查询（SELECT、 SHOW）语句。&lt;/p&gt;
&lt;p&gt;作用:&lt;/p&gt;
&lt;p&gt;①. 灾难时的数据恢复；&lt;/p&gt;
&lt;p&gt;②. MySQL 的主从复制。&lt;/p&gt;
&lt;p&gt;在 MySQL5.7 版本中，默认二进制日志是关闭着的，涉及到的参数如下:&lt;/p&gt;
&lt;h6 id=&#34;121-开启-bin-log记录&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#121-开启-bin-log记录&#34;&gt;#&lt;/a&gt; 1.2.1 开启 bin-log 记录&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;1.1改修配置文件
[root@db01 ~]# vim /etc/my.cnf
server-id=1
log-bin=mysql-bin
max_binlog_size=500M
expire_logs_days=15

1.2查看是否开启binlog.
mysql&amp;gt; show variables like &#39;log_%&#39;;
+----------------------------------------+--------------------------------+
| Variable_name                          | Value                          |
+----------------------------------------+--------------------------------+
| log_bin                                | ON                             |
| log_bin_basename                       | /var/lib/mysql/mysql-bin       |
| log_bin_index                          | /var/lib/mysql/mysql-bin.index |
| log_bin_trust_function_creators        | OFF                            |
| log_bin_use_v1_row_events              | OFF                            |
| log_builtin_as_identified_by_password  | OFF                            |
| log_error                              | /var/log/mysqld.log            |
| log_error_verbosity                    | 3                              |
| log_output                             | FILE                           |
| log_queries_not_using_indexes          | OFF                            |
| log_slave_updates                      | OFF                            |
| log_slow_admin_statements              | OFF                            |
| log_slow_slave_statements              | OFF                            |
| log_statements_unsafe_for_binlog       | ON                             |
| log_syslog                             | OFF                            |
| log_syslog_facility                    | daemon                         |
| log_syslog_include_pid                 | ON                             |
| log_syslog_tag                         |                                |
| log_throttle_queries_not_using_indexes | 0                              |
| log_timestamps                         | UTC                            |
| log_warnings                           | 2                              |
+----------------------------------------+--------------------------------+

1.3查看binlog
mysql&amp;gt; show binary logs;
+------------------+-----------+
| Log_name         | File_size |
+------------------+-----------+
| mysql-bin.000001 |     36825 |
| mysql-bin.000002 |    200464 |
| mysql-bin.000003 |    419809 |
+------------------+-----------+

1.4查看binlog日志保存天数 
# 0表示永久保留，expire_logs_days：保留指定日期范围内的binlog历史日志，上示例设置的15天内
mysql&amp;gt; show variables like &#39;expire_logs_days&#39;;
+------------------+-------+
| Variable_name    | Value |
+------------------+-------+
| expire_logs_days | 15    |
+------------------+-------+
1 row in set (0.00 sec)

1.5查看binlog日志保存大小
#max_binlog_size：bin log日志每达到设定大小后，会使用新的bin log日志。如mysql-bin.000002达到500M后，创建并使用mysql-bin.000003文件作为日志记录。
mysql&amp;gt; show variables like &#39;max_binlog_size&#39;;
+-----------------+-----------+
| Variable_name   | Value     |
+-----------------+-----------+
| max_binlog_size | 524288000 |
+-----------------+-----------+

1.6手动执行flush logs
#将会new一个新文件用于记录binlog
mysql&amp;gt; flush logs;

1.7手动清理binlog
#将mysql-bin.000010之前的日志清理掉
mysql&amp;gt; purge binary logs to &#39;mysql-bin.000010&#39;;
Query OK, 0 rows affected (0.01 sec)

#删除2022-04-21 18:08:00之前的binlog日志
mysql&amp;gt; purge binary logs before &#39;2022-04-21 18:08:00&#39;;

#清除全部binlog
mysql&amp;gt; reset master;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;122-日志格式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#122-日志格式&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.2.2 日志格式&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;MySQL 服务器中提供了多种格式来记录二进制日志，具体格式及特点如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;日志格式&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;含义&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;STATEMENT&lt;/td&gt;
&lt;td&gt;基于 SQL 语句的日志记录，记录的是 SQL 语句，对数据进行修改的 SQL 都会记录在日志文件中。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ROW&lt;/td&gt;
&lt;td&gt;基于行的日志记录，记录的是每一行的数据变更。(默认)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MIXED&lt;/td&gt;
&lt;td&gt;混合了 STATEMENT 和 ROW 两种格式，默认采用 STATEMENT, 在某些特殊情况下会自动切换为 ROW 进行记录。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &#39;binlog_format&#39;;
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| binlog_format | ROW   |
+---------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 &lt;code&gt;mysqlbinlog&lt;/code&gt;  来查看，具体语法:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysqlbinlog [ 参数选项] logfilename
参数选项:
	-d			指定数据库名称，只列出指定的数据库相关操作。
	-o			忽略掉日志中的前n行命令。
	-v			将行事件(数据变更)重构为SQL语句
	-vv			将行事件(数据变更)重构为SQL语句，并输出注释信息
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; use zh;
Database changed
mysql&amp;gt; show tables;
+----------------+
| Tables_in_zh   |
+----------------+
| account        |
| course         |
| dept           |
| emp            |
| score          |
| student        |
| student_course |
| tb_user        |
| tb_user_edu    |
| user           |
| user1          |
+----------------+
11 rows in set (0.00 sec)

mysql&amp;gt;  update tb_user_edu set university = &amp;quot;北京大学&amp;quot;;
Query OK, 4 rows affected (0.00 sec)
Rows matched: 4  Changed: 4  Warnings: 0

#二进制日志查看
[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000001 
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;123-修改binlog格式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#123-修改binlog格式&#34;&gt;#&lt;/a&gt; 1.2.3 修改 binlog 格式&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# vim /etc/my.cnf
...
binlog_format=STATEMENT
...
[root@db01 ~]# systemctl restart mysqld

mysql&amp;gt;  update tb_user_edu set university = &#39;清华大学&#39;;
[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000002 
...
SET TIMESTAMP=1701440373/*!*/;
update tb_user_edu set university = &#39;清华大学&#39;
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-查询日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-查询日志&#34;&gt;#&lt;/a&gt; 1.3 查询日志&lt;/h5&gt;
&lt;p&gt;查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的 SQL 语句。默认情况下，&lt;strong&gt;查询日志是未开启的&lt;/strong&gt;。如果需要开启查询日志，可以设置以下配置︰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &#39;%general%&#39;;
+------------------+-------------------------+
| Variable_name    | Value                   |
+------------------+-------------------------+
| general_log      | OFF                     |
| general_log_file | /var/lib/mysql/db01.log |
+------------------+-------------------------+
2 rows in set (0.00 sec)

#开启查询日志功能
[root@db01 ~]# cat /etc/my.cnf
general_log=1
general_log_file=/var/lib/mysql/mysql_query.log 
[root@db01 ~]# systemctl restart mysqld

[root@db01 ~]# tail -f /var/lib/mysql/mysql_query.log 
2023-12-01T14:31:28.554384Z	    2 Field List	student 
2023-12-01T14:31:28.554743Z	    2 Field List	student_course 
2023-12-01T14:31:35.737041Z	    2 Query	show variables like &#39;%general%&#39;
2023-12-01T14:31:37.345179Z	    2 Query	show variables like &#39;%general%&#39;
2023-12-01T14:32:17.593471Z	    2 Query	SELECT DATABASE()
2023-12-01T14:32:17.593651Z	    2 Init DB	zh
2023-12-01T14:32:25.249258Z	    2 Query	select * from emp
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-慢查询日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-慢查询日志&#34;&gt;#&lt;/a&gt; 1.4 慢查询日志&lt;/h5&gt;
&lt;p&gt;慢查询&lt;a href=&#34;https://so.csdn.net/so/search?q=%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95&amp;amp;spm=1001.2101.3001.7020&#34;&gt;日志记录&lt;/a&gt;了所有执行时间超过参数 &lt;code&gt;long_ query_time&lt;/code&gt;  设置值并且扫描记录数不小于 &lt;code&gt;min_examined_row_limit&lt;/code&gt;  的所有的 SQL 语句的日志，默认未开启。&lt;strong&gt; &lt;code&gt;long_query_time&lt;/code&gt;  默认为 10 秒，最小为 0，精度可以到微秒。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# vim /etc/my.cnf
#慢查询日志
slow_query_log=on
##执行时间参数
long_query_time=2
# 若没有指定，默认名字为hostname_slow.log
slow_query_log_file = /var/lib/mysql/slow-query.log
[root@db01 ~]# systemctl restart mysqld

#制造慢查询并执行
mysql&amp;gt; select sleep(3);
[root@db01 ~]# tail -f /var/lib/mysql/slow-query.log 
/usr/sbin/mysqld, Version: 5.7.43-log (MySQL Community Server (GPL)). started with:
Tcp port: 0  Unix socket: /var/lib/mysql/mysql.sock
Time                 Id Command    Argument
# Time: 2023-12-01T14:47:57.763735Z
# User@Host: root[root] @ localhost []  Id:     2
# Query_time: 3.001229  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0
use zh;
SET timestamp=1701442077;
select sleep(3);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;默认情况下，不会记录管理语句，也不会记录不使用索引进行查找的查询。可以使用 &lt;code&gt;log_slow_admin_statements&lt;/code&gt;  和更改此行为 &lt;code&gt;log_queries_not_using_indexes&lt;/code&gt; , 如下所述。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#记录执行较慢的管理语句
log_slow_admin_statements = 1
#记录执行较慢的未使用索引的语句
log_queries_not_using_indexes = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-主从复制&#34;&gt;#&lt;/a&gt; 2. 主从复制&lt;/h4&gt;
&lt;h5 id=&#34;21-主从复制的概述&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-主从复制的概述&#34;&gt;#&lt;/a&gt; 2.1 主从复制的概述&lt;/h5&gt;
&lt;p&gt;主从复制是指将&lt;strong&gt;主数据库的 DDL 和 DML 操作&lt;/strong&gt;通过&lt;strong&gt;二进制日志&lt;/strong&gt;传到&lt;strong&gt;从库服务器&lt;/strong&gt;中，然后在从库上对这些日志重新执行 (也叫重做) ，从而使得从库和主库的数据保持同步。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgO0Mj&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgO0Mj.png&#34; alt=&#34;pEgO0Mj.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MySQL 支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库， 实现链状复制。&lt;/p&gt;
&lt;p&gt;MySQL 复制的有点主要包含以下三个方面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;主库出现问题，可以快速切换到从库提供服务；&lt;/li&gt;
&lt;li&gt;实现读写分离，降低主库的访问压力；（如果增删改对主库 查询对从库）&lt;/li&gt;
&lt;li&gt;可以在从库中执行备份，以避免备份期间影响主库服务。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;22-主从复制的原理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-主从复制的原理&#34;&gt;#&lt;/a&gt; 2.2 主从复制的原理&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgOdzQ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgOdzQ.png&#34; alt=&#34;pEgOdzQ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;从上图来看，复制分成三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Master 主库在事务提交时，会把数据变更记录在二进制日志文件 Binlog 中。&lt;/li&gt;
&lt;li&gt;从库 IO 线程读取主库的二进制日志文件 Binlog，写入到从库的中继日志 Relay Log。&lt;/li&gt;
&lt;li&gt;slave 重做中继日志中的事件，SQL 线程将改变反映它自己的数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;23-主从复制的搭建&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-主从复制的搭建&#34;&gt;#&lt;/a&gt; 2.3 主从复制的搭建&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;主从复制的搭建步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;准备主从复制服务器环境&lt;/li&gt;
&lt;li&gt;完成主库配置&lt;/li&gt;
&lt;li&gt;完成从库配置&lt;/li&gt;
&lt;/ol&gt;
&lt;h6 id=&#34;231-服务器准备&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#231-服务器准备&#34;&gt;#&lt;/a&gt; 2.3.1 服务器准备&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgODLn&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgODLn.png&#34; alt=&#34;pEgODLn.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;232-主库配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#232-主库配置&#34;&gt;#&lt;/a&gt; 2.3.2 主库配置&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;#1. 安装 MySQL&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1、关闭防火墙、selinux、环境配置
[root@db01 ~]# hostnamectl set-hostname db01
[root@db01 ~]# systemctl stop firewalld
[root@db01 ~]# systemctl disable firewalld
[root@db01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@db01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@db01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@db01 ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@db01 ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@db01 ~]# ntpdate time2.aliyun.com
[root@db01 ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@db01 ~]# mkdir /soft /data /scripts /backup

#2、安装Mysql5.7
[root@db01 ~]# yum install -y mysql-community-server
[root@db01 ~]# systemctl start mysqld &amp;amp;&amp;amp; systemctl enable mysqld

[root@db01 ~]# mysql -uroot -p$(awk &#39;/temporary password/&amp;#123;print $NF&amp;#125;&#39; /var/log/mysqld.log)
mysql&amp;gt; ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;passwd&#39;;
mysql&amp;gt; grant all on *.* to &#39;root&#39;@&#39;192.168.1.%&#39; identified by &#39;passwd&#39;;

#3、允许root用户在任何地方进行远程登录，并具有所有库任何操作权限，具体操作如下：
mysql -u root -p&amp;quot;youpass&amp;quot;
mysql&amp;gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;passwd&#39; WITH GRANT OPTION;
FLUSH PRIVILEGES;

#4.配置主库
[root@db01 ~]# vim /etc/my.cnf
server-id=1                #mysql服务ID，保证整个集群环境中唯一， 取值范围: 1 - 2^&amp;#123;32&amp;#125;-1
log-bin=mysql-bin          #启动二进制日志
read-only=0                #是否只读,1代表只读, 0代表读写
#binlog-ignore-db=mysql    #忽略的数据，指不需要同步的数据库
#binlog-do-db=db01         #指定同步的数据库
[root@db01 ~]# systemctl restart mysqld

#5.创建repl用户，并设置密码，该用户可在任意主机连接该MySQL服务
mysql&amp;gt; grant replication slave on *.* to &#39;repl&#39;@&#39;%&#39; identified by &#39;passwd&#39;;

#6.查看master位置点
mysql&amp;gt; show master status;        
+------------------+----------+--------------+------------------+-------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+------------------+----------+--------------+------------------+-------------------+
| mysql-bin.000006 |      889 |              |                  |                   |
+------------------+----------+--------------+------------------+-------------------+
1 row in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;233-从库配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#233-从库配置&#34;&gt;#&lt;/a&gt; 2.3.3 从库配置&lt;/h6&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数名&lt;/th&gt;
&lt;th&gt;含义&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;8.0.23 之前&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_HOST&lt;/td&gt;
&lt;td&gt;主库 IP 地址&lt;/td&gt;
&lt;td&gt;MASTER_HOST&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_USER&lt;/td&gt;
&lt;td&gt;连接主库的用户名&lt;/td&gt;
&lt;td&gt;MASTER_USER&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_PASSWORD&lt;/td&gt;
&lt;td&gt;连接主库的密码&lt;/td&gt;
&lt;td&gt;MASTER_PASSWORD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_LOG FILE&lt;/td&gt;
&lt;td&gt;binlog 日志文件名&lt;/td&gt;
&lt;td&gt;MASTER LOG_FILE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_LOG POS&lt;/td&gt;
&lt;td&gt;binlog 日志文件位置&lt;/td&gt;
&lt;td&gt;MASTER_LOG_POS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;#1.配置从库
[root@db02 ~]# vim /etc/my.cnf
server-id=2           #mysql服务ID
read-only=1           #是否只读,1代表只读, 0代表读写
[root@db02 ~]# systemctl restart mysqld

#2..配置从服务器，连接主服务器
mysql&amp;gt; change master to master_host=&#39;192.168.40.150&#39;,master_user=&#39;repl&#39;,master_password=&#39;passwd&#39;,master_log_file=&#39;mysql-bin.000006&#39;,master_log_pos=889;

#3.开启从库
mysql&amp;gt; start slave;
Query OK, 0 rows affected (0.00 sec)

#4.检查主从复制状态
mysql&amp;gt; show slave status\G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 192.168.40.150
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000006
          Read_Master_Log_Pos: 889
               Relay_Log_File: db02-relay-bin.000002
                Relay_Log_Pos: 320
        Relay_Master_Log_File: mysql-bin.000006
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 889
              Relay_Log_Space: 526
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 1
                  Master_UUID: 9b911bea-43e6-11ee-b239-000c29074f5d
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind: 
      Last_IO_Error_Timestamp: 
     Last_SQL_Error_Timestamp: 
               Master_SSL_Crl: 
           Master_SSL_Crlpath: 
           Retrieved_Gtid_Set: 
            Executed_Gtid_Set: 
                Auto_Position: 0
         Replicate_Rewrite_DB: 
                 Channel_Name: 
           Master_TLS_Version: 
1 row in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-分库分表&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-分库分表&#34;&gt;#&lt;/a&gt; 3. &lt;a href=&#34;https://so.csdn.net/so/search?q=%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8&amp;amp;spm=1001.2101.3001.7020&#34;&gt;分库分表&lt;/a&gt;&lt;/h4&gt;
&lt;h5 id=&#34;31-分库分表介绍&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-分库分表介绍&#34;&gt;#&lt;/a&gt; 3.1 分库分表介绍&lt;/h5&gt;
&lt;h6 id=&#34;311-现在的问题&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#311-现在的问题&#34;&gt;#&lt;/a&gt; 3.1.1 现在的问题&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;单数据库&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所有数据都是存放在一个&lt;a href=&#34;https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%BA%93%E6%96%87%E4%BB%B6&amp;amp;spm=1001.2101.3001.7020&#34;&gt;数据库文件&lt;/a&gt;里的，经过常年累月，内存不足了怎么办？&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgOyd0&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgOyd0.png&#34; alt=&#34;pEgOyd0.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;随着互联网及移动互联网的发展，应用系统的数据量也是成指数式增长，若采用单数据库进行数据存储，存在以下性能瓶颈：&lt;/p&gt;
&lt;p&gt;IO 瓶颈：热点数据太多，数据库缓存不足，产生大量磁盘 IO，效率较低。请求数据太多，带宽不够，网络 IO 瓶颈。&lt;br /&gt;
CPU 瓶颈： 排序、分组、连接查询、聚合统计等 SQL 会耗费大量的 CPU 资源，请求数太多，CPU 出现瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgO6oV&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgO6oV.png&#34; alt=&#34;pEgO6oV.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分库分表的中心思想：&lt;br /&gt;
将数据分散存储，使得单一数据库 / 表的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的。&lt;/strong&gt;&lt;/p&gt;
&lt;h6 id=&#34;312-拆分策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#312-拆分策略&#34;&gt;#&lt;/a&gt; 3.1.2 拆分策略&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2dAXt&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2dAXt.png&#34; alt=&#34;pE2dAXt.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;313-垂直拆分策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#313-垂直拆分策略&#34;&gt;#&lt;/a&gt; 3.1.3 垂直拆分策略&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2dnAS&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2dnAS.png&#34; alt=&#34;pE2dnAS.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;特点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个库的表结构都不一样。&lt;/li&gt;
&lt;li&gt;每个库的数据也不一样 。&lt;/li&gt;
&lt;li&gt;所有，库的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2dQpj&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2dQpj.png&#34; alt=&#34;pE2dQpj.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;特点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个表的结构都不一样。&lt;/li&gt;
&lt;li&gt;每个表的数据也术一样，一般通过一列 (主键 / 外键) 关联。&lt;/li&gt;
&lt;li&gt;所有表的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h6 id=&#34;314-水平拆分策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#314-水平拆分策略&#34;&gt;#&lt;/a&gt; 3.1.4 水平拆分策略&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2d3Xq&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2d3Xq.png&#34; alt=&#34;pE2d3Xq.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;水平分库：以 “字段” 为依据，改为以 “行（记录）” 为依据。讲一个库的数据拆分到多个库&lt;/p&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个库的表结构都一样。&lt;/li&gt;
&lt;li&gt;每个库的数据都不一样。&lt;/li&gt;
&lt;li&gt;所有库的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/8Vp5L6j.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个表的表结构都一样 。&lt;/li&gt;
&lt;li&gt;每个表的数据都不一样 。&lt;/li&gt;
&lt;li&gt;所有表的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/2ctPFwi.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;shardingJDBC：基于 AOP 原理，在应用程序中对本地执行的 SQL 进行拦截，解析、改写、路由处理。需要自行编码配置实现，只支持 java 语言，性能较高。&lt;/li&gt;
&lt;li&gt;MyCat：数据库分库分表中间件，不用调整代码即可实现分库分表，支持多种语言，性能不及前者。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/lgs1r8g.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;h5 id=&#34;32-mycat概述&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-mycat概述&#34;&gt;#&lt;/a&gt; 3.2 Mycat 概述&lt;/h5&gt;
&lt;p&gt;Mycat 是开源的、活跃的、基于 Java 语言编写的&lt;strong&gt; MySQL 数据库中间件&lt;/strong&gt;。可以像使用 mysql 一样来使用 mycat，对于开发人员来说根本感觉不到 mycat 的存在。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/KFB4gQ8.png&#34; alt=&#34;5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性能可靠稳定&lt;/li&gt;
&lt;li&gt;强大的技术团队&lt;/li&gt;
&lt;li&gt;体系完善&lt;/li&gt;
&lt;li&gt;社区活跃&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mycat 是采用 java 语言开发的开源的数据库中间件，支持 Windows 和 Linux 运行环境，下面介绍 MyCat 的 Linux 中的环境搭建。 我们需要在准备好的服务器中安装如下软件。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;服务器&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.213&lt;/td&gt;
&lt;td&gt;JDK、Mycat&lt;/td&gt;
&lt;td&gt;MyCat 中间件服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.210&lt;/td&gt;
&lt;td&gt;MySQL&lt;/td&gt;
&lt;td&gt;分片服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.211&lt;/td&gt;
&lt;td&gt;MySQL&lt;/td&gt;
&lt;td&gt;分片服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.212&lt;/td&gt;
&lt;td&gt;MySQL&lt;/td&gt;
&lt;td&gt;分片服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;JDK 安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#解压jdk
[root@mycat ~]# tar xf jdk-8u371-linux-x64.tar.gz -C /usr/local
[root@mycat ~]# ln -s /usr/local/jdk1.8.0_371/ /usr/local/jdk

# 添加环境变量
[root@mycat ~]# vim /etc/profile.d/jdk.sh 
export JAVA_HOME=/usr/local/jdk
export PATH=$PATH:$JAVA_HOME/bin
export JRE_HOME=$JAVA_HOME/jre 
export CLASSPATH=$JAVA_HOME/lib/:$JRE_HOME/lib/

[root@mycat ~]# source /etc/profile
[root@mycat ~]# java -version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mycat 安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# tar xf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz -C /usr/local/
[root@mycat ~]# ll /usr/local/mycat/
total 12
drwxr-xr-x 2 root root  190 Dec  2 22:15 bin
drwxrwxrwx 2 root root    6 Mar  1  2016 catlet
drwxrwxrwx 4 root root 4096 Dec  2 22:15 conf
drwxr-xr-x 2 root root 4096 Dec  2 22:15 lib
drwxrwxrwx 2 root root    6 Oct 28  2016 logs
-rwxrwxrwx 1 root root  217 Oct 28  2016 version.txt

#上传jar包
[root@mycat ~]# rz /usr/local/mycat/lib/mysql-connector-java-8.0.25.jar
[root@mycat lib]# chmod 777 mysql-connector-java-8.0.25.jar 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/n86yXtx.png&#34; alt=&#34;4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/U26clQE.png&#34; alt=&#34;6.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;321-mycat入门&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#321-mycat入门&#34;&gt;#&lt;/a&gt; 3.2.1 Mycat 入门&lt;/h6&gt;
&lt;p&gt;由于 tb_gorder 表中数据量很大，磁盘 IO 及容量都到达了瓶颈，现在需要对 tb_order 表进行数据分片，分为三个数据节点，每一个节点主机位于不同的服务器上，具体的结构，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/YjmWPQf.png&#34; alt=&#34;5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/PQqJdjJ.png&#34; alt=&#34;8.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;322-mycat配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#322-mycat配置&#34;&gt;#&lt;/a&gt; 3.2.2 Mycat 配置&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/iXUxPhi.png&#34; alt=&#34;9.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;DB01&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;TB_ORDER&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; rule=&amp;quot;auto-sharding-long&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;db01&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;db01&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;db01&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/KkUttwJ.png&#34; alt=&#34;10.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat mycat]# cat /usr/local/mycat/conf/server.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;!-- - - Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;); 
	- you may not use this file except in compliance with the License. - You 
	may obtain a copy of the License at - - http://www.apache.org/licenses/LICENSE-2.0 
	- - Unless required by applicable law or agreed to in writing, software - 
	distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS, - WITHOUT 
	WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. - See the 
	License for the specific language governing permissions and - limitations 
	under the License. --&amp;gt;
&amp;lt;!DOCTYPE mycat:server SYSTEM &amp;quot;server.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:server xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;system&amp;gt;
	&amp;lt;property name=&amp;quot;useSqlStat&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;  &amp;lt;!-- 1为开启实时统计、0为关闭 --&amp;gt;
	&amp;lt;property name=&amp;quot;useGlobleTableCheck&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;  &amp;lt;!-- 1为开启全加班一致性检测、0为关闭 --&amp;gt;

		&amp;lt;property name=&amp;quot;sequnceHandlerType&amp;quot;&amp;gt;2&amp;lt;/property&amp;gt;
      &amp;lt;!--  &amp;lt;property name=&amp;quot;useCompression&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;--&amp;gt; &amp;lt;!--1为开启mysql压缩协议--&amp;gt;
        &amp;lt;!--  &amp;lt;property name=&amp;quot;fakeMySQLVersion&amp;quot;&amp;gt;5.6.20&amp;lt;/property&amp;gt;--&amp;gt; &amp;lt;!--设置模拟的MySQL版本号--&amp;gt;
	&amp;lt;!-- &amp;lt;property name=&amp;quot;processorBufferChunk&amp;quot;&amp;gt;40960&amp;lt;/property&amp;gt; --&amp;gt;
	&amp;lt;!-- 
	&amp;lt;property name=&amp;quot;processors&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt; 
	&amp;lt;property name=&amp;quot;processorExecutor&amp;quot;&amp;gt;32&amp;lt;/property&amp;gt; 
	 --&amp;gt;
		&amp;lt;!--默认为type 0: DirectByteBufferPool | type 1 ByteBufferArena--&amp;gt;
		&amp;lt;property name=&amp;quot;processorBufferPoolType&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;
		&amp;lt;!--默认是65535 64K 用于sql解析时最大文本长度 --&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;maxStringLiteralLength&amp;quot;&amp;gt;65535&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;sequnceHandlerType&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;backSocketNoDelay&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;frontSocketNoDelay&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;processorExecutor&amp;quot;&amp;gt;16&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--
			&amp;lt;property name=&amp;quot;serverPort&amp;quot;&amp;gt;8066&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;managerPort&amp;quot;&amp;gt;9066&amp;lt;/property&amp;gt; 
			&amp;lt;property name=&amp;quot;idleTimeout&amp;quot;&amp;gt;300000&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;bindIp&amp;quot;&amp;gt;0.0.0.0&amp;lt;/property&amp;gt; 
			&amp;lt;property name=&amp;quot;frontWriteQueueSize&amp;quot;&amp;gt;4096&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;processors&amp;quot;&amp;gt;32&amp;lt;/property&amp;gt; --&amp;gt;
		&amp;lt;!--分布式事务开关，0为不过滤分布式事务，1为过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤），2为不过滤分布式事务,但是记录分布式事务日志--&amp;gt;
		&amp;lt;property name=&amp;quot;handleDistributedTransactions&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;
		
			&amp;lt;!--
			off heap for merge/order/group/limit      1开启   0关闭
		--&amp;gt;
		&amp;lt;property name=&amp;quot;useOffHeapForMerge&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;

		&amp;lt;!--
			单位为m
		--&amp;gt;
		&amp;lt;property name=&amp;quot;memoryPageSize&amp;quot;&amp;gt;1m&amp;lt;/property&amp;gt;

		&amp;lt;!--
			单位为k
		--&amp;gt;
		&amp;lt;property name=&amp;quot;spillsFileBufferSize&amp;quot;&amp;gt;1k&amp;lt;/property&amp;gt;

		&amp;lt;property name=&amp;quot;useStreamOutput&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;

		&amp;lt;!--
			单位为m
		--&amp;gt;
		&amp;lt;property name=&amp;quot;systemReserveMemorySize&amp;quot;&amp;gt;384m&amp;lt;/property&amp;gt;


		&amp;lt;!--是否采用zookeeper协调切换  --&amp;gt;
		&amp;lt;property name=&amp;quot;useZKSwitch&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;


	&amp;lt;/system&amp;gt;
	
	&amp;lt;!-- 全局SQL防火墙设置 --&amp;gt;
	&amp;lt;!-- 
	&amp;lt;firewall&amp;gt; 
	   &amp;lt;whitehost&amp;gt;
	      &amp;lt;host host=&amp;quot;127.0.0.1&amp;quot; user=&amp;quot;mycat&amp;quot;/&amp;gt;
	      &amp;lt;host host=&amp;quot;127.0.0.2&amp;quot; user=&amp;quot;mycat&amp;quot;/&amp;gt;
	   &amp;lt;/whitehost&amp;gt;
       &amp;lt;blacklist check=&amp;quot;false&amp;quot;&amp;gt;
       &amp;lt;/blacklist&amp;gt;
	&amp;lt;/firewall&amp;gt;
	--&amp;gt;
	
	&amp;lt;user name=&amp;quot;root&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;DB01&amp;lt;/property&amp;gt;
		
		&amp;lt;!-- 表级 DML 权限设置 --&amp;gt;
		&amp;lt;!-- 		
		&amp;lt;privileges check=&amp;quot;false&amp;quot;&amp;gt;
			&amp;lt;schema name=&amp;quot;TESTDB&amp;quot; dml=&amp;quot;0110&amp;quot; &amp;gt;
				&amp;lt;table name=&amp;quot;tb01&amp;quot; dml=&amp;quot;0000&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
				&amp;lt;table name=&amp;quot;tb02&amp;quot; dml=&amp;quot;1111&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
			&amp;lt;/schema&amp;gt;
		&amp;lt;/privileges&amp;gt;		
		 --&amp;gt;
	&amp;lt;/user&amp;gt;

	&amp;lt;user name=&amp;quot;user&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;DB01&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;readOnly&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;
	&amp;lt;/user&amp;gt;

&amp;lt;/mycat:server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;323-mycat启动&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#323-mycat启动&#34;&gt;#&lt;/a&gt; 3.2.3 Mycat 启动&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;#1.启动mycat
[root@mycat mycat]# ./bin/mycat restart

#2.wrapper.log日志中常见错误
ERROR | wrapper | 2021/1/10 13:31:05 | Startup failed: Timed out waiting for signal from JVM.
ERROR | wrapper | 2021/1/10 13:31:05 | JVM did not exit on request, terminated

#3.启动Mycat超时,前往wrapper.conf配置超时策略
[root@mycat mycat]# vim /usr/local/mycat/conf/wrapper.conf
...
wrapper.startup.timeout=300     //添加此行，超时时间300秒
wrapper.ping.timeout=120

#4.查看mycat是否启动
[root@mycat mycat]# tail -f logs/wrapper.log
...
INFO   | jvm 1    | 2023/12/02 22:53:44 | MyCAT Server startup successfully. see logs in logs/mycat.log
[root@mycat mycat]# netstat -lntp|grep 8066
tcp6       0      0 :::8066                 :::*                    LISTEN      18028/java
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;324-分片测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#324-分片测试&#34;&gt;#&lt;/a&gt; 3.2.4 分片测试&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@db3 ~]#  mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3
Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| DB01     |
+----------+
1 row in set (0.00 sec)

mysql&amp;gt; use DB01;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&amp;gt; show tables;
+----------------+
| Tables in DB01 |
+----------------+
| tb_order       |
+----------------+
1 row in set (0.00 sec)
mysql&amp;gt; CREATE TABLE TB_ORDER(
    -&amp;gt; id BIGINT(20) NOT NULL,
    -&amp;gt; title VARCHAR(100) NOT NULL,
    -&amp;gt; PRIMARY KEY (id)
    -&amp;gt; )ENGINE=INNODB DEFAULT CHARSET=utf8;
Query OK, 0 rows affected (0.04 sec)
 OK!
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(1,&#39;guods1&#39;);
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(2,&#39;guods2&#39;);
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(3,&#39;guods3&#39;);
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(4,&#39;guods4&#39;);
mysql&amp;gt; select * from TB_ORDER;
+------+--------+
| id   | title  |
+------+--------+
|    1 | guods1 |
|    2 | guods2 |
|    3 | guods3 |
|    4 | guods4 |
+------+--------+
4 rows in set (0.03 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;数据写入到 db1 中，因为 mycat 分片规则为 0-50000000 存入节点 1,5000001-10000000 存入节点 2,10000001-15000000 存入节点 3，15000001 以上无法插入数据，需要增加数据节点。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat mycat]# vim conf/rule.xml
...
        &amp;lt;tableRule name=&amp;quot;auto-sharding-long&amp;quot;&amp;gt;
                &amp;lt;rule&amp;gt;
                        &amp;lt;columns&amp;gt;id&amp;lt;/columns&amp;gt;
                        &amp;lt;algorithm&amp;gt;rang-long&amp;lt;/algorithm&amp;gt;
                &amp;lt;/rule&amp;gt;
        &amp;lt;/tableRule&amp;gt;

....
       &amp;lt;function name=&amp;quot;rang-long&amp;quot;
                class=&amp;quot;io.mycat.route.function.AutoPartitionByLong&amp;quot;&amp;gt;
                &amp;lt;property name=&amp;quot;mapFile&amp;quot;&amp;gt;autopartition-long.txt&amp;lt;/property&amp;gt;
        &amp;lt;/function&amp;gt;

...

[root@mycat mycat]# cat conf/autopartition-long.txt
# range start-end ,data node index
# K=1000,M=10000.
0-500M=0
500M-1000M=1

#5000001-10000000存入节点2 
mysql&amp;gt; INSERT INTO TB_ORDER(id,title) VALUES(5000001,&#39;guods5000001&#39;);
Query OK, 1 row affected (0.01 sec)
 OK!
 
#10000001-15000000存入节点3 
mysql&amp;gt; INSERT INTO TB_ORDER(id,title) VALUES(10000001,&#39;guods10000001&#39;);
Query OK, 1 row affected (0.00 sec)
 OK!

#15000001以上无法插入数据，需要增加数据节点
mysql&amp;gt; INSERT INTO TB_ORDER(id,title) VALUES(15000001,&#39;guods15000001&#39;);
ERROR 1064 (HY000): can&#39;t find any valid datanode :TB_ORDER -&amp;gt; ID -&amp;gt; 15000001
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-mycat配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-mycat配置&#34;&gt;#&lt;/a&gt; 3.3 Mycat 配置&lt;/h5&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/I9QLBBR.png&#34; alt=&#34;11.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;331-schema标签&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#331-schema标签&#34;&gt;#&lt;/a&gt; 3.3.1 Schema 标签&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/TmYK7fP.png&#34; alt=&#34;13.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;schema 标签用于定义 MyCat 实例中的逻辑库，一个 MyCat 实例中，可以有多个逻辑库，可以通过 schema 标签来划分不同的逻辑库。MyCat 中的逻辑库的概念，等同于 MySQL 中的 database 概念，需要操作某个逻辑库下的表时也需要切换逻辑库 (use xxx)。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/SGo0DCv.png&#34; alt=&#34;14.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/XtDxXWj.png&#34; alt=&#34;15.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;332-datanode标签&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#332-datanode标签&#34;&gt;#&lt;/a&gt; 3.3.2 Datanode 标签&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/phHZ48F.png&#34; alt=&#34;16.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;333-datahost标签&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#333-datahost标签&#34;&gt;#&lt;/a&gt; 3.3.3 Datahost 标签&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/fXBnwcS.png&#34; alt=&#34;17.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;334-rulexml&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#334-rulexml&#34;&gt;#&lt;/a&gt; 3.3.4 rule.xml&lt;/h6&gt;
&lt;p&gt;rule.xml 中定义所有拆分表的规则，在使用过程中可以灵活的使用分片算法，或者对同一个分片算法使用不同的参数，它让分片过程可配置化。主要包含两类标签： &lt;code&gt;tableRule&lt;/code&gt; 、 &lt;code&gt;Function&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/Ecm1Nvr.png&#34; alt=&#34;18.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;335-serverxml&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#335-serverxml&#34;&gt;#&lt;/a&gt; 3.3.5 server.xml&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/xIDxYpu.png&#34; alt=&#34;19.png&#34; /&gt;&lt;/p&gt;
&lt;h5 id=&#34;34-mycat分片&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-mycat分片&#34;&gt;#&lt;/a&gt; 3.4 Mycat 分片&lt;/h5&gt;
&lt;h6 id=&#34;341-分库分表-mycat分片-垂直分库&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#341-分库分表-mycat分片-垂直分库&#34;&gt;#&lt;/a&gt; 3.4.1 分库分表 - MyCat 分片 - 垂直分库&lt;/h6&gt;
&lt;p&gt;场景：在业务系统中，涉及以下表结构，但是由于用户与订单每天都会产生大量的数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分，原有的数据库表如下。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/plSdAyY.png&#34; alt=&#34;20.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ps: 分库不需要指定 rule，涉及分表需要使用 rule；&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。&lt;br /&gt;
②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。&lt;br /&gt;
③三台服务器关闭防火墙或者开放对应的端口。&lt;br /&gt;
④分别在三台 MySQL 中创建数据库 shopping。&lt;br /&gt;
&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/uMZB18q.png&#34; alt=&#34;21.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;schema.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;server.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/server.xml 
...
	&amp;lt;user name=&amp;quot;root&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING&amp;lt;/property&amp;gt;
		
		&amp;lt;!-- 表级 DML 权限设置 --&amp;gt;
		&amp;lt;!-- 		
		&amp;lt;privileges check=&amp;quot;false&amp;quot;&amp;gt;
			&amp;lt;schema name=&amp;quot;TESTDB&amp;quot; dml=&amp;quot;0110&amp;quot; &amp;gt;
				&amp;lt;table name=&amp;quot;tb01&amp;quot; dml=&amp;quot;0000&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
				&amp;lt;table name=&amp;quot;tb02&amp;quot; dml=&amp;quot;1111&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
			&amp;lt;/schema&amp;gt;
		&amp;lt;/privileges&amp;gt;		
		 --&amp;gt;
	&amp;lt;/user&amp;gt;

	&amp;lt;user name=&amp;quot;user&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;readOnly&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;
	&amp;lt;/user&amp;gt;

&amp;lt;/mycat:server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;分库分表 - MyCat 分片 - 垂直分库 - 测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;垂直分库 - 测试&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.重启mycat
[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...
[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log 
...
INFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log
create database shopping default charset utf8mb4;

#2.在3台节点创建shopping数据库
mysql&amp;gt; create database shopping default charset utf8mb4;
mysql&amp;gt; create database shopping default charset utf8mb4;
mysql&amp;gt; create database shopping default charset utf8mb4;

#3.登入mycat
[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| SHOPPING |
+----------+
1 row in set (0.01 sec)

#4.查看逻辑库
mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| SHOPPING |
+----------+
1 row in set (0.01 sec)

#5.切换到SHOPPING数据库
mysql&amp;gt; use SHOPPING;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed

#6.查看逻辑表
mysql&amp;gt; show tables;
+--------------------+
| Tables in SHOPPING |
+--------------------+
| tb_areas_city      |
| tb_areas_provinces |
| tb_areas_region    |
| tb_goods_base      |
| tb_goods_brand     |
| tb_goods_cat       |
| tb_goods_desc      |
| tb_goods_item      |
| tb_order_item      |
| tb_order_master    |
| tb_order_pay_log   |
| tb_user            |
| tb_user_address    |
+--------------------+
13 rows in set (0.00 sec)

#7.上传shopping-table.sql表结构文件与shopping-insert.sql数据文件

#8.执行shopping-table.sql文件
mysql&amp;gt; source /root/shopping-table.sql

#9.执行shopping-insert.sql文件
mysql&amp;gt; source /root/shopping-insert.sql

#10.查看三个数据库可以发现（根据schema.xml配置文件的配置进行了实现）
①192.168.40.210的数据库中存放了 tb_goods_base、tb_goods_brand、tb_goods_cat、tb_goods_desc、tb_goods_item这五张表
②192.168.40.211的数据库中存放了 tb_order_item、tb_order_master、tb_order_pay_log这三张表；
③192.168.40.212的数据库中存放了 tb_user、tb_user_address、tb_areas_provinces、tb_areas_city、tb_areas_region这五张表
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;exam1: 查询用户的收件人及收件人地址信息 (包含省、市、区)。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;
+-----------+-----------+-----------+-----------+-----------+--------------------+
| user_id   | contact   | province  | city      | area      | address            |
+-----------+-----------+-----------+-----------+-----------+--------------------+
| deng      | 叶问      | 北京市    | 市辖区    | 西城区    | 咏春武馆总部       |
| java00001 | 李佳红    | 北京市    | 市辖区    | 崇文区    | 修正大厦           |
| deng      | 李小龙    | 北京市    | 市辖区    | 崇文区    | 永春武馆           |
| zhaoliu   | 赵三      | 北京市    | 市辖区    | 宣武区    | 西直门             |
| java00001 | 李嘉诚    | 北京市    | 市辖区    | 朝阳区    | 金燕龙办公楼       |
| java00001 | 李佳星    | 北京市    | 市辖区    | 朝阳区    | 中腾大厦           |
+-----------+-----------+-----------+-----------+-----------+--------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ps: 此查询语句只涉及了一个分片所以查询成功&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;exam2: 查询每一笔订单及订单的收件地址信息 (包含省、市、区)。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;
ERROR 1064 (HY000): invalid route in sql, multi tables found but datanode has no intersection  sql:SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ps: 此查询语句涉及多个分片所以查询报错，为了解决这个问题需要进行全局表配置&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全局表配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于省、市、区 / 县表 tb_areas_provinces，tb_areas_city，tb_areas_region，是属于数据字典表，在多个业务模块中都可能会遇到，可以将其设置为全局表，利于业务操作。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/EqJJ3Yv.png&#34; alt=&#34;22.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 修改 MyCat—schema.xml 文件配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;schema.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;

		&amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot;/&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot;/&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. 全局表测试&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.删除3个节点上原有表

#2.重启mycat
[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...
[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log 
...
INFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log
create database shopping default charset utf8mb4;

#3.执行shopping-table.sql文件
[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql&amp;gt; source /root/shopping-table.sql

#4.执行shopping-insert.sql文件
mysql&amp;gt; source /root/shopping-insert.sql

#5 exam1:查询用户的收件人及收件人地址信息(包含省、市、区)。
mysql&amp;gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;

#6 exam2:查询每一笔订单及订单的收件地址信息(包含省、市、区)
mysql&amp;gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;342-分库分表-mycat分片-水平分表&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#342-分库分表-mycat分片-水平分表&#34;&gt;#&lt;/a&gt; 3.4.2 分库分表 - MyCat 分片 - 水平分表&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;水平分表&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;场景&lt;/strong&gt;：在业务系统中，有一张表（日志表），业务系统每天都会产生大量的日志数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/0kMP4Ru.png&#34; alt=&#34;23.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;准备环境：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。&lt;br /&gt;
②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。&lt;br /&gt;
③三台服务器关闭防火墙或者开放对应的端口。&lt;br /&gt;
④分别在三台 MySQL 中创建数据库 itcast。&lt;br /&gt;
&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/zTm8XwU.png&#34; alt=&#34;24.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 三台 MySQL 中创建数据库 itcast&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; create database itcast default charset utf8mb4;
mysql&amp;gt; create database itcast default charset utf8mb4;
mysql&amp;gt; create database itcast default charset utf8mb4;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.MyCat—server.xml 文件配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;

                &amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;

        &amp;lt;schema name=&amp;quot;ITCAST&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
        	&amp;lt;table name=&amp;quot;tb_log&amp;quot; dataNode=&amp;quot;dn4,dn5,dn6&amp;quot; primaryKey=&amp;quot;id&amp;quot; rule=&amp;quot;mod-long&amp;quot; /&amp;gt;
        &amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;

	&amp;lt;dataNode name=&amp;quot;dn4&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn5&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn6&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3.MyCat—server.xml 文件配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/server.xml 
...
	&amp;lt;user name=&amp;quot;root&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING,ITCAST&amp;lt;/property&amp;gt;
		
		&amp;lt;!-- 表级 DML 权限设置 --&amp;gt;
		&amp;lt;!-- 		
		&amp;lt;privileges check=&amp;quot;false&amp;quot;&amp;gt;
			&amp;lt;schema name=&amp;quot;TESTDB&amp;quot; dml=&amp;quot;0110&amp;quot; &amp;gt;
				&amp;lt;table name=&amp;quot;tb01&amp;quot; dml=&amp;quot;0000&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
				&amp;lt;table name=&amp;quot;tb02&amp;quot; dml=&amp;quot;1111&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
			&amp;lt;/schema&amp;gt;
		&amp;lt;/privileges&amp;gt;		
		 --&amp;gt;
	&amp;lt;/user&amp;gt;

	&amp;lt;user name=&amp;quot;user&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING,ITCAST&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;readOnly&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;
	&amp;lt;/user&amp;gt;

&amp;lt;/mycat:server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;4.MyCat 启动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.重启mycat
[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...
[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log 
...
INFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log
create database shopping default charset utf8mb4;

#2.登入mycat
[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| ITCAST   |
| SHOPPING |
+----------+
2 rows in set (0.00 sec)

mysql&amp;gt; use ITCAST;
mysql&amp;gt; show tables;
+------------------+
| Tables in ITCAST |
+------------------+
| tb_log           |
+------------------+

#3.创建表结构及数据导入
mysql&amp;gt; CREATE TABLE tb_log (
    -&amp;gt;   id bigint(20) NOT NULL COMMENT &#39;ID&#39;,
    -&amp;gt;   model_name varchar(200) DEFAULT NULL COMMENT &#39;模块名&#39;,
    -&amp;gt;   model_value varchar(200) DEFAULT NULL COMMENT &#39;模块值&#39;,
    -&amp;gt;   return_value varchar(200) DEFAULT NULL COMMENT &#39;返回值&#39;,
    -&amp;gt;   return_class varchar(200) DEFAULT NULL COMMENT &#39;返回值类型&#39;,
    -&amp;gt;   operate_user varchar(20) DEFAULT NULL COMMENT &#39;操作用户&#39;,
    -&amp;gt;   operate_time varchar(20) DEFAULT NULL COMMENT &#39;操作时间&#39;,
    -&amp;gt;   param_and_value varchar(500) DEFAULT NULL COMMENT &#39;请求参数名及参数值&#39;,
    -&amp;gt;   operate_class varchar(200) DEFAULT NULL COMMENT &#39;操作类&#39;,
    -&amp;gt;   operate_method varchar(200) DEFAULT NULL COMMENT &#39;操作方法&#39;,
    -&amp;gt;   cost_time bigint(20) DEFAULT NULL COMMENT &#39;执行方法耗时, 单位 ms&#39;,
    -&amp;gt;   source int(1) DEFAULT NULL COMMENT &#39;来源 : 1 PC , 2 Android , 3 IOS&#39;,
    -&amp;gt;   PRIMARY KEY (id)
    -&amp;gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
Query OK, 0 rows affected (0.09 sec)
 OK!
查看三个数据库可以发现表和表结构都有了

#4.添加数据
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;1&#39;,&#39;user&#39;,&#39;insert&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:12:28&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;insert&#39;,&#39;10&#39;,1);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;2&#39;,&#39;user&#39;,&#39;insert&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:12:27&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;insert&#39;,&#39;23&#39;,1);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;3&#39;,&#39;user&#39;,&#39;update&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:16:45&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;update&#39;,&#39;34&#39;,1);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;4&#39;,&#39;user&#39;,&#39;update&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:16:45&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;update&#39;,&#39;13&#39;,2);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;5&#39;,&#39;user&#39;,&#39;insert&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:30:31&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;200\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;TomCat\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;0\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;insert&#39;,&#39;29&#39;,3);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;6&#39;,&#39;user&#39;,&#39;find&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:30:31&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;200\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;TomCat\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;0\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;find&#39;,&#39;29&#39;,2);

查看三个数据库内的tb_log表发现有数据了，数据的分布规则是 id模以3的结果为0的数据分布在第一个节点，id模以3的结果为1的数据分布在第二个节点，id模以3的结果为2的数据分布在第三个节点
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-分库分表-分片规则&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-分库分表-分片规则&#34;&gt;#&lt;/a&gt; 3.3 分库分表 - 分片规则&lt;/h5&gt;
&lt;h6 id=&#34;331-分库分表-分片规则-范围分片&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#331-分库分表-分片规则-范围分片&#34;&gt;#&lt;/a&gt; 3.3.1 分库分表 - 分片规则 - 范围分片&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;范围分片&lt;/strong&gt;：根据指定的字段及其配置的范围与数据节点的对应情况，来决定该数据属于哪一个分片。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/sdd8bvs.png&#34; alt=&#34;25.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/R3ecZ4k.png&#34; alt=&#34;26.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/autopartition-long.txt
# range start-end ,data node index
# K=1000,M=10000.
0-500M=0
500M-1000M=1
1000M-1500M=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;332-分库分表-分片规则-取模分片&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#332-分库分表-分片规则-取模分片&#34;&gt;#&lt;/a&gt; 3.3.2 分库分表 - 分片规则 - 取模分片&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;取模分片&lt;/strong&gt;：根据指定的字段值与节点数量进行求模运算，根据运算结果，来决定该数据属于哪一个分片。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/Xvn6sHi.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/aaey4H2.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;333-分库分表-分片规则-一致性hash算法&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#333-分库分表-分片规则-一致性hash算法&#34;&gt;#&lt;/a&gt; 3.3.3 分库分表 - 分片规则 - 一致性 hash 算法&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;一致性 hash 算法&lt;/strong&gt;：所谓一致性哈希，相同的哈希因子计算值总是被划分到相同的分区表中，不会因为分区节点的增加而改变原来数据的分区位置。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/6ANYtsD.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/8i8c5Le.png&#34; alt=&#34;4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一致性 hash 测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;schema.xml 配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;

                &amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;

        &amp;lt;schema name=&amp;quot;ITCAST&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
        	&amp;lt;table name=&amp;quot;tb_log&amp;quot; dataNode=&amp;quot;dn4,dn5,dn6&amp;quot; primaryKey=&amp;quot;id&amp;quot; rule=&amp;quot;mod-long&amp;quot; /&amp;gt;
        	&amp;lt;table name=&amp;quot;tb_order&amp;quot; dataNode=&amp;quot;dn4,dn5,dn6&amp;quot; primaryKey=&amp;quot;id&amp;quot; rule=&amp;quot;sharding-by-murmur&amp;quot; /&amp;gt;
        &amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;

	&amp;lt;dataNode name=&amp;quot;dn4&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn5&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn6&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;rule.xml 配置&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/rule.xml 
...
	&amp;lt;function name=&amp;quot;murmur&amp;quot;
		class=&amp;quot;io.mycat.route.function.PartitionByMurmurHash&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;seed&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;&amp;lt;!-- 默认是0 --&amp;gt;
		&amp;lt;property name=&amp;quot;count&amp;quot;&amp;gt;3&amp;lt;/property&amp;gt;&amp;lt;!-- 要分片的数据库节点数量，必须指定，否则没法分片 --&amp;gt;
		&amp;lt;property name=&amp;quot;virtualBucketTimes&amp;quot;&amp;gt;160&amp;lt;/property&amp;gt;&amp;lt;!-- 一个实际的数据库节点被映射为这么多虚拟节点，默认是160倍，也就是虚拟节点数是物理节点数的160倍 --&amp;gt;
		&amp;lt;!-- &amp;lt;property name=&amp;quot;weightMapFile&amp;quot;&amp;gt;weightMapFile&amp;lt;/property&amp;gt; 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写，以从0开始到count-1的整数值也就是节点索引为key，以节点权重值为值。所有权重值必须是正整数，否则以1代替 --&amp;gt;
		&amp;lt;!-- &amp;lt;property name=&amp;quot;bucketMapPath&amp;quot;&amp;gt;/etc/mycat/bucketMapPath&amp;lt;/property&amp;gt; 
			用于测试时观察各物理节点与虚拟节点的分布情况，如果指定了这个属性，会把虚拟节点的murmur hash值与物理节点的映射按行输出到这个文件，没有默认值，如果不指定，就不会输出任何东西 --&amp;gt;
	&amp;lt;/function&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;重启 mycat 并插入数据测试&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...

[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log
...
INFO   | jvm 1    | 2023/12/03 22:17:47 | MyCAT Server startup successfully. see logs in logs/mycat.log

[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)

mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| ITCAST   |
| SHOPPING |
+----------+
2 rows in set (0.00 sec)

mysql&amp;gt; use ITCAST;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&amp;gt; show tables;
+------------------+
| Tables in ITCAST |
+------------------+
| tb_log           |
| tb_order         |
+------------------+
2 rows in set (0.00 sec)

#创建表结构
create table tb_order(
    id  varchar(100) not null primary key,
    money   int null,
    content varchar(200) null
);

#插入数据
INSERT INTO tb_order (id, money, content) VALUES (&#39;b92fdaaf-6fc4-11ec-b831-482ae33c4a2d&#39;, 10, &#39;b92fdaf8-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b93482b6-6fc4-11ec-b831-482ae33c4a2d&#39;, 20, &#39;b93482d5-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b937e246-6fc4-11ec-b831-482ae33c4a2d&#39;, 50, &#39;b937e25d-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b93be2dd-6fc4-11ec-b831-482ae33c4a2d&#39;, 100, &#39;b93be2f9-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b93f2d68-6fc4-11ec-b831-482ae33c4a2d&#39;, 130, &#39;b93f2d7d-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9451b98-6fc4-11ec-b831-482ae33c4a2d&#39;, 30, &#39;b9451bcc-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9488ec1-6fc4-11ec-b831-482ae33c4a2d&#39;, 560, &#39;b9488edb-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b94be6e6-6fc4-11ec-b831-482ae33c4a2d&#39;, 10, &#39;b94be6ff-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b94ee10d-6fc4-11ec-b831-482ae33c4a2d&#39;, 123, &#39;b94ee12c-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b952492a-6fc4-11ec-b831-482ae33c4a2d&#39;, 145, &#39;b9524945-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b95553ac-6fc4-11ec-b831-482ae33c4a2d&#39;, 543, &#39;b95553c8-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9581cdd-6fc4-11ec-b831-482ae33c4a2d&#39;, 17, &#39;b9581cfa-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b95afc0f-6fc4-11ec-b831-482ae33c4a2d&#39;, 18, &#39;b95afc2a-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b95daa99-6fc4-11ec-b831-482ae33c4a2d&#39;, 134, &#39;b95daab2-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9667e3c-6fc4-11ec-b831-482ae33c4a2d&#39;, 156, &#39;b9667e60-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b96ab489-6fc4-11ec-b831-482ae33c4a2d&#39;, 175, &#39;b96ab4a5-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b96e2942-6fc4-11ec-b831-482ae33c4a2d&#39;, 180, &#39;b96e295b-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b97092ec-6fc4-11ec-b831-482ae33c4a2d&#39;, 123, &#39;b9709306-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b973727a-6fc4-11ec-b831-482ae33c4a2d&#39;, 230, &#39;b9737293-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b978840f-6fc4-11ec-b831-482ae33c4a2d&#39;, 560, &#39;b978843c-6fc4-11ec-b831-482ae33c4a2d&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PS：数据按一致性 hash 分布在不同节点&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/2771271649.html</guid>
            <title>云原生K8s安全专家CKS认证考题详解</title>
            <link>http://ixuyong.cn/posts/2771271649.html</link>
            <category>Kubernetes</category>
            <pubDate>Wed, 09 Apr 2025 21:38:39 +0800</pubDate>
            <description><![CDATA[ &lt;div class=&#34;hbe hbe-container&#34; id=&#34;hexo-blog-encrypt&#34; data-wpm=&#34;抱歉, 这个密码看着不太对, 请再试试。&#34; data-whm=&#34;抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。&#34;&gt;
  &lt;script id=&#34;hbeData&#34; type=&#34;hbeData&#34; data-hmacdigest=&#34;51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d&#34;&gt;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&lt;/script&gt;
  &lt;div class=&#34;hbe hbe-content&#34;&gt;
    &lt;div class=&#34;hbe hbe-input hbe-input-xray&#34;&gt;
      &lt;input class=&#34;hbe hbe-input-field hbe-input-field-xray&#34; type=&#34;password&#34; id=&#34;hbePass&#34;&gt;
      &lt;label class=&#34;hbe hbe-input-label hbe-input-label-xray&#34; for=&#34;hbePass&#34;&gt;
        &lt;span class=&#34;hbe hbe-input-label-content hbe-input-label-content-xray&#34;&gt;您好, 这里需要输入密码。&lt;/span&gt;
      &lt;/label&gt;
      &lt;svg class=&#34;hbe hbe-graphic hbe-graphic-xray&#34; width=&#34;300%&#34; height=&#34;100%&#34; viewBox=&#34;0 0 1200 60&#34; preserveAspectRatio=&#34;none&#34;&gt;
        &lt;path d=&#34;M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
        &lt;path d=&#34;M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
      &lt;/svg&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;script data-pjax src=&#34;/lib/hbe.js&#34;&gt;&lt;/script&gt;&lt;link href=&#34;/css/hbe.style.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt; ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/1414180692.html</guid>
            <title>Redis集群（主从+哨兵）模式</title>
            <link>http://ixuyong.cn/posts/1414180692.html</link>
            <category>Redis</category>
            <pubDate>Wed, 09 Apr 2025 19:50:06 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;redis集群主从哨兵模式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#redis集群主从哨兵模式&#34;&gt;#&lt;/a&gt; Redis 集群（主从 + 哨兵）模式&lt;/h3&gt;
&lt;h3 id=&#34;一-什么是redis主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#一-什么是redis主从复制&#34;&gt;#&lt;/a&gt; 一、什么是 redis 主从复制？&lt;/h3&gt;
&lt;p&gt;主从复制，是指将一台 Redis 服务器的数据，复制到其他的 Redis 服务器。前者称为主节点 (master)，后者称为从节点 (slave), 数据的复制是单向的，只能由主节点到从节点。master 以写为主，slave 以读为主。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgTlKx&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgTlKx.png&#34; alt=&#34;pEgTlKx.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;二-主从复制的作用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#二-主从复制的作用&#34;&gt;#&lt;/a&gt; 二、主从复制的作用&lt;/h3&gt;
&lt;p&gt;数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。&lt;br /&gt;
故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。&lt;br /&gt;
负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。&lt;br /&gt;
读写分离：用于实现读写分离，主库写、从库读，读写分离不仅可以提高服务器的负载能力，同时可根据需求的变化，改变从库的数量；&lt;br /&gt;
高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是 Redis 高可用的基础。&lt;/p&gt;
&lt;h3 id=&#34;三-实现主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#三-实现主从复制&#34;&gt;#&lt;/a&gt; 三、实现主从复制&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;redis01&lt;/td&gt;
&lt;td&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;redis02&lt;/td&gt;
&lt;td&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;slave&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;redis03&lt;/td&gt;
&lt;td&gt;192.168.40.103&lt;/td&gt;
&lt;td&gt;slave&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;31-关闭防火墙-selinux&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-关闭防火墙-selinux&#34;&gt;#&lt;/a&gt; 3.1 关闭防火墙、selinux&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@master01 ~]# hostnamectl set-hostname redis01
[root@redis01 ~]# systemctl stop firewalld
[root@redis01 ~]# systemctl disable firewalld
[root@redis01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@redis01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@redis01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@redis01 ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@redis01 ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@redis01 ~]# ntpdate time2.aliyun.com
[root@redis01 ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/null
[root@redis01 ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;32-安装redis&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-安装redis&#34;&gt;#&lt;/a&gt; 3.2 安装 redis&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 ~]# yum install gcc-c++ -y
[root@redis01 soft]# wget https://download.redis.io/releases/redis-6.2.11.tar.gz
[root@redis01 soft]# tar xf redis-6.2.11.tar.gz 
[root@redis01 soft]# ln -s /soft/redis-6.2.11 /soft/redis
[root@redis01 soft]# cd /soft/redis
[root@redis01 redis]# make            #执行make编译
[root@redis01 redis]# make install    #将 src下的许多可执行文件复制到/usr/local/bin 目录下
[root@redis01 redis]# redis-server /soft/redis/redis.conf &amp;amp;
[root@redis01 redis]# netstat -lntp|grep redis
tcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      69686/redis-server  
tcp6       0      0 ::1:6379                :::*                    LISTEN      69686/redis-server     
[root@redis01 redis]# redis-cli shutdown      #关闭Redis服务
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;33-redis配置文件说明&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-redis配置文件说明&#34;&gt;#&lt;/a&gt; 3.3 redis 配置文件说明&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 redis]# vim redis.conf 
bind 127.0.0.1      		# 绑定的ip
protected-mode yes  		# 保护模式
port 6379           		# 端口设置
daemonize yes               # 后台启动
bind 127.0.0.1      		# 绑定的ip
protected-mode yes  		# 保护模式
port 6379           		# 端口设置
loglevel notice     		# 记录日志级别
logfile &amp;quot;redis.log&amp;quot;         # 日志的文件位置名
dir ./               		# 日志存储目录
databases 16        		# 数据库的数量，默认是 16 个数据库
always-show-logo yes 		# 是否总是显示LOGO

# 如果900s内，如果至少有一个1 key进行了修改，我们及进行持久化操作
save 900 1
# 如果300s内，如果至少10 key进行了修改，我们及进行持久化操作
save 300 10
# 如果60s内，如果至少10000 key进行了修改，我们及进行持久化操作
save 60 10000
# 我们之后学习持久化，会自己定义这个测试！
stop-writes-on-bgsave-error yes   # 持久化如果出错，是否还需要继续工作！
rdbcompression yes                # 是否压缩 rdb 文件，需要消耗一些cpu资源！
rdbchecksum yes                   # 保存rdb文件的时候，进行错误的检查校验！
dbfilename dump.rdb               # rdb 文件保存的名称！
dir ./                            # rdb 文件保存的目录！

slaveof 192.168.1.154 6379        # 配置主从复制
requirepass foobared              # 配置redis登录密码

appendonly no    # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！
appendfilename &amp;quot;appendonly.aof&amp;quot;   # 持久化的文件的名字
# appendfsync always        # 每次修改都会 sync。消耗性能
appendfsync everysec        # 每秒执行一次 sync，可能会丢失这1s的数据！
# appendfsync no            # 不执行 sync，这个时候操作系统自己同步数据，速度最快！
no-appendfsync-on-rewrite   #重写时是否可以运用appendsync，默认no，可以保证数据的安全性
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;34-redis环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-redis环境配置&#34;&gt;#&lt;/a&gt; 3.4 redis 环境配置&lt;/h4&gt;
&lt;p&gt;#修改 maser 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.101 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#修改 slave01 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.102 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
replicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#修改 slave02 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.103 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
replicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;35-启动3台redis服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#35-启动3台redis服务&#34;&gt;#&lt;/a&gt; 3.5 启动 3 台 redis 服务&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#启动redis01
[root@redis01 redis]# redis-server /soft/redis/redis.conf
[root@redis0[root@redis01 redis]# redis-server /soft/redis/redis.conf redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server 

#启动redis02
[root@redis02 redis]# redis-server /soft/redis/redis.conf
[root@redis02 redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server

启动redis03
[root@redis03 redis]# redis-server /soft/redis/redis.conf
[root@redis03 redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;36-查看主从状态&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#36-查看主从状态&#34;&gt;#&lt;/a&gt; 3.6 查看主从状态&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#主节点
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023
192.168.40.101:6379&amp;gt; info replication
# Replication
role:master
connected_slaves:2
slave0:ip=192.168.40.102,port=6379,state=online,offset=616,lag=0
slave1:ip=192.168.40.103,port=6379,state=online,offset=616,lag=0
master_failover_state:no-failover
master_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:616
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:616

#从节点
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.103:6379&amp;gt; info replication
# Replication
role:slave
master_host:192.168.40.101
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:812
slave_repl_offset:812
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:812
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:295
repl_backlog_histlen:518
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;37-测试主从&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#37-测试主从&#34;&gt;#&lt;/a&gt; 3.7 测试主从&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.101:6379&amp;gt; set k1 v1
OK
192.168.40.101:6379&amp;gt; set k2 v2
OK

[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.103:6379&amp;gt; get k1
&amp;quot;v1&amp;quot;
192.168.40.103:6379&amp;gt; get k2
&amp;quot;v2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;注意:&lt;/strong&gt;&lt;br /&gt;
1、主机可以写，从机不能写，只能读。主机中的所有数据都会保存到从机中去。&lt;br /&gt;
2、主机断开连接，从机依旧连接到主机的，但是没有写操作，这个时候，主机如果回来了，从机依旧可以直接获取到主机写的信息！&lt;br /&gt;
3、如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从主机中获取值！&lt;br /&gt;
4、主从复制原理&lt;br /&gt;
 Slave 启动成功连接到 master 后会发送一个 sync 同步命令&lt;br /&gt;
 Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，并完成一次完全同步。&lt;br /&gt;
全量复制：slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。&lt;br /&gt;
增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave，完成同步，但是只要是重新连接 master，一次完全同步（全量复制）将被自动执行！ 主机的数据一定可以在从机中看到。&lt;/p&gt;
&lt;h3 id=&#34;四-哨兵模式搭建&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#四-哨兵模式搭建&#34;&gt;#&lt;/a&gt; 四、哨兵模式搭建&lt;/h3&gt;
&lt;p&gt;1、什么是 redis 哨兵？&lt;br /&gt;
RedisSentinel 是 Redis 的高可用性解决方案，由一个或多个 Sentinel（哨兵）实例组成。它可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，它的主要功能如下：&lt;br /&gt;
监控 (Monitoring)：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。&lt;br /&gt;
通知 (Notification)：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。&lt;br /&gt;
故障迁移：当主服务器不能正常工作时，Sentinel 会自动进行故障迁移，也就是主从切换。&lt;br /&gt;
统一的配置：管理连接者询问 sentinel 取得主从的地址。&lt;/p&gt;
&lt;p&gt;2、哨兵原理是什么？&lt;br /&gt;
Sentinel 使用的算法核心是 Raft 算法，主要用途就是用于分布式系统，系统容错，以及 Leader 选举，每个 Sentinel 都需要定期的执行以下任务：&lt;br /&gt;
每个 Sentinel 会自动发现其他 Sentinel 和从服务器，它以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。&lt;br /&gt;
如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。&lt;br /&gt;
如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有 Sentinel 要以每秒一次的频率确认主服务器的确进入了主观下线状态。&lt;br /&gt;
如果一个主服务器被标记为主观下线， 并且有足够数量的 Sentinel（至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断，那么这个主服务器被标记为客观下线。&lt;br /&gt;
在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO 命令。当一个主服务器 Sentinel 标记为客观下线时，Sentinel 向下线主服务器的所有从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。&lt;br /&gt;
当没有足够数量的 Sentinel 同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向 Sentinel 的 PING 命令返回有效回复时， 主服务器的主管下线状态就会被移除。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgT1r6&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgT1r6.png&#34; alt=&#34;pEgT1r6.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;41-搭建哨兵&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#41-搭建哨兵&#34;&gt;#&lt;/a&gt; 4.1 搭建哨兵&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;在每台服务器上部署一个哨兵，配置方式如下:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# vim sentinel.conf
#端口默认为26379。
port 26379
#关闭保护模式，可以外部访问。
protected-mode no
#设置为后台启动。
daemonize yes
#日志文件。
logfile &amp;quot;/soft/redis/sentinel.log&amp;quot;
#指定服务器IP地址和端口，并且指定当有2台哨兵认为主机挂了，则对主机进行容灾切换。注意:三台哨兵这里的ip配置均为主节点ip 和端口
sentinel monitor mymaster 192.168.40.101 6379 2
#当在Redis实例中开启了requirepass，这里就需要提供密码。
sentinel auth-pass mymaster psw66
#这里设置了主机多少秒无响应，则认为挂了。
sentinel down-after-milliseconds mymaster 3000
#主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的
snetinel parallel-syncs mymaster 1
#故障转移的超时时间，这里设置为三分钟。
sentinel failover-timeout mymaster 180000
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;42-启动三台服务器上的哨兵&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#42-启动三台服务器上的哨兵&#34;&gt;#&lt;/a&gt; 4.2 启动三台服务器上的哨兵&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#启动redis01的sentine
[root@redis01 redis]# redis-sentinel /soft/redis/sentinel.conf
[root@redis01 redis]#  netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      33536/redis-sentine 
tcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server 
tcp6       0      0 :::26379                :::*                    LISTEN      33536/redis-sentine

#启动redis02的sentine
[root@redis02 redis]# redis-sentinel /soft/redis/sentinel.conf
[root@redis02 redis]#  netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      18757/redis-sentine 
tcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server  
tcp6       0      0 :::26379                :::*                    LISTEN      18757/redis-sentine

#启动redis03的sentine
[root@redis03 redis]# redis-sentinel /soft/redis/sentinel.conf                     
[root@redis03 redis]# netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      19745/redis-sentine 
tcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server  
tcp6       0      0 :::26379                :::*                    LISTEN      19745/redis-sentine
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;43-连接客户端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#43-连接客户端&#34;&gt;#&lt;/a&gt; 4.3 连接客户端&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-cli -p 26379
127.0.0.1:26379&amp;gt;  info sentinel
# Sentinel
sentinel_masters:1
sentinel_tilt:0
sentinel_running_scripts:0
sentinel_scripts_queue_length:0
sentinel_simulate_failure_flags:0
master0:name=mymaster,status=ok,address=192.168.40.101:6379,slaves=2,sentinels=3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;44-redis容灾切换&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#44-redis容灾切换&#34;&gt;#&lt;/a&gt; 4.4 redis 容灾切换&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#连接redis客户端
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 
#验证密码
192.168.40.101:6379&amp;gt; auth Superman*2023
OK
#关闭redis服务
192.168.40.101:6379&amp;gt; shutdown
not connected&amp;gt;
#退出客户端
not connected&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;关闭主节点之后，我们去查看哨兵日志:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 ~]# tail -f /soft/redis/sentinel.log 
91936:X 14 Apr 2023 23:26:23.838 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
91936:X 14 Apr 2023 23:26:23.838 # Redis version=6.2.11, bits=64, commit=00000000, modified=0, pid=91936, just started
91936:X 14 Apr 2023 23:26:23.838 # Configuration loaded
91936:X 14 Apr 2023 23:26:23.838 * monotonic clock: POSIX clock_gettime
91936:X 14 Apr 2023 23:26:23.839 * Running mode=sentinel, port=26379.
91936:X 14 Apr 2023 23:26:23.839 # Sentinel ID is 835b4c8544fb250af5fd479f834ee369cc4f388e
91936:X 14 Apr 2023 23:26:23.839 # +monitor master mymaster 192.168.40.101 6379 quorum 2



91936:X 14 Apr 2023 23:31:25.329 # +sdown master mymaster 192.168.40.101 6379   #这里应该是发现主节点宕机
91936:X 14 Apr 2023 23:31:25.359 # +new-epoch 5
91936:X 14 Apr 2023 23:31:25.360 # +vote-for-leader ab43979285cb47b1b459aeb0ab91b63fa9d1a989 5
91936:X 14 Apr 2023 23:31:25.401 # +odown master mymaster 192.168.40.101 6379 #quorum 3/2 两个哨兵都觉得主节点宕机了
91936:X 14 Apr 2023 23:31:25.401 # Next failover delay: I will not start a failover before Fri Apr 14 23:37:25 2023
91936:X 14 Apr 2023 23:31:26.468 # +config-update-from sentinel ab43979285cb47b1b459aeb0ab91b63fa9d1a989 192.168.40.102 26379 @ mymaster 192.168.40.101 6379
91936:X 14 Apr 2023 23:31:26.468 # +switch-master mymaster 192.168.40.101 6379 192.168.40.103 6379 #通过投票选举40.103为新的主节点
91936:X 14 Apr 2023 23:31:26.468 * +slave slave 192.168.40.102:6379 192.168.40.102 6379 @ mymaster 192.168.40.103 6379
91936:X 14 Apr 2023 23:31:26.469 * +slave slave 192.168.40.101:6379 192.168.40.101 6379 @ mymaster 192.168.40.103 6379
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面我们去 40.103 下查看哨兵主从切换是否成功&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103
192.168.40.103:6379&amp;gt; auth Superman*2023
OK
192.168.40.103:6379&amp;gt; info replication
# Replication
role:master   # 40.103变成主节点了
connected_slaves:1   # 下面的从机个数为1
slave0:ip=192.168.40.102,port=6379,state=online,offset=108708,lag=1
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935
master_repl_offset:108851
second_repl_offset:59887
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:108851
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重新连接挂掉的主节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-server redis.conf 
[root@redis01 redis]#  redis-cli -p 6379 -h 192.168.40.101
192.168.40.101:6379&amp;gt; auth Superman*2023
OK
192.168.40.101:6379&amp;gt; info replication
# Replication
role:slave          #主节点连接回来之后自动变成了从节点，并且成功连上了主机
master_host:192.168.40.103
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:130607
slave_repl_offset:130607
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:130607
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:126982
repl_backlog_histlen:3626
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再去主节点确认一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;192.168.40.103:6379&amp;gt; info replication
# Replication
role:master
connected_slaves:2   #两个从节点
slave0:ip=192.168.40.102,port=6379,state=online,offset=147879,lag=1
slave1:ip=192.168.40.101,port=6379,state=online,offset=147879,lag=1
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935
master_repl_offset:148165
second_repl_offset:59887
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:148165
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;五、哨兵模式的优缺点&lt;br /&gt;
 1. 优点&lt;/p&gt;
&lt;p&gt;哨兵集群，基于主从复制模式，所有的主从配置优点，它全有&lt;/p&gt;
&lt;p&gt;主从可以切换，故障可以转移，系统的可用性就会更好&lt;/p&gt;
&lt;p&gt;哨兵模式就是主从模式的升级，手动到自动，更加健壮！&lt;/p&gt;
&lt;p&gt;2. 缺点&lt;/p&gt;
&lt;p&gt;Redis 不好在线扩容，集群容量一旦到达上限，在线扩容就十分麻烦&lt;/p&gt;
&lt;p&gt;哨兵模式的配置繁琐&lt;/p&gt;
&lt;p&gt;3. 哨兵模式的配置文件详解&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Example sentinel.conf
# 哨兵sentinel实例运行的端口 默认26379
port 26379
 
# 哨兵sentinel的工作目录
dir /tmp
 
# 哨兵sentinel监控的redis主节点的 ip port
# master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&amp;quot;.-_&amp;quot;组成。
# quorum 配置多少个sentinel哨兵统一认为master主节点失联 那么这时客观上认为主节点失联了
# sentinel monitor &amp;lt;master-name&amp;gt; &amp;lt;ip&amp;gt; &amp;lt;redis-port&amp;gt; &amp;lt;quorum&amp;gt;
sentinel monitor mymaster 127.0.0.1 6379 2
  
# 当在Redis实例中开启了requirepass foobared 授权密码这样所有连接Redis实例的客户端都要提供 密码
# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码
# sentinel auth-pass &amp;lt;master-name&amp;gt; &amp;lt;password&amp;gt;
sentinel auth-pass mymaster MySUPER--secret-0123passw0rd
 
# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时哨兵主观上认为主节点下线 默认30秒
# sentinel down-after-milliseconds &amp;lt;master-name&amp;gt; &amp;lt;milliseconds&amp;gt;
sentinel down-after-milliseconds mymaster 30000
 
# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长， 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。
# sentinel parallel-syncs &amp;lt;master-name&amp;gt; &amp;lt;numslaves&amp;gt;
sentinel parallel-syncs mymaster 1
 
# 故障转移的超时时间 failover-timeout 可以用在以下这些方面：
#1. 同一个sentinel对同一个master两次failover之间的间隔时间。
#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那 里同步数据时。
#3.当想要取消一个正在进行的failover所需要的时间。
#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时， slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了 # 默认三分钟
# sentinel failover-timeout &amp;lt;master-name&amp;gt; &amp;lt;milliseconds&amp;gt; bilibili：
sentinel failover-timeout mymaster 180000
 
# SCRIPTS EXECUTION
#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知 相关人员。
#对于脚本的运行结果有以下规则：
#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10
#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。
#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。
#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。
#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等）， 将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信 息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配 置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无 法正常启动成功。
#通知脚本
# shell编程
# sentinel notification-script &amp;lt;master-name&amp;gt; &amp;lt;script-path&amp;gt; sentinel
notification-script mymaster /var/redis/notify.sh
 
# 客户端重新配置主节点参数脚本
# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已 经发生改变的信息。
# 以下参数将会在调用脚本时传给脚本:
# &amp;lt;master-name&amp;gt; &amp;lt;role&amp;gt; &amp;lt;state&amp;gt; &amp;lt;from-ip&amp;gt; &amp;lt;from-port&amp;gt; &amp;lt;to-ip&amp;gt; &amp;lt;to-port&amp;gt; # 目前&amp;lt;state&amp;gt;总是“failover”,
# &amp;lt;role&amp;gt;是“leader”或者“observer”中的一个。
# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通 信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。
# sentinel client-reconfig-script &amp;lt;master-name&amp;gt; &amp;lt;script-path&amp;gt; sentinel client-reconfig-
script mymaster /var/redis/reconfig.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;再去看一下 redis 的配置文件和哨兵的配置文件，你会惊讶的发现，里边的配置文件已经被改过来了。&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat redis.con
...
replicaof 192.168.40.103 6379
&lt;/code&gt;&lt;/pre&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3166738000.html</guid>
            <title>Kubeadm高可用安装K8s集群</title>
            <link>http://ixuyong.cn/posts/3166738000.html</link>
            <category>Kubernetes</category>
            <pubDate>Wed, 09 Apr 2025 18:28:34 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;kubeadm高可用安装k8s集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#kubeadm高可用安装k8s集群&#34;&gt;#&lt;/a&gt; Kubeadm 高可用安装 K8s 集群&lt;/h2&gt;
&lt;h4 id=&#34;1-基本配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-基本配置&#34;&gt;#&lt;/a&gt; 1. 基本配置&lt;/h4&gt;
&lt;h5 id=&#34;11-基本环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-基本环境配置&#34;&gt;#&lt;/a&gt; 1.1 基本环境配置&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP 地址&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master 节点 * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived 虚拟 IP（不占用机器）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker 节点 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* 配置信息 *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;系统版本&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 网段&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service 网段&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改主机名（其它节点按需修改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 hosts，修改 /etc/hosts 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 yum 源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 配置基础源
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;必备工具安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭 swap 分区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ntpdate：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;同步时间并配置上海时区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 limit：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;升级系统：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;下载安装所有的源码文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-内核配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-内核配置&#34;&gt;#&lt;/a&gt; 1.2 内核配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ipvsadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 ipvs 模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 ipvs.conf，并配置开机自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# 加入以下内容
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;内核优化配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;应用配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置完内核后，重启机器，之后查看内核模块是否已自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-高可用组件安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-高可用组件安装&#34;&gt;#&lt;/a&gt; 2. 高可用组件安装&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;通过 yum 安装 HAProxy 和 KeepAlived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 HAProxy，需要注意黄色部分的 IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443       #HAProxy监听端口
  bind 127.0.0.1:16443     #HAProxy监听端口
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IP地址
  server k8s-master02	192.168.1.72:6443  check       #API Server IP地址
  server k8s-master03	192.168.1.73:6443  check       #API Server IP地址
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 KeepAlived，需要注意黄色部分的配置。&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #网卡名称
    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #网卡名称
    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #网卡名称
    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置 KeepAlived 健康检查文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置健康检查文件添加执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;启动 haproxy 和 keepalived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;所有节点测试VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld&lt;/li&gt;
&lt;li&gt;所有节点查看 selinux 状态，必须为 disable：getenforce&lt;/li&gt;
&lt;li&gt;master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master 节点查看监听端口：netstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果以上都没有问题，需要确认：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;是否是公有云机器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否是私有云机器（类似 OpenStack）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询&lt;/p&gt;
&lt;h4 id=&#34;3-runtime安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtime安装&#34;&gt;#&lt;/a&gt; 3. Runtime 安装&lt;/h4&gt;
&lt;p&gt;如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。&lt;/p&gt;
&lt;h5 id=&#34;31-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-安装containerd&#34;&gt;#&lt;/a&gt; 3.1 安装 Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置安装源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先配置 Containerd 所需的模块（&lt;mark&gt;所有节点&lt;/mark&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;，配置 Containerd 所需的内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;生成 Containerd 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改 Containerd 的 Cgroup 和 Pause 镜像配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 Containerd，并配置开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 crictl 客户端连接的运行时位置（可选）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-安装kubernetes组件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-安装kubernetes组件&#34;&gt;#&lt;/a&gt; 4 . 安装 Kubernetes 组件&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置源（注意更改版本号）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先在&lt;mark&gt; Master01 节点&lt;/mark&gt;查看最新的 Kubernetes 版本是多少：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum list kubeadm.x86_64 --showduplicates | sort -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;此时 kubelet 是起不来的，日志会有报错不影响！&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-集群初始化&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-集群初始化&#34;&gt;#&lt;/a&gt; 5 . 集群初始化&lt;/h4&gt;
&lt;p&gt;以下操作在&lt;mark&gt; master01&lt;/mark&gt;（注意黄色部分）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.71
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  name: k8s-master01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  certSANs:
  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443
controllerManager: &amp;#123;&amp;#125;
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致
networking:
  dnsDomain: cluster.local
  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突
  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突
scheduler: &amp;#123;&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;master01 节点&lt;/mark&gt;更新 kubeadm 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 new.yaml 文件复制到&lt;mark&gt;其他 master 节点&lt;/mark&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后&lt;mark&gt;所有 Master 节点&lt;/mark&gt;提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config images pull --config /root/new.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正确的反馈信息如下（&lt;em&gt;&lt;strong&gt;* 版本可能不一样 *&lt;/strong&gt;&lt;/em&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml 
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init --config /root/new.yaml  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

# 不要复制文档当中的，要去使用节点生成的
  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&amp;quot;kubeadm init phase upload-certs --upload-certs&amp;quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;配置环境变量，用于访问 Kubernetes 集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF
source /root/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;查看节点状态：（显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE   VERSION
k8s-master01   NotReady   control-plane   24s   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;# kubectl get pods -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-初始化失败排查&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-初始化失败排查&#34;&gt;#&lt;/a&gt; 5.1 初始化失败排查&lt;/h5&gt;
&lt;p&gt;如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/messages | grep -v &amp;quot;not found&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经常出错的原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突&lt;/li&gt;
&lt;li&gt;VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;52-高可用master&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-高可用master&#34;&gt;#&lt;/a&gt; 5.2 高可用 Master&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;其他 master&lt;/strong&gt; 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看当前状态：（如果显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;53-token过期处理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#53-token过期处理&#34;&gt;#&lt;/a&gt; 5.3 Token 过期处理&lt;/h5&gt;
&lt;p&gt;注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。&lt;/p&gt;
&lt;p&gt;Token 过期后生成新的 token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm token create --print-join-command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Master 需要生成 --certificate-key：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase upload-certs  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-node节点的配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-node节点的配置&#34;&gt;#&lt;/a&gt; 6. Node 节点的配置&lt;/h4&gt;
&lt;p&gt;Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点初始化完成后，查看集群状态（NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
k8s-node01     NotReady   &amp;lt;none&amp;gt;          13s     v1.32.3
k8s-node02     NotReady   &amp;lt;none&amp;gt;          10s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-calico组件的安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-calico组件的安装&#34;&gt;#&lt;/a&gt; 7. Calico 组件的安装&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt;/etc/NetworkManager/conf.d/calico.conf&amp;lt;&amp;lt;EOF
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
EOF
systemctl daemon-reload
systemctl restart NetworkManager
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下步骤只在&lt;mark&gt; master01&lt;/mark&gt; 执行（.x 不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x &amp;amp;&amp;amp; cd calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改 Pod 网段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= &#39;&amp;#123;print $NF&amp;#125;&#39;`

sed -i &amp;quot;s#POD_CIDR#$&amp;#123;POD_SUBNET&amp;#125;#g&amp;quot; calico.yaml
kubectl apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看容器和节点状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h
calico-node-7mzmb                          1/1     Running   0          24h
calico-node-ljqnl                          1/1     Running   0          24h
calico-node-njqlb                          1/1     Running   0          24h
calico-node-ph4m4                          1/1     Running   0          24h
calico-node-rx8rl                          1/1     Running   0          24h
coredns-76fccbbb6b-76559                   1/1     Running   0          24h
coredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h
etcd-k8s-master01                          1/1     Running   0          24h
etcd-k8s-master02                          1/1     Running   0          24h
etcd-k8s-master03                          1/1     Running   0          24h
kube-apiserver-k8s-master01                1/1     Running   0          24h
kube-apiserver-k8s-master02                1/1     Running   0          24h
kube-apiserver-k8s-master03                1/1     Running   0          24h
kube-controller-manager-k8s-master01       1/1     Running   0          24h
kube-controller-manager-k8s-master02       1/1     Running   0          24h
kube-controller-manager-k8s-master03       1/1     Running   0          24h
kube-proxy-9dtz4                           1/1     Running   0          24h
kube-proxy-jh7rl                           1/1     Running   0          24h
kube-proxy-jvvwt                           1/1     Running   0          24h
kube-proxy-sh89l                           1/1     Running   0          24h
kube-proxy-t2j49                           1/1     Running   0          24h
kube-scheduler-k8s-master01                1/1     Running   0          24h
kube-scheduler-k8s-master02                1/1     Running   0          24h
kube-scheduler-k8s-master03                1/1     Running   0          24h
metrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时节点全部变为 Ready 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   24h   v1.32.3
k8s-master02   Ready    control-plane   24h   v1.32.3
k8s-master03   Ready    control-plane   24h   v1.32.3
k8s-node01     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
k8s-node02     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-metrics部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-metrics部署&#34;&gt;#&lt;/a&gt; 8. Metrics 部署&lt;/h4&gt;
&lt;p&gt;在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。&lt;/p&gt;
&lt;p&gt;将&lt;mark&gt; Master01 节点&lt;/mark&gt;的 front-proxy-ca.crt 复制到所有 Node 节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt

scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下操作均在&lt;mark&gt; master01 节点&lt;/mark&gt;执行:&lt;/p&gt;
&lt;p&gt;安装 metrics server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/kubeadm-metrics-server

# kubectl  create -f comp.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl top node
NAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
k8s-master01   132m         3%       932Mi           5%          
k8s-master02   131m         3%       845Mi           5%          
k8s-master03   148m         3%       912Mi           5%          
k8s-node01     54m          1%       600Mi           3%          
k8s-node02     49m          1%       602Mi           3%          
[root@k8s-master01 ~]#  kubectl top po -A
NAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   
ingress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            
ingress-nginx          ingress-nginx-controller-r978m               1m           104Mi           
krm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            
krm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             
krm                    nginx-574cf48959-vcfjs                       0m           2Mi             
kube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            
kube-system            calico-node-7mzmb                            16m          176Mi           
kube-system            calico-node-ljqnl                            15m          182Mi           
kube-system            calico-node-njqlb                            19m          180Mi           
kube-system            calico-node-ph4m4                            15m          178Mi           
kube-system            calico-node-rx8rl                            17m          180Mi           
kube-system            coredns-76fccbbb6b-76559                     2m           16Mi            
kube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            
kube-system            etcd-k8s-master01                            22m          86Mi            
kube-system            etcd-k8s-master02                            27m          84Mi            
kube-system            etcd-k8s-master03                            22m          84Mi            
kube-system            kube-apiserver-k8s-master01                  22m          267Mi           
kube-system            kube-apiserver-k8s-master02                  20m          242Mi           
kube-system            kube-apiserver-k8s-master03                  18m          241Mi           
kube-system            kube-controller-manager-k8s-master01         6m           69Mi            
kube-system            kube-controller-manager-k8s-master02         2m           21Mi            
kube-system            kube-controller-manager-k8s-master03         1m           19Mi            
kube-system            kube-proxy-9dtz4                             11m          30Mi            
kube-system            kube-proxy-jh7rl                             1m           27Mi            
kube-system            kube-proxy-jvvwt                             17m          29Mi            
kube-system            kube-proxy-sh89l                             1m           29Mi            
kube-system            kube-proxy-t2j49                             16m          29Mi            
kube-system            kube-scheduler-k8s-master01                  6m           25Mi            
kube-system            kube-scheduler-k8s-master02                  6m           25Mi            
kube-system            kube-scheduler-k8s-master03                  6m           25Mi            
kube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            
kubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            
kubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9-dashboard部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-dashboard部署&#34;&gt;#&lt;/a&gt; 9. Dashboard 部署&lt;/h4&gt;
&lt;h5 id=&#34;91-安装dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#91-安装dashboard&#34;&gt;#&lt;/a&gt; 9.1 安装 Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;92-登录dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#92-登录dashboard&#34;&gt;#&lt;/a&gt; 9.2 登录 dashboard&lt;/h5&gt;
&lt;p&gt;在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更改 dashboard 的 svc 为 NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看端口号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：&lt;/p&gt;
&lt;p&gt;访问 Dashboard：&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建登录 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;10必看一些必须的配置更改&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10必看一些必须的配置更改&#34;&gt;#&lt;/a&gt; 10.【必看】一些必须的配置更改&lt;/h4&gt;
&lt;p&gt;将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：&lt;/p&gt;
&lt;p&gt;在 master01 节点执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
mode: ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新 Kube-Proxy 的 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;验证 Kube-Proxy 模式:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode
ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11必看注意事项&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11必看注意事项&#34;&gt;#&lt;/a&gt; 11.【必看】注意事项&lt;/h4&gt;
&lt;p&gt;注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。&lt;/p&gt;
&lt;p&gt;启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。&lt;/p&gt;
&lt;p&gt;其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。&lt;/p&gt;
&lt;p&gt;kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行更改，更改完成后，可以通过 patch 重启 kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-containerd配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-containerd配置镜像加速&#34;&gt;#&lt;/a&gt; 12. Containerd 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Containerd：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;13-docker配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-docker配置镜像加速&#34;&gt;#&lt;/a&gt; 13. Docker 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/1922841233.html</guid>
            <title>Rsync服务实践</title>
            <link>http://ixuyong.cn/posts/1922841233.html</link>
            <category>rsync</category>
            <pubDate>Sun, 30 Mar 2025 20:45:48 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;ursync服务实践u&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ursync服务实践u&#34;&gt;#&lt;/a&gt; &lt;u&gt;Rsync 服务实践&lt;/u&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;1rsync服务端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1rsync服务端&#34;&gt;#&lt;/a&gt; 1.rsync 服务端&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-rsync客户端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-rsync客户端&#34;&gt;#&lt;/a&gt; 2. rsync 客户端&lt;/h4&gt;
&lt;h5 id=&#34;21-安装rsync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-安装rsync&#34;&gt;#&lt;/a&gt; 2.1 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-配置传输密码&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-配置传输密码&#34;&gt;#&lt;/a&gt; 2.2 配置传输密码&lt;/h5&gt;
&lt;p&gt;方法 1：将密码写入文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]#  echo &#39;your passwd&#39; &amp;gt; /etc/rsync.pass
[root@db01 ~]# cat /etc/rsync.pass 
your passwd
[root@db01 ~]# chmod 600 /etc/rsync.pass
--测试收发数据：
[root@db01 ~]# rsync -avz --password-file=/etc/rsync.pass /root/test rsync_backup@192.168.40.101::backup_file
sending incremental file list

sent 47 bytes  received 20 bytes  134.00 bytes/sec
total size is 0  speedup is 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;方法 2：使用密码环境变量 RSYNC_PASSWORD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# export RSYNC_PASSWORD=&#39;your passwd&#39;
--测试收发数据：
[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file
sending incremental file list

sent 47 bytes  received 20 bytes  134.00 bytes/sec
total size is 0  speedup is 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ursync企业级备份案例u&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ursync企业级备份案例u&#34;&gt;#&lt;/a&gt; &lt;u&gt;Rsync 企业级备份案例&lt;/u&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;客户端需求&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端每天凌晨 3 点备份 MySQL 至 /backup 下以 &amp;quot;主机名_IP 地址_当前时间命名&amp;quot; 的目录中&lt;/li&gt;
&lt;li&gt;客户端推送 /backup 目录下数据备份目录至 Rsync 备份服务器&lt;/li&gt;
&lt;li&gt;客户端只保留最近七天的备份数据，避免浪费磁盘空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;服务端需求&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务端部署 rsync 服务，用于接收用户的备份数据&lt;/li&gt;
&lt;li&gt;服务端每天校验客户端推送过来的数据是否完整，并将结果以邮件的方式发送给管理员&lt;/li&gt;
&lt;li&gt;服务端仅保留 6 个月的备份数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：所有服务器的备份目录均为 /backup，所有脚本存放目录均为 /scripts。&lt;/p&gt;
&lt;h4 id=&#34;1-服务端部署rsync服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-服务端部署rsync服务&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1. 服务端部署 rsync 服务&lt;/strong&gt;&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux-2&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync-2&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件-2&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限-2&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启-2&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-rsync客户端-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-rsync客户端-2&#34;&gt;#&lt;/a&gt; 2. rsync 客户端&lt;/h4&gt;
&lt;h5 id=&#34;21-安装rsync-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-安装rsync-2&#34;&gt;#&lt;/a&gt; 2.1 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-测试客户端备份数据并推送至rsync服务器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-测试客户端备份数据并推送至rsync服务器&#34;&gt;#&lt;/a&gt; 2.2 测试客户端备份数据并推送至 rsync 服务器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# export RSYNC_PASSWORD=&#39;your passwd&#39;
[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-客户端备份数据并推送至rsync服务器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-客户端备份数据并推送至rsync服务器&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.3 客户端备份数据并推送至 rsync 服务器&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# mkdir /scripts
[root@db01 ~]# cat /scripts/mysql_backup.sh 
#!/bin/bash
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin

#1、定义变量
Host=$(hostname)
Ip=$(ifconfig ens192 | awk &#39;NR==2&amp;#123;print $2&amp;#125;&#39;)
Date=$(date +%F)
BackupDir=/backup/mysql
Dest=$&amp;#123;BackupDir&amp;#125;/$&amp;#123;Host&amp;#125;_$&amp;#123;Ip&amp;#125;_$&amp;#123;Date&amp;#125;
FILE_NAME=mysql_backup_`date &#39;+%Y%m%d%H%M%S&#39;`;
OLDBINLOG=/var/lib/mysql/oldbinlog

#2、创建备份目录
if [ ! -d $Dest ];then
  mkdir -p $Dest
fi

#3、备份目录
/usr/bin/mysqldump -u&#39;root&#39; -p&#39;your passwd&#39; nf_flms &amp;gt; $Dest/nf-flms_$&amp;#123;FILE_NAME&amp;#125;.sql
tar -czvf $Dest/$&amp;#123;FILE_NAME&amp;#125;.tar.gz $Dest/nf-flms_$&amp;#123;FILE_NAME&amp;#125;.sql
rm -rf $Dest/*$&amp;#123;FILE_NAME&amp;#125;.sql
echo &amp;quot;Your database backup successfully&amp;quot;

#4、校验
md5sum $Dest/* &amp;gt;$Dest/backup_check_$Date

#5、将备份目录推动到rsync服务端
Rsync_Ip=192.168.1.145
Rsync_user=rsync_backup
Rsync_Module=backup_mysql
export RSYNC_PASSWORD=your passwd
rsync -avz $Dest $Rsync_user@$Rsync_Ip::$Rsync_Module

#6、删除15天备份目录
find $Dest -type d -mtime +15 | xargs rm -rf
echo &amp;quot;remove file  successfully&amp;quot;

[root@db01 ~]# chmod +x /scripts/etc_backup.sh
[root@db01 ~]# crontab -e
00 03 * * * /bin/bash /scripts/mysql_backup.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-服务端校验数据并将结果以邮件发送给管理员&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-服务端校验数据并将结果以邮件发送给管理员&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.4 服务端校验数据并将结果以邮件发送给管理员&lt;/strong&gt;&lt;/h5&gt;
&lt;h6 id=&#34;241-配置邮件服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#241-配置邮件服务&#34;&gt;#&lt;/a&gt; 2.4.1 配置邮件服务&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum -y install mailx
[root@backup ~]# cat /etc/mail.rc      #最后一行插入
set from=373370405@qq.com
set smtp=smtps://smtp.qq.com:465
set smtp-auth-user=373370405@qq.com
set smtp-auth-password=**********   # 发件邮箱的授权码
set smtp-auth=login
set ssl-verify=ignore
set nss-config-dir=/etc/pki/nssdb
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;242-发送邮件测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#242-发送邮件测试&#34;&gt;#&lt;/a&gt; 2.4.2 发送邮件测试&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]#  echo Hello World | mail -s test 373370405@qq.com &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;243-配置脚本校验数据并将结果发送给管理员&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#243-配置脚本校验数据并将结果发送给管理员&#34;&gt;#&lt;/a&gt; 2.4.3 配置脚本校验数据并将结果发送给管理员&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup mysql]# cat /scripts/check_backup.sh 
#!/bin/bash
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin

#1、定义变量
Path=/backup/mysql
Date=$(date +%F)

#2、查看flag文件，并对对文件进行校验,然后将校验的结果保存至result_时间
find $Path -type f -name &amp;quot;backup_check_$&amp;#123;Date&amp;#125;*&amp;quot;|xargs md5sum -c &amp;gt;$Path/result_$&amp;#123;Date&amp;#125;

#3、将校验结果发送邮件给管理员
mail -s &amp;quot;Mysql Backup&amp;quot; 373370405@qq.com &amp;lt;$Path/result_$&amp;#123;Date&amp;#125; &amp;amp;&amp;gt; /dev/null

#4、删除超过7天的校验结果文件，删除超过180天的备份数据文件
find $Path -type f -name &amp;quot;result*&amp;quot; -mtime +7 | xargs rm -rf
find $Path -type f -mtime +180 | xargs rm -rf
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;244-写计划任务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#244-写计划任务&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.4.4 写计划任务&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# chmod +x /scripts/check_backup.sh 
[root@db01 ~]# crontab -e
00 06 * * * /bin/bash /scripts/mysql_backup.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;rsyncsersync实现数据实时同步&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#rsyncsersync实现数据实时同步&#34;&gt;#&lt;/a&gt; Rsync+sersync 实现数据实时同步&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;1rsync服务端-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1rsync服务端-2&#34;&gt;#&lt;/a&gt; 1.rsync 服务端&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux-3&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync-3&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件-3&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限-3&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启-3&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-客户端安装sersync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-客户端安装sersync&#34;&gt;#&lt;/a&gt; 2. 客户端安装 sersync&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;2.1 安装 sercync 依赖&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs ~]# yum install -y inotify-tools rsync
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.2 安装 sercync&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs ~]# mkdir -p /soft
[root@nfs ~]# cd /soft/
[root@nfs ~]# wget https://down.whsir.com/downloads/sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs soft]# tar -xf sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs soft]# mv GNU-Linux-x86 /usr/local/sersync
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-修改配置文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-修改配置文件&#34;&gt;#&lt;/a&gt; 2.3 &lt;strong&gt;修改配置文件&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs soft]# cd /usr/local/sersync/
[root@nfs sersync]# cp confxml.xml confxml.xml.bak
[root@nfs sersync]# vim confxml.xml
...
5    &amp;lt;fileSystem xfs=&amp;quot;true&amp;quot;/&amp;gt;    #第5行 false改为true
13          &amp;lt;delete start=&amp;quot;true&amp;quot;/&amp;gt; #第13-20行 false改为true,#说明：监控以上变化推送
14        &amp;lt;createFolder start=&amp;quot;true&amp;quot;/&amp;gt;
15        &amp;lt;createFile start=&amp;quot;false&amp;quot;/&amp;gt;
16        &amp;lt;closeWrite start=&amp;quot;true&amp;quot;/&amp;gt;
17        &amp;lt;moveFrom start=&amp;quot;true&amp;quot;/&amp;gt;
18        &amp;lt;moveTo start=&amp;quot;true&amp;quot;/&amp;gt;
19        &amp;lt;attrib start=&amp;quot;true&amp;quot;/&amp;gt;
20        &amp;lt;modify start=&amp;quot;true&amp;quot;/&amp;gt;
24        &amp;lt;localpath watch=&amp;quot;/data&amp;quot;&amp;gt;      #监控的本地目录
25             &amp;lt;remote ip=&amp;quot;192.168.1.145&amp;quot; name=&amp;quot;backup_file&amp;quot;/&amp;gt;  #rsync服务端IP和模块名backup_file
30      &amp;lt;commonParams params=&amp;quot;-avz&amp;quot;/&amp;gt;  #rsync命令选项
31      &amp;lt;auth start=&amp;quot;true&amp;quot; users=&amp;quot;rsync_backup&amp;quot; passwordfile=&amp;quot;/etc/rsync.passwd&amp;quot;/&amp;gt; #rsync认证信息
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-生成密码文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-生成密码文件&#34;&gt;#&lt;/a&gt; 2.4 生成密码文件&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# echo &#39;your passwd&#39; &amp;gt; /etc/rsync.passwd
[root@nfs sersync]# chmod 600 /etc/rsync.passwd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-启动sersync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-启动sersync&#34;&gt;#&lt;/a&gt; 2.5 启动 sersync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# ln -s /usr/local/sersync/sersync2 /usr/bin/
[root@nfs sersync]# sersync2 -dro /usr/local/sersync/confxml.xml     #针对配置文件confxml.xml启动sersync
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.5 设置 sersync 开机自启&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@qzj_nfs sersync]# vim /etc/rc.d/rc.local   
/usr/local/sersync/sersync2 -d -r -o  /usr/local/sersync/confxml.xml  #在最后添加一行
[root@qzj_nfs sersync]# chmod +x /etc/rc.d/rc.local
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.6 测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在客户端 /data 目录增删改目录文件，rsync 服务端数据存放目录变化&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@backup backup]# watch ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.7 添加脚本监控 sersync 是否正常运行&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# cat /scripts/check_sersync.sh 
#!/bin/sh
sersync=&amp;quot;/usr/local/sersync/sersync2&amp;quot;
confxml=&amp;quot;/usr/local/sersync/confxml.xml&amp;quot;
status=$(ps aux |grep &#39;sersync2&#39;|grep -v &#39;grep&#39;|wc -l)
if [ $status -eq 0 ];
then
$sersync -d -r -o $confxml &amp;amp;
else
exit 0;
fi

[root@nfs sersync]# chmod +x /scripts/check_sersync.sh
[root@nfs sersync]# crontab -l
*/5 * * * * /usr/bin/sh /scripts/check_sersync.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;补充： 多实例情况&lt;/strong&gt;&lt;/em&gt;&lt;br /&gt;
 1、配置多个 confxml.xml 文件（比如：www、bbs、blog.... 等等）&lt;br /&gt;
2、修改端口、同步路径、模块名称&lt;br /&gt;
 3、根据不同的需求同步对应的实例文件&lt;br /&gt;
 /usr/local/sersync/sersync2 -dro /usr/local/sersync/www_confxml.xml&lt;br /&gt;
/usr/local/sersync/sersync2 -dro /usr/local/sersync/bbs_confxml.xml&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">http://ixuyong.cn/posts/3071070978.html</guid>
            <title>企业级私有仓库Harbor搭建</title>
            <link>http://ixuyong.cn/posts/3071070978.html</link>
            <category>Harbor</category>
            <pubDate>Sun, 30 Mar 2025 16:17:00 +0800</pubDate>
            <description><![CDATA[ &lt;h3 id=&#34;企业级私有仓库harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#企业级私有仓库harbor&#34;&gt;#&lt;/a&gt; 企业级私有仓库 Harbor&lt;/h3&gt;
&lt;p&gt;企业部署 Kuberetes 集群环境之后，我们就可以将原来在传统虚拟机上运行的业务，迁移到 kubernetes 上，让 Kubernetes 通过容器的方式来管理。而一旦我们需要将传统业务使用容器的方式运行起来，就需要构建很多镜像，那么这些镜像就需要有一个专门的位置存储起来，为我们提供镜像上传和镜像下载等功能。但我们不能使用阿里云或者 Dockerhub 等仓库，首先拉取速度比较慢，其次镜像的安全性无法保证，所以就需要部署一个私有的镜像仓库来管理这些容器镜像。同时该仓库还需要提供高可用功能，确保随时都能上传和下载可用的容器镜像。&lt;/p&gt;
&lt;h4 id=&#34;1-关闭防火墙-selinux-环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-关闭防火墙-selinux-环境配置&#34;&gt;#&lt;/a&gt; 1、关闭防火墙、Selinux、环境配置&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# sudo mkdir -p /etc/docker
[root@harbor ~]# hostnamectl set-hostname harbor
[root@harbor ~]# systemctl stop firewalld
[root@harbor ~]# systemctl disable firewalld
[root@harbor ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@harbor ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate -y
[root@harbor ~]# yum update -y
[root@harbor ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-docker安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-docker安装&#34;&gt;#&lt;/a&gt; 2、Docker 安装&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# yum install -y yum-utils device-mapper-persistent-data lvm2
[root@harbor ~]# curl -o /etc/yum.repos.d/docker-ce.repo  https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@harbor ~]# yum list docker-ce --showduplicates |sort -r 
[root@harbor ~]# yum install docker-ce docker-compose -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-配置docker加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-配置docker加速&#34;&gt;#&lt;/a&gt; 3、配置 Docker 加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# sudo mkdir -p /etc/docker
[root@harbor ~]# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
[root@harbor ~]# systemctl enable docker --now
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-安装harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-安装harbor&#34;&gt;#&lt;/a&gt; 4、安装 Harbor&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# cd /soft/
[root@harbor ~]# wget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz
[root@harbor soft]# tar xf harbor-offline-installer-v2.6.1.tgz
[root@harbor soft]# cd harbor
[root@harbor harbor]# vim harbor.yml
hostname: 192.168.1.134
...
#https:
#  # https port for harbor, default is 443
#  port: 443
#  # The path of cert and key files for nginx
#  certificate: /your/certificate/path
#  private_key: /your/private/key/path
...
harbor_admin_password: Harbor12345
[root@harbor harbor]#  ./install.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-配置nginx负载均衡调度&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-配置nginx负载均衡调度&#34;&gt;#&lt;/a&gt; 5、配置 Nginx 负载均衡调度&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@lb ~]# vim s.hmallleasing.com.conf
server &amp;#123;
    listen 443 ssl;
    server_name harbor.hmallleasing.com;
    client_max_body_size 1G; 
    ssl_prefer_server_ciphers on;
    ssl_certificate  /etc/nginx/sslkey/_.hmallleasing.com_chain.crt;
    ssl_certificate_key  /etc/nginx/sslkey/_.hmallleasing.com_key.key;
    location / &amp;#123;
        proxy_pass http://192.168.1.134;
#      include proxy_params;
#        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        proxy_connect_timeout 30;
        proxy_send_timeout 60;
        proxy_read_timeout 60;
        
        proxy_buffering on;
        proxy_buffer_size 32k;
        proxy_buffers 4 128k;
        proxy_temp_file_write_size 10240k;		
        proxy_max_temp_file_size 10240k;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-推送镜像至harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-推送镜像至harbor&#34;&gt;#&lt;/a&gt; 6、推送镜像至 Harbor&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor harbor]# docker tag beae173ccac6 harbor.hmallleasing.com/ops/busybox.v1
[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1
[root@harbor harbor]# docker login harbor.hmallleasing.com
[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-harbor停止与启动&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-harbor停止与启动&#34;&gt;#&lt;/a&gt; 7、Harbor 停止与启动&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#停用Harbor
[root@harbor harbor]# pwd
/soft/harbor
[root@harbor harbor]# docker-compose stop
 #启动Harbor
[root@harbor harbor]# docker-compose up -d
[root@harbor harbor]# docker-compose start
&lt;/code&gt;&lt;/pre&gt;
 ]]></description>
        </item>
    </channel>
</rss>
