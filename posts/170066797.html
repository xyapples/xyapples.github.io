<!-- build time:Tue Jun 10 2025 21:55:34 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon.ico"><link rel="alternate" href="/rss.xml" title="LinuxSre云原生" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="LinuxSre云原生" type="application/atom+xml"><link rel="alternate" type="application/json" title="LinuxSre云原生" href="http://ixuyong.cn/feed.json"><link rel="preconnect" href="https://s4.zstatic.net"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/app.css?v=0.4.17"><link rel="modulepreload" href="/js/chunk-GV364XSK.js"><link rel="modulepreload" href="/js/chunk-NYSE5UKM.js"><link rel="modulepreload" href="/js/chunk-RONCYO2S.js"><link rel="modulepreload" href="/js/chunk-THHXCRSX.js"><link rel="modulepreload" href="/js/chunk-WIQECBEN.js"><link rel="modulepreload" href="/js/comments-DL2IYMPZ.js"><link rel="modulepreload" href="/js/copy-tex-NADCTXPG.js"><link rel="modulepreload" href="/js/post-DA635IH6.js"><link rel="modulepreload" href="/js/quicklink-WEDHL4BA.js"><link rel="modulepreload" href="/js/search-VCZRKTM5.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="modulepreload" href="/js/waline-NNBYRQEE.js"><link rel="stylesheet" href="/css/comments-F4ZGS7LD.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/waline-IDNZKML2.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="preload" href="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" as="image" fetchpriority="high"><meta name="keywords" content="ELKStack"><meta name="description" content="专注于 Linux 运维、云计算、云原⽣等技术"><link rel="canonical" href="http://ixuyong.cn/posts/170066797.html"><title>消费租赁项目Kubernetes基于ELK日志分析与实践</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">消费租赁项目Kubernetes基于ELK日志分析与实践</h1><div class="meta"><span class="item" title="创建时间：2025-05-25 14:35:21"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2025-05-25T14:35:21+08:00">2025-05-25</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>67k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>1:01</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">LinuxSre云原生</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><img src="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" loading="eager" decoding="async" fetchpriority="high" alt="LinuxSre云原生"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ELKStack/" itemprop="item" rel="index" title="分类于ELKStack"><span itemprop="name">ELKStack<meta itemprop="position" content="0"></span></a></span></div><article class="post block" itemscope itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://ixuyong.cn/posts/170066797.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.png"><meta itemprop="name" content="Xu Yong"><meta itemprop="description" content="致力于技术布道、普及前沿技术、打造云原生系列标杆博客, 专注于 Linux 运维、云计算、云原⽣等技术"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="LinuxSre云原生"></span><div class="body md" itemprop="articleBody"><h3 id="消费租赁项目kubernetes基于elk日志分析与实践"><a class="anchor" href="#消费租赁项目kubernetes基于elk日志分析与实践">#</a> 消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</h3><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg" alt="Snipaste_2025-05-25_13-43-46.jpg"></p><h4 id="一-elk创建namespace和secrets"><a class="anchor" href="#一-elk创建namespace和secrets">#</a> 一、ELK 创建 Namespace 和 Secrets</h4><pre><code># kubectl create ns logging
# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd
</code></pre><h4 id="二-交付zookeeper集群至k8s"><a class="anchor" href="#二-交付zookeeper集群至k8s">#</a> 二、交付 Zookeeper 集群至 K8S</h4><h5 id="21-制作zk集群镜像"><a class="anchor" href="#21-制作zk集群镜像">#</a> 2.1 制作 ZK 集群镜像</h5><h6 id="211-dockerfile"><a class="anchor" href="#211-dockerfile">#</a> 2.1.1 Dockerfile</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre

# 1、拷贝Zookeeper压缩包和配置文件
ENV VERSION=3.8.4
ADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /
ADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf

# 2、对Zookeeper文件夹名称重新命名
RUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper

# 3、拷贝eentrpoint的启动脚本文件
ADD ./entrypoint.sh /entrypoint.sh

# 4、暴露Zookeeper端口
EXPOSE 2181 2888 3888

# 5、执行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="212-zoocfg"><a class="anchor" href="#212-zoocfg">#</a> 2.1.2 zoo.cfg</h6><pre><code># cat zoo.cfg 
# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。
tickTime=&#123;ZOOK_TICKTIME&#125;

# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime
initLimit=&#123;ZOOK_INIT_LIMIT&#125;

# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime
syncLimit=&#123;ZOOK_SYNC_LIMIT&#125;
 
# 数据保存目录
dataDir=&#123;ZOOK_DATA_DIR&#125;

# 日志保存目录
dataLogDir=&#123;ZOOK_LOG_DIR&#125;

# 客户端连接端口
clientPort=&#123;ZOOK_CLIENT_PORT&#125;

# 客户端最大连接数。# 根据自己实际情况设置，默认为60个
maxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;

# 客户端获取 zookeeper 服务的当前状态及相关信息
4lw.commands.whitelist=*

# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口
</code></pre><h6 id="213-entrypoint"><a class="anchor" href="#213-entrypoint">#</a> 2.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
#设定变量
ZOOK_BIN_DIR=/zookeeper/bin
ZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg

# 2、对配置文件中的字符串进行变量替换
sed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;

# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来
for server in $&#123;ZOOK_SERVERS&#125;
do
	echo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;
done

# 4、在datadir目录中创建myid的文件，并填入对应的编号
ZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))
echo $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid

#5、前台运行Zookeeper
cd $&#123;ZOOK_BIN_DIR&#125;
./zkServer.sh start-foreground
</code></pre><h6 id="214-构建镜像并推送仓库"><a class="anchor" href="#214-构建镜像并推送仓库">#</a> 2.1.4 构建镜像并推送仓库</h6><pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .
# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4
</code></pre><h5 id="22-迁移zookeeper至k8s"><a class="anchor" href="#22-迁移zookeeper至k8s">#</a> 2.2 迁移 zookeeper 至 K8S</h5><h6 id="221-zookeeper-headless"><a class="anchor" href="#221-zookeeper-headless">#</a> 2.2.1 zookeeper-headless</h6><pre><code># cat 01-zookeeper-headless.yaml 
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: zookeeper
  ports:
  - name: client
    port: 2181
    targetPort: 2181
  - name: leader-follwer
    port: 2888
    targetPort: 2888
  - name: selection
    port: 3888
    targetPort: 3888
</code></pre><h6 id="222-zookeeper-sts"><a class="anchor" href="#222-zookeeper-sts">#</a> 2.2.2 zookeeper-sts</h6><pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper               
  namespace: logging
spec:
  serviceName: &quot;zookeeper-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: [&quot;zookeeper&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: zookeeper
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           
        imagePullPolicy: Always
        ports:
        - name: client
          containerPort: 2181
        - name: leader-follwer
          containerPort: 2888
        - name: selection
          containerPort: 3888
        env:
        - name: ZOOK_SERVERS
          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;
        readinessProbe:         # 就绪探针，不就绪则不介入流量
          exec:
            command:
            - &quot;/bin/bash&quot;
            - &quot;-c&quot;
            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'
          initialDelaySeconds: 5
        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启
          exec:
            command:
            - &quot;/bin/bash&quot;
            - &quot;-c&quot;
            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'
          initialDelaySeconds: 5
        volumeMounts:
        - name: data
          mountPath: /data
          subPath: data
        - name: data
          mountPath: /logs
          subPath: logs
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="223-更新资源清单"><a class="anchor" href="#223-更新资源清单">#</a> 2.2.3 更新资源清单</h6><pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml 
[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml
[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging
NAME          READY   STATUS    RESTARTS   AGE
zookeeper-0   1/1     Running   0          17m
zookeeper-1   1/1     Running   0          14m
zookeeper-2   1/1     Running   0          11m
</code></pre><h6 id="224-检查zookeeper集群状态"><a class="anchor" href="#224-检查zookeeper集群状态">#</a> 2.2.4 检查 zookeeper 集群状态</h6><pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
</code></pre><h6 id="225-连接zookeeper集群"><a class="anchor" href="#225-连接zookeeper集群">#</a> 2.2.5 连接 Zookeeper 集群</h6><pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh
# /zookeeper/bin/zkCli.sh -server zookeeper-svc
[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu
Created /hello
[zk: zookeeper-svc(CONNECTED) 1] get /hello
oldxu
</code></pre><h4 id="三-交付kafka集群至k8s"><a class="anchor" href="#三-交付kafka集群至k8s">#</a> 三、 交付 Kafka 集群至 K8S</h4><h5 id="31-制作kafka集群镜像"><a class="anchor" href="#31-制作kafka集群镜像">#</a> 3.1 制作 Kafka 集群镜像</h5><h6 id="311-dockerfile"><a class="anchor" href="#311-dockerfile">#</a> 3.1.1 Dockerfile</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre

# 1、调整时区
RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \
    echo 'Asia/Shanghai' &gt; /etc/timezone

# 2、拷贝kafka软件以及kafka的配置
ENV VERSION=2.12-2.2.0
ADD ./kafka_$&#123;VERSION&#125;.tgz /
ADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties

# 3、修改kafka的名称
RUN mv /kafka_$&#123;VERSION&#125; /kafka

# 4、启动脚本（修改kafka配置）
ADD ./entrypoint.sh /entrypoint.sh

# 5、暴露kafka端口 9999是jmx的端口
EXPOSE 9092 9999

# 6、运行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="312-serverproperties"><a class="anchor" href="#312-serverproperties">#</a> 3.1.2 server.properties</h6><pre><code class="language-'"># cat server.properties 
############################# Server Basics ############################# 
# broker的id，值为整数，且必须唯一，在一个集群中不能重复
broker.id=&#123;BROKER_ID&#125;

############################# Socket Server Settings ############################# 
# kafka监听端口，默认9092
listeners=PLAINTEXT://&#123;LISTENERS&#125;:9092

# 处理网络请求的线程数量，默认为3个
num.network.threads=3

# 执行磁盘IO操作的线程数量，默认为8个 
num.io.threads=8

# socket服务发送数据的缓冲区大小，默认100KB
socket.send.buffer.bytes=102400

# socket服务接受数据的缓冲区大小，默认100KB
socket.receive.buffer.bytes=102400

# socket服务所能接受的一个请求的最大大小，默认为100M
socket.request.max.bytes=104857600

############################# Log Basics ############################# 
# kafka存储消息数据的目录
log.dirs=&#123;KAFKA_DATA_DIR&#125;

# 每个topic默认的partition
num.partitions=1

# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。
default.replication.factor=3

# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量
num.recovery.threads.per.data.dir=1

############################# Log Flush Policy ############################# 
# 消息刷新到磁盘中的消息条数阈值
log.flush.interval.messages=10000

# 消息刷新到磁盘中的最大时间间隔,1s
log.flush.interval.ms=1000

############################# Log Retention Policy ############################# 
# 日志保留小时数，超时会自动删除，默认为7天
log.retention.hours=168

# 日志保留大小，超出大小会自动删除，默认为1G
#log.retention.bytes=1073741824

# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件
log.segment.bytes=1073741824

# 每隔多长时间检测数据是否达到删除条件,300s
log.retention.check.interval.ms=300000

############################# Zookeeper ############################# 
# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开
zookeeper.connect=&#123;ZOOK_SERVERS&#125;

# 连接zookeeper的超时时间,6s
zookeeper.connection.timeout.ms=6000
</code></pre><h6 id="313-entrypoint"><a class="anchor" href="#313-entrypoint">#</a> 3.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
# 变量
KAFKA_DIR=/kafka
KAFKA_CONF=/kafka/config/server.properties

# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一
BROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))
LISTENERS=$(hostname -i)

# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递
sed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;

# 3、启动Kafka
cd $&#123;KAFKA_DIR&#125;/bin
sed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh
./kafka-server-start.sh ../config/server.properties
</code></pre><h6 id="314-构建镜像并推送仓库"><a class="anchor" href="#314-构建镜像并推送仓库">#</a> 3.1.4 构建镜像并推送仓库</h6><pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2
</code></pre><h5 id="32-迁移kafka集群至k8s"><a class="anchor" href="#32-迁移kafka集群至k8s">#</a> 3.2 迁移 Kafka 集群至 K8S</h5><h6 id="321-kafka-headless"><a class="anchor" href="#321-kafka-headless">#</a> 3.2.1 kafka-headless</h6><pre><code># cat 01-kafka-headless.yaml 
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
  - name: client
    port: 9092
    targetPort: 9092
  - name: jmx
    port: 9999
    targetPort: 9999
</code></pre><h6 id="322-kafka-sts"><a class="anchor" href="#322-kafka-sts">#</a> 3.2.2 kafka-sts</h6><pre><code># cat 02-kafka-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: logging
spec:
  serviceName: &quot;kafka-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: [&quot;kafka&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: kafka
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 
        imagePullPolicy: Always
        ports:
        - name: client
          containerPort: 9092
        - name: jmxport
          containerPort: 9999
        env:
        - name: ZOOK_SERVERS
          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;
        readinessProbe:         # 就绪探针，不就绪则不介入流量
          tcpSocket:
            port: 9092
          initialDelaySeconds: 5
        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启
          tcpSocket:
            port: 9092
          initialDelaySeconds: 5
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="323-更新资源清单"><a class="anchor" href="#323-更新资源清单">#</a> 3.2.3 更新资源清单</h6><pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml 
[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml
[root@k8s-master01 02-kafka]# kubectl get pods -n logging 
NAME          READY   STATUS    RESTARTS       AGE
kafka-0       1/1     Running   0              5m49s
kafka-1       1/1     Running   0              4m43s
kafka-2       1/1     Running   0              3m40s

#查看kafka是否注册到zookeeper
[root@k8s-master01 02-kafka]# kubectl exec -it zookeeper-0 -n logging -- /bin/bash
root@zookeeper-0:/# /zookeeper/bin/zkCli.sh 
[zk: localhost:2181(CONNECTED) 2] get /brokers/ids/1
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.85.201:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.85.201&quot;,&quot;timestamp&quot;:&quot;1748162470218&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
[zk: localhost:2181(CONNECTED) 3] get /brokers/ids/2
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.58.205:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.58.205&quot;,&quot;timestamp&quot;:&quot;1748162532658&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
[zk: localhost:2181(CONNECTED) 4] get /brokers/ids/3
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.195.1:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.195.1&quot;,&quot;timestamp&quot;:&quot;1748162649250&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
</code></pre><h6 id="324-检查kafka集群"><a class="anchor" href="#324-检查kafka集群">#</a> 3.2.4 检查 Kafka 集群</h6><pre><code>1.创建一个topic
root@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu

2.模拟消息发布
root@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu
&gt;hello kubernetes
&gt;hello world

3.模拟消息订阅
root@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning
hello kubernetes
hello world
</code></pre><h4 id="四-交付efak至k8s"><a class="anchor" href="#四-交付efak至k8s">#</a> 四、交付 efak 至 K8S</h4><h5 id="41-制作efak镜像"><a class="anchor" href="#41-制作efak镜像">#</a> 4.1 制作 efak 镜像</h5><h6 id="411-dockerfile"><a class="anchor" href="#411-dockerfile">#</a> 4.1.1 Dockerfile</h6><pre><code>[root@manager 03-efak]# cat Dockerfile 
FROM openjdk:8

# 1、调整时区
RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \
    echo 'Asia/Shanghai' &gt; /etc/timezone

# 2、拷贝kafka软件以及kafka的配置
ENV VERSION=3.0.1
ADD ./efak-web-$&#123;VERSION&#125;-bin.tar.gz /
ADD ./system-config.properties /efak-web-$&#123;VERSION&#125;/conf/system-config.properties

# 3、修改efak的名称
RUN mv /efak-web-$&#123;VERSION&#125; /efak

# 4、环境变量
ENV KE_HOME=/efak
ENV PATH=$PATH:$KE_HOME/bin

# 5、启动脚本（修改kafka配置）
ADD ./entrypoint.sh /entrypoint.sh

# 6、暴露kafka端口 9999是jmx的端口
EXPOSE 8048

# 7、运行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="412-system-config"><a class="anchor" href="#412-system-config">#</a> 4.1.2 system-config</h6><pre><code># cat system-config.properties 
######################################
# 填写 zookeeper集群列表
######################################
efak.zk.cluster.alias=cluster1
cluster1.zk.list=&#123;ZOOK_SERVERS&#125;

######################################
# broker 最大规模数量
######################################
cluster1.efak.broker.size=20

######################################
# zk 客户端线程数
######################################
kafka.zk.limit.size=32

######################################
# EFAK webui 端口
######################################
efak.webui.port=8048

######################################
# kafka offset storage
######################################
cluster1.efak.offset.storage=kafka

######################################
# kafka jmx uri
######################################
cluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi

######################################
# kafka metrics 指标，默认存储15天
######################################
efak.metrics.charts=true
efak.metrics.retain=15

######################################
# kafka sql topic records max
######################################
efak.sql.topic.records.max=5000
efak.sql.topic.preview.records.max=10

######################################
# delete kafka topic token
######################################
efak.topic.token=keadmin

######################################
# kafka sqlite 数据库地址（需要修改存储路径）
######################################
efak.driver=org.sqlite.JDBC
efak.url=jdbc:sqlite:&#123;EFAK_DATA_DIR&#125;/db/ke.db
efak.username=root
efak.password=www.kafka-eagle.org
</code></pre><h6 id="413-entrypoint"><a class="anchor" href="#413-entrypoint">#</a> 4.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
# 1、变量
EFAK_DIR=/efak
EFAK_CONF=/efak/conf/system-config.properties

# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递
sed -i s@&#123;EFAK_DATA_DIR&#125;@$&#123;EFAK_DIR&#125;@g  $&#123;EFAK_CONF&#125;
sed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;EFAK_CONF&#125;

# 3、启动efka
$&#123;EFAK_DIR&#125;/bin/ke.sh start
tail -f $&#123;EFAK_DIR&#125;/logs/ke_console.out
</code></pre><h6 id="414-构建镜像并推送仓库"><a class="anchor" href="#414-构建镜像并推送仓库">#</a> 4.1.4 构建镜像并推送仓库</h6><pre><code># wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0
</code></pre><h5 id="42-迁移efak至k8s"><a class="anchor" href="#42-迁移efak至k8s">#</a> 4.2 迁移 efak 至 K8S</h5><h6 id="421-efak-deploy"><a class="anchor" href="#421-efak-deploy">#</a> 4.2.1 efak-deploy</h6><pre><code># cat 01-efak-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: efak
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: efak
  template:
    metadata:
      labels:
        app: efak
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: efak
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 8048
        env:
        - name: ZOOK_SERVERS
          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;
</code></pre><h6 id="422-efak-service"><a class="anchor" href="#422-efak-service">#</a> 4.2.2 efak-service</h6><pre><code># cat 02-efak-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: efak-svc
  namespace: logging
spec:
  selector:
    app: efak
  ports:
  - port: 8048
    targetPort: 8048
</code></pre><h6 id="423-efak-ingress"><a class="anchor" href="#423-efak-ingress">#</a> 4.2.3 efak-ingress</h6><pre><code># cat 03-efak-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: efak-ingress
  namespace: logging
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;efak.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: efak-svc
            port: 
              number: 8048
</code></pre><h6 id="424-更新资源清单"><a class="anchor" href="#424-更新资源清单">#</a> 4.2.4 更新资源清单</h6><pre><code>[root@k8s-master01 03-efak]# kubectl apply -f 01-efak-deploy.yaml 
[root@k8s-master01 03-efak]# kubectl apply -f 02-efak-service.yaml 
[root@k8s-master01 03-efak]# kubectl apply -f 03-efak-ingress.yaml 
</code></pre><h6 id="425-访问efka"><a class="anchor" href="#425-访问efka">#</a> 4.2.5 访问 efka</h6><p>1、初始用户名密码 admin 123456</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Nq16u4z.png" alt="1.png"></p><p>2、查看 Topics</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/9Bin9cr.png" alt="2.png"></p><p>3、查看 kafka 集群状态</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/U76YIck.png" alt="3.png"></p><p>4、查看 Zookeeper 集群状态</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/cY5LeWx.png" alt="4.png"></p><h4 id="五-交付elastic集群"><a class="anchor" href="#五-交付elastic集群">#</a> 五、交付 Elastic 集群</h4><ul><li>ES 集群是由多个节点组成的，通过 <a target="_blank" rel="noopener" href="http://cluster.name">cluster.name</a> 设置 ES 集群名称，同时用于区分其它的 ES 集群。</li><li>每个节点通过 <a target="_blank" rel="noopener" href="http://node.name">node.name</a> 参数来设定所在集群的节点名称。</li><li>节点使用 discovery.send_hosts 参数来设定集群节点的列表。</li><li>集群在第一次启动时，需要初始化，同时需要指定参与选举的 master 节点 IP，或节点名称。</li><li>每个节点可以通过 node.master:true 设定为 master 角色，通过 node.data:true 设定为 data 角色。</li></ul><pre><code>[root@k8s-master01 ~]# grep &quot;^[a-Z]&quot; /etc/elasticsearch/elasticsearch.yml
# 集群名称cluster.name: my-oldxu
# 节点名称node.name: node1
# 数据存储路径path.data: /var/lib/elasticsearch
# 日志存储路径path.logs: /var/log/elasticsearch
# 监听在本地哪个地址上network.host: 10.0.0.100
# 监听端口http.port: 9200
# 集群主机列表discovery.seed_hosts: [&quot;ip1&quot;, &quot;ip2&quot;, &quot;ip3&quot;]
# 仅第一次启动集群时进行选举（可以填写node.name的名称）cluster.initial_master_nodes: [&quot;node01&quot;, &quot;node02&quot;, &quot;node03&quot;]
</code></pre><h5 id="51-下载elastic镜像"><a class="anchor" href="#51-下载elastic镜像">#</a> 5.1 下载 elastic 镜像</h5><pre><code># docker pull elasticsearch:7.17.6
# docker tag elasticsearch:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6
</code></pre><h5 id="52-交付es-service"><a class="anchor" href="#52-交付es-service">#</a> 5.2 交付 ES-Service</h5><p>创建 es-headlessService，为每个 ES Pod 设定固定的 DNS 名称，无论它是 Master 或是 Data，易或是 Coordinating</p><pre><code># cat 01-es-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: es-svc
  namespace: logging
spec:
  selector:
    app: es
  clusterIP: None
  ports:
  - name: cluster
    port: 9200
    targetPort: 9200
  - name: transport
    port: 9300
    targetPort: 9300
</code></pre><h5 id="53-交付es-master节点"><a class="anchor" href="#53-交付es-master节点">#</a> 5.3 交付 ES-Master 节点</h5><ol><li><p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data ；</p></li><li><p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p></li><li><p>ES 启动是通过 ENV 环境变量传参来完成的；</p><ul><li><p>集群名称、节点名称、角色类型；</p></li><li><p>discovery.seed_hosts 集群地址列表；</p></li><li><p>cluster.initial_master_nodes 初始集群参与选举的 master 节点名称；</p></li></ul></li></ol><pre><code># cat 02-es-master.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-master
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 3           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: master
  template:
    metadata:
      labels:
        app: es
        role: master
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;master&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;true&quot;
        - name: node.data
          value: &quot;false&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
        - name: cluster.initial_master_nodes
          value: &quot;es-master-0,es-master-1,es-master-2&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><pre><code>[root@k8s-master01 04-elasticsearch]# cat 03-es-data.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-data
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 2           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: data
  template:
    metadata:
      labels:
        app: es
        role: data
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;data&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;false&quot;
        - name: node.data
          value: &quot;true&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="54-交付es-data节点"><a class="anchor" href="#54-交付es-data节点">#</a> 5.4 交付 ES-Data 节点</h5><ol><li><p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data</p></li><li><p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p></li><li><p>ES 启动是通过 ENV 环境变量传参来完成的</p><ul><li><p>集群名称、节点名称、角色类型</p></li><li><p>discovery.seed_hosts 集群地址列表</p></li></ul></li></ol><pre><code># cat 03-es-data.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-data
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 2           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: data
  template:
    metadata:
      labels:
        app: es
        role: data
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;data&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;false&quot;
        - name: node.data
          value: &quot;true&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="55-更新资源清单"><a class="anchor" href="#55-更新资源清单">#</a> 5.5 更新资源清单</h5><pre><code>[root@k8s-master01 04-elasticsearch]# kubectl apply -f 01-es-svc.yaml 
[root@k8s-master01 04-elasticsearch]# kubectl apply -f 02-es-master.yaml 
[root@k8s-master01 04-elasticsearch]# kubectl apply -f 03-es-data.yaml 
</code></pre><h5 id="56-验证es集群"><a class="anchor" href="#56-验证es集群">#</a> 5.6 验证 ES 集群</h5><pre><code>#1.解析headlessService获取对应ES集群任一节点的IP地址
# dig @10.96.0.10 es-svc.logging.svc.cluster.local  +short
172.16.58.229
172.16.122.191
172.16.195.21
172.16.122.129
172.16.32.164

#2.通过curl访问ES，检查ES集群是否正常（如果仅交付Master，没有data节点，集群状态可能会Red，因为没有数据节点进行数据存储；）
# curl -XGET &quot;http://172.16.122.129:9200/_cluster/health?pretty&quot;
&#123;
  &quot;cluster_name&quot; : &quot;es-cluster&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 5,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 3,
  &quot;active_shards&quot; : 6,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;delayed_unassigned_shards&quot; : 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0,
  &quot;task_max_waiting_in_queue_millis&quot; : 0,
  &quot;active_shards_percent_as_number&quot; : 100.0
&#125;

#3.查看ES各个节点详情
# curl -XGET &quot;http://172.16.122.129:9200/_cat/nodes&quot;
172.16.122.129 16 33 20 0.38 0.56 0.38 ilmr       - es-master-2
172.16.58.229  66 33 22 0.64 0.66 0.44 ilmr       * es-master-1
172.16.122.191 52 34 15 0.38 0.56 0.38 cdfhilrstw - es-data-0
172.16.195.21  38 35 19 0.38 0.53 0.36 cdfhilrstw - es-data-1
172.16.32.164  31 33 12 0.28 0.50 0.59 ilmr       - es-master-0
</code></pre><h4 id="六-交付kibana可视化"><a class="anchor" href="#六-交付kibana可视化">#</a> 六、交付 Kibana 可视化</h4><h5 id="61-下载kibana镜像"><a class="anchor" href="#61-下载kibana镜像">#</a> 6.1 下载 kibana 镜像</h5><pre><code># docker pull kibana:7.17.6
# docker tag kibana:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6
</code></pre><h5 id="62-kibana-deploy"><a class="anchor" href="#62-kibana-deploy">#</a> 6.2 kibana-deploy</h5><ol><li>Kibana 需要连接 ES 集群，通过 ELASTICSEARCH_HOSTS 变量来传递 ES 集群地址</li><li>kibana 通过 I18N_LOCALE 来传递语言环境</li><li>Kibana 通过 SERVER_PUBLICBASEURL 来传递服务访问的公开地址</li></ol><pre><code># cat 01-kibana-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: kibana
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6 
        resources:
          limits:
            cpu: 1000m
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: '[&quot;http://es-data-0.es-svc:9200&quot;,&quot;http://es-data-1.es-svc:9200&quot;]'
        - name: I18N_LOCALE
          value: &quot;zh-CN&quot;
        - name: SERVER_PUBLICBASEURL
          value: &quot;http://kibana.hmallleasing.com&quot;   #kibana访问UI
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
</code></pre><h5 id="63-kibana-svc"><a class="anchor" href="#63-kibana-svc">#</a> 6.3 kibana-svc</h5><pre><code># cat 02-kibana-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: kibana-svc
  namespace: logging
spec:
  selector:
    app: kibana
  ports:
  - name: web
    port: 5601
    targetPort: 5601
</code></pre><h5 id="64-kibana-ingress"><a class="anchor" href="#64-kibana-ingress">#</a> 6.4 kibana-ingress</h5><pre><code># cat 03-kibana-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana-ingress
  namespace: logging
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;kibana.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana-svc
            port:
              number: 5601
</code></pre><h5 id="65-更新资源清单"><a class="anchor" href="#65-更新资源清单">#</a> 6.5 更新资源清单</h5><pre><code>[root@k8s-master01 05-kibana]# kubectl apply -f 01-kibana-deploy.yaml 
[root@k8s-master01 05-kibana]# kubectl apply -f 02-kibana-svc.yaml 
[root@k8s-master01 05-kibana]# kubectl apply -f 03-kibana-ingress.yaml

[root@k8s-master01 05-kibana]# kubectl get pods -n logging
NAME                      READY   STATUS    RESTARTS   AGE
efak-5cdc74bf59-nrhb4     1/1     Running   0          5h33m
es-data-0                 1/1     Running   0          16m
es-data-1                 1/1     Running   0          15m
es-master-0               1/1     Running   0          17m
es-master-1               1/1     Running   0          15m
es-master-2               1/1     Running   0          12m
kafka-0                   1/1     Running   0          5h39m
kafka-1                   1/1     Running   0          5h39m
kafka-2                   1/1     Running   0          5h38m
kibana-5ccc46864b-ndzx9   1/1     Running   0          118s
zookeeper-0               1/1     Running   0          5h42m
zookeeper-1               1/1     Running   0          5h42m
zookeeper-2               1/1     Running   0          5h41m
</code></pre><h5 id="66-访问kibana"><a class="anchor" href="#66-访问kibana">#</a> 6.6 访问 kibana</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/sUXTx1J.png" alt="1.png"></p><h4 id="七-filebeat-sidecar收集业务应用日志"><a class="anchor" href="#七-filebeat-sidecar收集业务应用日志">#</a> 七、filebeat-sidecar 收集业务应用日志</h4><h5 id="71-部署架构说明"><a class="anchor" href="#71-部署架构说明">#</a> 7.1 部署架构说明</h5><p>对于那些能够将日志输出到本地文件的 Pod，我们可以使用 Sidecar 模式方式运行一个日志采集 Agent，对其进行单独收集日志。</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/u20K1ll.png" alt="1.png"></p><ol><li>首先需要将 Pod 中的业务容器日志输出至本地文件，而后运行一个 Filebeat 边车容器，采集本地路径下的日志；</li><li>Filebeat 容器需要传递如下变量；<ul><li>ENV：了解 Pod 属于隶属于哪个环境；</li><li>PROJECT_NAME：为了后期能在单个索引中区分出不同的项目；</li><li>PodIP：为了让用户清楚该 Pod 属于哪个 IP；</li><li>Node：用于获取该 Pod 所处的节点；</li></ul></li><li>Logstash 根据不同的环境，拉取不同的 topic 数据，然后将数据存储至 ES 对应的索引中；</li><li>Kibana 添加不同环境的 index pattern，而后选择对应环境不同的项目进行日志探索与展示；</li></ol><h5 id="72-sidecar部署思路"><a class="anchor" href="#72-sidecar部署思路">#</a> 7.2 Sidecar 部署思路</h5><ol><li>制作一个业务镜像，要求镜像输出日志至本地；</li><li>制作 Filebeat 镜像，配置 Input、output 等信息；</li><li>采用边车模式运行不同环境的 Pod，确保日志信息能输出至 Kafka 集群；</li><li>准备不同环境下 Logstash 配置文件，而后读取数据写入 ES 集群；</li><li>使用 kibana 添加索引，进行日志探索与展示；</li></ol><h5 id="73-制作filebeat镜像"><a class="anchor" href="#73-制作filebeat镜像">#</a> 7.3 制作 Filebeat 镜像</h5><p><strong>7.3.1 下载 filebeat</strong></p><pre><code>curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.17.6-x86_64.rpm
</code></pre><p><strong>7.3.2 编写 Dockerfile</strong></p><pre><code># cat Dockerfile 
# 1、基础镜像
FROM centos:7

# 2、拷贝filebeat
ENV VERSION=7.17.6
ADD ./filebeat-$&#123;VERSION&#125;-x86_64.rpm /
RUN rpm -ivh /filebeat-$&#123;VERSION&#125;-x86_64.rpm &amp;&amp; \
    rm -f /filebeat-$&#123;VERSION&#125;-x86_64.rpm

# 3、拷贝filebeat配置文件（核心）
ADD ./filebeat.yml /etc/filebeat/filebeat.yml

# 4、拷贝启动脚本
ADD ./entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# 5、执行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><p><strong>7.3.3 编写 entrypoint</strong></p><pre><code># cat entrypoint.sh 
#启动脚本
#1、替换filbeat配置文件中的内容
Beat_Conf=/etc/filebeat/filebeat.yml

sed -i s@&#123;ENV&#125;@$&#123;ENV:-test&#125;@g $&#123;Beat_Conf&#125;
sed -i s@&#123;PodIP&#125;@$&#123;PodIP:-&quot;no-ip&quot;&#125;@g $&#123;Beat_Conf&#125;
sed -i s@&#123;Node&#125;@$&#123;Node:-&quot;none&quot;&#125;@g $&#123;Beat_Conf&#125;
sed -i s@&#123;PROJECT_NAME&#125;@$&#123;PROJECT_NAME:-&quot;no-define&quot;&#125;@g $&#123;Beat_Conf&#125;
sed -i s@&#123;MULTILINE&#125;@$&#123;MULTILINE:-&quot;^\\\d&#123;2&#125;&quot;&#125;@g $&#123;Beat_Conf&#125;		# \\用来转义
sed -i s@&#123;KAFKA_HOSTS&#125;@$&#123;KAFKA_HOSTS&#125;@g $&#123;Beat_Conf&#125;

# 2、运行filebeat
filebeat -e -c /etc/filebeat/filebeat.yml
</code></pre><p><strong>7.3.4 编写 filebeat 配置</strong></p><p>{ENV}：用于定义环境的变量；<br>{PROJECT_NAME}：用于定义项目名称的变量；<br>{MULTILINE}：用于定义多行合并的正则变量；<br>{KAFKA_HOSTS}：用于定义 KAFKA 集群地址的变量；<br>{PodIP}：用于获取该 Pod 地址的变量；<br>{Node}：用于获取该 Pod 所处的节点；</p><pre><code>[root@k8s-master01 filebeat_sidecar_dockerfile]# cat filebeat.yml 
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /logu/*.log
    - /logu/*/*.log
  tags: [&quot;logu&quot;]
  fields:
    topic: &#123;PROJECT_NAME&#125;
    podip: &#123;PodIP&#125;
    node: &#123;Node&#125;
  fields_under_root: true               # 增加的所有字段都为顶级字段

- type: log
  enabled: true
  paths:
    - /logm/*.log
    - /logm/*/*.log
  tags: [&quot;logm&quot;]
  fields:
    topic: &#123;PROJECT_NAME&#125;
    podip: &#123;PodIP&#125;
    node: &#123;Node&#125;
  fields_under_root: true               # 增加的所有字段都为顶级字段
  multiline.pattern: '&#123;MULTILINE&#125;'      
  multiline.negate: true
  multiline.match: after
  multiline.max_lines: 10000    #默认最大合并行为500，可根据实际情况调整。

output.kafka:
  hosts: [&#123;KAFKA_HOSTS&#125;]
  topic: app-&#123;ENV&#125;-%&#123;[topic]&#125;
  required_acks: 1              # 保证消息可靠，0不保证，1等待写入主分区（默认），-1等待写入副本分区
  compression: gzip             # 压缩
  max_message_bytes: 1000000    # 每条消息最大的长度，多余的被删除
</code></pre><p><strong>7.3.5 构建并推送镜像</strong></p><pre><code># docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6
</code></pre><h5 id="74-nf-flms-gateway日志收集"><a class="anchor" href="#74-nf-flms-gateway日志收集">#</a> 7.4 nf-flms-gateway 日志收集</h5><p><strong>7.4.1 创建 Namespace 和 Secrets</strong></p><pre><code># sed -i &quot;s#dev#prod#g&quot; *.yaml
# kubectl create ns prod
# kubectl create secret tls prod-api.hmallleasig.com --key hmallleasing.com.key --cert hmallleasing.com.pem -n prod
# kubectl create secret docker-registry harbor-admin --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd -n prod
</code></pre><p><strong>7.4.2 创建 nf-flms-gateway</strong></p><pre><code># cat 01-nf-flms-gateway.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-gateway
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-gateway
  template:
    metadata:
      labels:
        app: nf-flms-gateway
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-gateway
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-gateway:v2.2 
        command:
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-gateway.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        - name: log
          mountPath: /logs    # 业务容器日志目录
      - name: filebeat
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6 
        imagePullPolicy: Always
        volumeMounts:
        - name: log
          mountPath: /logm    # 匹配多行日志
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ENV
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PodIP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: Node
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: PROJECT_NAME
          value: &quot;nf-flms-gateway&quot;
        - name: KAFKA_HOSTS
          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
      - name: log
        emptyDir: &#123;&#125;
---
apiVersion: v1
kind: Service
metadata:
  name: gateway-svc
  namespace: prod
spec:
  selector:
    app: nf-flms-gateway
  ports:
  - port: 8080
    targetPort: 8080

---

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: gateway-ingress
  namespace: prod
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #禁用https强制跳转
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;prod-api.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: gateway-svc
            port:
              number: 8080
  tls:                  #https
  - hosts:
    - prod-api.hmallleasing.com
    secretName: &quot;prod-api.hmallleasig.com&quot;   #配置默认证书可不添加secretNam
</code></pre><p><strong>7.4.3 检查 KafkaTopic</strong></p><p>1、检查是否有对应的 topic</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/SkS0yA0.png" alt="2.png"></p><p>2、点击对应的 Preview，查看 topic 中的最新数据</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/0SOP2qi.png" alt="3.png"></p><h5 id="75-nf-flms-order日志收集"><a class="anchor" href="#75-nf-flms-order日志收集">#</a> 7.5 nf-flms-order 日志收集</h5><p><strong>7.5.1 创建 PVC</strong></p><p>创建 PVC 存储订单合同、身份证复印件等附件</p><pre><code># cat 02-data-image.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-image
  namespace: prod
spec:
  storageClassName: &quot;nfs-storage&quot;     # 明确指定使用哪个sc的供应商来创建pv
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi                      # 根据业务实际大小进行资源申请
</code></pre><p><strong>7.5.2 创建 nf-flms-order</strong></p><pre><code># cat 02-nf-flms-order.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-order
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-order
  template:
    metadata:
      labels:
        app: nf-flms-order
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-order
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-order:v2.0 
        command:
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-order.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        - name: data-image
          mountPath: /data
        - name: log
          mountPath: /logs    # 业务容器日志目录
      - name: filebeat
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6
        imagePullPolicy: Always
        volumeMounts:
        - name: log
          mountPath: /logm    # 匹配多行日志
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ENV
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PodIP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: Node
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: PROJECT_NAME
          value: &quot;nf-flms-order&quot;
        - name: KAFKA_HOSTS
          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
      - name: data-image
        persistentVolumeClaim:      
          claimName: data-image
      - name: log
        emptyDir: &#123;&#125;
</code></pre><p><strong>7.5.3 检查 KafkaTopic</strong></p><p>1、检查是否有对应的 topic</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/AxF7dhj.png" alt="4.png"></p><p>2、点击对应的 Preview，查看 topic 中的最新数据</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/bYpKAsJ.png" alt="5.png"></p><h5 id="76-nf-flms-statistics日志收集"><a class="anchor" href="#76-nf-flms-statistics日志收集">#</a> 7.6 nf-flms-statistics 日志收集</h5><p><strong>7.6.1 创建 nf-flms-statistics</strong></p><pre><code># cat 03-nf-flms-statistics.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-statistics
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-statistics
  template:
    metadata:
      labels:
        app: nf-flms-statistics
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-statistics
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-statistics:v2.0 
        command: 
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-statistics.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        - name: log
          mountPath: /logs    # 业务容器日志目录
      - name: filebeat
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6
        imagePullPolicy: Always
        volumeMounts:
        - name: log
          mountPath: /logm    # 匹配多行日志
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ENV
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PodIP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: Node
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: PROJECT_NAME
          value: &quot;nf-flms-statistics&quot;
        - name: KAFKA_HOSTS
          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
      - name: log
        emptyDir: &#123;&#125;
</code></pre><p><strong>7.6.2 检查 KafkaTopic</strong></p><p>1、检查是否有对应的 topic</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/kD0B7C3.png" alt="6.png"></p><p>2、点击对应的 Preview，查看 topic 中的最新数据</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/JO9HMip.png" alt="7.png"></p><h5 id="77-nf-flms-system日志收集"><a class="anchor" href="#77-nf-flms-system日志收集">#</a> 7.7 nf-flms-system 日志收集</h5><p><strong>7.7.1 创建 nf-flms-system</strong></p><pre><code># cat 04-nf-flms-system.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-system
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-system
  template:
    metadata:
      labels:
        app: nf-flms-system
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-system
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-system:v2.0 
        command:
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-system.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        livenessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 8080
          failureThreshold: 2
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        - name: log
          mountPath: /logs    # 业务容器日志目录
      - name: filebeat
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6
        imagePullPolicy: Always
        volumeMounts:
        - name: log
          mountPath: /logm    # 匹配多行日志
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ENV
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PodIP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: Node
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: PROJECT_NAME
          value: &quot;nf-flms-system&quot;
        - name: KAFKA_HOSTS
          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
      - name: log
        emptyDir: &#123;&#125;
</code></pre><p><strong>7.7.2 检查 KafkaTopic</strong></p><p>1、检查是否有对应的 topic</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Rj4wUMF.png" alt="1.png"></p><p>2、点击对应的 Preview，查看 topic 中的最新数据</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/JYCPcgP.png" alt="2.png"></p><h5 id="78-nf-flms-openapi日志收集"><a class="anchor" href="#78-nf-flms-openapi日志收集">#</a> 7.8 nf-flms-openapi 日志收集</h5><p><strong>7.8.1 创建 nf-flms-openapi</strong></p><pre><code># cat 06-nf-flms-openapi.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-openapi
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-openapi
  template:
    metadata:
      labels:
        app: nf-flms-openapi
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-openapi
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-openapi:v2.2 
        command: 
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-openapi.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        livenessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 8080
          failureThreshold: 2
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        - name: log
          mountPath: /logs    # 业务容器日志目录
      - name: filebeat
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6
        imagePullPolicy: Always
        volumeMounts:
        - name: log
          mountPath: /logm    # 匹配多行日志
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ENV
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PodIP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: Node
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: PROJECT_NAME
          value: &quot;nf-flms-openapi&quot;
        - name: KAFKA_HOSTS
          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
      - name: log
        emptyDir: &#123;&#125;
</code></pre><p><strong>7.8.2 检查 KafkaTopic</strong></p><p>1、检查是否有对应的 topic</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/esmEJF7.png" alt="3.png"></p><p>2、点击对应的 Preview，查看 topic 中的最新数据</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/2HvC3KR.png" alt="4.png"></p><h5 id="79-nf-flms-ui日志收集"><a class="anchor" href="#79-nf-flms-ui日志收集">#</a> 7.9 nf-flms-ui 日志收集</h5><p><strong>7.9.1 准备 Nginx 配置文件</strong></p><pre><code># cat prod.hmallleasing.com.conf 
server &#123;
        listen 80;
        server_name prod.hmallleasing.com;
        root /code/prod;

        location / &#123;
            index  index.html index.htm;
        &#125;
&#125;

server &#123;
        listen 80;
        server_name prod-api.hmallleasing.com;

        location / &#123;
                proxy_set_header Host $http_host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header REMOTE-HOST $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_pass http://gateway-svc.prod.svc.cluster.local:8080;
        &#125;
&#125;
</code></pre><p><strong>7.9.2 创建 ConfigMap</strong></p><pre><code>kubectl create configmap nf-flms-ui-conf --from-file=./prod.hmallleasing.com.conf -n prod
</code></pre><p><strong>7.9.3 创建 nf-flms-ui</strong></p><pre><code>[root@k8s-master01 06-service-all]# cat 07-ui-deploy-ingress.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-ui
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nf-flms-ui
  template:
    metadata:
      labels:
        app: nf-flms-ui
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-ui
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-ui:v1.0
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 80
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 80
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        volumeMounts:
        - name: ngxconfs
          mountPath: /etc/nginx/conf.d/
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        - name: log
          mountPath: /var/log/nginx/    # 业务容器日志目录
      - name: filebeat
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6
        imagePullPolicy: Always
        volumeMounts:
        - name: log
          mountPath: /logu    # 匹配多行日志
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ENV
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PodIP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: Node
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: PROJECT_NAME
          value: &quot;nf-flms-ui&quot;
        - name: KAFKA_HOSTS
          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'
      volumes:
      - name: ngxconfs
        configMap:
          name: nf-flms-ui-conf
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
      - name: log
        emptyDir: &#123;&#125;
---
apiVersion: v1
kind: Service
metadata:
  name: nf-flms-ui-svc
  namespace: prod
spec:
  selector:
    app: nf-flms-ui
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nf-flms-ui-ingress
  namespace: prod
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #禁用https强制跳转
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;prod.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nf-flms-ui-svc
            port:
              number: 80
  tls:                  #https
  - hosts:
    - prod.hmallleasing.com
    secretName: &quot;prod-api.hmallleasig.com&quot;   #配置默认证书可不添加secretName
</code></pre><p><strong>7.9.2 检查 KafkaTopic</strong></p><p>1、检查是否有对应的 topic</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/9Nh2ywz.png" alt="1.png"></p><p>2、点击对应的 Preview，查看 topic 中的最新数据</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/xB0bjWA.png" alt="2.png"></p><h4 id="八-交付生产环境logstash"><a class="anchor" href="#八-交付生产环境logstash">#</a> 八、交付生产环境 Logstash</h4><h5 id="81-拉取镜像"><a class="anchor" href="#81-拉取镜像">#</a> 8.1 拉取镜像</h5><pre><code># docker pull docker.elastic.co/logstash/logstash-oss:7.17.6
# docker tag docker.elastic.co/logstash/logstash-oss:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
</code></pre><h5 id="82-编写logstash配置"><a class="anchor" href="#82-编写logstash配置">#</a> 8.2 编写 logstash 配置</h5><pre><code>[root@k8s-master01 conf]# cat logstash-prod.conf 
input &#123;
    kafka &#123;
        bootstrap_servers =&gt; &quot;kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092&quot;
        group_id =&gt; &quot;logstash-prod&quot;      # 消费者组名称
        consumer_threads =&gt; &quot;3&quot;          # 理想情况下，配置与分区数一样多的线程，实现均衡
        topics_pattern =&gt; &quot;app-prod-.*&quot;  # 通过正则表达式匹配要订阅的主题
    &#125;
&#125;

filter &#123;
	json &#123;
		source =&gt; &quot;message&quot;
	&#125;
&#125;

output &#123;
    stdout &#123;
        codec =&gt; rubydebug
    &#125;
    elasticsearch &#123;
        hosts =&gt; [&quot;es-data-0.es-svc:9200&quot;,&quot;es-data-1.es-svc:9200&quot;]
        index =&gt; &quot;app-prod-%&#123;+YYYY.MM.dd&#125;&quot;
        template_overwrite =&gt; true
    &#125;
&#125;
</code></pre><h5 id="83-创建生产环境configmap"><a class="anchor" href="#83-创建生产环境configmap">#</a> 8.3 创建生产环境 configmap</h5><pre><code>kubectl create configmap logstash-prod-conf --from-file=logstash.conf=conf/logstash-prod.conf -n logging
</code></pre><h5 id="84-创建生产环境logstash"><a class="anchor" href="#84-创建生产环境logstash">#</a> 8.4 创建生产环境 Logstash</h5><p>1、创建 logstash-svc</p><pre><code># cat 01-logstash-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: logstash-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: logstash
  ports:
  - port: 9600
    targetPort: 9600
</code></pre><p>2、创建 logstash-StatefulSet</p><pre><code># cat 05-logstash-prod-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logstash-prod
  namespace: logging
spec:
  serviceName: &quot;logstash-svc&quot;           # 使用此前创建的svc，则无需重复创建
  replicas: 1
  selector:
    matchLabels:
      app: logstash
      env: prod
  template:
    metadata:
      labels:
        app: logstash
        env: prod
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: logstash
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
        args: [&quot;-f&quot;,&quot;config/logstash.conf&quot;]                     # 启动时指定加载的配置文件
        resources:
          limits:
            memory: 1024Mi
        env:
        - name: PIPELINE_WORKERS
          value: &quot;2&quot;
        - name: PIPELINE_BATCH_SIZE
          value: &quot;10000&quot;
        lifecycle:
          postStart:                                            # 设定JVM
            exec:
              command:
              - &quot;/bin/bash&quot;
              - &quot;-c&quot;
              - &quot;sed -i -e '/^-Xms/c-Xms1024m' -e '/^-Xmx/c-Xmx1024m' /usr/share/logstash/config/jvm.options&quot;
        volumeMounts:
        - name: data                                            # 持久化数据目录
          mountPath: /usr/share/logstash/data
        - name: conf
          mountPath: /usr/share/logstash/config/logstash.conf
          subPath: logstash.conf
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: conf
        configMap:
          name: logstash-prod-conf
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;         
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="85-更新资源清单"><a class="anchor" href="#85-更新资源清单">#</a> 8.5 更新资源清单</h5><pre><code>[root@k8s-master01 08-logstash]# kubectl apply -f 01-logstash-svc.yaml 
[root@k8s-master01 08-logstash]# kubectl apply -f 05-logstash-prod-sts.yaml
</code></pre><h5 id="86-检查es生产环境索引"><a class="anchor" href="#86-检查es生产环境索引">#</a> 8.6 检查 ES 生产环境索引</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/0OV3zd4.png" alt="1.png"></p><h4 id="九-交付测试环境logstash"><a class="anchor" href="#九-交付测试环境logstash">#</a> 九、交付测试环境 Logstash</h4><h5 id="91-拉取镜像"><a class="anchor" href="#91-拉取镜像">#</a> 9.1 拉取镜像</h5><pre><code># docker pull docker.elastic.co/logstash/logstash-oss:7.17.6
# docker tag docker.elastic.co/logstash/logstash-oss:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
</code></pre><h5 id="92-编写logstash配置"><a class="anchor" href="#92-编写logstash配置">#</a> 9.2 编写 logstash 配置</h5><pre><code>[root@k8s-master01 08-logstash]# cat conf/logstash-test.conf 
input &#123;
    kafka &#123;
        bootstrap_servers =&gt; &quot;kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092&quot;
        group_id =&gt; &quot;logstash-test&quot;      # 消费者组名称
        consumer_threads =&gt; &quot;3&quot;          # 理想情况下，配置与分区数一样多的线程，实现均衡
        topics_pattern =&gt; &quot;app-test-.*&quot;  # 通过正则表达式匹配要订阅的主题
    &#125;
&#125;

filter &#123;
	json &#123;
		source =&gt; &quot;message&quot;
	&#125;
&#125;

output &#123;
    stdout &#123;
        codec =&gt; rubydebug
    &#125;
    elasticsearch &#123;
        hosts =&gt; [&quot;es-data-0.es-svc:9200&quot;,&quot;es-data-1.es-svc:9200&quot;]
        index =&gt; &quot;app-test-%&#123;+YYYY.MM.dd&#125;&quot;
        template_overwrite =&gt; true
    &#125;
&#125;
</code></pre><h5 id="93-创建测试环境configmap"><a class="anchor" href="#93-创建测试环境configmap">#</a> 9.3 创建测试环境 configmap</h5><pre><code>kubectl create configmap logstash-test-conf --from-file=logstash.conf=conf/logstash-test.conf -n logging
</code></pre><h5 id="94-创建测试环境logstash"><a class="anchor" href="#94-创建测试环境logstash">#</a> 9.4 创建测试环境 Logstash</h5><p>1、创建 logstash-svc</p><pre><code># cat 01-logstash-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: logstash-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: logstash
  ports:
  - port: 9600
    targetPort: 9600
</code></pre><p>2、创建 logstash-StatefulSet</p><pre><code>[root@k8s-master01 08-logstash]# cat 03-logstash-test-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logstash-test
  namespace: logging
spec:
  serviceName: &quot;logstash-svc&quot;           # 使用此前创建的svc，则无需重复创建
  replicas: 1
  selector:
    matchLabels:
      app: logstash
      env: test
  template:
    metadata:
      labels:
        app: logstash
        env: test
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: logstash
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
        args: [&quot;-f&quot;,&quot;config/logstash.conf&quot;]                     # 启动时指定加载的配置文件
        resources:
          limits:
            memory: 1024Mi
        env:
        - name: PIPELINE_WORKERS
          value: &quot;2&quot;
        - name: PIPELINE_BATCH_SIZE
          value: &quot;10000&quot;
        lifecycle:
          postStart:                                            # 设定JVM
            exec:
              command:
              - &quot;/bin/bash&quot;
              - &quot;-c&quot;
              - &quot;sed -i -e '/^-Xms/c-Xms1024m' -e '/^-Xmx/c-Xmx1024m' /usr/share/logstash/config/jvm.options&quot;
        volumeMounts:
        - name: data                                            # 持久化数据目录
          mountPath: /usr/share/logstash/data
        - name: conf
          mountPath: /usr/share/logstash/config/logstash.conf
          subPath: logstash.conf
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone          
      volumes:
      - name: conf
        configMap:
          name: logstash-test-conf
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;          
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="95-更新资源清单"><a class="anchor" href="#95-更新资源清单">#</a> 9.5 更新资源清单</h5><pre><code>[root@k8s-master01 08-logstash]# kubectl apply -f 01-logstash-svc.yaml 
[root@k8s-master01 08-logstash]# kubectl apply -f 03-logstash-test-sts.yaml
</code></pre><h5 id="96-检查es测试环境索引"><a class="anchor" href="#96-检查es测试环境索引">#</a> 9.6 检查 ES 测试环境索引</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/0OV3zd4.png" alt="1.png"></p><h4 id="十-kibana数据展示"><a class="anchor" href="#十-kibana数据展示">#</a> 十、Kibana 数据展示</h4><h5 id="101-添加生产环境索引"><a class="anchor" href="#101-添加生产环境索引">#</a> 10.1 添加生产环境索引</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/B5uofDF.png" alt="2.png"></p><h5 id="102-查看生产环境数据"><a class="anchor" href="#102-查看生产环境数据">#</a> 10.2 查看生产环境数据</h5><p>kibana-&gt;Discover</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/yY4Y4Sz.png" alt="3.png"></p><p>①点击开发环境索引 -&gt;②选择需要查看的字段 -&gt;③进行 filter 筛选项目 -&gt;④选择对应时间段</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/9i3HsC6.png" alt="4.png"></p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/PPAQczE.png" alt="5.png"></p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/NkOwqO3.png" alt="6.png"></p><h6><a class="anchor" href="#">#</a></h6><div class="tags"><a href="/tags/ELKStack/" rel="tag"><i class="ic i-tag"></i>ELKStack</a></div></div><footer><div class="meta"><span class="icon"><i class="ic i-eye"></i></span><span>此文章已被阅读次数:</span><span class="waline-pageview-count" id="twikoo_visitors" data-path="/posts/170066797.html">正在加载...</span><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于</span><time title="修改时间：2025-06-05 19:03:07" itemprop="dateModified" datetime="2025-06-05T19:03:07+08:00">2025-06-05</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.png" alt="Xu Yong 微信支付"><p>微信支付</p></div><div><img loading="lazy" data-src="/assets/alipay.png" alt="Xu Yong 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者：</strong>Xu Yong<i class="ic i-at"><em>@</em></i>LinuxSre云原生</li><li class="link"><strong>本文链接：</strong><a href="http://ixuyong.cn/posts/170066797.html" title="消费租赁项目Kubernetes基于ELK日志分析与实践">http://ixuyong.cn/posts/170066797.html</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/posts/626047790.html" rel="prev" itemprop="url" data-background-image="https:&#x2F;&#x2F;wp-cdn.4ce.cn&#x2F;v2&#x2F;er1zpXS.jpeg" title="消费租赁系统微服务应用交付实践"><span class="type">上一篇</span><span class="category"><i class="ic i-flag"></i>Kubernetes</span><h3>消费租赁系统微服务应用交付实践</h3></a></div><div class="item right"><a href="/posts/2692178654.html" rel="next" itemprop="url" data-background-image="https:&#x2F;&#x2F;wp-cdn.4ce.cn&#x2F;v2&#x2F;a061dum.jpeg" title="Rabbitmq集群部署"><span class="type">下一篇</span><span class="category"><i class="ic i-flag"></i>Rabbitmq</span><h3>Rabbitmq集群部署</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E7%A7%9F%E8%B5%81%E9%A1%B9%E7%9B%AEkubernetes%E5%9F%BA%E4%BA%8Eelk%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.</span> <span class="toc-text">消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-elk%E5%88%9B%E5%BB%BAnamespace%E5%92%8Csecrets"><span class="toc-number">1.1.</span> <span class="toc-text">一、ELK 创建 Namespace 和 Secrets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E4%BA%A4%E4%BB%98zookeeper%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.2.</span> <span class="toc-text">二、交付 Zookeeper 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#21-%E5%88%B6%E4%BD%9Czk%E9%9B%86%E7%BE%A4%E9%95%9C%E5%83%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 制作 ZK 集群镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#211-dockerfile"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">2.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#212-zoocfg"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">2.1.2 zoo.cfg</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#213-entrypoint"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">2.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#214-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.2.1.4.</span> <span class="toc-text">2.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#22-%E8%BF%81%E7%A7%BBzookeeper%E8%87%B3k8s"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 迁移 zookeeper 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#221-zookeeper-headless"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">2.2.1 zookeeper-headless</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#222-zookeeper-sts"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2.2.2 zookeeper-sts</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#223-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">2.2.3 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#224-%E6%A3%80%E6%9F%A5zookeeper%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">2.2.4 检查 zookeeper 集群状态</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#225-%E8%BF%9E%E6%8E%A5zookeeper%E9%9B%86%E7%BE%A4"><span class="toc-number">1.2.2.5.</span> <span class="toc-text">2.2.5 连接 Zookeeper 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-%E4%BA%A4%E4%BB%98kafka%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.3.</span> <span class="toc-text">三、 交付 Kafka 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#31-%E5%88%B6%E4%BD%9Ckafka%E9%9B%86%E7%BE%A4%E9%95%9C%E5%83%8F"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 制作 Kafka 集群镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#311-dockerfile"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">3.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#312-serverproperties"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">3.1.2 server.properties</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#313-entrypoint"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">3.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#314-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.3.1.4.</span> <span class="toc-text">3.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#32-%E8%BF%81%E7%A7%BBkafka%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 迁移 Kafka 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#321-kafka-headless"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">3.2.1 kafka-headless</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#322-kafka-sts"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">3.2.2 kafka-sts</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#323-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">3.2.3 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#324-%E6%A3%80%E6%9F%A5kafka%E9%9B%86%E7%BE%A4"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">3.2.4 检查 Kafka 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B-%E4%BA%A4%E4%BB%98efak%E8%87%B3k8s"><span class="toc-number">1.4.</span> <span class="toc-text">四、交付 efak 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#41-%E5%88%B6%E4%BD%9Cefak%E9%95%9C%E5%83%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 制作 efak 镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#411-dockerfile"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">4.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#412-system-config"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">4.1.2 system-config</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#413-entrypoint"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">4.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#414-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">4.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#42-%E8%BF%81%E7%A7%BBefak%E8%87%B3k8s"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 迁移 efak 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#421-efak-deploy"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">4.2.1 efak-deploy</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#422-efak-service"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">4.2.2 efak-service</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#423-efak-ingress"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">4.2.3 efak-ingress</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#424-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">4.2.4 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#425-%E8%AE%BF%E9%97%AEefka"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">4.2.5 访问 efka</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94-%E4%BA%A4%E4%BB%98elastic%E9%9B%86%E7%BE%A4"><span class="toc-number">1.5.</span> <span class="toc-text">五、交付 Elastic 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#51-%E4%B8%8B%E8%BD%BDelastic%E9%95%9C%E5%83%8F"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 下载 elastic 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#52-%E4%BA%A4%E4%BB%98es-service"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 交付 ES-Service</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#53-%E4%BA%A4%E4%BB%98es-master%E8%8A%82%E7%82%B9"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 交付 ES-Master 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#54-%E4%BA%A4%E4%BB%98es-data%E8%8A%82%E7%82%B9"><span class="toc-number">1.5.4.</span> <span class="toc-text">5.4 交付 ES-Data 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#55-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.5.5.</span> <span class="toc-text">5.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#56-%E9%AA%8C%E8%AF%81es%E9%9B%86%E7%BE%A4"><span class="toc-number">1.5.6.</span> <span class="toc-text">5.6 验证 ES 集群</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AD-%E4%BA%A4%E4%BB%98kibana%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.6.</span> <span class="toc-text">六、交付 Kibana 可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#61-%E4%B8%8B%E8%BD%BDkibana%E9%95%9C%E5%83%8F"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 下载 kibana 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#62-kibana-deploy"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 kibana-deploy</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#63-kibana-svc"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 kibana-svc</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#64-kibana-ingress"><span class="toc-number">1.6.4.</span> <span class="toc-text">6.4 kibana-ingress</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#65-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.6.5.</span> <span class="toc-text">6.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#66-%E8%AE%BF%E9%97%AEkibana"><span class="toc-number">1.6.6.</span> <span class="toc-text">6.6 访问 kibana</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%83-filebeat-sidecar%E6%94%B6%E9%9B%86%E4%B8%9A%E5%8A%A1%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97"><span class="toc-number">1.7.</span> <span class="toc-text">七、filebeat-sidecar 收集业务应用日志</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#71-%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 部署架构说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#72-sidecar%E9%83%A8%E7%BD%B2%E6%80%9D%E8%B7%AF"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 Sidecar 部署思路</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#73-%E5%88%B6%E4%BD%9Cfilebeat%E9%95%9C%E5%83%8F"><span class="toc-number">1.7.3.</span> <span class="toc-text">7.3 制作 Filebeat 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#74-nf-flms-gateway%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86"><span class="toc-number">1.7.4.</span> <span class="toc-text">7.4 nf-flms-gateway 日志收集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#75-nf-flms-order%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86"><span class="toc-number">1.7.5.</span> <span class="toc-text">7.5 nf-flms-order 日志收集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#76-nf-flms-statistics%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86"><span class="toc-number">1.7.6.</span> <span class="toc-text">7.6 nf-flms-statistics 日志收集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#77-nf-flms-system%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86"><span class="toc-number">1.7.7.</span> <span class="toc-text">7.7 nf-flms-system 日志收集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#78-nf-flms-openapi%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86"><span class="toc-number">1.7.8.</span> <span class="toc-text">7.8 nf-flms-openapi 日志收集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#79-nf-flms-ui%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86"><span class="toc-number">1.7.9.</span> <span class="toc-text">7.9 nf-flms-ui 日志收集</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AB-%E4%BA%A4%E4%BB%98%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83logstash"><span class="toc-number">1.8.</span> <span class="toc-text">八、交付生产环境 Logstash</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#81-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F"><span class="toc-number">1.8.1.</span> <span class="toc-text">8.1 拉取镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#82-%E7%BC%96%E5%86%99logstash%E9%85%8D%E7%BD%AE"><span class="toc-number">1.8.2.</span> <span class="toc-text">8.2 编写 logstash 配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#83-%E5%88%9B%E5%BB%BA%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83configmap"><span class="toc-number">1.8.3.</span> <span class="toc-text">8.3 创建生产环境 configmap</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#84-%E5%88%9B%E5%BB%BA%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83logstash"><span class="toc-number">1.8.4.</span> <span class="toc-text">8.4 创建生产环境 Logstash</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#85-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.8.5.</span> <span class="toc-text">8.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#86-%E6%A3%80%E6%9F%A5es%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%B4%A2%E5%BC%95"><span class="toc-number">1.8.6.</span> <span class="toc-text">8.6 检查 ES 生产环境索引</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B9%9D-%E4%BA%A4%E4%BB%98%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83logstash"><span class="toc-number">1.9.</span> <span class="toc-text">九、交付测试环境 Logstash</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#91-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F"><span class="toc-number">1.9.1.</span> <span class="toc-text">9.1 拉取镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#92-%E7%BC%96%E5%86%99logstash%E9%85%8D%E7%BD%AE"><span class="toc-number">1.9.2.</span> <span class="toc-text">9.2 编写 logstash 配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#93-%E5%88%9B%E5%BB%BA%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83configmap"><span class="toc-number">1.9.3.</span> <span class="toc-text">9.3 创建测试环境 configmap</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#94-%E5%88%9B%E5%BB%BA%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83logstash"><span class="toc-number">1.9.4.</span> <span class="toc-text">9.4 创建测试环境 Logstash</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#95-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.9.5.</span> <span class="toc-text">9.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#96-%E6%A3%80%E6%9F%A5es%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E7%B4%A2%E5%BC%95"><span class="toc-number">1.9.6.</span> <span class="toc-text">9.6 检查 ES 测试环境索引</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%81-kibana%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA"><span class="toc-number">1.10.</span> <span class="toc-text">十、Kibana 数据展示</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#101-%E6%B7%BB%E5%8A%A0%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%B4%A2%E5%BC%95"><span class="toc-number">1.10.1.</span> <span class="toc-text">10.1 添加生产环境索引</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#102-%E6%9F%A5%E7%9C%8B%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.10.2.</span> <span class="toc-text">10.2 查看生产环境数据</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link"><span class="toc-number">1.10.2.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/posts/170066797.html" rel="bookmark" title="消费租赁项目Kubernetes基于ELK日志分析与实践">消费租赁项目Kubernetes基于ELK日志分析与实践</a></li><li><a href="/posts/570469260.html" rel="bookmark" title="ELK收集Kubernetes组件日志分析与实践">ELK收集Kubernetes组件日志分析与实践</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="Xu Yong" src="/assets/avatar.png"><p class="name" itemprop="name">Xu Yong</p><div class="description" itemprop="description">专注于 Linux 运维、云计算、云原⽣等技术</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">40</span><span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">14</span><span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">15</span><span class="name">标签</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/xyapples" class="item github" title="https:&#x2F;&#x2F;github.com&#x2F;xyapples"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://gitee.com/chinagei" class="item gitee" title="https:&#x2F;&#x2F;gitee.com&#x2F;chinagei"><i class="ic i-gitee"></i></a><a href="mailto:373370405@qq.com" class="item email" title="mailto:373370405@qq.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-about"></i>关于</a></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"><a href="/posts/2692178654.html" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/posts/626047790.html" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/176412055.html">K8s准入控制ResourceQuota、LimitRange、QoS服务质量</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/DevOps/" title="分类于DevOps">DevOps</a></div><span><a href="/posts/1208493697.html">K8S基于Jenkins实现SpringCloud微服务CI与CD实践（一）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/858611107.html">K8s服务发布Service</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Linux/" title="分类于Linux">Linux</a></div><span><a href="/posts/1922841233.html">Rsync服务实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3992668367.html">K8s配置管理Configmap</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Rabbitmq/" title="分类于Rabbitmq">Rabbitmq</a></div><span><a href="/posts/2692178654.html">Rabbitmq集群部署</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Rabbitmq/" title="分类于Rabbitmq">Rabbitmq</a></div><span><a href="/posts/1384812626.html">Kubenetes部署Rabbitmq集群</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/DevOps/" title="分类于DevOps">DevOps</a></div><span><a href="/posts/2015283801.html">K8S基于Jenkins实现SpringCloud微服务CI与CD实践（三）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3890389502.html">K8S持久化存储NFS+StorageClass</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ELKStack/" title="分类于ELKStack">ELKStack</a></div><span><a href="/posts/570469260.html">ELK收集Kubernetes组件日志分析与实践</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2><ul class="leancloud-recent-comment" id="new-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2022 -<span itemprop="copyrightYear">2025</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">Xu Yong @ LinuxSre云原生</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">676k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">10:15</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL = {
    ispost: true,
        path: `posts/170066797.html`,
        favicon: {
        show: `（●´3｀●）やれやれだぜ`,
        hide: `(´Д｀)大変だ！`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};</script><script src="https://s4.zstatic.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha384-ZvpUoO&#x2F;+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn&#x2F;6Z&#x2F;hRTt8+pR6L4N2" crossorigin="anonymous" fetchpriority="high"></script><script src="/js/siteInit.js?v=0.4.17" type="module" fetchpriority="high" defer></script></body></html><!-- rebuild by hrmmi -->