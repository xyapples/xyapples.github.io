<!-- build time:Wed May 28 2025 19:22:28 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon.ico"><link rel="alternate" href="/rss.xml" title="LinuxSre云原生" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="LinuxSre云原生" type="application/atom+xml"><link rel="alternate" type="application/json" title="LinuxSre云原生" href="http://ixuyong.cn/feed.json"><link rel="preconnect" href="https://s4.zstatic.net"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/app.css?v=0.4.17"><link rel="modulepreload" href="/js/chunk-GV364XSK.js"><link rel="modulepreload" href="/js/chunk-NYSE5UKM.js"><link rel="modulepreload" href="/js/chunk-RONCYO2S.js"><link rel="modulepreload" href="/js/chunk-THHXCRSX.js"><link rel="modulepreload" href="/js/chunk-WIQECBEN.js"><link rel="modulepreload" href="/js/comments-DL2IYMPZ.js"><link rel="modulepreload" href="/js/copy-tex-NADCTXPG.js"><link rel="modulepreload" href="/js/post-DA635IH6.js"><link rel="modulepreload" href="/js/quicklink-WEDHL4BA.js"><link rel="modulepreload" href="/js/search-VCZRKTM5.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="modulepreload" href="/js/waline-NNBYRQEE.js"><link rel="stylesheet" href="/css/comments-F4ZGS7LD.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/waline-IDNZKML2.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="preload" href="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" as="image" fetchpriority="high"><meta name="keywords" content="ELKStack"><meta name="description" content="专注于 Linux 运维、云计算、云原⽣等技术"><link rel="canonical" href="http://ixuyong.cn/posts/170066797.html"><title>消费租赁项目Kubernetes基于ELK日志分析与实践</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">消费租赁项目Kubernetes基于ELK日志分析与实践</h1><div class="meta"><span class="item" title="创建时间：2025-05-25 14:35:21"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2025-05-25T14:35:21+08:00">2025-05-25</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>34k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>31 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">LinuxSre云原生</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><img src="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" loading="eager" decoding="async" fetchpriority="high" alt="LinuxSre云原生"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ELKStack/" itemprop="item" rel="index" title="分类于ELKStack"><span itemprop="name">ELKStack<meta itemprop="position" content="0"></span></a></span></div><article class="post block" itemscope itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://ixuyong.cn/posts/170066797.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.png"><meta itemprop="name" content="Xu Yong"><meta itemprop="description" content="致力于技术布道、普及前沿技术、打造云原生系列标杆博客, 专注于 Linux 运维、云计算、云原⽣等技术"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="LinuxSre云原生"></span><div class="body md" itemprop="articleBody"><h3 id="消费租赁项目kubernetes基于elk日志分析与实践"><a class="anchor" href="#消费租赁项目kubernetes基于elk日志分析与实践">#</a> 消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</h3><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg" alt="Snipaste_2025-05-25_13-43-46.jpg"></p><h4 id="1-elk创建namespace和secrets"><a class="anchor" href="#1-elk创建namespace和secrets">#</a> 1. ELK 创建 Namespace 和 Secrets</h4><pre><code># kubectl create ns logging
# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=Talent*19871988
</code></pre><h4 id="2-交付zookeeper集群至k8s"><a class="anchor" href="#2-交付zookeeper集群至k8s">#</a> 2. 交付 Zookeeper 集群至 K8S</h4><h5 id="21-制作zk集群镜像"><a class="anchor" href="#21-制作zk集群镜像">#</a> 2.1 制作 ZK 集群镜像</h5><h6 id="211-dockerfile"><a class="anchor" href="#211-dockerfile">#</a> 2.1.1 Dockerfile</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre

# 1、拷贝Zookeeper压缩包和配置文件
ENV VERSION=3.8.4
ADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /
ADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf

# 2、对Zookeeper文件夹名称重新命名
RUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper

# 3、拷贝eentrpoint的启动脚本文件
ADD ./entrypoint.sh /entrypoint.sh

# 4、暴露Zookeeper端口
EXPOSE 2181 2888 3888

# 5、执行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="212-zoocfg"><a class="anchor" href="#212-zoocfg">#</a> 2.1.2 zoo.cfg</h6><pre><code># cat zoo.cfg 
# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。
tickTime=&#123;ZOOK_TICKTIME&#125;

# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime
initLimit=&#123;ZOOK_INIT_LIMIT&#125;

# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime
syncLimit=&#123;ZOOK_SYNC_LIMIT&#125;
 
# 数据保存目录
dataDir=&#123;ZOOK_DATA_DIR&#125;

# 日志保存目录
dataLogDir=&#123;ZOOK_LOG_DIR&#125;

# 客户端连接端口
clientPort=&#123;ZOOK_CLIENT_PORT&#125;

# 客户端最大连接数。# 根据自己实际情况设置，默认为60个
maxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;

# 客户端获取 zookeeper 服务的当前状态及相关信息
4lw.commands.whitelist=*

# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口
</code></pre><h6 id="213-entrypoint"><a class="anchor" href="#213-entrypoint">#</a> 2.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
#设定变量
ZOOK_BIN_DIR=/zookeeper/bin
ZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg

# 2、对配置文件中的字符串进行变量替换
sed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;

# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来
for server in $&#123;ZOOK_SERVERS&#125;
do
	echo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;
done

# 4、在datadir目录中创建myid的文件，并填入对应的编号
ZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))
echo $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid

#5、前台运行Zookeeper
cd $&#123;ZOOK_BIN_DIR&#125;
./zkServer.sh start-foreground
</code></pre><h6 id="214-构建镜像并推送仓库"><a class="anchor" href="#214-构建镜像并推送仓库">#</a> 2.1.4 构建镜像并推送仓库</h6><pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .
# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4
</code></pre><h5 id="22-迁移zookeeper至k8s"><a class="anchor" href="#22-迁移zookeeper至k8s">#</a> 2.2 迁移 zookeeper 至 K8S</h5><h6 id="221-zookeeper-headless"><a class="anchor" href="#221-zookeeper-headless">#</a> 2.2.1 zookeeper-headless</h6><pre><code># cat 01-zookeeper-headless.yaml 
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: zookeeper
  ports:
  - name: client
    port: 2181
    targetPort: 2181
  - name: leader-follwer
    port: 2888
    targetPort: 2888
  - name: selection
    port: 3888
    targetPort: 3888
</code></pre><h6 id="222-zookeeper-sts"><a class="anchor" href="#222-zookeeper-sts">#</a> 2.2.2 zookeeper-sts</h6><pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper               
  namespace: logging
spec:
  serviceName: &quot;zookeeper-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: [&quot;zookeeper&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: zookeeper
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           
        imagePullPolicy: Always
        ports:
        - name: client
          containerPort: 2181
        - name: leader-follwer
          containerPort: 2888
        - name: selection
          containerPort: 3888
        env:
        - name: ZOOK_SERVERS
          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;
        readinessProbe:         # 就绪探针，不就绪则不介入流量
          exec:
            command:
            - &quot;/bin/bash&quot;
            - &quot;-c&quot;
            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'
          initialDelaySeconds: 5
        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启
          exec:
            command:
            - &quot;/bin/bash&quot;
            - &quot;-c&quot;
            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'
          initialDelaySeconds: 5
        volumeMounts:
        - name: data
          mountPath: /data
          subPath: data
        - name: data
          mountPath: /logs
          subPath: logs
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="223-更新资源清单"><a class="anchor" href="#223-更新资源清单">#</a> 2.2.3 更新资源清单</h6><pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml 
[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml
[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging
NAME          READY   STATUS    RESTARTS   AGE
zookeeper-0   1/1     Running   0          17m
zookeeper-1   1/1     Running   0          14m
zookeeper-2   1/1     Running   0          11m
</code></pre><h6 id="224-检查zookeeper集群状态"><a class="anchor" href="#224-检查zookeeper集群状态">#</a> 2.2.4 检查 zookeeper 集群状态</h6><pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
</code></pre><h6 id="225-连接zookeeper集群"><a class="anchor" href="#225-连接zookeeper集群">#</a> 2.2.5 连接 Zookeeper 集群</h6><pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh
# /zookeeper/bin/zkCli.sh -server zookeeper-svc
[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu
Created /hello
[zk: zookeeper-svc(CONNECTED) 1] get /hello
oldxu
</code></pre><h4 id="3-交付kafka集群至k8s"><a class="anchor" href="#3-交付kafka集群至k8s">#</a> 3. 交付 Kafka 集群至 K8S</h4><h5 id="31-制作kafka集群镜像"><a class="anchor" href="#31-制作kafka集群镜像">#</a> 3.1 制作 Kafka 集群镜像</h5><h6 id="311-dockerfile"><a class="anchor" href="#311-dockerfile">#</a> 3.1.1 Dockerfile</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre

# 1、调整时区
RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \
    echo 'Asia/Shanghai' &gt; /etc/timezone

# 2、拷贝kafka软件以及kafka的配置
ENV VERSION=2.12-2.2.0
ADD ./kafka_$&#123;VERSION&#125;.tgz /
ADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties

# 3、修改kafka的名称
RUN mv /kafka_$&#123;VERSION&#125; /kafka

# 4、启动脚本（修改kafka配置）
ADD ./entrypoint.sh /entrypoint.sh

# 5、暴露kafka端口 9999是jmx的端口
EXPOSE 9092 9999

# 6、运行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="312-serverproperties"><a class="anchor" href="#312-serverproperties">#</a> 3.1.2 server.properties</h6><pre><code class="language-'"># cat server.properties 
############################# Server Basics ############################# 
# broker的id，值为整数，且必须唯一，在一个集群中不能重复
broker.id=&#123;BROKER_ID&#125;

############################# Socket Server Settings ############################# 
# kafka监听端口，默认9092
listeners=PLAINTEXT://&#123;LISTENERS&#125;:9092

# 处理网络请求的线程数量，默认为3个
num.network.threads=3

# 执行磁盘IO操作的线程数量，默认为8个 
num.io.threads=8

# socket服务发送数据的缓冲区大小，默认100KB
socket.send.buffer.bytes=102400

# socket服务接受数据的缓冲区大小，默认100KB
socket.receive.buffer.bytes=102400

# socket服务所能接受的一个请求的最大大小，默认为100M
socket.request.max.bytes=104857600

############################# Log Basics ############################# 
# kafka存储消息数据的目录
log.dirs=&#123;KAFKA_DATA_DIR&#125;

# 每个topic默认的partition
num.partitions=1

# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。
default.replication.factor=3

# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量
num.recovery.threads.per.data.dir=1

############################# Log Flush Policy ############################# 
# 消息刷新到磁盘中的消息条数阈值
log.flush.interval.messages=10000

# 消息刷新到磁盘中的最大时间间隔,1s
log.flush.interval.ms=1000

############################# Log Retention Policy ############################# 
# 日志保留小时数，超时会自动删除，默认为7天
log.retention.hours=168

# 日志保留大小，超出大小会自动删除，默认为1G
#log.retention.bytes=1073741824

# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件
log.segment.bytes=1073741824

# 每隔多长时间检测数据是否达到删除条件,300s
log.retention.check.interval.ms=300000

############################# Zookeeper ############################# 
# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开
zookeeper.connect=&#123;ZOOK_SERVERS&#125;

# 连接zookeeper的超时时间,6s
zookeeper.connection.timeout.ms=6000
</code></pre><h6 id="313-entrypoint"><a class="anchor" href="#313-entrypoint">#</a> 3.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
# 变量
KAFKA_DIR=/kafka
KAFKA_CONF=/kafka/config/server.properties

# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一
BROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))
LISTENERS=$(hostname -i)

# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递
sed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;

# 3、启动Kafka
cd $&#123;KAFKA_DIR&#125;/bin
sed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh
./kafka-server-start.sh ../config/server.properties
</code></pre><h6 id="314-构建镜像并推送仓库"><a class="anchor" href="#314-构建镜像并推送仓库">#</a> 3.1.4 构建镜像并推送仓库</h6><pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2
</code></pre><h5 id="32-迁移kafka集群至k8s"><a class="anchor" href="#32-迁移kafka集群至k8s">#</a> 3.2 迁移 Kafka 集群至 K8S</h5><h6 id="321-kafka-headless"><a class="anchor" href="#321-kafka-headless">#</a> 3.2.1 kafka-headless</h6><pre><code># cat 01-kafka-headless.yaml 
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
  - name: client
    port: 9092
    targetPort: 9092
  - name: jmx
    port: 9999
    targetPort: 9999
</code></pre><h6 id="322-kafka-sts"><a class="anchor" href="#322-kafka-sts">#</a> 3.2.2 kafka-sts</h6><pre><code># cat 02-kafka-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: logging
spec:
  serviceName: &quot;kafka-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: [&quot;kafka&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: kafka
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 
        imagePullPolicy: Always
        ports:
        - name: client
          containerPort: 9092
        - name: jmxport
          containerPort: 9999
        env:
        - name: ZOOK_SERVERS
          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;
        readinessProbe:         # 就绪探针，不就绪则不介入流量
          tcpSocket:
            port: 9092
          initialDelaySeconds: 5
        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启
          tcpSocket:
            port: 9092
          initialDelaySeconds: 5
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="323-更新资源清单"><a class="anchor" href="#323-更新资源清单">#</a> 3.2.3 更新资源清单</h6><pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml 
[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml
[root@k8s-master01 02-kafka]# kubectl get pods -n logging 
NAME          READY   STATUS    RESTARTS       AGE
kafka-0       1/1     Running   0              5m49s
kafka-1       1/1     Running   0              4m43s
kafka-2       1/1     Running   0              3m40s

#查看kafka是否注册到zookeeper
[root@k8s-master01 02-kafka]# kubectl exec -it zookeeper-0 -n logging -- /bin/bash
root@zookeeper-0:/# /zookeeper/bin/zkCli.sh 
[zk: localhost:2181(CONNECTED) 2] get /brokers/ids/1
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.85.201:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.85.201&quot;,&quot;timestamp&quot;:&quot;1748162470218&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
[zk: localhost:2181(CONNECTED) 3] get /brokers/ids/2
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.58.205:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.58.205&quot;,&quot;timestamp&quot;:&quot;1748162532658&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
[zk: localhost:2181(CONNECTED) 4] get /brokers/ids/3
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.195.1:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.195.1&quot;,&quot;timestamp&quot;:&quot;1748162649250&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
</code></pre><h6 id="324-检查kafka集群"><a class="anchor" href="#324-检查kafka集群">#</a> 3.2.4 检查 Kafka 集群</h6><pre><code>1.创建一个topic
root@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu

2.模拟消息发布
root@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu
&gt;hello kubernetes
&gt;hello world

3.模拟消息订阅
root@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning
hello kubernetes
hello world
</code></pre><h4 id="4-交付efak至k8s"><a class="anchor" href="#4-交付efak至k8s">#</a> 4. 交付 efak 至 K8S</h4><h5 id="41-制作efak镜像"><a class="anchor" href="#41-制作efak镜像">#</a> 4.1 制作 efak 镜像</h5><h6 id="411-dockerfile"><a class="anchor" href="#411-dockerfile">#</a> 4.1.1 Dockerfile</h6><pre><code>[root@manager 03-efak]# cat Dockerfile 
FROM openjdk:8

# 1、调整时区
RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \
    echo 'Asia/Shanghai' &gt; /etc/timezone

# 2、拷贝kafka软件以及kafka的配置
ENV VERSION=3.0.1
ADD ./efak-web-$&#123;VERSION&#125;-bin.tar.gz /
ADD ./system-config.properties /efak-web-$&#123;VERSION&#125;/conf/system-config.properties

# 3、修改efak的名称
RUN mv /efak-web-$&#123;VERSION&#125; /efak

# 4、环境变量
ENV KE_HOME=/efak
ENV PATH=$PATH:$KE_HOME/bin

# 5、启动脚本（修改kafka配置）
ADD ./entrypoint.sh /entrypoint.sh

# 6、暴露kafka端口 9999是jmx的端口
EXPOSE 8048

# 7、运行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="412-system-config"><a class="anchor" href="#412-system-config">#</a> 4.1.2 system-config</h6><pre><code># cat system-config.properties 
######################################
# 填写 zookeeper集群列表
######################################
efak.zk.cluster.alias=cluster1
cluster1.zk.list=&#123;ZOOK_SERVERS&#125;

######################################
# broker 最大规模数量
######################################
cluster1.efak.broker.size=20

######################################
# zk 客户端线程数
######################################
kafka.zk.limit.size=32

######################################
# EFAK webui 端口
######################################
efak.webui.port=8048

######################################
# kafka offset storage
######################################
cluster1.efak.offset.storage=kafka

######################################
# kafka jmx uri
######################################
cluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi

######################################
# kafka metrics 指标，默认存储15天
######################################
efak.metrics.charts=true
efak.metrics.retain=15

######################################
# kafka sql topic records max
######################################
efak.sql.topic.records.max=5000
efak.sql.topic.preview.records.max=10

######################################
# delete kafka topic token
######################################
efak.topic.token=keadmin

######################################
# kafka sqlite 数据库地址（需要修改存储路径）
######################################
efak.driver=org.sqlite.JDBC
efak.url=jdbc:sqlite:&#123;EFAK_DATA_DIR&#125;/db/ke.db
efak.username=root
efak.password=www.kafka-eagle.org
</code></pre><h6 id="413-entrypoint"><a class="anchor" href="#413-entrypoint">#</a> 4.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
# 1、变量
EFAK_DIR=/efak
EFAK_CONF=/efak/conf/system-config.properties

# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递
sed -i s@&#123;EFAK_DATA_DIR&#125;@$&#123;EFAK_DIR&#125;@g  $&#123;EFAK_CONF&#125;
sed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;EFAK_CONF&#125;

# 3、启动efka
$&#123;EFAK_DIR&#125;/bin/ke.sh start
tail -f $&#123;EFAK_DIR&#125;/logs/ke_console.out
</code></pre><h6 id="414-构建镜像并推送仓库"><a class="anchor" href="#414-构建镜像并推送仓库">#</a> 4.1.4 构建镜像并推送仓库</h6><pre><code># wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0
</code></pre><h5 id="42-迁移efak至k8s"><a class="anchor" href="#42-迁移efak至k8s">#</a> 4.2 迁移 efak 至 K8S</h5><h6 id="421-efak-deploy"><a class="anchor" href="#421-efak-deploy">#</a> 4.2.1 efak-deploy</h6><pre><code># cat 01-efak-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: efak
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: efak
  template:
    metadata:
      labels:
        app: efak
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: efak
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 8048
        env:
        - name: ZOOK_SERVERS
          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;
</code></pre><h6 id="422-efak-service"><a class="anchor" href="#422-efak-service">#</a> 4.2.2 efak-service</h6><pre><code># cat 02-efak-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: efak-svc
  namespace: logging
spec:
  selector:
    app: efak
  ports:
  - port: 8048
    targetPort: 8048
</code></pre><h6 id="423-efak-ingress"><a class="anchor" href="#423-efak-ingress">#</a> 4.2.3 efak-ingress</h6><pre><code># cat 03-efak-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: efak-ingress
  namespace: logging
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;efak.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: efak-svc
            port: 
              number: 8048
</code></pre><h6 id="424-更新资源清单"><a class="anchor" href="#424-更新资源清单">#</a> 4.2.4 更新资源清单</h6><pre><code>[root@k8s-master01 03-efak]# kubectl apply -f 01-efak-deploy.yaml 
[root@k8s-master01 03-efak]# kubectl apply -f 02-efak-service.yaml 
[root@k8s-master01 03-efak]# kubectl apply -f 03-efak-ingress.yaml 
</code></pre><h6 id="425-访问efka"><a class="anchor" href="#425-访问efka">#</a> 4.2.5 访问 efka</h6><p>1、初始用户名密码 admin 123456</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Nq16u4z.png" alt="1.png"></p><p>2、查看 Topics</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/9Bin9cr.png" alt="2.png"></p><p>3、查看 kafka 集群状态</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/U76YIck.png" alt="3.png"></p><p>4、查看 Zookeeper 集群状态</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/cY5LeWx.png" alt="4.png"></p><h4 id="5-交付elastic集群"><a class="anchor" href="#5-交付elastic集群">#</a> 5. 交付 Elastic 集群</h4><ul><li>ES 集群是由多个节点组成的，通过 <a target="_blank" rel="noopener" href="http://cluster.name">cluster.name</a> 设置 ES 集群名称，同时用于区分其它的 ES 集群。</li><li>每个节点通过 <a target="_blank" rel="noopener" href="http://node.name">node.name</a> 参数来设定所在集群的节点名称。</li><li>节点使用 discovery.send_hosts 参数来设定集群节点的列表。</li><li>集群在第一次启动时，需要初始化，同时需要指定参与选举的 master 节点 IP，或节点名称。</li><li>每个节点可以通过 node.master:true 设定为 master 角色，通过 node.data:true 设定为 data 角色。</li></ul><pre><code>[root@k8s-master01 ~]# grep &quot;^[a-Z]&quot; /etc/elasticsearch/elasticsearch.yml
# 集群名称cluster.name: my-oldxu
# 节点名称node.name: node1
# 数据存储路径path.data: /var/lib/elasticsearch
# 日志存储路径path.logs: /var/log/elasticsearch
# 监听在本地哪个地址上network.host: 10.0.0.100
# 监听端口http.port: 9200
# 集群主机列表discovery.seed_hosts: [&quot;ip1&quot;, &quot;ip2&quot;, &quot;ip3&quot;]
# 仅第一次启动集群时进行选举（可以填写node.name的名称）cluster.initial_master_nodes: [&quot;node01&quot;, &quot;node02&quot;, &quot;node03&quot;]
</code></pre><h5 id="51-下载elastic镜像"><a class="anchor" href="#51-下载elastic镜像">#</a> 5.1 下载 elastic 镜像</h5><pre><code># docker pull elasticsearch:7.17.6
# docker tag elasticsearch:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6
</code></pre><h5 id="52-交付es-service"><a class="anchor" href="#52-交付es-service">#</a> 5.2 交付 ES-Service</h5><p>创建 es-headlessService，为每个 ES Pod 设定固定的 DNS 名称，无论它是 Master 或是 Data，易或是 Coordinating</p><pre><code># cat 01-es-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: es-svc
  namespace: logging
spec:
  selector:
    app: es
  clusterIP: None
  ports:
  - name: cluster
    port: 9200
    targetPort: 9200
  - name: transport
    port: 9300
    targetPort: 9300
</code></pre><h5 id="53-交付es-master节点"><a class="anchor" href="#53-交付es-master节点">#</a> 5.3 交付 ES-Master 节点</h5><ol><li><p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data ；</p></li><li><p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p></li><li><p>ES 启动是通过 ENV 环境变量传参来完成的；</p><ul><li><p>集群名称、节点名称、角色类型；</p></li><li><p>discovery.seed_hosts 集群地址列表；</p></li><li><p>cluster.initial_master_nodes 初始集群参与选举的 master 节点名称；</p></li></ul></li></ol><pre><code># cat 02-es-master.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-master
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 3           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: master
  template:
    metadata:
      labels:
        app: es
        role: master
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;master&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;true&quot;
        - name: node.data
          value: &quot;false&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
        - name: cluster.initial_master_nodes
          value: &quot;es-master-0,es-master-1,es-master-2&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><pre><code>[root@k8s-master01 04-elasticsearch]# cat 03-es-data.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-data
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 2           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: data
  template:
    metadata:
      labels:
        app: es
        role: data
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;data&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;false&quot;
        - name: node.data
          value: &quot;true&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="54-交付es-data节点"><a class="anchor" href="#54-交付es-data节点">#</a> 5.4 交付 ES-Data 节点</h5><ol><li><p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data</p></li><li><p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p></li><li><p>ES 启动是通过 ENV 环境变量传参来完成的</p><ul><li><p>集群名称、节点名称、角色类型</p></li><li><p>discovery.seed_hosts 集群地址列表</p></li></ul></li></ol><pre><code># cat 03-es-data.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-data
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 2           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: data
  template:
    metadata:
      labels:
        app: es
        role: data
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;data&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;false&quot;
        - name: node.data
          value: &quot;true&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="55-更新资源清单"><a class="anchor" href="#55-更新资源清单">#</a> 5.5 更新资源清单</h5><pre><code>[root@k8s-master01 04-elasticsearch]# kubectl apply -f 01-es-svc.yaml 
[root@k8s-master01 04-elasticsearch]# kubectl apply -f 02-es-master.yaml 
[root@k8s-master01 04-elasticsearch]# kubectl apply -f 03-es-data.yaml 
</code></pre><h5 id="56-验证es集群"><a class="anchor" href="#56-验证es集群">#</a> 5.6 验证 ES 集群</h5><pre><code>#1.解析headlessService获取对应ES集群任一节点的IP地址
# dig @10.96.0.10 es-svc.logging.svc.cluster.local  +short
172.16.58.229
172.16.122.191
172.16.195.21
172.16.122.129
172.16.32.164

#2.通过curl访问ES，检查ES集群是否正常（如果仅交付Master，没有data节点，集群状态可能会Red，因为没有数据节点进行数据存储；）
# curl -XGET &quot;http://172.16.122.129:9200/_cluster/health?pretty&quot;
&#123;
  &quot;cluster_name&quot; : &quot;es-cluster&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 5,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 3,
  &quot;active_shards&quot; : 6,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;delayed_unassigned_shards&quot; : 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0,
  &quot;task_max_waiting_in_queue_millis&quot; : 0,
  &quot;active_shards_percent_as_number&quot; : 100.0
&#125;

#3.查看ES各个节点详情
# curl -XGET &quot;http://172.16.122.129:9200/_cat/nodes&quot;
172.16.122.129 16 33 20 0.38 0.56 0.38 ilmr       - es-master-2
172.16.58.229  66 33 22 0.64 0.66 0.44 ilmr       * es-master-1
172.16.122.191 52 34 15 0.38 0.56 0.38 cdfhilrstw - es-data-0
172.16.195.21  38 35 19 0.38 0.53 0.36 cdfhilrstw - es-data-1
172.16.32.164  31 33 12 0.28 0.50 0.59 ilmr       - es-master-0
</code></pre><h4 id="6-交付kibana可视化"><a class="anchor" href="#6-交付kibana可视化">#</a> 6. 交付 Kibana 可视化</h4><h5 id="61-下载kibana镜像"><a class="anchor" href="#61-下载kibana镜像">#</a> 6.1 下载 kibana 镜像</h5><pre><code># docker pull kibana:7.17.6
# docker tag kibana:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6
</code></pre><h5 id="62-kibana-deploy"><a class="anchor" href="#62-kibana-deploy">#</a> 6.2 kibana-deploy</h5><ol><li>Kibana 需要连接 ES 集群，通过 ELASTICSEARCH_HOSTS 变量来传递 ES 集群地址</li><li>kibana 通过 I18N_LOCALE 来传递语言环境</li><li>Kibana 通过 SERVER_PUBLICBASEURL 来传递服务访问的公开地址</li></ol><pre><code># cat 01-kibana-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: kibana
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6 
        resources:
          limits:
            cpu: 1000m
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: '[&quot;http://es-data-0.es-svc:9200&quot;,&quot;http://es-data-1.es-svc:9200&quot;]'
        - name: I18N_LOCALE
          value: &quot;zh-CN&quot;
        - name: SERVER_PUBLICBASEURL
          value: &quot;http://kibana.hmallleasing.com&quot;   #kibana访问UI
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
</code></pre><h5 id="63-kibana-svc"><a class="anchor" href="#63-kibana-svc">#</a> 6.3 kibana-svc</h5><pre><code># cat 02-kibana-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: kibana-svc
  namespace: logging
spec:
  selector:
    app: kibana
  ports:
  - name: web
    port: 5601
    targetPort: 5601
</code></pre><h5 id="64-kibana-ingress"><a class="anchor" href="#64-kibana-ingress">#</a> 6.4 kibana-ingress</h5><pre><code># cat 03-kibana-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana-ingress
  namespace: logging
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;kibana.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana-svc
            port:
              number: 5601
</code></pre><h5 id="65-更新资源清单"><a class="anchor" href="#65-更新资源清单">#</a> 6.5 更新资源清单</h5><pre><code>[root@k8s-master01 05-kibana]# kubectl apply -f 01-kibana-deploy.yaml 
[root@k8s-master01 05-kibana]# kubectl apply -f 02-kibana-svc.yaml 
[root@k8s-master01 05-kibana]# kubectl apply -f 03-kibana-ingress.yaml

[root@k8s-master01 05-kibana]# kubectl get pods -n logging
NAME                      READY   STATUS    RESTARTS   AGE
efak-5cdc74bf59-nrhb4     1/1     Running   0          5h33m
es-data-0                 1/1     Running   0          16m
es-data-1                 1/1     Running   0          15m
es-master-0               1/1     Running   0          17m
es-master-1               1/1     Running   0          15m
es-master-2               1/1     Running   0          12m
kafka-0                   1/1     Running   0          5h39m
kafka-1                   1/1     Running   0          5h39m
kafka-2                   1/1     Running   0          5h38m
kibana-5ccc46864b-ndzx9   1/1     Running   0          118s
zookeeper-0               1/1     Running   0          5h42m
zookeeper-1               1/1     Running   0          5h42m
zookeeper-2               1/1     Running   0          5h41m
</code></pre><h5 id="66-访问kibana"><a class="anchor" href="#66-访问kibana">#</a> 6.6 访问 kibana</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/sUXTx1J.png" alt="1.png"></p><div class="tags"><a href="/tags/ELKStack/" rel="tag"><i class="ic i-tag"></i>ELKStack</a></div></div><footer><div class="meta"><span class="icon"><i class="ic i-eye"></i></span><span>此文章已被阅读次数:</span><span class="waline-pageview-count" id="twikoo_visitors" data-path="/posts/170066797.html">正在加载...</span><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于</span><time title="修改时间：2025-05-26 19:50:46" itemprop="dateModified" datetime="2025-05-26T19:50:46+08:00">2025-05-26</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.png" alt="Xu Yong 微信支付"><p>微信支付</p></div><div><img loading="lazy" data-src="/assets/alipay.png" alt="Xu Yong 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者：</strong>Xu Yong<i class="ic i-at"><em>@</em></i>LinuxSre云原生</li><li class="link"><strong>本文链接：</strong><a href="http://ixuyong.cn/posts/170066797.html" title="消费租赁项目Kubernetes基于ELK日志分析与实践">http://ixuyong.cn/posts/170066797.html</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/posts/626047790.html" rel="prev" itemprop="url" data-background-image="https:&#x2F;&#x2F;wp-cdn.4ce.cn&#x2F;v2&#x2F;er1zpXS.jpeg" title="消费租赁系统微服务应用交付实践"><span class="type">上一篇</span><span class="category"><i class="ic i-flag"></i>Kubernetes</span><h3>消费租赁系统微服务应用交付实践</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E7%A7%9F%E8%B5%81%E9%A1%B9%E7%9B%AEkubernetes%E5%9F%BA%E4%BA%8Eelk%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.</span> <span class="toc-text">消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-elk%E5%88%9B%E5%BB%BAnamespace%E5%92%8Csecrets"><span class="toc-number">1.1.</span> <span class="toc-text">1. ELK 创建 Namespace 和 Secrets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BA%A4%E4%BB%98zookeeper%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.2.</span> <span class="toc-text">2. 交付 Zookeeper 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#21-%E5%88%B6%E4%BD%9Czk%E9%9B%86%E7%BE%A4%E9%95%9C%E5%83%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 制作 ZK 集群镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#211-dockerfile"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">2.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#212-zoocfg"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">2.1.2 zoo.cfg</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#213-entrypoint"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">2.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#214-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.2.1.4.</span> <span class="toc-text">2.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#22-%E8%BF%81%E7%A7%BBzookeeper%E8%87%B3k8s"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 迁移 zookeeper 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#221-zookeeper-headless"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">2.2.1 zookeeper-headless</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#222-zookeeper-sts"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2.2.2 zookeeper-sts</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#223-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">2.2.3 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#224-%E6%A3%80%E6%9F%A5zookeeper%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">2.2.4 检查 zookeeper 集群状态</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#225-%E8%BF%9E%E6%8E%A5zookeeper%E9%9B%86%E7%BE%A4"><span class="toc-number">1.2.2.5.</span> <span class="toc-text">2.2.5 连接 Zookeeper 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BA%A4%E4%BB%98kafka%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.3.</span> <span class="toc-text">3. 交付 Kafka 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#31-%E5%88%B6%E4%BD%9Ckafka%E9%9B%86%E7%BE%A4%E9%95%9C%E5%83%8F"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 制作 Kafka 集群镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#311-dockerfile"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">3.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#312-serverproperties"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">3.1.2 server.properties</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#313-entrypoint"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">3.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#314-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.3.1.4.</span> <span class="toc-text">3.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#32-%E8%BF%81%E7%A7%BBkafka%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 迁移 Kafka 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#321-kafka-headless"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">3.2.1 kafka-headless</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#322-kafka-sts"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">3.2.2 kafka-sts</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#323-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">3.2.3 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#324-%E6%A3%80%E6%9F%A5kafka%E9%9B%86%E7%BE%A4"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">3.2.4 检查 Kafka 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BA%A4%E4%BB%98efak%E8%87%B3k8s"><span class="toc-number">1.4.</span> <span class="toc-text">4. 交付 efak 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#41-%E5%88%B6%E4%BD%9Cefak%E9%95%9C%E5%83%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 制作 efak 镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#411-dockerfile"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">4.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#412-system-config"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">4.1.2 system-config</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#413-entrypoint"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">4.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#414-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">4.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#42-%E8%BF%81%E7%A7%BBefak%E8%87%B3k8s"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 迁移 efak 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#421-efak-deploy"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">4.2.1 efak-deploy</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#422-efak-service"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">4.2.2 efak-service</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#423-efak-ingress"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">4.2.3 efak-ingress</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#424-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">4.2.4 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#425-%E8%AE%BF%E9%97%AEefka"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">4.2.5 访问 efka</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E4%BA%A4%E4%BB%98elastic%E9%9B%86%E7%BE%A4"><span class="toc-number">1.5.</span> <span class="toc-text">5. 交付 Elastic 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#51-%E4%B8%8B%E8%BD%BDelastic%E9%95%9C%E5%83%8F"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 下载 elastic 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#52-%E4%BA%A4%E4%BB%98es-service"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 交付 ES-Service</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#53-%E4%BA%A4%E4%BB%98es-master%E8%8A%82%E7%82%B9"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 交付 ES-Master 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#54-%E4%BA%A4%E4%BB%98es-data%E8%8A%82%E7%82%B9"><span class="toc-number">1.5.4.</span> <span class="toc-text">5.4 交付 ES-Data 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#55-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.5.5.</span> <span class="toc-text">5.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#56-%E9%AA%8C%E8%AF%81es%E9%9B%86%E7%BE%A4"><span class="toc-number">1.5.6.</span> <span class="toc-text">5.6 验证 ES 集群</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E4%BA%A4%E4%BB%98kibana%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.6.</span> <span class="toc-text">6. 交付 Kibana 可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#61-%E4%B8%8B%E8%BD%BDkibana%E9%95%9C%E5%83%8F"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 下载 kibana 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#62-kibana-deploy"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 kibana-deploy</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#63-kibana-svc"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 kibana-svc</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#64-kibana-ingress"><span class="toc-number">1.6.4.</span> <span class="toc-text">6.4 kibana-ingress</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#65-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.6.5.</span> <span class="toc-text">6.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#66-%E8%AE%BF%E9%97%AEkibana"><span class="toc-number">1.6.6.</span> <span class="toc-text">6.6 访问 kibana</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/posts/170066797.html" rel="bookmark" title="消费租赁项目Kubernetes基于ELK日志分析与实践">消费租赁项目Kubernetes基于ELK日志分析与实践</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="Xu Yong" src="/assets/avatar.png"><p class="name" itemprop="name">Xu Yong</p><div class="description" itemprop="description">专注于 Linux 运维、云计算、云原⽣等技术</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">27</span><span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">8</span><span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">8</span><span class="name">标签</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/xyapples" class="item github" title="https:&#x2F;&#x2F;github.com&#x2F;xyapples"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://gitee.com/chinagei" class="item gitee" title="https:&#x2F;&#x2F;gitee.com&#x2F;chinagei"><i class="ic i-gitee"></i></a><a href="mailto:373370405@qq.com" class="item email" title="mailto:373370405@qq.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-about"></i>关于</a></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/posts/626047790.html" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/858611107.html">K8s服务发布Service</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ELKStack/" title="分类于ELKStack">ELKStack</a></div><span><a href="/posts/170066797.html">消费租赁项目Kubernetes基于ELK日志分析与实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Linux/" title="分类于Linux">Linux</a></div><span><a href="/posts/1922841233.html">Rsync服务实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Harbor/" title="分类于Harbor">Harbor</a></div><span><a href="/posts/3071070978.html">企业级私有仓库Harbor搭建</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/169153047.html">K8s持久化存储</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/MySQL/" title="分类于MySQL">MySQL</a></div><span><a href="/posts/2628187572.html">MySQL运维DBA应用与实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3992668367.html">K8s配置管理Configmap</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/985149017.html">二进制高可用安装K8S集群</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3166738000.html">Kubeadm高可用安装K8s集群</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Windows/" title="分类于Windows">Windows</a></div><span><a href="/posts/3071070979.html">一键永久激活Window、office教程</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2><ul class="leancloud-recent-comment" id="new-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2022 -<span itemprop="copyrightYear">2025</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">Xu Yong @ LinuxSre云原生</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">510k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">7:44</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL = {
    ispost: true,
        path: `posts/170066797.html`,
        favicon: {
        show: `（●´3｀●）やれやれだぜ`,
        hide: `(´Д｀)大変だ！`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};</script><script src="https://s4.zstatic.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha384-ZvpUoO&#x2F;+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn&#x2F;6Z&#x2F;hRTt8+pR6L4N2" crossorigin="anonymous" fetchpriority="high"></script><script src="/js/siteInit.js?v=0.4.17" type="module" fetchpriority="high" defer></script></body></html><!-- rebuild by hrmmi -->