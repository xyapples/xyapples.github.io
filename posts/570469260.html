<!-- build time:Tue Jun 10 2025 22:13:55 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon.ico"><link rel="alternate" href="/rss.xml" title="LinuxSre云原生" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="LinuxSre云原生" type="application/atom+xml"><link rel="alternate" type="application/json" title="LinuxSre云原生" href="http://ixuyong.cn/feed.json"><link rel="preconnect" href="https://s4.zstatic.net"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/app.css?v=0.4.17"><link rel="modulepreload" href="/js/chunk-GV364XSK.js"><link rel="modulepreload" href="/js/chunk-NYSE5UKM.js"><link rel="modulepreload" href="/js/chunk-RONCYO2S.js"><link rel="modulepreload" href="/js/chunk-THHXCRSX.js"><link rel="modulepreload" href="/js/chunk-WIQECBEN.js"><link rel="modulepreload" href="/js/comments-DL2IYMPZ.js"><link rel="modulepreload" href="/js/copy-tex-NADCTXPG.js"><link rel="modulepreload" href="/js/post-DA635IH6.js"><link rel="modulepreload" href="/js/quicklink-WEDHL4BA.js"><link rel="modulepreload" href="/js/search-VCZRKTM5.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="modulepreload" href="/js/waline-NNBYRQEE.js"><link rel="stylesheet" href="/css/comments-F4ZGS7LD.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/waline-IDNZKML2.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="preload" href="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" as="image" fetchpriority="high"><meta name="keywords" content="ELKStack"><meta name="description" content="专注于 Linux 运维、云计算、云原⽣等技术"><link rel="canonical" href="http://ixuyong.cn/posts/570469260.html"><title>ELK收集Kubernetes组件日志分析与实践</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">ELK收集Kubernetes组件日志分析与实践</h1><div class="meta"><span class="item" title="创建时间：2025-06-05 19:06:21"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2025-06-05T19:06:21+08:00">2025-06-05</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>48k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>44 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">LinuxSre云原生</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><img src="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" loading="eager" decoding="async" fetchpriority="high" alt="LinuxSre云原生"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ELKStack/" itemprop="item" rel="index" title="分类于ELKStack"><span itemprop="name">ELKStack<meta itemprop="position" content="0"></span></a></span></div><article class="post block" itemscope itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://ixuyong.cn/posts/570469260.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.png"><meta itemprop="name" content="Xu Yong"><meta itemprop="description" content="致力于技术布道、普及前沿技术、打造云原生系列标杆博客, 专注于 Linux 运维、云计算、云原⽣等技术"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="LinuxSre云原生"></span><div class="body md" itemprop="articleBody"><h3 id="elk收集kubernetes组件日志分析与实践"><a class="anchor" href="#elk收集kubernetes组件日志分析与实践">#</a> ELK 收集 Kubernetes 组件日志分析与实践</h3><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg" alt="Snipaste_2025-05-25_13-43-46.jpg"></p><h4 id="一-elk创建namespace和secrets"><a class="anchor" href="#一-elk创建namespace和secrets">#</a> 一、ELK 创建 Namespace 和 Secrets</h4><pre><code># kubectl create ns logging
# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd
</code></pre><h4 id="二-交付zookeeper集群至k8s"><a class="anchor" href="#二-交付zookeeper集群至k8s">#</a> 二、交付 Zookeeper 集群至 K8S</h4><h5 id="21-制作zk集群镜像"><a class="anchor" href="#21-制作zk集群镜像">#</a> 2.1 制作 ZK 集群镜像</h5><h6 id="211-dockerfile"><a class="anchor" href="#211-dockerfile">#</a> 2.1.1 Dockerfile</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre

# 1、拷贝Zookeeper压缩包和配置文件
ENV VERSION=3.8.4
ADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /
ADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf

# 2、对Zookeeper文件夹名称重新命名
RUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper

# 3、拷贝eentrpoint的启动脚本文件
ADD ./entrypoint.sh /entrypoint.sh

# 4、暴露Zookeeper端口
EXPOSE 2181 2888 3888

# 5、执行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="212-zoocfg"><a class="anchor" href="#212-zoocfg">#</a> 2.1.2 zoo.cfg</h6><pre><code># cat zoo.cfg 
# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。
tickTime=&#123;ZOOK_TICKTIME&#125;

# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime
initLimit=&#123;ZOOK_INIT_LIMIT&#125;

# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime
syncLimit=&#123;ZOOK_SYNC_LIMIT&#125;
 
# 数据保存目录
dataDir=&#123;ZOOK_DATA_DIR&#125;

# 日志保存目录
dataLogDir=&#123;ZOOK_LOG_DIR&#125;

# 客户端连接端口
clientPort=&#123;ZOOK_CLIENT_PORT&#125;

# 客户端最大连接数。# 根据自己实际情况设置，默认为60个
maxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;

# 客户端获取 zookeeper 服务的当前状态及相关信息
4lw.commands.whitelist=*

# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口
</code></pre><h6 id="213-entrypoint"><a class="anchor" href="#213-entrypoint">#</a> 2.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
#设定变量
ZOOK_BIN_DIR=/zookeeper/bin
ZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg

# 2、对配置文件中的字符串进行变量替换
sed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;
sed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;

# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来
for server in $&#123;ZOOK_SERVERS&#125;
do
	echo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;
done

# 4、在datadir目录中创建myid的文件，并填入对应的编号
ZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))
echo $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid

#5、前台运行Zookeeper
cd $&#123;ZOOK_BIN_DIR&#125;
./zkServer.sh start-foreground
</code></pre><h6 id="214-构建镜像并推送仓库"><a class="anchor" href="#214-构建镜像并推送仓库">#</a> 2.1.4 构建镜像并推送仓库</h6><pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .
# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4
</code></pre><h5 id="22-迁移zookeeper至k8s"><a class="anchor" href="#22-迁移zookeeper至k8s">#</a> 2.2 迁移 zookeeper 至 K8S</h5><h6 id="221-zookeeper-headless"><a class="anchor" href="#221-zookeeper-headless">#</a> 2.2.1 zookeeper-headless</h6><pre><code># cat 01-zookeeper-headless.yaml 
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: zookeeper
  ports:
  - name: client
    port: 2181
    targetPort: 2181
  - name: leader-follwer
    port: 2888
    targetPort: 2888
  - name: selection
    port: 3888
    targetPort: 3888
</code></pre><h6 id="222-zookeeper-sts"><a class="anchor" href="#222-zookeeper-sts">#</a> 2.2.2 zookeeper-sts</h6><pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper               
  namespace: logging
spec:
  serviceName: &quot;zookeeper-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: [&quot;zookeeper&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: zookeeper
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           
        imagePullPolicy: Always
        ports:
        - name: client
          containerPort: 2181
        - name: leader-follwer
          containerPort: 2888
        - name: selection
          containerPort: 3888
        env:
        - name: ZOOK_SERVERS
          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;
        readinessProbe:         # 就绪探针，不就绪则不介入流量
          exec:
            command:
            - &quot;/bin/bash&quot;
            - &quot;-c&quot;
            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'
          initialDelaySeconds: 5
        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启
          exec:
            command:
            - &quot;/bin/bash&quot;
            - &quot;-c&quot;
            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'
          initialDelaySeconds: 5
        volumeMounts:
        - name: data
          mountPath: /data
          subPath: data
        - name: data
          mountPath: /logs
          subPath: logs
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="223-更新资源清单"><a class="anchor" href="#223-更新资源清单">#</a> 2.2.3 更新资源清单</h6><pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml 
[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml
[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging
NAME          READY   STATUS    RESTARTS   AGE
zookeeper-0   1/1     Running   0          17m
zookeeper-1   1/1     Running   0          14m
zookeeper-2   1/1     Running   0          11m
</code></pre><h6 id="224-检查zookeeper集群状态"><a class="anchor" href="#224-检查zookeeper集群状态">#</a> 2.2.4 检查 zookeeper 集群状态</h6><pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
ZooKeeper JMX enabled by default
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
</code></pre><h6 id="225-连接zookeeper集群"><a class="anchor" href="#225-连接zookeeper集群">#</a> 2.2.5 连接 Zookeeper 集群</h6><pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh
# /zookeeper/bin/zkCli.sh -server zookeeper-svc
[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu
Created /hello
[zk: zookeeper-svc(CONNECTED) 1] get /hello
oldxu
</code></pre><h4 id="三-交付kafka集群至k8s"><a class="anchor" href="#三-交付kafka集群至k8s">#</a> 三、 交付 Kafka 集群至 K8S</h4><h5 id="31-制作kafka集群镜像"><a class="anchor" href="#31-制作kafka集群镜像">#</a> 3.1 制作 Kafka 集群镜像</h5><h6 id="311-dockerfile"><a class="anchor" href="#311-dockerfile">#</a> 3.1.1 Dockerfile</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre

# 1、调整时区
RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \
    echo 'Asia/Shanghai' &gt; /etc/timezone

# 2、拷贝kafka软件以及kafka的配置
ENV VERSION=2.12-2.2.0
ADD ./kafka_$&#123;VERSION&#125;.tgz /
ADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties

# 3、修改kafka的名称
RUN mv /kafka_$&#123;VERSION&#125; /kafka

# 4、启动脚本（修改kafka配置）
ADD ./entrypoint.sh /entrypoint.sh

# 5、暴露kafka端口 9999是jmx的端口
EXPOSE 9092 9999

# 6、运行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="312-serverproperties"><a class="anchor" href="#312-serverproperties">#</a> 3.1.2 server.properties</h6><pre><code class="language-'"># cat server.properties 
############################# Server Basics ############################# 
# broker的id，值为整数，且必须唯一，在一个集群中不能重复
broker.id=&#123;BROKER_ID&#125;

############################# Socket Server Settings ############################# 
# kafka监听端口，默认9092
listeners=PLAINTEXT://&#123;LISTENERS&#125;:9092

# 处理网络请求的线程数量，默认为3个
num.network.threads=3

# 执行磁盘IO操作的线程数量，默认为8个 
num.io.threads=8

# socket服务发送数据的缓冲区大小，默认100KB
socket.send.buffer.bytes=102400

# socket服务接受数据的缓冲区大小，默认100KB
socket.receive.buffer.bytes=102400

# socket服务所能接受的一个请求的最大大小，默认为100M
socket.request.max.bytes=104857600

############################# Log Basics ############################# 
# kafka存储消息数据的目录
log.dirs=&#123;KAFKA_DATA_DIR&#125;

# 每个topic默认的partition
num.partitions=1

# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。
default.replication.factor=3

# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量
num.recovery.threads.per.data.dir=1

############################# Log Flush Policy ############################# 
# 消息刷新到磁盘中的消息条数阈值
log.flush.interval.messages=10000

# 消息刷新到磁盘中的最大时间间隔,1s
log.flush.interval.ms=1000

############################# Log Retention Policy ############################# 
# 日志保留小时数，超时会自动删除，默认为7天
log.retention.hours=168

# 日志保留大小，超出大小会自动删除，默认为1G
#log.retention.bytes=1073741824

# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件
log.segment.bytes=1073741824

# 每隔多长时间检测数据是否达到删除条件,300s
log.retention.check.interval.ms=300000

############################# Zookeeper ############################# 
# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开
zookeeper.connect=&#123;ZOOK_SERVERS&#125;

# 连接zookeeper的超时时间,6s
zookeeper.connection.timeout.ms=6000
</code></pre><h6 id="313-entrypoint"><a class="anchor" href="#313-entrypoint">#</a> 3.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
# 变量
KAFKA_DIR=/kafka
KAFKA_CONF=/kafka/config/server.properties

# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一
BROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))
LISTENERS=$(hostname -i)

# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递
sed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;
sed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;

# 3、启动Kafka
cd $&#123;KAFKA_DIR&#125;/bin
sed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh
./kafka-server-start.sh ../config/server.properties
</code></pre><h6 id="314-构建镜像并推送仓库"><a class="anchor" href="#314-构建镜像并推送仓库">#</a> 3.1.4 构建镜像并推送仓库</h6><pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2
</code></pre><h5 id="32-迁移kafka集群至k8s"><a class="anchor" href="#32-迁移kafka集群至k8s">#</a> 3.2 迁移 Kafka 集群至 K8S</h5><h6 id="321-kafka-headless"><a class="anchor" href="#321-kafka-headless">#</a> 3.2.1 kafka-headless</h6><pre><code># cat 01-kafka-headless.yaml 
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
  - name: client
    port: 9092
    targetPort: 9092
  - name: jmx
    port: 9999
    targetPort: 9999
</code></pre><h6 id="322-kafka-sts"><a class="anchor" href="#322-kafka-sts">#</a> 3.2.2 kafka-sts</h6><pre><code># cat 02-kafka-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: logging
spec:
  serviceName: &quot;kafka-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: [&quot;kafka&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: kafka
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 
        imagePullPolicy: Always
        ports:
        - name: client
          containerPort: 9092
        - name: jmxport
          containerPort: 9999
        env:
        - name: ZOOK_SERVERS
          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;
        readinessProbe:         # 就绪探针，不就绪则不介入流量
          tcpSocket:
            port: 9092
          initialDelaySeconds: 5
        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启
          tcpSocket:
            port: 9092
          initialDelaySeconds: 5
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="323-更新资源清单"><a class="anchor" href="#323-更新资源清单">#</a> 3.2.3 更新资源清单</h6><pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml 
[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml
[root@k8s-master01 02-kafka]# kubectl get pods -n logging 
NAME          READY   STATUS    RESTARTS       AGE
kafka-0       1/1     Running   0              5m49s
kafka-1       1/1     Running   0              4m43s
kafka-2       1/1     Running   0              3m40s

#查看kafka是否注册到zookeeper
[root@k8s-master01 02-kafka]# kubectl exec -it zookeeper-0 -n logging -- /bin/bash
root@zookeeper-0:/# /zookeeper/bin/zkCli.sh 
[zk: localhost:2181(CONNECTED) 2] get /brokers/ids/1
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.85.201:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.85.201&quot;,&quot;timestamp&quot;:&quot;1748162470218&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
[zk: localhost:2181(CONNECTED) 3] get /brokers/ids/2
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.58.205:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.58.205&quot;,&quot;timestamp&quot;:&quot;1748162532658&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
[zk: localhost:2181(CONNECTED) 4] get /brokers/ids/3
&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.195.1:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.195.1&quot;,&quot;timestamp&quot;:&quot;1748162649250&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;
</code></pre><h6 id="324-检查kafka集群"><a class="anchor" href="#324-检查kafka集群">#</a> 3.2.4 检查 Kafka 集群</h6><pre><code>1.创建一个topic
root@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu

2.模拟消息发布
root@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu
&gt;hello kubernetes
&gt;hello world

3.模拟消息订阅
root@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning
hello kubernetes
hello world
</code></pre><h4 id="四-交付efak至k8s"><a class="anchor" href="#四-交付efak至k8s">#</a> 四、交付 efak 至 K8S</h4><h5 id="41-制作efak镜像"><a class="anchor" href="#41-制作efak镜像">#</a> 4.1 制作 efak 镜像</h5><h6 id="411-dockerfile"><a class="anchor" href="#411-dockerfile">#</a> 4.1.1 Dockerfile</h6><pre><code>[root@manager 03-efak]# cat Dockerfile 
FROM openjdk:8

# 1、调整时区
RUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \
    echo 'Asia/Shanghai' &gt; /etc/timezone

# 2、拷贝kafka软件以及kafka的配置
ENV VERSION=3.0.1
ADD ./efak-web-$&#123;VERSION&#125;-bin.tar.gz /
ADD ./system-config.properties /efak-web-$&#123;VERSION&#125;/conf/system-config.properties

# 3、修改efak的名称
RUN mv /efak-web-$&#123;VERSION&#125; /efak

# 4、环境变量
ENV KE_HOME=/efak
ENV PATH=$PATH:$KE_HOME/bin

# 5、启动脚本（修改kafka配置）
ADD ./entrypoint.sh /entrypoint.sh

# 6、暴露kafka端口 9999是jmx的端口
EXPOSE 8048

# 7、运行启动脚本
CMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]
</code></pre><h6 id="412-system-config"><a class="anchor" href="#412-system-config">#</a> 4.1.2 system-config</h6><pre><code># cat system-config.properties 
######################################
# 填写 zookeeper集群列表
######################################
efak.zk.cluster.alias=cluster1
cluster1.zk.list=&#123;ZOOK_SERVERS&#125;

######################################
# broker 最大规模数量
######################################
cluster1.efak.broker.size=20

######################################
# zk 客户端线程数
######################################
kafka.zk.limit.size=32

######################################
# EFAK webui 端口
######################################
efak.webui.port=8048

######################################
# kafka offset storage
######################################
cluster1.efak.offset.storage=kafka

######################################
# kafka jmx uri
######################################
cluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi

######################################
# kafka metrics 指标，默认存储15天
######################################
efak.metrics.charts=true
efak.metrics.retain=15

######################################
# kafka sql topic records max
######################################
efak.sql.topic.records.max=5000
efak.sql.topic.preview.records.max=10

######################################
# delete kafka topic token
######################################
efak.topic.token=keadmin

######################################
# kafka sqlite 数据库地址（需要修改存储路径）
######################################
efak.driver=org.sqlite.JDBC
efak.url=jdbc:sqlite:&#123;EFAK_DATA_DIR&#125;/db/ke.db
efak.username=root
efak.password=www.kafka-eagle.org
</code></pre><h6 id="413-entrypoint"><a class="anchor" href="#413-entrypoint">#</a> 4.1.3 entrypoint</h6><pre><code># cat entrypoint.sh 
# 1、变量
EFAK_DIR=/efak
EFAK_CONF=/efak/conf/system-config.properties

# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递
sed -i s@&#123;EFAK_DATA_DIR&#125;@$&#123;EFAK_DIR&#125;@g  $&#123;EFAK_CONF&#125;
sed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;EFAK_CONF&#125;

# 3、启动efka
$&#123;EFAK_DIR&#125;/bin/ke.sh start
tail -f $&#123;EFAK_DIR&#125;/logs/ke_console.out
</code></pre><h6 id="414-构建镜像并推送仓库"><a class="anchor" href="#414-构建镜像并推送仓库">#</a> 4.1.4 构建镜像并推送仓库</h6><pre><code># wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0
</code></pre><h5 id="42-迁移efak至k8s"><a class="anchor" href="#42-迁移efak至k8s">#</a> 4.2 迁移 efak 至 K8S</h5><h6 id="421-efak-deploy"><a class="anchor" href="#421-efak-deploy">#</a> 4.2.1 efak-deploy</h6><pre><code># cat 01-efak-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: efak
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: efak
  template:
    metadata:
      labels:
        app: efak
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: efak
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 8048
        env:
        - name: ZOOK_SERVERS
          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;
</code></pre><h6 id="422-efak-service"><a class="anchor" href="#422-efak-service">#</a> 4.2.2 efak-service</h6><pre><code># cat 02-efak-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: efak-svc
  namespace: logging
spec:
  selector:
    app: efak
  ports:
  - port: 8048
    targetPort: 8048
</code></pre><h6 id="423-efak-ingress"><a class="anchor" href="#423-efak-ingress">#</a> 4.2.3 efak-ingress</h6><pre><code># cat 03-efak-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: efak-ingress
  namespace: logging
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;efak.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: efak-svc
            port: 
              number: 8048
</code></pre><h6 id="424-更新资源清单"><a class="anchor" href="#424-更新资源清单">#</a> 4.2.4 更新资源清单</h6><pre><code>[root@k8s-master01 03-efak]# kubectl apply -f 01-efak-deploy.yaml 
[root@k8s-master01 03-efak]# kubectl apply -f 02-efak-service.yaml 
[root@k8s-master01 03-efak]# kubectl apply -f 03-efak-ingress.yaml 
</code></pre><h6 id="425-访问efka"><a class="anchor" href="#425-访问efka">#</a> 4.2.5 访问 efka</h6><p>1、初始用户名密码 admin 123456</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Nq16u4z.png" alt="1.png"></p><p>2、查看 Topics</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/9Bin9cr.png" alt="2.png"></p><p>3、查看 kafka 集群状态</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/U76YIck.png" alt="3.png"></p><p>4、查看 Zookeeper 集群状态</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/cY5LeWx.png" alt="4.png"></p><h4 id="五-交付elastic集群"><a class="anchor" href="#五-交付elastic集群">#</a> 五、交付 Elastic 集群</h4><ul><li>ES 集群是由多个节点组成的，通过 <a target="_blank" rel="noopener" href="http://cluster.name">cluster.name</a> 设置 ES 集群名称，同时用于区分其它的 ES 集群。</li><li>每个节点通过 <a target="_blank" rel="noopener" href="http://node.name">node.name</a> 参数来设定所在集群的节点名称。</li><li>节点使用 discovery.send_hosts 参数来设定集群节点的列表。</li><li>集群在第一次启动时，需要初始化，同时需要指定参与选举的 master 节点 IP，或节点名称。</li><li>每个节点可以通过 node.master:true 设定为 master 角色，通过 node.data:true 设定为 data 角色。</li></ul><pre><code>[root@k8s-master01 ~]# grep &quot;^[a-Z]&quot; /etc/elasticsearch/elasticsearch.yml
# 集群名称cluster.name: my-oldxu
# 节点名称node.name: node1
# 数据存储路径path.data: /var/lib/elasticsearch
# 日志存储路径path.logs: /var/log/elasticsearch
# 监听在本地哪个地址上network.host: 10.0.0.100
# 监听端口http.port: 9200
# 集群主机列表discovery.seed_hosts: [&quot;ip1&quot;, &quot;ip2&quot;, &quot;ip3&quot;]
# 仅第一次启动集群时进行选举（可以填写node.name的名称）cluster.initial_master_nodes: [&quot;node01&quot;, &quot;node02&quot;, &quot;node03&quot;]
</code></pre><h5 id="51-下载elastic镜像"><a class="anchor" href="#51-下载elastic镜像">#</a> 5.1 下载 elastic 镜像</h5><pre><code># docker pull elasticsearch:7.17.6
# docker tag elasticsearch:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6
</code></pre><h5 id="52-交付es-service"><a class="anchor" href="#52-交付es-service">#</a> 5.2 交付 ES-Service</h5><p>创建 es-headlessService，为每个 ES Pod 设定固定的 DNS 名称，无论它是 Master 或是 Data，易或是 Coordinating</p><pre><code># cat 01-es-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: es-svc
  namespace: logging
spec:
  selector:
    app: es
  clusterIP: None
  ports:
  - name: cluster
    port: 9200
    targetPort: 9200
  - name: transport
    port: 9300
    targetPort: 9300
</code></pre><h5 id="53-交付es-master节点"><a class="anchor" href="#53-交付es-master节点">#</a> 5.3 交付 ES-Master 节点</h5><ol><li><p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data ；</p></li><li><p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p></li><li><p>ES 启动是通过 ENV 环境变量传参来完成的；</p><ul><li><p>集群名称、节点名称、角色类型；</p></li><li><p>discovery.seed_hosts 集群地址列表；</p></li><li><p>cluster.initial_master_nodes 初始集群参与选举的 master 节点名称；</p></li></ul></li></ol><pre><code># cat 02-es-master.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-master
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 3           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: master
  template:
    metadata:
      labels:
        app: es
        role: master
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;master&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;true&quot;
        - name: node.data
          value: &quot;false&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
        - name: cluster.initial_master_nodes
          value: &quot;es-master-0,es-master-1,es-master-2&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><pre><code>[root@k8s-master01 04-elasticsearch]# cat 03-es-data.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-data
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 2           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: data
  template:
    metadata:
      labels:
        app: es
        role: data
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;data&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;false&quot;
        - name: node.data
          value: &quot;true&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="54-交付es-data节点"><a class="anchor" href="#54-交付es-data节点">#</a> 5.4 交付 ES-Data 节点</h5><ol><li><p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data</p></li><li><p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p></li><li><p>ES 启动是通过 ENV 环境变量传参来完成的</p><ul><li><p>集群名称、节点名称、角色类型</p></li><li><p>discovery.seed_hosts 集群地址列表</p></li></ul></li></ol><pre><code># cat 03-es-data.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-data
  namespace: logging
spec:
  serviceName: &quot;es-svc&quot;
  replicas: 2           # es-pod运行的实例
  selector:             # 需要管理的ES-Pod标签
    matchLabels:
      app: es
      role: data
  template:
    metadata:
      labels:
        app: es
        role: data
    spec:                       # 定义pod规范
      imagePullSecrets:         # 镜像拉取使用的认证信息
      - name: harbor-admin
      affinity:                 # 设定pod反亲和
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [&quot;es&quot;]
              - key: role
                operator: In
                values: [&quot;data&quot;]
            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:               # ES主容器
      - name: es
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 
        resources:
          limits:
            cpu: 1000m
            memory: 4096Mi
          requests:
            cpu: 300m
            memory: 1024Mi
        ports:
        - name: cluster
          containerPort: 9200
        - name: transport
          containerPort: 9300
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        env:
        - name: ES_JAVA_OPTS
          value: &quot;-Xms1g -Xmx1g&quot;
        - name: cluster.name
          value: es-cluster
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: node.master
          value: &quot;false&quot;
        - name: node.data
          value: &quot;true&quot;
        - name: discovery.seed_hosts
          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates: # 动态pvc
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="55-更新资源清单"><a class="anchor" href="#55-更新资源清单">#</a> 5.5 更新资源清单</h5><pre><code>[root@k8s-master01 04-elasticsearch]# kubectl apply -f 01-es-svc.yaml 
[root@k8s-master01 04-elasticsearch]# kubectl apply -f 02-es-master.yaml 
[root@k8s-master01 04-elasticsearch]# kubectl apply -f 03-es-data.yaml 
</code></pre><h5 id="56-验证es集群"><a class="anchor" href="#56-验证es集群">#</a> 5.6 验证 ES 集群</h5><pre><code>#1.解析headlessService获取对应ES集群任一节点的IP地址
# dig @10.96.0.10 es-svc.logging.svc.cluster.local  +short
172.16.58.229
172.16.122.191
172.16.195.21
172.16.122.129
172.16.32.164

#2.通过curl访问ES，检查ES集群是否正常（如果仅交付Master，没有data节点，集群状态可能会Red，因为没有数据节点进行数据存储；）
# curl -XGET &quot;http://172.16.122.129:9200/_cluster/health?pretty&quot;
&#123;
  &quot;cluster_name&quot; : &quot;es-cluster&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 5,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 3,
  &quot;active_shards&quot; : 6,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;delayed_unassigned_shards&quot; : 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0,
  &quot;task_max_waiting_in_queue_millis&quot; : 0,
  &quot;active_shards_percent_as_number&quot; : 100.0
&#125;

#3.查看ES各个节点详情
# curl -XGET &quot;http://172.16.122.129:9200/_cat/nodes&quot;
172.16.122.129 16 33 20 0.38 0.56 0.38 ilmr       - es-master-2
172.16.58.229  66 33 22 0.64 0.66 0.44 ilmr       * es-master-1
172.16.122.191 52 34 15 0.38 0.56 0.38 cdfhilrstw - es-data-0
172.16.195.21  38 35 19 0.38 0.53 0.36 cdfhilrstw - es-data-1
172.16.32.164  31 33 12 0.28 0.50 0.59 ilmr       - es-master-0
</code></pre><h4 id="六-交付kibana可视化"><a class="anchor" href="#六-交付kibana可视化">#</a> 六、交付 Kibana 可视化</h4><h5 id="61-下载kibana镜像"><a class="anchor" href="#61-下载kibana镜像">#</a> 6.1 下载 kibana 镜像</h5><pre><code># docker pull kibana:7.17.6
# docker tag kibana:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6
</code></pre><h5 id="62-kibana-deploy"><a class="anchor" href="#62-kibana-deploy">#</a> 6.2 kibana-deploy</h5><ol><li>Kibana 需要连接 ES 集群，通过 ELASTICSEARCH_HOSTS 变量来传递 ES 集群地址</li><li>kibana 通过 I18N_LOCALE 来传递语言环境</li><li>Kibana 通过 SERVER_PUBLICBASEURL 来传递服务访问的公开地址</li></ol><pre><code># cat 01-kibana-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: kibana
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6 
        resources:
          limits:
            cpu: 1000m
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: '[&quot;http://es-data-0.es-svc:9200&quot;,&quot;http://es-data-1.es-svc:9200&quot;]'
        - name: I18N_LOCALE
          value: &quot;zh-CN&quot;
        - name: SERVER_PUBLICBASEURL
          value: &quot;http://kibana.hmallleasing.com&quot;   #kibana访问UI
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
</code></pre><h5 id="63-kibana-svc"><a class="anchor" href="#63-kibana-svc">#</a> 6.3 kibana-svc</h5><pre><code># cat 02-kibana-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: kibana-svc
  namespace: logging
spec:
  selector:
    app: kibana
  ports:
  - name: web
    port: 5601
    targetPort: 5601
</code></pre><h5 id="64-kibana-ingress"><a class="anchor" href="#64-kibana-ingress">#</a> 6.4 kibana-ingress</h5><pre><code># cat 03-kibana-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana-ingress
  namespace: logging
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;kibana.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana-svc
            port:
              number: 5601
</code></pre><h5 id="65-更新资源清单"><a class="anchor" href="#65-更新资源清单">#</a> 6.5 更新资源清单</h5><pre><code>[root@k8s-master01 05-kibana]# kubectl apply -f 01-kibana-deploy.yaml 
[root@k8s-master01 05-kibana]# kubectl apply -f 02-kibana-svc.yaml 
[root@k8s-master01 05-kibana]# kubectl apply -f 03-kibana-ingress.yaml

[root@k8s-master01 05-kibana]# kubectl get pods -n logging
NAME                      READY   STATUS    RESTARTS   AGE
efak-5cdc74bf59-nrhb4     1/1     Running   0          5h33m
es-data-0                 1/1     Running   0          16m
es-data-1                 1/1     Running   0          15m
es-master-0               1/1     Running   0          17m
es-master-1               1/1     Running   0          15m
es-master-2               1/1     Running   0          12m
kafka-0                   1/1     Running   0          5h39m
kafka-1                   1/1     Running   0          5h39m
kafka-2                   1/1     Running   0          5h38m
kibana-5ccc46864b-ndzx9   1/1     Running   0          118s
zookeeper-0               1/1     Running   0          5h42m
zookeeper-1               1/1     Running   0          5h42m
zookeeper-2               1/1     Running   0          5h41m
</code></pre><h5 id="66-访问kibana"><a class="anchor" href="#66-访问kibana">#</a> 6.6 访问 kibana</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/sUXTx1J.png" alt="1.png"></p><h4 id="七-daemonset运行日志agent实践"><a class="anchor" href="#七-daemonset运行日志agent实践">#</a> 七、DaemonSet 运行日志 Agent 实践</h4><h5 id="71-部署架构说明"><a class="anchor" href="#71-部署架构说明">#</a> 7.1 部署架构说明</h5><p>对于那些将日志输出到，stdout 与 stderr 的 Pod，可以直接使用 DaemonSet 控制器在每个 Node 节点上运行一个 filebeat、logstash、fluentd 容器进行统一的收集，而后写入到日志存储系统</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/UOlaNE1.png" alt="1.png"></p><h5 id="72-创建serviceaccount"><a class="anchor" href="#72-创建serviceaccount">#</a> 7.2 创建 ServiceAccount</h5><pre><code> kubectl create serviceaccount filebeat -n logging
</code></pre><h5 id="73-创建clusterrole"><a class="anchor" href="#73-创建clusterrole">#</a> 7.3 创建 ClusterRole</h5><pre><code>kubectl create clusterrole filebeat --verb=get,list,watch --resource=namespace,pods,nodes
</code></pre><h5 id="74-创建clusterrolebinding"><a class="anchor" href="#74-创建clusterrolebinding">#</a> 7.4 创建 ClusterRolebinding</h5><pre><code>kubectl create clusterrolebinding filebeat --serviceaccount=logging:filebeat --clusterrole=filebeat
</code></pre><h5 id="75-交付filebeat"><a class="anchor" href="#75-交付filebeat">#</a> 7.5 交付 Filebeat</h5><h6 id="751-下载filebeat镜像"><a class="anchor" href="#751-下载filebeat镜像">#</a> <strong>7.5.1 下载 filebeat 镜像</strong></h6><pre><code># docker pull docker.elastic.co/beats/filebeat:7.17.6
# docker tag docker.elastic.co/beats/filebeat:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat:7.17.6
</code></pre><h6 id="752-交付filebeat"><a class="anchor" href="#752-交付filebeat">#</a> <strong>7.5.2 交付 filebeat</strong></h6><ol><li>从 ConfigMap 中挂载 filebeat.yaml 配置文件；</li><li>挂载 /var/log、/var/lib/docker/containers 日志相关目录；</li><li>使用 hostPath 方式挂载 /usr/share/filebeat/data 数据目录，该目录下有一个 registry 文件，里面记录了 filebeat 采集日志位置的相关内容，比如文件 offset、source、timestamp 等，如果 Pod 发生异常后 K8S 自动将 Pod 进行重启，不挂载的情况下 registry 会被重置，将导致日志文件又从 offset=0 开始采集，会造成重复收集日志。</li></ol><pre><code>[root@k8s-master01 07-filebeat-daemoset]# cat 02-filebeat-ds.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: logging
spec:
  selector:
    matchLabels:
      app: filebeat
  template:
    metadata:
      labels:
        app: filebeat
    spec:
      serviceAccountName: &quot;filebeat&quot;
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: &quot;Exists&quot;
        effect: &quot;NoSchedule&quot;
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: filebeat
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat:7.17.6
        args: [
          &quot;-c&quot;,&quot;/etc/filebeat.yml&quot;,
          &quot;-e&quot;
        ]
        securityContext:
          runAsUser: 0
        resources:
          limits:
            memory: 300Mi
        volumeMounts:
        - name: config                          # 从ConfigMap中读取
          mountPath: /etc/filebeat.yml
          subPath: filebeat.yml
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: config
        configMap:
          name: filebeat-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: data
        hostPath:
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot; 
</code></pre><h6 id="753-filebeat配置文件"><a class="anchor" href="#753-filebeat配置文件">#</a> 7.5.3 Filebeat 配置文件</h6><pre><code># cat 01-filebeat-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: logging

data:
  filebeat.yml: |-
    # ============================== Filebeat inputs ==============================
    logging.level: warning
    filebeat.inputs:
    - type: log
      enabled: true
      encoding: utf-8
      paths: /var/log/messages
      include_lines: ['kubelet']            # 获取与kubelet相关的日志
      fields:                               # 添加filebeat字段
        namespace: kubelet
      fields_under_root: true

    # ============================== Filebeat autodiscover ============================
    filebeat.autodiscover:
      providers:
        - type: kubernetes
          templates:
          - condition:						# 匹配kube-system名称空间下所有日志
              equals:
                kubernetes.namespace: kube-system
            config:
              - type: container
                stream: all					# 收集stdout、stderr类型日志，all是所有
                encoding: utf-8
                paths: /var/log/containers/*-$&#123;data.kubernetes.container.id&#125;.log
                exclude_lines: ['info']     # 排除info相关的日志

          - condition:                      # 收集ingress-nginx命名空间下stdout日志
              equals:
                kubernetes.namespace: ingress-nginx
            config:
              - type: container
                stream: stdout
                encoding: utf-8
                paths: /var/log/containers/*-$&#123;data.kubernetes.container.id&#125;.log
                json.keys_under_root: true  # 默认将json解析存储至messages，true则不存储至message
                json.overwrite_keys: true   # 覆盖默认message字段，使用自定义json格式的key
                #exclude_lines: ['kibana']   # 与kibana相关的则排除

          - condition:                              # 收集ingress-nginx命名空间下stderr日志
              equals:
                kubernetes.namespace: ingress-nginx
            config:
              - type: container
                stream: stderr
                encoding: utf-8
                paths:
                  - /var/log/containers/*-$&#123;data.kubernetes.container.id&#125;.log

    # ============================== Filebeat Processors ===========================
    processors:
      - rename:                              # 重写kubernetes源数据信息
          fields:
          - from: &quot;kubernetes.namespace&quot;
            to: &quot;namespace&quot;
          - from: &quot;kubernetes.pod.name&quot;
            to: &quot;podname&quot;
          - from: &quot;kubernetes.pod.ip&quot;
            to: &quot;podip&quot;
      - drop_fields:                        # 删除无用的字段
          fields: [&quot;host&quot;,&quot;agent&quot;,&quot;ecs&quot;,&quot;input&quot;,&quot;container&quot;,&quot;kubernetes&quot;]

    # ================================== Kafka Output ===================================
    output.kafka:
      hosts: [&quot;kafka-0.kafka-svc:9092&quot;,&quot;kafka-1.kafka-svc:9092&quot;,&quot;kafka-2.kafka-svc:9092&quot;]
      topic: &quot;app-%&#123;[namespace]&#125;&quot;	# %&#123;[namespace]&#125; 会自动将其转换为namespace对应的值
      required_acks: 1              # 保证消息可靠，0不保证，1等待写入主分区（默认）-1等待写入副本分区
      compression: gzip             # 压缩
      max_message_bytes: 1000000    # 每条消息最大的长度，多余的被删除
</code></pre><h6 id="754-收集ingress-nginx名称空间"><a class="anchor" href="#754-收集ingress-nginx名称空间">#</a> 7.5.4 收集 ingress-nginx 名称空间</h6><p>修改 Ingress 日志输出格式</p><pre><code># kubectl edit configmap -n ingress-nginx ingress-nginx-controller 
apiVersion: v1
data:
  ...
  log-format-upstream: '&#123;&quot;timestamp&quot;:&quot;$time_iso8601&quot;,&quot;domain&quot;:&quot;$server_name&quot;,&quot;hostname&quot;:&quot;$hostname&quot;,&quot;remote_user&quot;:&quot;$remote_user&quot;,&quot;clientip&quot;:&quot;$remote_addr&quot;,&quot;proxy_protocol_addr&quot;:&quot;$proxy_protocol_addr&quot;,&quot;@source&quot;:&quot;$server_addr&quot;,&quot;host&quot;:&quot;$http_host&quot;,&quot;request&quot;:&quot;$request&quot;,&quot;args&quot;:&quot;$args&quot;,&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&quot;status&quot;:&quot;$status&quot;,&quot;upstream_status&quot;:&quot;$upstream_status&quot;,&quot;bytes&quot;:&quot;$body_bytes_sent&quot;,&quot;responsetime&quot;:&quot;$request_time&quot;,&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&quot;proxy_upstream_name&quot;:&quot;$proxy_upstream_name&quot;,&quot;x_forwarded&quot;:&quot;$http_x_forwarded_for&quot;,&quot;upstream_response_length&quot;:&quot;$upstream_response_length&quot;,&quot;referer&quot;:&quot;$http_referer&quot;,&quot;user_agent&quot;:&quot;$http_user_agent&quot;,&quot;request_length&quot;:&quot;$request_length&quot;,&quot;request_method&quot;:&quot;$request_method&quot;,&quot;scheme&quot;:&quot;$scheme&quot;,&quot;k8s_ingress_name&quot;:&quot;$ingress_name&quot;,&quot;k8s_service_name&quot;:&quot;$service_name&quot;,&quot;k8s_service_port&quot;:&quot;$service_port&quot;&#125;'
</code></pre><h6 id="755-更新资源清单"><a class="anchor" href="#755-更新资源清单">#</a> 7.5.5 更新资源清单</h6><pre><code>kubectl apply -f 01-filebeat-configmap.yaml
kubectl apply -f 02-filebeat-ds.yaml
</code></pre><h6 id="756-检查kafka对应topic"><a class="anchor" href="#756-检查kafka对应topic">#</a> 7.5.6 检查 kafka 对应 Topic</h6><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/shg5F6T.png" alt="PixPin_2025-06-05_16-02-45.png"></p><h4 id="八-交付logstash"><a class="anchor" href="#八-交付logstash">#</a> 八、 交付 Logstash</h4><h5 id="81-下载logstash镜像"><a class="anchor" href="#81-下载logstash镜像">#</a> 8.1 下载 Logstash 镜像</h5><pre><code># docker pull docker.elastic.co/logstash/logstash-oss:7.17.6
# docker tag docker.elastic.co/logstash/logstash-oss:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6
</code></pre><h5 id="82-如何交付logstash"><a class="anchor" href="#82-如何交付logstash">#</a> 8.2 如何交付 Logstash</h5><ol><li>Logstash 需要设定环境变量来调整主配置文件参数，比如：worker 运行数量，以及批量处理的最大条目是多少；</li><li>Logstash 需要调整 JVM 堆内存使用的范围，没办法传参调整，但可以通过 postStart 来修改其文件对应的 jvm 参数；</li><li>Logstash 需要配置文件，读取 Kafka 数据，而后通过 filter 处理，最后输出至 ES</li></ol><pre><code># /usr/share/logstash/config/logstash.yml
# 可通过变量传参修改
pipeline.workers: 2
pipeline.batch.size: 1000
# /usr/share/logstash/config/jvm.options-Xms512m-Xmx512m
# /usr/share/logstash/config/logstash.conf
input &#123;
	kafka
&#125;
filter &#123;

&#125;
</code></pre><h5 id="83-准备logstash配置"><a class="anchor" href="#83-准备logstash配置">#</a> 8.3 准备 logstash 配置</h5><p><strong>input 段含义</strong></p><ol><li>所有数据都从 kafka 集群中获取；</li><li>获取 kafka 集群中 topic，主要有 app-kube-system、app-ingress-nginx、app-kubelet</li></ol><p><strong>filter 段含义</strong></p><ol><li>判断 namespace 等于 kubelet，则为其添加一个索引字段名称；</li><li>判断 namespace 等于 kube-system，则为其添加一个索引字段名称；</li><li>判断 namespace 等于 ingress-nginx，并且 stream 等于 stderr，则为其添加一个索引字段名称；</li><li>判断 namespace 等于 ingress-nginx，并且 stream 等于 stdout，则使用 geoip 获取地址来源，使用 useragent 模块分析来访客户端设备，使用 date 处理时间，使用 mutate 转换对应字段格式，最后添加一个索引字段名称；</li></ol><pre><code># cat logstash-node.conf 
input &#123;
	kafka &#123;
	bootstrap_servers =&gt; &quot;kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092&quot;
        group_id =&gt; &quot;logstash-node&quot;     # 消费者组名称
        consumer_threads =&gt; &quot;3&quot;         # 理想情况下，您应该拥有与分区数一样多的线程,以实现完美的平衡
        topics =&gt; [&quot;app-kube-system&quot;,&quot;app-ingress-nginx&quot;,&quot;app-kubelet&quot;]
	    codec =&gt; json
    &#125;
&#125;

filter &#123;
##########################################################################
	if &quot;kubelet&quot; in [namespace] &#123;
		mutate &#123; 
 			add_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;
		&#125;
	&#125;

##########################################################################
	if &quot;kube-system&quot; in [namespace] &#123;
		mutate &#123; 
 			add_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;
		&#125;
	&#125;
	
##########################################################################
	if [namespace] == &quot;ingress-nginx&quot; and [stream] == &quot;stdout&quot; &#123;
		geoip &#123;
        		source =&gt; &quot;clientip&quot;
    		&#125;
		useragent &#123;
        		source =&gt; &quot;user_agent&quot;
			target =&gt; &quot;user_agent&quot;
        	&#125;

    		date &#123;
			# 2022-10-08T13:13:20.000Z
        		match =&gt; [&quot;timestamp&quot;,&quot;ISO8601&quot;]
        		target =&gt; &quot;@timestamp&quot;
        		timezone =&gt; &quot;Asia/Shanghai&quot;	
    		&#125;
		
		mutate &#123; 
			convert =&gt; &#123; 
				&quot;bytes&quot; =&gt; &quot;integer&quot;
				&quot;responsetime&quot; =&gt; &quot;float&quot;
				&quot;upstreamtime&quot; =&gt; &quot;float&quot;
			&#125;
			add_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;[stream]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;
		&#125;
	&#125;

##########################################################################
	if [namespace] == &quot;ingress-nginx&quot; and [stream] == &quot;stderr&quot; &#123;
		mutate &#123; 
 		  add_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;[stream]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;
		&#125;
	&#125;
&#125;

output &#123;
    stdout &#123;
        codec =&gt; rubydebug
    &#125;
    elasticsearch &#123;
        hosts =&gt; [&quot;es-data-0.es-svc:9200&quot;,&quot;es-data-1.es-svc:9200&quot;]
        index =&gt; &quot;%&#123;[target_index]&#125;&quot;
        template_overwrite =&gt; true
    &#125;
&#125;
</code></pre><h5 id="84-创建configmap"><a class="anchor" href="#84-创建configmap">#</a> 8.4 创建 ConfigMap</h5><pre><code>kubectl create configmap logstash-node-conf --from-file=logstash.conf=conf/logstash-node.conf -n logging
</code></pre><h5 id="85-创建service"><a class="anchor" href="#85-创建service">#</a> 8.5 创建 Service</h5><pre><code># cat 01-logstash-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: logstash-svc
  namespace: logging
spec:
  clusterIP: None
  selector:
    app: logstash
  ports:
  - port: 9600
    targetPort: 9600
</code></pre><h5 id="86-交付logstash"><a class="anchor" href="#86-交付logstash">#</a> 8.6 交付 Logstash</h5><pre><code>[root@k8s-master01 08-logstash]# cat 02-logstash-node-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logstash-node
  namespace: logging
spec:
  serviceName: &quot;logstash-svc&quot;
  replicas: 1
  selector:
    matchLabels:
      app: logstash
      env: node
  template:
    metadata:
      labels:
        app: logstash
        env: node
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: logstash
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6 
        args: [&quot;-f&quot;,&quot;config/logstash.conf&quot;]                     # 启动时指定加载的配置文件
        resources:
          limits:
            memory: 1024Mi
        env:
        - name: PIPELINE_WORKERS
          value: &quot;2&quot;
        - name: PIPELINE_BATCH_SIZE
          value: &quot;10000&quot;
        lifecycle:
          postStart:                                            # 设定JVM
            exec:
              command:
              - &quot;/bin/bash&quot;
              - &quot;-c&quot;
              - &quot;sed -i -e '/^-Xms/c-Xms512m' -e '/^-Xmx/c-Xmx512m' /usr/share/logstash/config/jvm.options&quot;
        volumeMounts:
        - name: data                                            # 持久化数据目录
          mountPath: /usr/share/logstash/data
        - name: conf
          mountPath: /usr/share/logstash/config/logstash.conf
          subPath: logstash.conf
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: conf
        configMap:
          name: logstash-node-conf
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h5 id="87-更新资源清单"><a class="anchor" href="#87-更新资源清单">#</a> 8.7 更新资源清单</h5><pre><code>kubectl apply -f 01-logstash-svc.yaml 
kubectl apply -f 02-logstash-node-sts.yaml
</code></pre><h5 id="88-检查kibana索引"><a class="anchor" href="#88-检查kibana索引">#</a> 8.8 检查 kibana 索引</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/DB4w8Ln.png" alt="2.png"></p><h4 id="九-kibana可视化"><a class="anchor" href="#九-kibana可视化">#</a> 九、Kibana 可视化</h4><h5 id="91-创建索引"><a class="anchor" href="#91-创建索引">#</a> 9.1 创建索引</h5><p>kube-system 索引</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/agOzRmb.png" alt="3.png"></p><p>ingress-stdout 索引</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Iv0xBBe.png" alt="4.png"></p><p>ingress-stderr 索引</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/RpbsKpc.png" alt="1.png"></p><p>kubelet 索引</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/2JBN6fi.png" alt="5.png"></p><h5 id="92-日志展示"><a class="anchor" href="#92-日志展示">#</a> 9.2 日志展示</h5><p>app-ingress-nginx-stdout 索引日志</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/Yyf0gyS.png" alt="1.png"></p><p>app-ingress-nginx-stderr 索引日志</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/dUWbjv3.png" alt="2.png"></p><p>app-kube-system 索引日志</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/HQO7ECM.png" alt="3.png"></p><p>app-kubelet 索引日志</p><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/HN2BpY7.png" alt="4.png"></p><h5 id="93-图形展示"><a class="anchor" href="#93-图形展示">#</a> 9.3 图形展示</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/fcx5E8m.png" alt="5.png"></p><div class="tags"><a href="/tags/ELKStack/" rel="tag"><i class="ic i-tag"></i>ELKStack</a></div></div><footer><div class="meta"><span class="icon"><i class="ic i-eye"></i></span><span>此文章已被阅读次数:</span><span class="waline-pageview-count" id="twikoo_visitors" data-path="/posts/570469260.html">正在加载...</span><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于</span><time title="修改时间：2025-06-05 19:12:23" itemprop="dateModified" datetime="2025-06-05T19:12:23+08:00">2025-06-05</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.png" alt="Xu Yong 微信支付"><p>微信支付</p></div><div><img loading="lazy" data-src="/assets/alipay.png" alt="Xu Yong 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者：</strong>Xu Yong<i class="ic i-at"><em>@</em></i>LinuxSre云原生</li><li class="link"><strong>本文链接：</strong><a href="http://ixuyong.cn/posts/570469260.html" title="ELK收集Kubernetes组件日志分析与实践">http://ixuyong.cn/posts/570469260.html</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/posts/1384812626.html" rel="prev" itemprop="url" data-background-image="https:&#x2F;&#x2F;wp-cdn.4ce.cn&#x2F;v2&#x2F;ysB0MfV.jpeg" title="Kubenetes部署Rabbitmq集群"><span class="type">上一篇</span><span class="category"><i class="ic i-flag"></i>Rabbitmq</span><h3>Kubenetes部署Rabbitmq集群</h3></a></div><div class="item right"><a href="/posts/2126514413.html" rel="next" itemprop="url" data-background-image="https:&#x2F;&#x2F;wp-cdn.4ce.cn&#x2F;v2&#x2F;RxKV39a.jpeg" title="虚拟隧道网络Openvpn"><span class="type">下一篇</span><span class="category"><i class="ic i-flag"></i>Openvpn</span><h3>虚拟隧道网络Openvpn</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#elk%E6%94%B6%E9%9B%86kubernetes%E7%BB%84%E4%BB%B6%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.</span> <span class="toc-text">ELK 收集 Kubernetes 组件日志分析与实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-elk%E5%88%9B%E5%BB%BAnamespace%E5%92%8Csecrets"><span class="toc-number">1.1.</span> <span class="toc-text">一、ELK 创建 Namespace 和 Secrets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E4%BA%A4%E4%BB%98zookeeper%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.2.</span> <span class="toc-text">二、交付 Zookeeper 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#21-%E5%88%B6%E4%BD%9Czk%E9%9B%86%E7%BE%A4%E9%95%9C%E5%83%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 制作 ZK 集群镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#211-dockerfile"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">2.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#212-zoocfg"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">2.1.2 zoo.cfg</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#213-entrypoint"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">2.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#214-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.2.1.4.</span> <span class="toc-text">2.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#22-%E8%BF%81%E7%A7%BBzookeeper%E8%87%B3k8s"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 迁移 zookeeper 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#221-zookeeper-headless"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">2.2.1 zookeeper-headless</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#222-zookeeper-sts"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2.2.2 zookeeper-sts</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#223-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">2.2.3 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#224-%E6%A3%80%E6%9F%A5zookeeper%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">2.2.4 检查 zookeeper 集群状态</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#225-%E8%BF%9E%E6%8E%A5zookeeper%E9%9B%86%E7%BE%A4"><span class="toc-number">1.2.2.5.</span> <span class="toc-text">2.2.5 连接 Zookeeper 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-%E4%BA%A4%E4%BB%98kafka%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.3.</span> <span class="toc-text">三、 交付 Kafka 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#31-%E5%88%B6%E4%BD%9Ckafka%E9%9B%86%E7%BE%A4%E9%95%9C%E5%83%8F"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 制作 Kafka 集群镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#311-dockerfile"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">3.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#312-serverproperties"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">3.1.2 server.properties</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#313-entrypoint"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">3.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#314-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.3.1.4.</span> <span class="toc-text">3.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#32-%E8%BF%81%E7%A7%BBkafka%E9%9B%86%E7%BE%A4%E8%87%B3k8s"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 迁移 Kafka 集群至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#321-kafka-headless"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">3.2.1 kafka-headless</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#322-kafka-sts"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">3.2.2 kafka-sts</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#323-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">3.2.3 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#324-%E6%A3%80%E6%9F%A5kafka%E9%9B%86%E7%BE%A4"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">3.2.4 检查 Kafka 集群</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B-%E4%BA%A4%E4%BB%98efak%E8%87%B3k8s"><span class="toc-number">1.4.</span> <span class="toc-text">四、交付 efak 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#41-%E5%88%B6%E4%BD%9Cefak%E9%95%9C%E5%83%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 制作 efak 镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#411-dockerfile"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">4.1.1 Dockerfile</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#412-system-config"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">4.1.2 system-config</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#413-entrypoint"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">4.1.3 entrypoint</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#414-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">4.1.4 构建镜像并推送仓库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#42-%E8%BF%81%E7%A7%BBefak%E8%87%B3k8s"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 迁移 efak 至 K8S</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#421-efak-deploy"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">4.2.1 efak-deploy</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#422-efak-service"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">4.2.2 efak-service</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#423-efak-ingress"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">4.2.3 efak-ingress</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#424-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">4.2.4 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#425-%E8%AE%BF%E9%97%AEefka"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">4.2.5 访问 efka</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94-%E4%BA%A4%E4%BB%98elastic%E9%9B%86%E7%BE%A4"><span class="toc-number">1.5.</span> <span class="toc-text">五、交付 Elastic 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#51-%E4%B8%8B%E8%BD%BDelastic%E9%95%9C%E5%83%8F"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 下载 elastic 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#52-%E4%BA%A4%E4%BB%98es-service"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 交付 ES-Service</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#53-%E4%BA%A4%E4%BB%98es-master%E8%8A%82%E7%82%B9"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 交付 ES-Master 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#54-%E4%BA%A4%E4%BB%98es-data%E8%8A%82%E7%82%B9"><span class="toc-number">1.5.4.</span> <span class="toc-text">5.4 交付 ES-Data 节点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#55-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.5.5.</span> <span class="toc-text">5.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#56-%E9%AA%8C%E8%AF%81es%E9%9B%86%E7%BE%A4"><span class="toc-number">1.5.6.</span> <span class="toc-text">5.6 验证 ES 集群</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AD-%E4%BA%A4%E4%BB%98kibana%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.6.</span> <span class="toc-text">六、交付 Kibana 可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#61-%E4%B8%8B%E8%BD%BDkibana%E9%95%9C%E5%83%8F"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 下载 kibana 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#62-kibana-deploy"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 kibana-deploy</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#63-kibana-svc"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 kibana-svc</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#64-kibana-ingress"><span class="toc-number">1.6.4.</span> <span class="toc-text">6.4 kibana-ingress</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#65-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.6.5.</span> <span class="toc-text">6.5 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#66-%E8%AE%BF%E9%97%AEkibana"><span class="toc-number">1.6.6.</span> <span class="toc-text">6.6 访问 kibana</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%83-daemonset%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97agent%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.7.</span> <span class="toc-text">七、DaemonSet 运行日志 Agent 实践</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#71-%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E8%AF%B4%E6%98%8E"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 部署架构说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#72-%E5%88%9B%E5%BB%BAserviceaccount"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 创建 ServiceAccount</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#73-%E5%88%9B%E5%BB%BAclusterrole"><span class="toc-number">1.7.3.</span> <span class="toc-text">7.3 创建 ClusterRole</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#74-%E5%88%9B%E5%BB%BAclusterrolebinding"><span class="toc-number">1.7.4.</span> <span class="toc-text">7.4 创建 ClusterRolebinding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#75-%E4%BA%A4%E4%BB%98filebeat"><span class="toc-number">1.7.5.</span> <span class="toc-text">7.5 交付 Filebeat</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#751-%E4%B8%8B%E8%BD%BDfilebeat%E9%95%9C%E5%83%8F"><span class="toc-number">1.7.5.1.</span> <span class="toc-text">7.5.1 下载 filebeat 镜像</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#752-%E4%BA%A4%E4%BB%98filebeat"><span class="toc-number">1.7.5.2.</span> <span class="toc-text">7.5.2 交付 filebeat</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#753-filebeat%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">1.7.5.3.</span> <span class="toc-text">7.5.3 Filebeat 配置文件</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#754-%E6%94%B6%E9%9B%86ingress-nginx%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4"><span class="toc-number">1.7.5.4.</span> <span class="toc-text">7.5.4 收集 ingress-nginx 名称空间</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#755-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.7.5.5.</span> <span class="toc-text">7.5.5 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#756-%E6%A3%80%E6%9F%A5kafka%E5%AF%B9%E5%BA%94topic"><span class="toc-number">1.7.5.6.</span> <span class="toc-text">7.5.6 检查 kafka 对应 Topic</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AB-%E4%BA%A4%E4%BB%98logstash"><span class="toc-number">1.8.</span> <span class="toc-text">八、 交付 Logstash</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#81-%E4%B8%8B%E8%BD%BDlogstash%E9%95%9C%E5%83%8F"><span class="toc-number">1.8.1.</span> <span class="toc-text">8.1 下载 Logstash 镜像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#82-%E5%A6%82%E4%BD%95%E4%BA%A4%E4%BB%98logstash"><span class="toc-number">1.8.2.</span> <span class="toc-text">8.2 如何交付 Logstash</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#83-%E5%87%86%E5%A4%87logstash%E9%85%8D%E7%BD%AE"><span class="toc-number">1.8.3.</span> <span class="toc-text">8.3 准备 logstash 配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#84-%E5%88%9B%E5%BB%BAconfigmap"><span class="toc-number">1.8.4.</span> <span class="toc-text">8.4 创建 ConfigMap</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#85-%E5%88%9B%E5%BB%BAservice"><span class="toc-number">1.8.5.</span> <span class="toc-text">8.5 创建 Service</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#86-%E4%BA%A4%E4%BB%98logstash"><span class="toc-number">1.8.6.</span> <span class="toc-text">8.6 交付 Logstash</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#87-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.8.7.</span> <span class="toc-text">8.7 更新资源清单</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#88-%E6%A3%80%E6%9F%A5kibana%E7%B4%A2%E5%BC%95"><span class="toc-number">1.8.8.</span> <span class="toc-text">8.8 检查 kibana 索引</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B9%9D-kibana%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.9.</span> <span class="toc-text">九、Kibana 可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#91-%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95"><span class="toc-number">1.9.1.</span> <span class="toc-text">9.1 创建索引</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#92-%E6%97%A5%E5%BF%97%E5%B1%95%E7%A4%BA"><span class="toc-number">1.9.2.</span> <span class="toc-text">9.2 日志展示</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#93-%E5%9B%BE%E5%BD%A2%E5%B1%95%E7%A4%BA"><span class="toc-number">1.9.3.</span> <span class="toc-text">9.3 图形展示</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/posts/170066797.html" rel="bookmark" title="消费租赁项目Kubernetes基于ELK日志分析与实践">消费租赁项目Kubernetes基于ELK日志分析与实践</a></li><li class="active"><a href="/posts/570469260.html" rel="bookmark" title="ELK收集Kubernetes组件日志分析与实践">ELK收集Kubernetes组件日志分析与实践</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="Xu Yong" src="/assets/avatar.png"><p class="name" itemprop="name">Xu Yong</p><div class="description" itemprop="description">专注于 Linux 运维、云计算、云原⽣等技术</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">41</span><span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">14</span><span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">15</span><span class="name">标签</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/xyapples" class="item github" title="https:&#x2F;&#x2F;github.com&#x2F;xyapples"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://gitee.com/chinagei" class="item gitee" title="https:&#x2F;&#x2F;gitee.com&#x2F;chinagei"><i class="ic i-gitee"></i></a><a href="mailto:373370405@qq.com" class="item email" title="mailto:373370405@qq.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-about"></i>关于</a></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"><a href="/posts/2126514413.html" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/posts/1384812626.html" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/DevOps/" title="分类于DevOps">DevOps</a></div><span><a href="/posts/889219339.html">K8S基于Jenkins实现SpringCloud微服务CI与CD实践（三）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/108692210.html">K8s资源调度deployment、statefulset、daemonset</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/169153047.html">K8s持久化存储</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/312010518.html">K8s亲和力Affinity</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Redis/" title="分类于Redis">Redis</a></div><span><a href="/posts/1490514396.html">Redis Cluster集群部署</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3254599477.html">K8s容忍和污点</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Linux/" title="分类于Linux">Linux</a></div><span><a href="/posts/2260957686.html">Linux—Centos7修改网卡名称</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/858611107.html">K8s服务发布Service</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/722512536.html">K8s细粒度权限控制RBAC</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3142072607.html">K8s初始化容器、临时容器</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2><ul class="leancloud-recent-comment" id="new-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2022 -<span itemprop="copyrightYear">2025</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">Xu Yong @ LinuxSre云原生</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">676k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">10:15</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL = {
    ispost: true,
        path: `posts/570469260.html`,
        favicon: {
        show: `（●´3｀●）やれやれだぜ`,
        hide: `(´Д｀)大変だ！`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};</script><script src="https://s4.zstatic.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha384-ZvpUoO&#x2F;+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn&#x2F;6Z&#x2F;hRTt8+pR6L4N2" crossorigin="anonymous" fetchpriority="high"></script><script src="/js/siteInit.js?v=0.4.17" type="module" fetchpriority="high" defer></script></body></html><!-- rebuild by hrmmi -->