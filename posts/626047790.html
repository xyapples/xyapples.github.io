<!-- build time:Thu May 29 2025 20:41:42 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon.ico"><link rel="alternate" href="/rss.xml" title="LinuxSre云原生" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="LinuxSre云原生" type="application/atom+xml"><link rel="alternate" type="application/json" title="LinuxSre云原生" href="http://ixuyong.cn/feed.json"><link rel="preconnect" href="https://s4.zstatic.net"><link rel="preconnect" href="https://at.alicdn.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/app.css?v=0.4.17"><link rel="modulepreload" href="/js/chunk-GV364XSK.js"><link rel="modulepreload" href="/js/chunk-NYSE5UKM.js"><link rel="modulepreload" href="/js/chunk-RONCYO2S.js"><link rel="modulepreload" href="/js/chunk-THHXCRSX.js"><link rel="modulepreload" href="/js/chunk-WIQECBEN.js"><link rel="modulepreload" href="/js/comments-DL2IYMPZ.js"><link rel="modulepreload" href="/js/copy-tex-NADCTXPG.js"><link rel="modulepreload" href="/js/post-DA635IH6.js"><link rel="modulepreload" href="/js/quicklink-WEDHL4BA.js"><link rel="modulepreload" href="/js/search-VCZRKTM5.js"><link rel="modulepreload" href="/js/siteInit.js"><link rel="modulepreload" href="/js/waline-NNBYRQEE.js"><link rel="stylesheet" href="/css/comments-F4ZGS7LD.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/waline-IDNZKML2.css" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="preload" href="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" as="image" fetchpriority="high"><meta name="keywords" content="Kubernetes"><meta name="description" content="专注于 Linux 运维、云计算、云原⽣等技术"><link rel="canonical" href="http://ixuyong.cn/posts/626047790.html"><title>消费租赁系统微服务应用交付实践</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">消费租赁系统微服务应用交付实践</h1><div class="meta"><span class="item" title="创建时间：2025-05-18 21:42:46"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2025-05-18T21:42:46+08:00">2025-05-18</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>81k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>1:14</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">LinuxSre云原生</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><img src="https://s21.ax1x.com/2025/03/29/pEsS0hD.png" loading="eager" decoding="async" fetchpriority="high" alt="LinuxSre云原生"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemlistelement itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Kubernetes/" itemprop="item" rel="index" title="分类于Kubernetes"><span itemprop="name">Kubernetes<meta itemprop="position" content="0"></span></a></span></div><article class="post block" itemscope itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://ixuyong.cn/posts/626047790.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.png"><meta itemprop="name" content="Xu Yong"><meta itemprop="description" content="致力于技术布道、普及前沿技术、打造云原生系列标杆博客, 专注于 Linux 运维、云计算、云原⽣等技术"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="LinuxSre云原生"></span><div class="body md" itemprop="articleBody"><h3 id="消费租赁系统微服务应用交付实践"><a class="anchor" href="#消费租赁系统微服务应用交付实践">#</a> 消费租赁系统微服务应用交付实践</h3><h4 id="一-部署中间件"><a class="anchor" href="#一-部署中间件">#</a> 一、部署中间件</h4><h5 id="11-部署mysql"><a class="anchor" href="#11-部署mysql">#</a> 1.1 部署 MySQL</h5><h6 id="111-mysql-configmap"><a class="anchor" href="#111-mysql-configmap">#</a> 1.1.1 MySQL-ConfigMap</h6><pre><code>[root@k8s-master01 01-nf-flms-mysql]# cat 01-mysql-cm.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-cm
  namespace: prod
data:
  my.cnf: |-
    [mysqld]
    #performance setttings
    lock_wait_timeout = 3600
    open_files_limit = 65535
    back_log = 1024
    max_connections = 1024
    max_connect_errors = 1000000
    table_open_cache = 1024
    table_definition_cache = 1024
    thread_stack = 512K
    sort_buffer_size = 4M
    join_buffer_size = 4M
    read_buffer_size = 8M
    read_rnd_buffer_size = 4M
    bulk_insert_buffer_size = 64M
    thread_cache_size = 768
    interactive_timeout = 600
    wait_timeout = 600
    tmp_table_size = 32M
    max_heap_table_size = 32M
    max_allowed_packet = 128M
</code></pre><h6 id="112-mysql-secret"><a class="anchor" href="#112-mysql-secret">#</a> 1.1.2 MySQL-Secret</h6><pre><code>[root@k8s-master01 01-nf-flms-mysql]# cat 02-mysql-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
  namespace: prod
stringData:
  MYSQL_ROOT_PASSWORD: Superman*2023
type: Opaque
</code></pre><h6 id="113-mysql-statefulset"><a class="anchor" href="#113-mysql-statefulset">#</a> 1.1.3 MySQL-StatefulSet</h6><pre><code># cat 03-mysql-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-nf-flms
  namespace: prod
spec:
  serviceName: &quot;mysql-nf-flms-svc&quot;
  replicas: 1
  selector:
    matchLabels:
      app: mysql
      role: nf-flms
  template:
    metadata:
      labels:
        app: mysql
        role: nf-flms
    spec:
      containers:
      - name: db
        image: mysql:8.0
        args:
        - &quot;--character-set-server=utf8&quot;
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_ROOT_PASSWORD
        - name: MYSQL_DATABASE      #数据库名称
          value: nf-flms        
        ports:
        - name: tcp-3306
          containerPort: 3306
          protocol: TCP
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        resources:
          limits:
            cpu: 2000m
            memory: 4000Mi
          requests:
            cpu: 200m
            memory: 500Mi
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql/
        - name: config
          mountPath: /etc/mysql/conf.d/my.cnf
          subPath: my.cnf
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: config
        configMap:
          name: mysql-cm
          items:
            - key: my.cnf
              path: my.cnf
          defaultMode: 420
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      storageClassName: &quot;nfs-storage&quot;
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="114-mysql-service"><a class="anchor" href="#114-mysql-service">#</a> 1.1.4 MySQL Service</h6><pre><code># cat 04-mysql-nf-flms-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-nf-flms-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: mysql
    role: nf-flms
  ports:
  - name: tcp-mysql-svc
    protocol: TCP
    port: 3306
    targetPort: 3306
---
kind: Service
apiVersion: v1
metadata:
  name: mysql-nf-flms-svc-balance
  namespace: prod 
spec:
  selector:
    app: mysql
    role: nf-flms
  ports:
  - name: tcp-mysql-balance
    protocol: TCP
    port: 3306
    targetPort: 3306
    nodePort: 32206
  type: NodePort
</code></pre><h6 id="115-更新资源清单"><a class="anchor" href="#115-更新资源清单">#</a> 1.1.5 更新资源清单</h6><pre><code>[root@k8s-master01 01-nf-flms-mysql]# sed -i &quot;s#dev#prod#g&quot; *.yaml
[root@k8s-master01 01-nf-flms-mysql]# kubectl create ns prod
[root@k8s-master01 01-nf-flms-mysql]# kubectl apply -f .
</code></pre><h6 id="116-导入数据库"><a class="anchor" href="#116-导入数据库">#</a> 1.1.6 导入数据库</h6><pre><code>[root@k8s-master01 ~]# dig @10.96.0.10 mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local +short
172.16.85.231
[root@k8s-master01 ~]# mysql -h 172.16.85.231 -uroot -p&quot;Superman*2023&quot; -B nf_flms &lt; 202503310038/sggyl_nf_flms_202503310038.sql
</code></pre><h5 id="12-部署redis-single"><a class="anchor" href="#12-部署redis-single">#</a> 1.2 部署 Redis-single</h5><h6 id="121-redis-configmap"><a class="anchor" href="#121-redis-configmap">#</a> 1.2.1 Redis-ConfigMap</h6><pre><code># cat 01-redis-cm.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-conf
  namespace: prod
data:
  redis.conf: |
    bind 0.0.0.0
    appendonly yes
    protected-mode no
    dir /data
    port 6379
    requirepass Superman*2023
</code></pre><h6 id="122-redis-statefulset"><a class="anchor" href="#122-redis-statefulset">#</a> 1.2.2 Redis-StatefulSet</h6><pre><code># cat 02-redis-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: prod
spec:
  serviceName: redis-svc
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:6.2.7
        command:
        - &quot;redis-server&quot;
        args:
        - &quot;/etc/redis/redis.conf&quot;
        ports:
        - name: redis-6379
          containerPort: 6379
          protocol: TCP
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 6379
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 6379
        volumeMounts:
        - name: config
          mountPath: /etc/redis
        - name: data
          mountPath: /data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        resources:
          limits:
            cpu: '2'
            memory: 4000Mi
          requests:
            cpu: 100m
            memory: 500Mi
      volumes:
      - name: config
        configMap:
          name: redis-conf
          items:
          - key: redis.conf
            path: redis.conf
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 2Gi
</code></pre><h6 id="123-redis-service"><a class="anchor" href="#123-redis-service">#</a> 1.2.3 Redis-Service</h6><pre><code># cat 03-redis-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: redis-svc
  namespace: prod
  labels:
    app: redis
spec:
  ports:
    - name: redis-6379
      protocol: TCP
      port: 6379
      targetPort: 6379
  selector:
    app: redis
  clusterIP: None
</code></pre><h6 id="124-更新资源清单"><a class="anchor" href="#124-更新资源清单">#</a> 1.2.4 更新资源清单</h6><pre><code>[root@k8s-master01 02-redis]# sed -i &quot;s#dev#prod#g&quot; *.yaml
[root@k8s-master01 02-redis]# kubectl apply -f .
</code></pre><h5 id="14-部署nacos集群"><a class="anchor" href="#14-部署nacos集群">#</a> 1.4 部署 Nacos 集群</h5><h6 id="141-部署nacos-mysql"><a class="anchor" href="#141-部署nacos-mysql">#</a> 1.4.1 部署 Nacos-MySQL</h6><pre><code># cat 01-mysql-nacos-sts-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-nacos-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: mysql
    role: nacos
  ports:
  - port: 3306
    targetPort: 3306
---
kind: Service
apiVersion: v1
metadata:
  name: mysql-nacos-balance
  namespace: prod
spec:
  selector:
    app: mysql
    role: nacos
  ports:
  - name: tcp-mysql-balance
    protocol: TCP
    port: 3306
    targetPort: 3306
    nodePort: 31106
  type: NodePort
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-nacos
  namespace: prod
spec:
  serviceName: &quot;mysql-nacos-svc&quot;
  replicas: 1
  selector:
    matchLabels:
      app: mysql
      role: nacos
  template:
    metadata:
      labels:
        app: mysql
        role: nacos
    spec:
      containers:
      - name: db
        image: mysql:8.0
        args:
        - &quot;--character-set-server=utf8&quot;
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: Superman*2023
        - name: MYSQL_DATABASE    #nacos库名称
          value: nacos
        ports:
        - containerPort: 3306
        resources:
          limits:
            cpu: '2'
            memory: 4000Mi
          requests:
            cpu: 100m
            memory: 500Mi
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql/
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="142-导入数据库"><a class="anchor" href="#142-导入数据库">#</a> 1.4.2 导入数据库</h6><p>nacos 下载地址：<a target="_blank" rel="noopener" href="https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK">https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK</a></p><pre><code>[root@k8s-master01 03-nacos]#  sed -i &quot;s#dev#prod#g&quot; *.yaml
[root@k8s-master01 03-nacos]# kubectl apply -f 01-mysql-nacos-sts-svc.yaml
[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-nacos-svc.prod.svc.cluster.local +short
172.16.32.159
[root@k8s-master01 03-nacos]# mysql -h 172.16.32.159 -uroot -p&quot;Superman*2023&quot; -B nacos &lt; nacos/conf/mysql-schema.sql
[root@k8s-master01 ~]# mysql -h 172.16.32.159 -uroot -p&quot;Superman*2023&quot; -B nacos &lt; sggyl_nf_nacos_202505210038.sql
</code></pre><h6 id="143-部署nacos-configmap"><a class="anchor" href="#143-部署nacos-configmap">#</a> 1.4.3 部署 Nacos-ConfigMap</h6><pre><code># cat 02-nacos-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: nacos-cm
  namespace: prod
data:
  mysql.host: &quot;mysql-nacos-svc.prod.svc.cluster.local&quot;
  mysql.db.name: &quot;nacos&quot;   #nacos数据库名称
  mysql.port: &quot;3306&quot;
  mysql.user: &quot;root&quot;    #nacos数据库用户名
  mysql.password: &quot;Superman*2023&quot;   #nacos数据库密码
</code></pre><h6 id="144-部署nacos-service-statefulset"><a class="anchor" href="#144-部署nacos-service-statefulset">#</a> 1.4.4 部署 Nacos-Service-StatefulSet</h6><p><strong>1. 开启鉴权</strong></p><pre><code># cat 03-nacos-sts-deploy-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nacos-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: nacos
  ports:
  - name: server
    port: 8848
    targetPort: 8848
  - name: client-rpc
    port: 9848
    targetPort: 9848
  - name: raft-rpc
    port: 9849
    targetPort: 9849
  - name: old-raft-rpc
    port: 7848
    targetPort: 7848
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nacos
  namespace: prod
spec:
  serviceName: &quot;nacos-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: nacos
  template:
    metadata:
      labels:
        app: nacos
    spec:
      affinity:                                                 # 避免Pod运行到同一个节点上了
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values: [&quot;nacos&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;  
      initContainers:
      - name: peer-finder-plugin-install
        image: nacos/nacos-peer-finder-plugin:1.1
        imagePullPolicy: IfNotPresent 
        volumeMounts:
          - name: data
            mountPath: /home/nacos/plugins/peer-finder
            subPath: peer-finder
      containers:
      - name: nacos
        image: nacos/nacos-server:v2.4.3
        resources:
          limits:
            cpu: '2'
            memory: 4Gi
          requests:
            cpu: &quot;100m&quot;
            memory: &quot;1Gi&quot;
        ports:
        - name: client-port
          containerPort: 8848
        - name: client-rpc
          containerPort: 9848
        - name: raft-rpc
          containerPort: 9849
        - name: old-raft-rpc
          containerPort: 7848
        env:
        - name: NACOS_REPLICAS
          value: &quot;3&quot;
        - name: SERVICE_NAME
          value: &quot;nacos-svc&quot;
        - name: DOMAIN_NAME
          value: &quot;cluster.local&quot;
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: MYSQL_SERVICE_HOST
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.host
        - name: MYSQL_SERVICE_DB_NAME
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.db.name
        - name: MYSQL_SERVICE_PORT
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.port
        - name: MYSQL_SERVICE_USER
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.user
        - name: MYSQL_SERVICE_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.password
        - name: SPRING_DATASOURCE_PLATFORM
          value: &quot;mysql&quot;
        - name: NACOS_SERVER_PORT
          value: &quot;8848&quot;
        - name: NACOS_APPLICATION_PORT
          value: &quot;8848&quot;
        - name: PREFER_HOST_MODE
          value: &quot;hostname&quot;
        - name: NACOS_AUTH_ENABLE
          value: &quot;true&quot;
        - name: NACOS_AUTH_IDENTITY_KEY
          value: &quot;nacosAuthKey&quot;
        - name: NACOS_AUTH_IDENTITY_VALUE
          value: &quot;nacosSecurtyValue&quot;
        - name: NACOS_AUTH_TOKEN
          value: &quot;SecretKey012345678901234567890123456789012345678901234567890123456789&quot;
        - name: NACOS_AUTH_TOKEN_EXPIRE_SECONDS
          value: &quot;18000&quot;
        volumeMounts:
        - name: data
          mountPath: /home/nacos/plugins/peer-finder
          subPath: peer-finder
        - name: data
          mountPath: /home/nacos/data
          subPath: data
        - name: data
          mountPath: /home/nacos/logs
          subPath: logs
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        storageClassName: &quot;nfs-storage&quot;
        accessModes: [&quot;ReadWriteMany&quot;]
        resources:
          requests:
            storage: 5Gi
</code></pre><p><strong>2. 关闭鉴权</strong></p><pre><code># cat 03-nacos-sts-deploy-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nacos-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: nacos
  ports:
  - name: server
    port: 8848
    targetPort: 8848
  - name: client-rpc
    port: 9848
    targetPort: 9848
  - name: raft-rpc
    port: 9849
    targetPort: 9849
  - name: old-raft-rpc
    port: 7848
    targetPort: 7848
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nacos
  namespace: prod
spec:
  serviceName: &quot;nacos-svc&quot;
  replicas: 3
  selector:
    matchLabels:
      app: nacos
  template:
    metadata:
      labels:
        app: nacos
    spec:
      affinity:                                                 # 避免Pod运行到同一个节点上了
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values: [&quot;nacos&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;  
      initContainers:
      - name: peer-finder-plugin-install
        image: nacos/nacos-peer-finder-plugin:1.1
        imagePullPolicy: Always
        volumeMounts:
          - name: data
            mountPath: /home/nacos/plugins/peer-finder
            subPath: peer-finder
      containers:
      - name: nacos
        image: nacos/nacos-server:v2.4.3
        resources:
          limits:
            cpu: '2'
            memory: 4Gi
          requests:
            cpu: &quot;100m&quot;
            memory: &quot;1Gi&quot;
        ports:
        - name: client-port
          containerPort: 8848
        - name: client-rpc
          containerPort: 9848
        - name: raft-rpc
          containerPort: 9849
        - name: old-raft-rpc
          containerPort: 7848
        env:
        - name: NACOS_AUTH_ENABLE
          value: &quot;false&quot;
        - name: MODE  
          value: &quot;cluster&quot;
        - name: NACOS_SERVERS
          value: &quot;nacos-0.nacos-svc.prod.svc.cluster.local:8848  nacos-1.nacos-svc.prod.svc.cluster.local:8848 nacos-2.nacos-svc.prod.svc.cluster.local:8848&quot;
        - name: NACOS_VERSION
          value: 2.4.3
        - name: SPRING_DATASOURCE_PLATFORM
          value: &quot;mysql&quot;
        - name: NACOS_REPLICAS
          value: &quot;3&quot;
        - name: SERVICE_NAME 
          value: &quot;nacos-svc&quot;
        - name: DOMAIN_NAME 
          value: &quot;cluster.local&quot;
        - name: NACOS_SERVER_PORT   
          value: &quot;8848&quot;
        - name: NACOS_APPLICATION_PORT
          value: &quot;8848&quot;
        - name: PREFER_HOST_MODE
          value: &quot;hostname&quot;
        - name: POD_NAMESPACE      
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: MYSQL_SERVICE_HOST
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.host
        - name: MYSQL_SERVICE_DB_NAME
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.db.name
        - name: MYSQL_SERVICE_PORT
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.port
        - name: MYSQL_SERVICE_USER
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.user
        - name: MYSQL_SERVICE_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.password
        volumeMounts:
        - name: data
          mountPath: /home/nacos/plugins/peer-finder
          subPath: peer-finder
        - name: data
          mountPath: /home/nacos/data
          subPath: data
        - name: data
          mountPath: /home/nacos/logs
          subPath: logs
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        storageClassName: &quot;nfs-storage&quot;
        accessModes: [&quot;ReadWriteMany&quot;]
        resources:
          requests:
            storage: 5Gi
</code></pre><h6 id="145-部署nacos-ingress"><a class="anchor" href="#145-部署nacos-ingress">#</a> 1.4.5 部署 Nacos-Ingress</h6><pre><code># cat 04-nacos-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nacos-ingress
  namespace: prod
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: nacos.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nacos-svc
            port:
              number: 8848
        path: /
        pathType: ImplementationSpecific
</code></pre><h6 id="146-更新资源清单"><a class="anchor" href="#146-更新资源清单">#</a> 1.4.6 更新资源清单</h6><pre><code>[root@k8s-master01 03-nacos]# sed -i &quot;s#dev#prod#g&quot; *.yaml
[root@k8s-master01 03-nacos]# kubectl apply -f .

#检查cluster是否一致
[root@k8s-master01 03-nacos]# for i in &#123;0..2&#125;; do echo nacos-$i; kubectl exec nacos-$i -c nacos -n prod -- cat conf/cluster.conf; donenacos-0
#2025-05-21T10:43:12.668
nacos-0.nacos-svc.prod.svc.cluster.local:8848
nacos-1.nacos-svc.prod.svc.cluster.local:8848
nacos-2.nacos-svc.prod.svc.cluster.local:8848
nacos-1
#2025-05-21T10:43:14.879
nacos-0.nacos-svc.prod.svc.cluster.local:8848
nacos-1.nacos-svc.prod.svc.cluster.local:8848
nacos-2.nacos-svc.prod.svc.cluster.local:8848
nacos-2
#2025-05-21T10:43:17.299
nacos-0.nacos-svc.prod.svc.cluster.local:8848
nacos-1.nacos-svc.prod.svc.cluster.local:8848
nacos-2.nacos-svc.prod.svc.cluster.local:8848
</code></pre><h6 id="147-web访问nacos"><a class="anchor" href="#147-web访问nacos">#</a> 1.4.7 Web 访问 nacos</h6><pre><code>Url：http://nacos.hmallleasing.com/nacos 
User: nacos
Passwd: nacos 
</code></pre><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/iNiUErY.jpeg" alt="Snipaste_2025-05-18_21-13-44.jpg"></p><h5 id="15-部署xxl-job"><a class="anchor" href="#15-部署xxl-job">#</a> 1.5 部署 xxl-job</h5><h6 id="151-部署xxl-job-mysql"><a class="anchor" href="#151-部署xxl-job-mysql">#</a> 1.5.1 部署 xxl-job-MySQL</h6><pre><code># cat 01-mysql-xxljob-sts-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-xxljob-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: mysql
    role: xxljob
  ports:
    - name: tcp-mysql-svc
      protocol: TCP
      port: 3306
      targetPort: 3306
---
apiVersion: v1
kind: Service
apiVersion: v1
metadata:
  name: mysql-xxljob-external
  namespace: prod
spec:
  ports:
    - name: tcp-mysql-external
      protocol: TCP
      port: 3306
      targetPort: 3306
      nodePort: 31206
  selector:
    app: mysql
    role: xxljob
  type: NodePort
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-xxljob
  namespace: prod
spec:
  serviceName: &quot;mysql-xxljob-svc&quot;
  replicas: 1
  selector:
    matchLabels:
      app: mysql
      role: xxljob
  template:
    metadata:
      labels:
        app: mysql
        role: xxljob
    spec:
      containers:
      - name: db
        image: mysql:8.0
        args:
        - &quot;--character-set-server=utf8&quot;
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: Superman*2023
        ports:
        - containerPort: 3306
        resources:
          limits:
            cpu: 2000m
            memory: 4000Mi
          requests:
            cpu: 200m
            memory: 500Mi
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql/
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteMany&quot;]
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="152-导入数据库"><a class="anchor" href="#152-导入数据库">#</a> 1.5.2 导入数据库</h6><p>xxljob 表结构下载地址：<a target="_blank" rel="noopener" href="https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db">https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db</a></p><pre><code>[root@k8s-master01 05-xxl-job]# kubectl apply -f 01-mysql-xxljob-sts-svc.yaml
[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-xxljob-svc.prod.svc.cluster.local +short
172.16.85.250
[root@k8s-master01 05-xxl-job]# mysql -h 172.16.85.250  -uroot -p&quot;Superman*2023&quot;  &lt; tables_xxl_job.sql
</code></pre><h6 id="153-部署xxl-job-service-deployment"><a class="anchor" href="#153-部署xxl-job-service-deployment">#</a> 1.5.3 部署 xxl-job-Service-Deployment</h6><pre><code># cat 02-xxljob-deploy-svc.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: xxl-job
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: xxl-job
  template:
    metadata:
      labels:
        app: xxl-job
    spec:
      containers:
      - image: xuxueli/xxl-job-admin:3.1.0
        name: xxl-job
        ports:
        - containerPort: 8080
        env:
        - name: PARAMS
          value: &quot;--spring.datasource.url=jdbc:mysql://mysql-xxljob-svc.prod.svc.cluster.local:3306/xxl_job?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;serverTimezone=Asia/Shanghai --spring.datasource.username=root --spring.datasource.password=Superman*2023&quot;
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        resources:
          limits:
            cpu: '1'
            memory: 2000Mi
          requests:
            cpu: 100m
            memory: 500Mi
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
---
apiVersion: v1
kind: Service
metadata:
  name: xxljob-svc
  namespace: prod
spec:
  ports:
  - port: 8080
    protocol: TCP
    name: http
  selector:
    app: xxl-job
</code></pre><h6 id="154-部署xxl-job-service"><a class="anchor" href="#154-部署xxl-job-service">#</a> 1.5.4 部署 xxl-job-service</h6><pre><code>[root@k8s-master01 05-xxl-job]# cat 04-xxljob-external.yaml 
apiVersion: v1
kind: Service
metadata:
  name: xxljob-balancer
  namespace: prod
spec:
  type: NodePort
  ports:
    - name: xxljob-balancer
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app: xxl-job
</code></pre><h6 id="155-部署xxl-job-ingress"><a class="anchor" href="#155-部署xxl-job-ingress">#</a> 1.5.5 部署 xxl-job-Ingress</h6><pre><code>[root@k8s-master01 05-xxl-job]# cat 03-xxljob-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: xxljob-ingress
  namespace: prod
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: xxljob.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: xxljob-svc
            port:
              number: 8080
        path: /
        pathType: ImplementationSpecific
</code></pre><h6 id="156-更新资源清单"><a class="anchor" href="#156-更新资源清单">#</a> 1.5.6 更新资源清单</h6><pre><code>[root@k8s-master01 05-xxl-job]# sed -i &quot;s#dev#prod#g&quot; *.yaml
[root@k8s-master01 05-xxl-job]# kubectl apply -f .
</code></pre><h6 id="157-web访问xxl-job"><a class="anchor" href="#157-web访问xxl-job">#</a> 1.5.7 Web 访问 xxl-job</h6><pre><code>http://192.168.40.101:30904/xxl-job-admin/
http://xxljob.hmallleasing.com/xxl-job-admin/ 
user:admin    
pwd:1223456
</code></pre><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/A5NbU2Z.jpeg" alt="Snipaste_2025-05-18_14-54-59.jpg"></p><h5 id="16-部署rabbitmq集群"><a class="anchor" href="#16-部署rabbitmq集群">#</a> 1.6 部署 rabbitmq 集群</h5><h6 id="161-创建rbac权限"><a class="anchor" href="#161-创建rbac权限">#</a> 1.6.1 创建 RBAC 权限</h6><pre><code># cat 01-rabbitmq-rbac.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rabbitmq-cluster
  namespace: prod
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rabbitmq-cluster
  namespace: prod
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;endpoints&quot;]
  verbs: [&quot;get&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rabbitmq-cluster
  namespace: prod
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rabbitmq-cluster
subjects:
- kind: ServiceAccount
  name: rabbitmq-cluster
  namespace: prod
</code></pre><h6 id="162-创建集群的secret"><a class="anchor" href="#162-创建集群的secret">#</a> 1.6.2 创建集群的 Secret</h6><pre><code># cat 02-rabbitmq-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: rabbitmq-secret
  namespace: prod
stringData:
  password: talent
  url: amqp://RABBITMQ_USER:RABBITMQ_PASS@rmq-cluster-balancer
  username: superman
type: Opaque
</code></pre><h6 id="163-创建configmap"><a class="anchor" href="#163-创建configmap">#</a> 1.6.3 创建 ConfigMap</h6><pre><code># cat 03-rabbitmq-cm.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-cluster-config
  namespace: prod
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
data:
    enabled_plugins: |
      [rabbitmq_management,rabbitmq_peer_discovery_k8s].
    rabbitmq.conf: |
      loopback_users.guest = false
      default_user = RABBITMQ_USER
      default_pass = RABBITMQ_PASS
      ## Cluster 
      cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s
      cluster_formation.k8s.host = kubernetes.default.svc
      #cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
      cluster_formation.k8s.address_type = hostname
      #################################################
      # prod is rabbitmq-cluster's namespace#
      #################################################
      cluster_formation.k8s.hostname_suffix = .rabbitmq-cluster.prod.svc.cluster.local
      cluster_formation.node_cleanup.interval = 30
      cluster_formation.node_cleanup.only_log_warning = true
      cluster_partition_handling = autoheal
      ## queue master locator
      queue_master_locator = min-masters
      cluster_formation.randomized_startup_delay_range.min = 0
      cluster_formation.randomized_startup_delay_range.max = 2
      # memory
      vm_memory_high_watermark.absolute = 100MB
      # disk
      disk_free_limit.absolute = 2GB
</code></pre><p><em>注：配置文件 cluster_formation.k8s.host 设置为 kubernetes.default.svc.cluster.local，然后就是各种连不上，后来换上 kubernetes.default.svc 就可以了，不知道是不是 k8s 新版本的问题。</em></p><h6 id="164-创建集群的svc"><a class="anchor" href="#164-创建集群的svc">#</a> 1.6.4 创建集群的 svc</h6><pre><code># cat 04-rabbitmq-cluster-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: rabbitmq-cluster
  name: rabbitmq-cluster
  namespace: prod
spec:
  clusterIP: None
  ports:
  - name: rmqport
    port: 5672
    targetPort: 5672
  - name: http
    port: 15672
    protocol: TCP
    targetPort: 15672
  selector:
    app: rabbitmq-cluster
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: rabbitmq-cluster-balancer
  name: rabbitmq-cluster-balancer
  namespace: prod
spec:
  ports:
  - name: rmqport
    port: 5672
    targetPort: 5672
  - name: http
    port: 15672
    protocol: TCP
    targetPort: 15672
  selector:
    app: rabbitmq-cluster
  type: NodePort
</code></pre><h6 id="165-创建statefulset"><a class="anchor" href="#165-创建statefulset">#</a> 1.6.5 创建 StatefulSet</h6><pre><code># cat 05-rabbitmq-cluster-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: rabbitmq-cluster
  name: rabbitmq-cluster
  namespace: prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rabbitmq-cluster
  serviceName: rabbitmq-cluster
  template:
    metadata:
      labels:
        app: rabbitmq-cluster
    spec:
      affinity:                                                 # 避免Pod运行到同一个节点上了
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values: [&quot;rabbitmq-cluster&quot;]
              topologyKey: &quot;kubernetes.io/hostname&quot;  
      containers:
      - args:
        - -c
        - cp -v /etc/rabbitmq/rabbitmq.conf $&#123;RABBITMQ_CONFIG_FILE&#125;; exec docker-entrypoint.sh rabbitmq-server
        command:
        - sh
        env:
        - name: RABBITMQ_DEFAULT_USER
          valueFrom:
            secretKeyRef:
              key: username
              name: rabbitmq-secret
        - name: RABBITMQ_DEFAULT_PASS 
          valueFrom:
            secretKeyRef:
              key: password 
              name: rabbitmq-secret
        - name: TZ
          value: 'Asia/Shanghai'
        - name: RABBITMQ_ERLANG_COOKIE
          value: 'SWvCP0Hrqv43NG7GybHC95ntCJKoW8UyNFWnBEWG8TY='
        - name: K8S_SERVICE_NAME
          value: rabbitmq-cluster
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: RABBITMQ_USE_LONGNAME
          value: &quot;true&quot;
        - name: RABBITMQ_NODENAME
          value: rabbit@$(POD_NAME).$(K8S_SERVICE_NAME).$(POD_NAMESPACE).svc.cluster.local
        - name: RABBITMQ_CONFIG_FILE
          value: /var/lib/rabbitmq/rabbitmq.conf
        image: rabbitmq:3.9-management
        imagePullPolicy: IfNotPresent
        name: rabbitmq
        ports:
        - containerPort: 15672
          name: http
          protocol: TCP
        - containerPort: 5672
          name: amqp
          protocol: TCP
        livenessProbe:
          exec:
            command: [&quot;rabbitmq-diagnostics&quot;, &quot;status&quot;]
          initialDelaySeconds: 60
          periodSeconds: 60
          failureThreshold: 2
          timeoutSeconds: 10
        readinessProbe:
          exec:
            command: [&quot;rabbitmq-diagnostics&quot;, &quot;status&quot;]
          failureThreshold: 2
          initialDelaySeconds: 60
          periodSeconds: 60
          timeoutSeconds: 10
        volumeMounts:
        - mountPath: /etc/rabbitmq
          name: config-volume
          readOnly: false
        - mountPath: /var/lib/rabbitmq
          name: rabbitmq-storage
          readOnly: false
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      serviceAccountName: rabbitmq-cluster
      terminationGracePeriodSeconds: 30
      volumes:
      - name: config-volume
        configMap:
          items:
          - key: rabbitmq.conf
            path: rabbitmq.conf
          - key: enabled_plugins
            path: enabled_plugins
          name: rabbitmq-cluster-config
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-storage
    spec:
      accessModes:
      - ReadWriteMany
      storageClassName: &quot;nfs-storage&quot;
      resources:
        requests:
          storage: 5Gi
</code></pre><h6 id="166-创建ingress"><a class="anchor" href="#166-创建ingress">#</a> 1.6.6 创建 Ingress</h6><pre><code>[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rabbitmq-ingress
  namespace: prod
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: rabbitmq.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: rabbitmq-cluster
            port:
              number: 15672
        path: /
        pathType: ImplementationSpecific
</code></pre><h6 id="167-更新资源清单"><a class="anchor" href="#167-更新资源清单">#</a> 1.6.7 更新资源清单</h6><pre><code>[root@k8s-master01 04-rabbitmq]# sed -i &quot;s#dev#prod#g&quot; *.yaml
[root@k8s-master01 04-rabbitmq]# kubectl apply -f .
[root@k8s-master01 04-rabbitmq]# kubectl get pods -n prod
NAME                 READY   STATUS    RESTARTS   AGE
rabbitmq-cluster-0   1/1     Running   0          9m53s
rabbitmq-cluster-1   1/1     Running   0          8m47s
rabbitmq-cluster-2   1/1     Running   0          7m40s

[root@k8s-master01 04-rabbitmq]# kubectl exec -it rabbitmq-cluster-0 -n prod -- /bin/bash
root@rabbitmq-cluster-0:/# rabbitmqctl cluster_status
RABBITMQ_ERLANG_COOKIE env variable support is deprecated and will be REMOVED in a future version. Use the $HOME/.erlang.cookie file or the --erlang-cookie switch instead.
Cluster status of node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local ...
Basics

Cluster name: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local

Disk Nodes

rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local

Running Nodes

rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local

Versions

rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9
rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9
rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9

Maintenance status

Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance

Alarms

Memory alarm on node rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local
Memory alarm on node rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local
Memory alarm on node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local

Network Partitions

(none)

Listeners

Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0

Feature flags

Flag: drop_unroutable_metric, state: enabled
Flag: empty_basic_get_metric, state: enabled
Flag: implicit_default_bindings, state: enabled
Flag: maintenance_mode_status, state: enabled
Flag: quorum_queue, state: enabled
Flag: stream_queue, state: enabled
Flag: user_limits, state: enabled
Flag: virtual_host_metadata, state: enabled
</code></pre><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/XqURJbg.jpeg" alt="Snipaste_2025-05-17_17-42-47.jpg"></p><h6 id="168-web访问rabbitmq"><a class="anchor" href="#168-web访问rabbitmq">#</a> 1.6.8 Web 访问 rabbitmq</h6><pre><code>http://rabbitmq.hmallleasing.com/#/
user:superman
pwd:talent
</code></pre><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/7qKxUBf.jpeg" alt="Snipaste_2025-05-17_17-14-54.jpg"></p><h6 id="168-rabbitmq全部挂了无法重启解决方案"><a class="anchor" href="#168-rabbitmq全部挂了无法重启解决方案">#</a> 1.6.8 rabbitMQ 全部挂了，无法重启解决方案</h6><p>Kubernetes 环境中，遇到 RabbitMQ 集群无法启动的问题。原因是 RabbitMQ 所有实例均失效，需要在每个 Pod 对应的持久化存储路径下创建 force_load 文件来强制启动。通过获取 PV 存储路径，在指定目录创建该文件后，重新启动 RabbitMQ 服务，成功解决了集群启动问题</p><pre><code>[root@k8s-node02 ~# cd /data/dev-rabbitmq-storage-rabbitmq-cluster-0-pvc-3abca920-3c68-44eb-b0fd-406a4358b153/mnesia/rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local
[root@k8s-node02 rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local]# touch force_load
</code></pre><h5 id="17-部署rabbitmq-single"><a class="anchor" href="#17-部署rabbitmq-single">#</a> 1.7 部署 rabbitmq-single</h5><pre><code>[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-single.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rabbitmq-single-data
  namespace: prod
spec:
  storageClassName: &quot;nfs-storage&quot;     # 明确指定使用哪个sc的供应商来创建pv
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi                      # 根据业务实际大小进行资源申请
---

apiVersion: v1
kind: Service
metadata:
  name: rabbitmq-single-svc
  namespace: prod
  labels:
    name: rabbitmq-single-svc
spec:
  ports:
  - port: 5672 
    protocol: TCP
    name: web
    targetPort: 5672
  - name: http
    port: 15672
    protocol: TCP
    targetPort: 15672
  selector:
    app: rabbitmq-single

---
apiVersion: networking.k8s.io/v1 # k8s &gt;= 1.22 必须 v1
kind: Ingress
metadata:
  name: rabbitmq-single-ingress
  namespace: prod
spec:
  ingressClassName: nginx
  rules:
  - host: rabbitmq.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: rabbitmq-single-svc
            port:
              number: 15672
        path: /
        pathType: Prefix

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rabbitmq-single
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq-single
  template:
    metadata:
      labels:
        app: rabbitmq-single
    spec:
      containers:
      - name: rabbitmq-single
        image: rabbitmq:3.9-management
        ports:
        - containerPort: 5672
          name: web
          protocol: TCP
        - containerPort: 15672
          name: http
          protocol: TCP
        env:
        - name: RABBITMQ_DEFAULT_USER  # 自定义环境变量
          value: admin
        - name: RABBITMQ_DEFAULT_PASS
          value: Superman*2025
        resources:
          requests:
            memory: &quot;1Gi&quot;
            cpu: &quot;500m&quot;
        livenessProbe:
          exec:
            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 10
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: rabbitmq-storage
          mountPath: /var/lib/rabbitmq
      volumes:
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File
      - name: rabbitmq-storage
        persistentVolumeClaim:
          claimName: rabbitmq-single-data
</code></pre><h4 id="二-微服务配置"><a class="anchor" href="#二-微服务配置">#</a> 二、微服务配置</h4><h5 id="21-代码编译前配置"><a class="anchor" href="#21-代码编译前配置">#</a> 2.1 代码编译前配置</h5><p>每个微服务需要配置</p><pre><code>[root@jenkins nf-flms]# cat nf-flms-order/src/main/resources/bootstrap-prd.yml 
info:
  groupId: @project.groupId@
  artifactId: @project.artifactId@
  version: @project.version@
  name: 南方手机租赁平台
  copyright: 2021
  description: 诚信 务实 专注 专业 创新

server:
  port: 8080

spring:
  application:
    name: @artifactId@
  profiles:
    active: prd
  cloud:
    nacos:
      discovery:
        server-addr: nacos-svc.prod.svc.cluster.local:8848
        namespace: 1d994267-0b13-45aa-bdbd-b810a37725ef
        group: nf-flms
      config:
        server-addr: $&#123;spring.cloud.nacos.discovery.server-addr&#125;
        namespace: $&#123;spring.cloud.nacos.discovery.namespace&#125;
        group: $&#123;spring.cloud.nacos.discovery.group&#125;
        file-extension: yml
        shared-configs:
          - data-id: nf-flms-application-$&#123;spring.profiles.active&#125;.$&#123;spring.cloud.nacos.config.file-extension&#125;
            group: nf-flms
            refresh: true
</code></pre><h5 id="22-导入mvn本地依赖"><a class="anchor" href="#22-导入mvn本地依赖">#</a> 2.2 导入 mvn 本地依赖</h5><pre><code>[root@jenkins repository]# ll ~/.m2/repository
total 8
drwxr-xr-x  3 root root   25 Jun 20  2023 aopalliance
drwxr-xr-x  4 root root   35 Jun 20  2023 asm
drwxr-xr-x  3 root root   30 Jun 20  2023 avalon-framework
drwxr-xr-x  3 root root   38 Mar 29  2023 backport-util-concurrent
drwxr-xr-x  3 root root   17 Mar 29  2023 ch
drwxr-xr-x  3 root root   25 Mar 29  2023 classworlds
drwxr-xr-x  6 root root   62 Mar 29  2023 cn
drwxr-xr-x 36 root root 4096 Jun 20  2023 com
drwxr-xr-x  4 root root   61 Jun 20  2023 commons-beanutils
drwxr-xr-x  3 root root   25 Mar 29  2023 commons-cli
drwxr-xr-x  3 root root   27 Mar 29  2023 commons-codec
drwxr-xr-x  3 root root   33 Mar 29  2023 commons-collections
drwxr-xr-x  3 root root   35 Mar 29  2023 commons-configuration
drwxr-xr-x  3 root root   26 Mar 29  2023 commons-dbcp
drwxr-xr-x  3 root root   30 Jun 20  2023 commons-digester
drwxr-xr-x  3 root root   24 Jun 20  2023 commons-el
drwxr-xr-x  3 root root   32 Mar 29  2023 commons-fileupload
drwxr-xr-x  3 root root   32 Jun 20  2023 commons-httpclient
drwxr-xr-x  3 root root   24 Mar 29  2023 commons-io
drwxr-xr-x  3 root root   26 Mar 29  2023 commons-lang
drwxr-xr-x  3 root root   29 Mar 29  2023 commons-logging
drwxr-xr-x  3 root root   25 Jun 20  2023 commons-net
drwxr-xr-x  3 root root   26 Mar 29  2023 commons-pool
drwxr-xr-x  3 root root   24 Mar 29  2023 concurrent
drwxr-xr-x  3 root root   25 Mar 29  2023 de
drwxr-xr-x  3 root root   19 Mar 29  2023 dom4j
drwxr-xr-x  3 root root   28 Mar 29  2023 doxia
drwxr-xr-x 17 root root  251 Mar 29  2023 io
drwxr-xr-x  8 root root   96 Jun 20  2023 jakarta
drwxr-xr-x 14 root root  175 Jun 20  2023 javax
drwxr-xr-x  3 root root   23 Mar 29  2023 joda-time
drwxr-xr-x  3 root root   19 Mar 29  2023 junit
drwxr-xr-x  3 root root   19 Mar 29  2023 log4j
drwxr-xr-x  3 root root   20 Jun 20  2023 logkit
drwxr-xr-x  3 root root   20 Jun 20  2023 math
drwxr-xr-x  3 root root   18 Jun 20  2023 me
drwxr-xr-x  3 root root   34 Mar 29  2023 mysql
drwxr-xr-x 13 root root  167 Mar 29  2023 net
drwxr-xr-x  3 root root   18 Mar 29  2023 ognl
drwxr-xr-x 56 root root 4096 Jun 20  2023 org
drwxr-xr-x  3 root root   21 Mar 29  2023 redis
drwxr-xr-x  3 root root   22 Mar 29  2023 stax
drwxr-xr-x  5 root root   72 Jun 20  2023 tomcat
drwxr-xr-x  3 root root   19 Jun 20  2023 xalan
drwxr-xr-x  3 root root   24 Mar 29  2023 xerces
drwxr-xr-x  4 root root   42 Jun 20  2023 xml-apis
drwxr-xr-x  3 root root   20 Jun 20  2023 xmlenc
</code></pre><h5 id="23-mvn打包代码"><a class="anchor" href="#23-mvn打包代码">#</a> 2.3 mvn 打包代码</h5><pre><code>[root@jenkins nf-flms]# pwd
/root/qzj-system-back/nf-flms
[root@jenkins nf-flms]# mvn -B -U clean package -Dmaven.test.skip=true -Dautoconfig.skip
</code></pre><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/3zRSM8b.png" alt="PixPin_2025-05-22_15-40-30.png"></p><h5 id="24-nacos配置"><a class="anchor" href="#24-nacos配置">#</a> 2.4 Nacos 配置</h5><h6 id="241-nf-flms-application-prdyml"><a class="anchor" href="#241-nf-flms-application-prdyml">#</a> 2.4.1 nf-flms-application-prd.yml</h6><pre><code>timedigit:
  mq:
    qos: 10
    maxRetries : 1
  security:
    jwt:
      issuer: nf-fmls
      #到期时间，单位毫秒
      expiration: &#123;SYSTEM: 1800000, ALIPAY_MEMBER: 31536000000, SALES_MEMBER: 31536000000&#125;
      maxRetries: 5
      patterns:
        - /**
    feign:
      allowHeads:
        - SOURCE_TYPE
spring:
  rabbitmq:
    host: rabbitmq-cluster.prod.svc.cluster.local
    port: 5672
    username: superman
    password: talent
    publisher-confirms: true
    publisher-returns: true
    virtual-host: /
    listener:
      simple:
        acknowledge-mode: manual
        concurrency: 1
        max-concurrency: 5
        retry:
          enabled: true
  klock:
    # redis地址
    address: redis-0.redis-svc.prod.svc.cluster.local:6379
    # redis密码
    password: Superman*2023
    # redis数据索引
    database: 8
    # 获取锁最长阻塞时间（默认：60，单位：秒）
    waitTime: 5
    # 已获取锁后自动释放时间（默认：60，单位：秒）
    leaseTime: 60

jetcache:
  statIntervalMinutes: 15
  areaInCacheName: false
  hiddenPackages: com.timedigit
  local:
    default:
      type: linkedhashmap
      keyConvertor: fastjson
  remote:
    default:
      type: redis
      keyConvertor: fastjson
      valueEncoder: java
      valueDecoder: java
      poolConfig:
        minIdle: 5
        maxIdle: 20
        maxTotal: 50
      host: redis-0.redis-svc.prod.svc.cluster.local
      port: 6379
      password: Superman*2023
      database: 8

# feign 配置
feign:
  client:
    config:
      default:
        connectTimeout: 30000
        readTimeout: 30000

#ribbon 请求处理的超时时间
ribbon:
  #全局请求连接的超时时间
  ReadTimeout: 30000
  #全局请求的超时时间
  ConnectTimeout: 30000

xgyq:
  url: https://qiaozuji-flms-api.test.qiaozuji.com
  accessKey: 7d82b81b9b6075f7
  secretKey: 17b85704043f57759983af5a744d82c0
  path: /data/


ant:
    #正式参数
  blockchainv3:
    endpoint: https://openapi.alipay.com/gateway.do
    appId: '2021002169667748'
    privateKey: MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCxCce/eHKuTv+joeDwQv1a2Po0HsoJWdnBK8DMlflC3P8bUbPLom690tIWSo1JQyAFEFDGnE3Wqc/GFEySNZvHJ693YRiyLoLtBTJw9i/yH8aINX8fQ+SbM9SxyiMl9wqc+16hxp93Rar7zZY9UbMI3bRhRquPA7rGm/66I5BldDaL1Hnh1ZQ2WwNMOpiw5vAN8ulqm3HAkUC4pReG+SCtgzu6jj2pfbViakXDCO0G7JdNKUI2G4Bhstx3V+mLYKD7+jRwaVWyZDmtiFfjPDXx2u9UYFYDrfLZb7FBs9EjzLO5z6qPerjxu5T8P5qKmsXpmyu6hfj1R3pKwILTIGGzAgMBAAECggEBAJt9U4q/Zzng+HXnP4DF1W9tEpOkVx5PZAldPEBzmDE5mHWOFLPNPiZKe2pIoD6wTfcklU1bCqJ3Ep2ORpJDs0X/fQUEqoQUhblWzy6XixTFA8Gt+rCjGK2XoD9moeg+SXwG6t57bKN89OejcUj58Jzg3ARz5Un+pJS7fcZOZgwzxoyDnhnfe7mEcN8bkuy/2PhRkZWcTkY6ND/Ey5VHk8dqjIQ7uLf3TEilL+mNdcNoguju3I0yhAEtJlhsefeKKcFADWxWZRY129RJPn22TlHNk6h5GmCktnNthaMH1yDWq7mhi/2dUYqE6KlKD37zveQnVUtR6OtMV0wcFeKqY8ECgYEA5aIDJzRicxKdbuX6+aIoGHkH3iU6gfHQRUdI4FGf3+ZqfZ3AKbsBQ5EtzIns9HXVLsPdqq7ZTeFcAeIR7hdeiTWec+fxTu0VROu4z8SMQFQQW2QlvBMfNcPhACOlCFw2aZGbsaqwFmXqshxOBeqzR57MYvTAERvtgN6NxZm0Ar8CgYEAxV3DnOi8uFnttYtf4jI06Fba2LcRJgVL10jUZBDdZLfgMCcWLVHKYc50VQIK7lnzH7uP8+mJEVwT7fMWAmi1+x4XX2o+hbx4rsLbAwTMlWgnj3bFtR1/3r4VW8IF2BN9U95+KuxdQ9UMCsStVYlL4RldEDS3Q2jSwfyRX7B5Qg0CgYAVogOmB9tWd+R49BWGuu4IEC7bkKpIX519SU/mQgpLr4tMtjXKOKHP2bd003GNPiSNOUqCr+Is4hQm4UNLKMxxJKn+xVUIWHFugr5wZFXKIaFA2thrNWn1SLTDrJf5h6Zgn6UJQclA8uz/RodbK1ckYiNjFyeY9QaU42J7wRUiRQKBgQCKxigh7x+rPEhBS3Oq94RuDYwpr2cWZcjy4hm9FoKlLAktsn4MdaMo7GKt1xbai1LA8EAC0CV5mFXHDRJftUKoBHuIsoqtvFza/NXEJJ65Oxf97xSLCef8NYmNEDrNuL55t0rdYX8ej/G8rJf4OeapqwzdtUNa2Zy/m5iYQNyyDQKBgDYtXji8LJFtuF5hOVLC5X9xZKUCiFnblD9Rkv2P5OwyfFkq8tQq1a122ha/8Vwyuv4M/C4yfeEVPbE45OMBxlcggLbtSyubEdno1etmNBftOSHeG7xXjoNqJqP7cRiFE3EgjHsbDOCiED7YCk0RsFnvXmCqL8jgUJUsuXQFZwMK
    alipayPublicKey: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAgi1cntxez3ul/BPoOBpp2/4VczWnBZ2Hv1+r7hbsOjYqflOsBQYid/p8bN7WlyZ6QgwQO32288mORWH6scXBDAMc5g+nn4rBhSOqDHh0ZxVnf+RTqSMpp+207DcCO8MbEP5EucpYOsTIvOqufSY7QkTMcaNfaYxxTPLi20Y+VKY/EsB+m8UpY93f9cxKl8vwmpJUfOtU3ENxKfrAZhm1+h4QcFy5W7ERae1Htk40bMLCvWFCrNkhTXQ0LY9bJAPLGlr8zqv0Vb7PxauNdStgIuM7SPdFQ+ZJisj7kbvfeUbPFWMRRBJaBJJWmZYWsKP7RF1f7wSocZxodEUHyJp8IwIDAQAB
    sceneCode: NFZL
    subSceneCode: '0000000000025599'
    siteUserId: '2088731937333632'
    notifyUrl: 
    timeout: 25h    
</code></pre><h6 id="242-nf-flms-gateway-prdyml"><a class="anchor" href="#242-nf-flms-gateway-prdyml">#</a> 2.4.2 nf-flms-gateway-prd.yml</h6><pre><code>spring:
  cloud:
    gateway:
      discovery:
         locator:
           lowerCaseServiceId: true
           enabled: true
      routes:
        - id: nf-flms-system
          uri: lb://nf-flms-system
          order: 1
          predicates:
          - Path=/system/**
          filters:
          - StripPrefix=1
        - id: nf-flms-order
          uri: lb://nf-flms-order
          order: 2
          predicates:
          - Path=/order/**
          filters:
          - StripPrefix=1
        - id: nf-flms-statistics
          uri: lb://nf-flms-statistics
          order: 3
          predicates:
          - Path=/statistics/**
          filters:
          - StripPrefix=1
        - id: nf-flms-openapi
          uri: lb://nf-flms-openapi
          order: 4
          predicates:
          - Path=/openapi/**
          filters:
          - StripPrefix=1
</code></pre><h6 id="243-nf-flms-order-prdyml"><a class="anchor" href="#243-nf-flms-order-prdyml">#</a> 2.4.3 nf-flms-order-prd.yml</h6><pre><code>spring:
  datasource:
    url: jdbc:mysql://mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local:3306/nf-flms?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false
    username: root
    password: Superman*2023

swagger:
  enabled: false
  title: 订单服务接口
  description: 订单服务接口
  version: 1.0.0.SNAPSHOT
  base-package: com.timedigit
  authorization: 
    name: Authorization
    key-name: Authorization
    auth-regex:  ^.*$

prometheus:
  enabled: false
  project-enviroment: $&#123;spring.profiles.active&#125;
  project-name: $&#123;spring.application.name&#125;    

# 档案
document:
  path: /data/

timedigit:
  job:
    adminAddresses: http://xxljob-svc.prod.svc.cluster.local:8080/xxl-job-admin
    appName: nf-flms
    ip:
    port: 9996
    logPath: /logs/jobhandler
    accessToken:
    logRetentionDays: 5  

sign:
  saas:
    #appId: 7438855101
    #appKey: c8def27d26d9493d745cfba4a96fa3b5
    appId: 7438905950
    appKey: 9554565359962695be2842dda660587a
    host: https://smlopenapi.esign.cn

bairong:
  apiCode: 3030942
  appKey: f997bd90b4457e7407b249a228d903e35f71acf7a65544c08620fa41e32e1867
  url: https://sandbox-api2.100credit.cn
  strategyId: STR_BR0003107
  confId: MCP_BR0001643
  befor:
    path: /strategy_api/v3/hx_query
  valid:
    path: /infoverify/v3/info_verify  

knife4j:
  enable: true
  setting:
    language: zh-CN
    enableVersion: true
    enableSearch: true
    enableFooter: false
    enableFooterCustom: true
    footerCustomContent: Copyright  2020-[深圳市租享生活科技有限公司](https://www.qiaozuji.com)
  basic:
    enable: false   
</code></pre><h6 id="244-nf-flms-statistics-prdyml"><a class="anchor" href="#244-nf-flms-statistics-prdyml">#</a> 2.4.4 nf-flms-statistics-prd.yml</h6><pre><code>spring:
  datasource:
    url: jdbc:mysql://mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local:3306/nf-flms?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false
    username: root
    password: Superman*2023

swagger:
  enabled: false
  title: 统计服务接口
  description: 统计服务接口
  version: 1.0.0.SNAPSHOT
  base-package: com.timedigit
  authorization: 
    name: Authorization
    key-name: Authorization
    auth-regex:  ^.*$

prometheus:
  enabled: false
  project-enviroment: $&#123;spring.profiles.active&#125;
  project-name: $&#123;spring.application.name&#125;   

timedigit:
  job:
    adminAddresses: http://xxljob-svc.prod.svc.cluster.local:8080/xxl-job-admin
    appName: nf-flms-statistics
    ip:
    port: 9996
    logPath: /logs/jobhandler
    accessToken:
    logRetentionDays: 5 

knife4j:
  enable: true
  setting:
    language: zh-CN
    enableVersion: true
    enableSearch: true
    enableFooter: false
    enableFooterCustom: true
    footerCustomContent: Copyright  2020-[深圳市租享生活科技有限公司](https://www.qiaozuji.com)
  basic:
    enable: false 
</code></pre><h6 id="245-nf-flms-system-prdyml"><a class="anchor" href="#245-nf-flms-system-prdyml">#</a> 2.4.5 nf-flms-system-prd.yml</h6><pre><code>spring:
  datasource:
    url: jdbc:mysql://mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local:3306/nf-flms?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false
    username: root
    password: Superman*2023

swagger:
  enabled: false
  title: 系统服务接口
  description: 系统服务接口
  version: 1.0.0.SNAPSHOT
  base-package: com.timedigit
  authorization: 
    name: Authorization
    key-name: Authorization
    auth-regex:  ^.*$

prometheus:
  enabled: false
  project-enviroment: $&#123;spring.profiles.active&#125;
  project-name: $&#123;spring.application.name&#125;    

knife4j:
  enable: true
  setting:
    language: zh-CN
    enableVersion: true
    enableSearch: true
    enableFooter: false
    enableFooterCustom: true
    footerCustomContent: Copyright  2020-[深圳市租享生活科技有限公司](https://www.qiaozuji.com)
  basic:
    enable: false
</code></pre><h6 id="246-nf-flms-openapi-prdyml"><a class="anchor" href="#246-nf-flms-openapi-prdyml">#</a> 2.4.6 nf-flms-openapi-prd.yml</h6><pre><code>spring:
  datasource:
    url: jdbc:mysql://mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local:3306/nf-flms?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false
    username: root
    password: Superman*2023

swagger:
  enabled: false
  title: 订单服务接口
  description: 订单服务接口
  version: 1.0.0.SNAPSHOT
  base-package: com.timedigit
  authorization: 
    name: Authorization
    key-name: Authorization
    auth-regex:  ^.*$

prometheus:
  enabled: false
  project-enviroment: $&#123;spring.profiles.active&#125;
  project-name: $&#123;spring.application.name&#125;    



timedigit:
  job:
    adminAddresses: http://xxljob-svc.prod.svc.cluster.local:8080/xxl-job-admin
    appName: nf-flms
    ip:
    port: 9996
    logPath: /logs/jobhandler
    accessToken:
    logRetentionDays: 5  

#蚂蚁区块链代扣
ant:
  blockchainv2:
     callbackKey: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAgi1cntxez3ul/BPoOBpp2/4VczWnBZ2Hv1+r7hbsOjYqflOsBQYid/p8bN7WlyZ6QgwQO32288mORWH6scXBDAMc5g+nn4rBhSOqDHh0ZxVnf+RTqSMpp+207DcCO8MbEP5EucpYOsTIvOqufSY7QkTMcaNfaYxxTPLi20Y+VKY/EsB+m8UpY93f9cxKl8vwmpJUfOtU3ENxKfrAZhm1+h4QcFy5W7ERae1Htk40bMLCvWFCrNkhTXQ0LY9bJAPLGlr8zqv0Vb7PxauNdStgIuM7SPdFQ+ZJisj7kbvfeUbPFWMRRBJaBJJWmZYWsKP7RF1f7wSocZxodEUHyJp8IwIDAQAB
     #callbackKey: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAigh2X40D7Rm6zu3UFkFVnnLU0higr3Q/IRN+qyAOO2CGddQ1xhyqNr82mYUHcUvKUkTV/QDi3yvfMfo2DUaffURWab0Ucth02mz70Jknse/BmkZLX0r3jiJGDErbwP1xb149GwkgM3ffwDB+LhXe/4Y0cOXJV2Qen0p3Krl5I5QFiuGfFNHPHsBJ6WRMNie8J/rvdOriVZlYmevzDxbeuvsdrXqLRIiLazvK1B0+8NcGhInCkVLFw/Zvu7piCUkyh01AyVDB13Qau6M4l93usp5jQXcTLLxMhjJTnO1L2kwGUCekKgutLbUXLa0Ar8DHrD6Z2sw8iz2hVJUXjufYMwIDAQAB
knife4j:
  enable: true
  setting:
    language: zh-CN
    enableVersion: true
    enableSearch: true
    enableFooter: false
    enableFooterCustom: true
    footerCustomContent: Copyright  2020-[深圳市租享生活科技有限公司](https://www.qiaozuji.com)
  basic:
    enable: false   
</code></pre><h6 id="247-nf-flms-admin-prdyml"><a class="anchor" href="#247-nf-flms-admin-prdyml">#</a> 2.4.7 nf-flms-admin-prd.yml</h6><pre><code>spring:
  security:
    user:
      name: admin
      password: 4Q3NGIqsnU3Arwg9
  boot:
    admin:
      ui:
        title: '俏租机 服务状态监控'
        brand: '俏租机 服务状态监控'
</code></pre><h6 id="248-nf-flms-file-prdyml"><a class="anchor" href="#248-nf-flms-file-prdyml">#</a> 2.4.8 nf-flms-file-prd.yml</h6><pre><code>spring:
  datasource:
    url: jdbc:mysql://mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local:3306/nf-flms?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false
    username: root
    password: Superman*2023

swagger:
  enabled: false
  title: 文件服务接口
  description: 文件服务接口
  version: 1.0.0.SNAPSHOT
  base-package: com.timedigit
  authorization: 
    name: Authorization
    key-name: Authorization
    auth-regex:  ^.*$

prometheus:
  enabled: false
  project-enviroment: $&#123;spring.profiles.active&#125;
  project-name: $&#123;spring.application.name&#125;   

timedigit:
  job:
    adminAddresses: http://192.168.1.70:30959/xxl-job-admin/
    appName: nf-flms-statistics
    ip:
    port: 9996
    logPath: /logs/jobhandler
    accessToken:
    logRetentionDays: 5 

knife4j:
  enable: true
  setting:
    language: zh-CN
    enableVersion: true
    enableSearch: true
    enableFooter: false
    enableFooterCustom: true
    footerCustomContent: Copyright  2020-[深圳市租享生活科技有限公司](https://www.qiaozuji.com)
  basic:
    enable: false 

# 档案
document:
  path: /data/
</code></pre><h5 id="25-构建镜像"><a class="anchor" href="#25-构建镜像">#</a> 2.5 构建镜像</h5><h6 id="251-构建nf-flms-order"><a class="anchor" href="#251-构建nf-flms-order">#</a> 2.5.1 构建 nf-flms-order</h6><pre><code># vim Dockerfile 
[root@jenkins nf-flms-order]# cat Dockerfile 
FROM openjdk:8-jre
VOLUME /tmp
COPY target/nf-flms-order.jar nf-flms-order.jar
EXPOSE 8080
CMD java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar nf-flms-order.jar

# pwd
/root/qzj-system-back/nf-flms/nf-flms-order

# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-order:v2.0 .
# docker login --username=xyapples@163.com registry.cn-hangzhou.aliyuncs.com
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-order:v2.0
</code></pre><h6 id="252-构建nf-flms-statistics"><a class="anchor" href="#252-构建nf-flms-statistics">#</a> 2.5.2 构建 nf-flms-statistics</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre
VOLUME /tmp
COPY target/nf-flms-statistics.jar nf-flms-statistics.jar
EXPOSE 8080
CMD java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar nf-flms-statistics.jar

# pwd
/root/qzj-system-back/nf-flms/nf-flms-statistics

# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-statistics:v1 .
# docker login --username=xyapples@163.com registry.cn-hangzhou.aliyuncs.com
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-statistics:v1
</code></pre><h6 id="253-构建nf-flms-system"><a class="anchor" href="#253-构建nf-flms-system">#</a> 2.5.3 构建 nf-flms-system</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre
VOLUME /tmp
COPY target/nf-flms-system.jar nf-flms-system.jar
EXPOSE 30011
CMD java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar nf-flms-system.jar

# pwd
/root/qzj-system-back/nf-flms/nf-flms-system

# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-system:v1 .
# docker login --username=xyapples@163.com registry.cn-hangzhou.aliyuncs.com
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-system:v1
</code></pre><h6 id="254-构建nf-flms-openapi"><a class="anchor" href="#254-构建nf-flms-openapi">#</a> 2.5.4 构建 nf-flms-openapi</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre
VOLUME /tmp
COPY target/nf-flms-openapi.jar nf-flms-openapi.jar
EXPOSE 30022
CMD java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar nf-flms-openapi.jar

# pwd
/root/qzj-system-back/nf-flms/nf-flms-openapi

# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-openapi:v1 .
# docker login --username=xyapples@163.com registry.cn-hangzhou.aliyuncs.com
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-openapi:v1
</code></pre><h6 id="255-构建nf-flms-gateway"><a class="anchor" href="#255-构建nf-flms-gateway">#</a> 2.5.5 构建 nf-flms-gateway</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre
VOLUME /tmp
COPY target/nf-flms-gateway.jar nf-flms-gateway.jar
EXPOSE 8080
CMD java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar nf-flms-gateway.jar

# pwd
/root/qzj-system-back/nf-flms/nf-flms-gateway

# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-gateway:v2 .
# docker login --username=xyapples@163.com registry.cn-hangzhou.aliyuncs.com
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-gateway:v2
</code></pre><h6 id="256-构建nf-flms-file"><a class="anchor" href="#256-构建nf-flms-file">#</a> 2.5.6 构建 nf-flms-file</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre
VOLUME /tmp
COPY target/nf-flms-file.jar nf-flms-file.jar
EXPOSE 30017
CMD java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar nf-flms-file.jar

# pwd
/root/qzj-system-back/nf-flms/nf-flms-file

# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-file:v1 .
# docker login --username=xyapples@163.com registry.cn-hangzhou.aliyuncs.com
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-file:v1
</code></pre><h6 id="257-构建nf-flms-admin"><a class="anchor" href="#257-构建nf-flms-admin">#</a> 2.5.7 构建 nf-flms-admin</h6><pre><code># cat Dockerfile 
FROM openjdk:8-jre
VOLUME /tmp
COPY target/nf-flms-admin.jar nf-flms-admin.jar
EXPOSE 30013
CMD java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar nf-flms-admin.jar

# pwd
/root/qzj-system-back/nf-flms/nf-flms-admin

# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-admin:v1 .
# docker login --username=xyapples@163.com registry.cn-hangzhou.aliyuncs.com
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-admin:v1
</code></pre><h5 id="26-配置xxl-job执行器"><a class="anchor" href="#26-配置xxl-job执行器">#</a> 2.6 配置 XXL-JOB 执行器</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/LrPhTlM.png" alt="微信图片_20250523105717.png"></p><h4 id="三-部署微服务应用"><a class="anchor" href="#三-部署微服务应用">#</a> 三、部署微服务应用</h4><h5 id="31-创建secret"><a class="anchor" href="#31-创建secret">#</a> 3.1 创建 secret</h5><pre><code>#  sed -i &quot;s#dev#prod#g&quot; *.yaml
# kubectl create secret tls prod-api.hmallleasig.com --key hmallleasing.com.key --cert hmallleasing.com.pem -n prod
# kubectl create secret docker-registry harbor-admin --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd -n prod
</code></pre><h5 id="32-创建nf-flms-gateway"><a class="anchor" href="#32-创建nf-flms-gateway">#</a> 3.2 创建 nf-flms-gateway</h5><pre><code># kubectl apply -f 01-nf-flms-gateway.yaml 
deployment.apps/nf-flms-gateway created
service/gateway-svc created
ingress.networking.k8s.io/gateway-ingress created
[root@k8s-master01 06-service-all]# cat 01-nf-flms-gateway.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-gateway
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-gateway
  template:
    metadata:
      labels:
        app: nf-flms-gateway
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-gateway
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-gateway:v2.2 
        command:
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-gateway.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;

---

apiVersion: v1
kind: Service
metadata:
  name: gateway-svc
  namespace: prod
spec:
  selector:
    app: nf-flms-gateway
  ports:
  - port: 8080
    targetPort: 8080

---

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: gateway-ingress
  namespace: prod
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #禁用https强制跳转
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;prod-api.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: gateway-svc
            port:
              number: 8080
  tls:                  #https
  - hosts:
    - prod-api.hmallleasing.com
    secretName: &quot;prod-api.hmallleasig.com&quot;   #配置默认证书可不添加secretName
</code></pre><h5 id="33-创建nf-flms-statistics"><a class="anchor" href="#33-创建nf-flms-statistics">#</a> 3.3 创建 nf-flms-statistics</h5><pre><code># cat 03-nf-flms-statistics.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-statistics
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-statistics
  template:
    metadata:
      labels:
        app: nf-flms-statistics
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-statistics
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-statistics:v2.0 
        command: 
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-statistics.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
</code></pre><h5 id="34-创建nf-flms-order"><a class="anchor" href="#34-创建nf-flms-order">#</a> 3.4 创建 nf-flms-order</h5><h6 id="341-创建pvc"><a class="anchor" href="#341-创建pvc">#</a> 3.4.1 创建 PVC</h6><pre><code># cat 02-data-image.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-image
  namespace: prod
spec:
  storageClassName: &quot;nfs-storage&quot;     # 明确指定使用哪个sc的供应商来创建pv
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi                      # 根据业务实际大小进行资源申请
</code></pre><h6 id="342-创建nf-flms-order"><a class="anchor" href="#342-创建nf-flms-order">#</a> 3.4.2 创建 nf-flms-order</h6><pre><code># cat 02-nf-flms-order.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-order
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-order
  template:
    metadata:
      labels:
        app: nf-flms-order
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-order
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-order:v2.0 
        command:
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-order.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        - name: data-image
          mountPath: /data
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
      - name: data-image
        persistentVolumeClaim:      
          claimName: data-image
</code></pre><h5 id="35-创建nf-flms-system"><a class="anchor" href="#35-创建nf-flms-system">#</a> 3.5 创建 nf-flms-system</h5><pre><code># cat 04-nf-flms-system.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-system
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-system
  template:
    metadata:
      labels:
        app: nf-flms-system
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-system
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-system:v2.0 
        command:
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-system.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        livenessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
        readinessProbe:
          tcpSocket:
            port: 8080
          failureThreshold: 2
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
</code></pre><h5 id="36-创建nf-flms-admin"><a class="anchor" href="#36-创建nf-flms-admin">#</a> 3.6 创建 nf-flms-admin</h5><pre><code># cat 05-nf-flms-admin.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-admin   
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nf-flms-admin
  template:
    metadata:
      labels:
        app: nf-flms-admin
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-admin
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-admin:v2.0 
        command: 
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-admin.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        livenessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
        readinessProbe:
          tcpSocket:
            port: 8080
          failureThreshold: 2
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 3
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;

---

apiVersion: v1
kind: Service
metadata:
  name: nf-flms-admin-svc
  namespace: prod
spec:
  selector:
    app: nf-flms-admin
  ports:
  - port: 8080
    targetPort: 8080

---

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nf-flms-admin-ingress
  namespace: prod
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;monitor.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nf-flms-admin-svc
            port:
              number: 8080
</code></pre><h5 id="37-创建nf-flms-openapi"><a class="anchor" href="#37-创建nf-flms-openapi">#</a> 3.7 创建 nf-flms-openapi</h5><pre><code># cat 06-nf-flms-openapi.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-openapi
  namespace: prod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nf-flms-openapi
  template:
    metadata:
      labels:
        app: nf-flms-openapi
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-openapi
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-openapi:v2.2 
        command: 
        - &quot;/bin/sh&quot;
        - &quot;-c&quot;
        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-openapi.jar&quot;
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        ports:
        - containerPort: 8080
        livenessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 8080
          failureThreshold: 2
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
</code></pre><h5 id="38-查看服务是否注册nacos"><a class="anchor" href="#38-查看服务是否注册nacos">#</a> 3.8 查看服务是否注册 Nacos</h5><p><img loading="lazy" data-src="https://wp-cdn.4ce.cn/v2/I3obzQJ.png" alt="1.png"></p><h4 id="四-部署前端ui"><a class="anchor" href="#四-部署前端ui">#</a> 四、部署前端 UI</h4><h5 id="41-修改前端配置"><a class="anchor" href="#41-修改前端配置">#</a> 4.1 修改前端配置</h5><pre><code>[root@jenkins qzj-system-front]# git checkout nf-flms-ui-v2.2
[root@jenkins qzj-system-front]# vim src/environments/environment.prod.ts
[root@jenkins qzj-system-front]# cat src/environments/environment.prod.ts
export const environment = &#123;
  // SERVER_URL: `https://nf-flms-api.prd.qiaozuji.com`,
  SERVER_URL: `https://prod-api.hmallleasing.com`,
  production: true,
  useHash: true,
  hmr: false,
&#125;;
</code></pre><h5 id="42-编译项目"><a class="anchor" href="#42-编译项目">#</a> 4.2 编译项目</h5><pre><code>[root@k8s-master01 ~]# cd /soft/
[root@k8s-master01 soft]# wget https://nodejs.org/dist/v12.20.0/node-v12.20.0-linux-x64.tar.xz
[root@k8s-master01 soft]# tar xf node-v12.20.0-linux-x64.tar.xz -C /usr/local/
[root@k8s-master01 soft]# cd /usr/local/node-v12.20.0-linux-x64/
[root@k8s-master01 node]# mv node-v12.20.0-linux-x64 node 
[root@k8s-master01 node]# cat /etc/profile.d/nodejs.sh 
export NODE_HOME=/usr/local/node
export PATH=$NODE_HOME/bin:$PATH
[root@k8s-master01 soft]# source /etc/profile
[root@k8s-master01 soft]# node -v &amp;&amp; npm -v
v12.20.0
6.14.8
[root@jenkins soft]# npm install yarn -g
[root@jenkins soft]# yarn -v
1.22.19

[root@k8s-master01 ~]# cd nf-flms-ui/
[root@k8s-master01 nf-flms-ui]# npm install -g yarn -registry=https://registry.npm.taobao.org
[root@k8s-master01 nf-flms-ui]# yarn install
[root@k8s-master01 nf-flms-ui]# npm run build
[root@k8s-master01 nf-flms-ui]# npm run build-qnyp 
[root@k8s-master01 nf-flms-ui]# npm run build-test
</code></pre><h5 id="43编写dockerfile"><a class="anchor" href="#43编写dockerfile">#</a> 4.3 编写 Dockerfile</h5><pre><code>[root@jenkins qzj-system-front]# cat Dockerfile 
FROM nginx
COPY ./dist/ /code
RUN rm -f /etc/nginx/conf.d/default.conf
</code></pre><h5 id="44-制作镜像并推送仓库"><a class="anchor" href="#44-制作镜像并推送仓库">#</a> 4.4 制作镜像并推送仓库</h5><pre><code># cd /root/nf-flms-ui
# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-ui:v1.0 .
# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-ui:v1.0
</code></pre><h5 id="45-创建configmap"><a class="anchor" href="#45-创建configmap">#</a> 4.5 创建 ConfigMap</h5><h6 id="451-准备nginx配置文件"><a class="anchor" href="#451-准备nginx配置文件">#</a> 4.5.1 准备 Nginx 配置文件</h6><pre><code># cat prod.hmallleasing.com.conf 
server &#123;
        listen 80;
        server_name prod.hmallleasing.com;
        root /code/prod;

        location / &#123;
            index  index.html index.htm;
        &#125;
&#125;

server &#123;
        listen 80;
        server_name prod-api.hmallleasing.com;

        location / &#123;
                proxy_set_header Host $http_host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header REMOTE-HOST $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_pass http://gateway-svc.prod.svc.cluster.local:8080;
        &#125;
&#125;
</code></pre><h6 id="452-创建configmap"><a class="anchor" href="#452-创建configmap">#</a> 4.5.2 创建 ConfigMap</h6><p>在启动 ui 项目时，通过 configmap 挂载配置，便于后期动态修改；</p><pre><code># kubectl create configmap nf-flms-ui-conf --from-file=./prod.hmallleasing.com.conf -n prod
</code></pre><h5 id="46-创建前端ui"><a class="anchor" href="#46-创建前端ui">#</a> 4.6 创建前端 UI</h5><pre><code># cat 07-ui-deploy-ingress.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nf-flms-ui
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nf-flms-ui
  template:
    metadata:
      labels:
        app: nf-flms-ui
    spec:
      imagePullSecrets:
      - name: harbor-admin
      containers:
      - name: nf-flms-ui
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-ui:v1.0
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: '1000m'
            memory: 1Gi
          requests:
            cpu: &quot;200m&quot;
            memory: &quot;500Mi&quot;
        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除
          tcpSocket:
            port: 80
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        livenessProbe:          # 存活探针，不存活会重启
          tcpSocket:
            port: 80
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
        volumeMounts:
        - name: ngxconfs
          mountPath: /etc/nginx/conf.d/
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: ngxconfs
        configMap:
          name: nf-flms-ui-conf
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &quot;&quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &quot;&quot;
---
apiVersion: v1
kind: Service
metadata:
  name: nf-flms-ui-svc
  namespace: prod
spec:
  selector:
    app: nf-flms-ui
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nf-flms-ui-ingress
  namespace: prod
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #禁用https强制跳转
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: &quot;prod.hmallleasing.com&quot;
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nf-flms-ui-svc
            port:
              number: 80
  tls:                  #https
  - hosts:
    - prod.hmallleasing.com
    secretName: &quot;prod-api.hmallleasig.com&quot;   #配置默认证书可不添加secretName
</code></pre><h5 id="47-创建ssl-secret"><a class="anchor" href="#47-创建ssl-secret">#</a> 4.7 创建 ssl secret</h5><pre><code>[root@k8s-master01 06-service-all]# cd ssl/
[root@k8s-master01 ssl]# kubectl create secret tls prod-api.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev
</code></pre><h5 id="48-更新资源清单"><a class="anchor" href="#48-更新资源清单">#</a> 4.8 更新资源清单</h5><pre><code>[root@k8s-master01 06-service-all]# kubectl apply -f 07-ui-deploy-ingress.yaml 

[root@k8s-master01 06-service-all]# kubectl get pods -n prod
NAME                                  READY   STATUS    RESTARTS   AGE
mysql-nacos-0                         1/1     Running   0          8d
mysql-nf-flms-0                       1/1     Running   0          7d4h
mysql-xxljob-0                        1/1     Running   0          8d
nacos-0                               1/1     Running   0          4d22h
nacos-1                               1/1     Running   0          4d22h
nacos-2                               1/1     Running   0          4d22h
nf-flms-admin-576b8fb949-2w6vg        1/1     Running   0          13m
nf-flms-gateway-55cb54cc7c-jtmjj      1/1     Running   0          28m
nf-flms-gateway-55cb54cc7c-zmgpc      1/1     Running   0          28m
nf-flms-openapi-67d66d97cf-ldhgz      1/1     Running   0          16m
nf-flms-openapi-67d66d97cf-vq82b      1/1     Running   0          15m
nf-flms-order-599dcd884c-k7rtf        1/1     Running   0          7m57s
nf-flms-order-599dcd884c-wjtn7        1/1     Running   0          11m
nf-flms-statistics-77bf7dd847-n7k46   1/1     Running   0          23m
nf-flms-statistics-77bf7dd847-qjp7q   1/1     Running   0          23m
nf-flms-system-8d5c784b9-2cwrj        1/1     Running   0          15m
nf-flms-system-8d5c784b9-lhs2f        1/1     Running   0          14m
nf-flms-ui-55467cd98b-gl99s           1/1     Running   0          57m
rabbitmq-cluster-0                    1/1     Running   0          8d
rabbitmq-cluster-1                    1/1     Running   0          8d
rabbitmq-cluster-2                    1/1     Running   0          8d
redis-0                               1/1     Running   0          8d
xxl-job-76c9464876-dzntc              1/1     Running   0          8d
</code></pre><div class="tags"><a href="/tags/Kubernetes/" rel="tag"><i class="ic i-tag"></i>Kubernetes</a></div></div><footer><div class="meta"><span class="icon"><i class="ic i-eye"></i></span><span>此文章已被阅读次数:</span><span class="waline-pageview-count" id="twikoo_visitors" data-path="/posts/626047790.html">正在加载...</span><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于</span><time title="修改时间：2025-05-28 19:24:27" itemprop="dateModified" datetime="2025-05-28T19:24:27+08:00">2025-05-28</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.png" alt="Xu Yong 微信支付"><p>微信支付</p></div><div><img loading="lazy" data-src="/assets/alipay.png" alt="Xu Yong 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者：</strong>Xu Yong<i class="ic i-at"><em>@</em></i>LinuxSre云原生</li><li class="link"><strong>本文链接：</strong><a href="http://ixuyong.cn/posts/626047790.html" title="消费租赁系统微服务应用交付实践">http://ixuyong.cn/posts/626047790.html</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/posts/1888662579.html" rel="prev" itemprop="url" data-background-image="https:&#x2F;&#x2F;wp-cdn.4ce.cn&#x2F;v2&#x2F;Uy68edS.jpeg" title="Containerd常用命令"><span class="type">上一篇</span><span class="category"><i class="ic i-flag"></i>Docker</span><h3>Containerd常用命令</h3></a></div><div class="item right"><a href="/posts/170066797.html" rel="next" itemprop="url" data-background-image="https:&#x2F;&#x2F;wp-cdn.4ce.cn&#x2F;v2&#x2F;Q4Mg1Vy.jpeg" title="消费租赁项目Kubernetes基于ELK日志分析与实践"><span class="type">下一篇</span><span class="category"><i class="ic i-flag"></i>ELKStack</span><h3>消费租赁项目Kubernetes基于ELK日志分析与实践</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E7%A7%9F%E8%B5%81%E7%B3%BB%E7%BB%9F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%BA%94%E7%94%A8%E4%BA%A4%E4%BB%98%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.</span> <span class="toc-text">消费租赁系统微服务应用交付实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80-%E9%83%A8%E7%BD%B2%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">1.1.</span> <span class="toc-text">一、部署中间件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#11-%E9%83%A8%E7%BD%B2mysql"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 部署 MySQL</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#111-mysql-configmap"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">1.1.1 MySQL-ConfigMap</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#112-mysql-secret"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">1.1.2 MySQL-Secret</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#113-mysql-statefulset"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">1.1.3 MySQL-StatefulSet</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#114-mysql-service"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">1.1.4 MySQL Service</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#115-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.1.1.5.</span> <span class="toc-text">1.1.5 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#116-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.1.1.6.</span> <span class="toc-text">1.1.6 导入数据库</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#12-%E9%83%A8%E7%BD%B2redis-single"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 部署 Redis-single</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#121-redis-configmap"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">1.2.1 Redis-ConfigMap</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#122-redis-statefulset"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">1.2.2 Redis-StatefulSet</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#123-redis-service"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">1.2.3 Redis-Service</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#124-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.1.2.4.</span> <span class="toc-text">1.2.4 更新资源清单</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#14-%E9%83%A8%E7%BD%B2nacos%E9%9B%86%E7%BE%A4"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.4 部署 Nacos 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#141-%E9%83%A8%E7%BD%B2nacos-mysql"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">1.4.1 部署 Nacos-MySQL</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#142-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">1.4.2 导入数据库</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#143-%E9%83%A8%E7%BD%B2nacos-configmap"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">1.4.3 部署 Nacos-ConfigMap</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#144-%E9%83%A8%E7%BD%B2nacos-service-statefulset"><span class="toc-number">1.1.3.4.</span> <span class="toc-text">1.4.4 部署 Nacos-Service-StatefulSet</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#145-%E9%83%A8%E7%BD%B2nacos-ingress"><span class="toc-number">1.1.3.5.</span> <span class="toc-text">1.4.5 部署 Nacos-Ingress</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#146-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.1.3.6.</span> <span class="toc-text">1.4.6 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#147-web%E8%AE%BF%E9%97%AEnacos"><span class="toc-number">1.1.3.7.</span> <span class="toc-text">1.4.7 Web 访问 nacos</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#15-%E9%83%A8%E7%BD%B2xxl-job"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.5 部署 xxl-job</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#151-%E9%83%A8%E7%BD%B2xxl-job-mysql"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">1.5.1 部署 xxl-job-MySQL</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#152-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">1.5.2 导入数据库</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#153-%E9%83%A8%E7%BD%B2xxl-job-service-deployment"><span class="toc-number">1.1.4.3.</span> <span class="toc-text">1.5.3 部署 xxl-job-Service-Deployment</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#154-%E9%83%A8%E7%BD%B2xxl-job-service"><span class="toc-number">1.1.4.4.</span> <span class="toc-text">1.5.4 部署 xxl-job-service</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#155-%E9%83%A8%E7%BD%B2xxl-job-ingress"><span class="toc-number">1.1.4.5.</span> <span class="toc-text">1.5.5 部署 xxl-job-Ingress</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#156-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.1.4.6.</span> <span class="toc-text">1.5.6 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#157-web%E8%AE%BF%E9%97%AExxl-job"><span class="toc-number">1.1.4.7.</span> <span class="toc-text">1.5.7 Web 访问 xxl-job</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#16-%E9%83%A8%E7%BD%B2rabbitmq%E9%9B%86%E7%BE%A4"><span class="toc-number">1.1.5.</span> <span class="toc-text">1.6 部署 rabbitmq 集群</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#161-%E5%88%9B%E5%BB%BArbac%E6%9D%83%E9%99%90"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">1.6.1 创建 RBAC 权限</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#162-%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%9A%84secret"><span class="toc-number">1.1.5.2.</span> <span class="toc-text">1.6.2 创建集群的 Secret</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#163-%E5%88%9B%E5%BB%BAconfigmap"><span class="toc-number">1.1.5.3.</span> <span class="toc-text">1.6.3 创建 ConfigMap</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#164-%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%9A%84svc"><span class="toc-number">1.1.5.4.</span> <span class="toc-text">1.6.4 创建集群的 svc</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#165-%E5%88%9B%E5%BB%BAstatefulset"><span class="toc-number">1.1.5.5.</span> <span class="toc-text">1.6.5 创建 StatefulSet</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#166-%E5%88%9B%E5%BB%BAingress"><span class="toc-number">1.1.5.6.</span> <span class="toc-text">1.6.6 创建 Ingress</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#167-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.1.5.7.</span> <span class="toc-text">1.6.7 更新资源清单</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#168-web%E8%AE%BF%E9%97%AErabbitmq"><span class="toc-number">1.1.5.8.</span> <span class="toc-text">1.6.8 Web 访问 rabbitmq</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#168-rabbitmq%E5%85%A8%E9%83%A8%E6%8C%82%E4%BA%86%E6%97%A0%E6%B3%95%E9%87%8D%E5%90%AF%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">1.1.5.9.</span> <span class="toc-text">1.6.8 rabbitMQ 全部挂了，无法重启解决方案</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#17-%E9%83%A8%E7%BD%B2rabbitmq-single"><span class="toc-number">1.1.6.</span> <span class="toc-text">1.7 部署 rabbitmq-single</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.</span> <span class="toc-text">二、微服务配置</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#21-%E4%BB%A3%E7%A0%81%E7%BC%96%E8%AF%91%E5%89%8D%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 代码编译前配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#22-%E5%AF%BC%E5%85%A5mvn%E6%9C%AC%E5%9C%B0%E4%BE%9D%E8%B5%96"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 导入 mvn 本地依赖</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#23-mvn%E6%89%93%E5%8C%85%E4%BB%A3%E7%A0%81"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 mvn 打包代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#24-nacos%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.4.</span> <span class="toc-text">2.4 Nacos 配置</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#241-nf-flms-application-prdyml"><span class="toc-number">1.2.4.1.</span> <span class="toc-text">2.4.1 nf-flms-application-prd.yml</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#242-nf-flms-gateway-prdyml"><span class="toc-number">1.2.4.2.</span> <span class="toc-text">2.4.2 nf-flms-gateway-prd.yml</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#243-nf-flms-order-prdyml"><span class="toc-number">1.2.4.3.</span> <span class="toc-text">2.4.3 nf-flms-order-prd.yml</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#244-nf-flms-statistics-prdyml"><span class="toc-number">1.2.4.4.</span> <span class="toc-text">2.4.4 nf-flms-statistics-prd.yml</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#245-nf-flms-system-prdyml"><span class="toc-number">1.2.4.5.</span> <span class="toc-text">2.4.5 nf-flms-system-prd.yml</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#246-nf-flms-openapi-prdyml"><span class="toc-number">1.2.4.6.</span> <span class="toc-text">2.4.6 nf-flms-openapi-prd.yml</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#247-nf-flms-admin-prdyml"><span class="toc-number">1.2.4.7.</span> <span class="toc-text">2.4.7 nf-flms-admin-prd.yml</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#248-nf-flms-file-prdyml"><span class="toc-number">1.2.4.8.</span> <span class="toc-text">2.4.8 nf-flms-file-prd.yml</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#25-%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F"><span class="toc-number">1.2.5.</span> <span class="toc-text">2.5 构建镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#251-%E6%9E%84%E5%BB%BAnf-flms-order"><span class="toc-number">1.2.5.1.</span> <span class="toc-text">2.5.1 构建 nf-flms-order</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#252-%E6%9E%84%E5%BB%BAnf-flms-statistics"><span class="toc-number">1.2.5.2.</span> <span class="toc-text">2.5.2 构建 nf-flms-statistics</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#253-%E6%9E%84%E5%BB%BAnf-flms-system"><span class="toc-number">1.2.5.3.</span> <span class="toc-text">2.5.3 构建 nf-flms-system</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#254-%E6%9E%84%E5%BB%BAnf-flms-openapi"><span class="toc-number">1.2.5.4.</span> <span class="toc-text">2.5.4 构建 nf-flms-openapi</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#255-%E6%9E%84%E5%BB%BAnf-flms-gateway"><span class="toc-number">1.2.5.5.</span> <span class="toc-text">2.5.5 构建 nf-flms-gateway</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#256-%E6%9E%84%E5%BB%BAnf-flms-file"><span class="toc-number">1.2.5.6.</span> <span class="toc-text">2.5.6 构建 nf-flms-file</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#257-%E6%9E%84%E5%BB%BAnf-flms-admin"><span class="toc-number">1.2.5.7.</span> <span class="toc-text">2.5.7 构建 nf-flms-admin</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#26-%E9%85%8D%E7%BD%AExxl-job%E6%89%A7%E8%A1%8C%E5%99%A8"><span class="toc-number">1.2.6.</span> <span class="toc-text">2.6 配置 XXL-JOB 执行器</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89-%E9%83%A8%E7%BD%B2%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%BA%94%E7%94%A8"><span class="toc-number">1.3.</span> <span class="toc-text">三、部署微服务应用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#31-%E5%88%9B%E5%BB%BAsecret"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 创建 secret</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#32-%E5%88%9B%E5%BB%BAnf-flms-gateway"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 创建 nf-flms-gateway</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#33-%E5%88%9B%E5%BB%BAnf-flms-statistics"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 创建 nf-flms-statistics</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#34-%E5%88%9B%E5%BB%BAnf-flms-order"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 创建 nf-flms-order</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#341-%E5%88%9B%E5%BB%BApvc"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">3.4.1 创建 PVC</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#342-%E5%88%9B%E5%BB%BAnf-flms-order"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">3.4.2 创建 nf-flms-order</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#35-%E5%88%9B%E5%BB%BAnf-flms-system"><span class="toc-number">1.3.5.</span> <span class="toc-text">3.5 创建 nf-flms-system</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#36-%E5%88%9B%E5%BB%BAnf-flms-admin"><span class="toc-number">1.3.6.</span> <span class="toc-text">3.6 创建 nf-flms-admin</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#37-%E5%88%9B%E5%BB%BAnf-flms-openapi"><span class="toc-number">1.3.7.</span> <span class="toc-text">3.7 创建 nf-flms-openapi</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#38-%E6%9F%A5%E7%9C%8B%E6%9C%8D%E5%8A%A1%E6%98%AF%E5%90%A6%E6%B3%A8%E5%86%8Cnacos"><span class="toc-number">1.3.8.</span> <span class="toc-text">3.8 查看服务是否注册 Nacos</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B-%E9%83%A8%E7%BD%B2%E5%89%8D%E7%AB%AFui"><span class="toc-number">1.4.</span> <span class="toc-text">四、部署前端 UI</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#41-%E4%BF%AE%E6%94%B9%E5%89%8D%E7%AB%AF%E9%85%8D%E7%BD%AE"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 修改前端配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#42-%E7%BC%96%E8%AF%91%E9%A1%B9%E7%9B%AE"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 编译项目</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#43%E7%BC%96%E5%86%99dockerfile"><span class="toc-number">1.4.3.</span> <span class="toc-text">4.3 编写 Dockerfile</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#44-%E5%88%B6%E4%BD%9C%E9%95%9C%E5%83%8F%E5%B9%B6%E6%8E%A8%E9%80%81%E4%BB%93%E5%BA%93"><span class="toc-number">1.4.4.</span> <span class="toc-text">4.4 制作镜像并推送仓库</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#45-%E5%88%9B%E5%BB%BAconfigmap"><span class="toc-number">1.4.5.</span> <span class="toc-text">4.5 创建 ConfigMap</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#451-%E5%87%86%E5%A4%87nginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">1.4.5.1.</span> <span class="toc-text">4.5.1 准备 Nginx 配置文件</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#452-%E5%88%9B%E5%BB%BAconfigmap"><span class="toc-number">1.4.5.2.</span> <span class="toc-text">4.5.2 创建 ConfigMap</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#46-%E5%88%9B%E5%BB%BA%E5%89%8D%E7%AB%AFui"><span class="toc-number">1.4.6.</span> <span class="toc-text">4.6 创建前端 UI</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#47-%E5%88%9B%E5%BB%BAssl-secret"><span class="toc-number">1.4.7.</span> <span class="toc-text">4.7 创建 ssl secret</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#48-%E6%9B%B4%E6%96%B0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95"><span class="toc-number">1.4.8.</span> <span class="toc-text">4.8 更新资源清单</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/posts/3166738000.html" rel="bookmark" title="Kubeadm高可用安装K8s集群">Kubeadm高可用安装K8s集群</a></li><li><a href="/posts/2771271649.html" rel="bookmark" title="云原生K8s安全专家CKS认证考题详解">云原生K8s安全专家CKS认证考题详解</a></li><li><a href="/posts/985149017.html" rel="bookmark" title="二进制高可用安装K8S集群">二进制高可用安装K8S集群</a></li><li><a href="/posts/1771242682.html" rel="bookmark" title="K8s零宕机服务发布-探针">K8s零宕机服务发布-探针</a></li><li><a href="/posts/108692210.html" rel="bookmark" title="K8s资源调度deployment、statefulset、daemonset">K8s资源调度deployment、statefulset、daemonset</a></li><li><a href="/posts/858611107.html" rel="bookmark" title="K8s服务发布Service">K8s服务发布Service</a></li><li><a href="/posts/3992668367.html" rel="bookmark" title="K8s配置管理Configmap">K8s配置管理Configmap</a></li><li><a href="/posts/169153047.html" rel="bookmark" title="K8s持久化存储">K8s持久化存储</a></li><li><a href="/posts/3833778957.html" rel="bookmark" title="K8s计划任务Job、Cronjob">K8s计划任务Job、Cronjob</a></li><li><a href="/posts/3142072607.html" rel="bookmark" title="K8s初始化容器、临时容器">K8s初始化容器、临时容器</a></li><li><a href="/posts/3254599477.html" rel="bookmark" title="K8s容忍和污点">K8s容忍和污点</a></li><li><a href="/posts/312010518.html" rel="bookmark" title="K8s亲和力Affinity">K8s亲和力Affinity</a></li><li><a href="/posts/176412055.html" rel="bookmark" title="K8s准入控制ResourceQuota、LimitRange、QoS服务质量">K8s准入控制ResourceQuota、LimitRange、QoS服务质量</a></li><li><a href="/posts/722512536.html" rel="bookmark" title="K8s细粒度权限控制RBAC">K8s细粒度权限控制RBAC</a></li><li><a href="/posts/3890389502.html" rel="bookmark" title="K8S持久化存储NFS+StorageClass">K8S持久化存储NFS+StorageClass</a></li><li><a href="/posts/3030097036.html" rel="bookmark" title="K8S云原生存储Rook-Ceph">K8S云原生存储Rook-Ceph</a></li><li><a href="/posts/170573601.html" rel="bookmark" title="K8s服务发布Ingress">K8s服务发布Ingress</a></li><li class="active"><a href="/posts/626047790.html" rel="bookmark" title="消费租赁系统微服务应用交付实践">消费租赁系统微服务应用交付实践</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="Xu Yong" src="/assets/avatar.png"><p class="name" itemprop="name">Xu Yong</p><div class="description" itemprop="description">专注于 Linux 运维、云计算、云原⽣等技术</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">28</span><span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">9</span><span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">9</span><span class="name">标签</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/xyapples" class="item github" title="https:&#x2F;&#x2F;github.com&#x2F;xyapples"><i class="ic i-github"></i></a><a target="_blank" rel="noopener" href="https://gitee.com/chinagei" class="item gitee" title="https:&#x2F;&#x2F;gitee.com&#x2F;chinagei"><i class="ic i-gitee"></i></a><a href="mailto:373370405@qq.com" class="item email" title="mailto:373370405@qq.com"><i class="ic i-envelope"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-about"></i>关于</a></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"><a href="/posts/170066797.html" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/posts/1888662579.html" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/108692210.html">K8s资源调度deployment、statefulset、daemonset</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/MySQL/" title="分类于MySQL">MySQL</a></div><span><a href="/posts/2628187572.html">MySQL运维DBA应用与实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Harbor/" title="分类于Harbor">Harbor</a></div><span><a href="/posts/3071070978.html">企业级私有仓库Harbor搭建</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Redis/" title="分类于Redis">Redis</a></div><span><a href="/posts/1414180692.html">Redis集群（主从+哨兵）模式</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3254599477.html">K8s容忍和污点</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3166738000.html">Kubeadm高可用安装K8s集群</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Linux/" title="分类于Linux">Linux</a></div><span><a href="/posts/1922841233.html">Rsync服务实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/3142072607.html">K8s初始化容器、临时容器</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Docker/" title="分类于Docker">Docker</a></div><span><a href="/posts/1888662579.html">Containerd常用命令</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Kubernetes/" title="分类于Kubernetes">Kubernetes</a></div><span><a href="/posts/176412055.html">K8s准入控制ResourceQuota、LimitRange、QoS服务质量</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2><ul class="leancloud-recent-comment" id="new-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2022 -<span itemprop="copyrightYear">2025</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">Xu Yong @ LinuxSre云原生</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">516k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">7:49</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL = {
    ispost: true,
        path: `posts/626047790.html`,
        favicon: {
        show: `（●´3｀●）やれやれだぜ`,
        hide: `(´Д｀)大変だ！`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};</script><script src="https://s4.zstatic.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha384-ZvpUoO&#x2F;+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn&#x2F;6Z&#x2F;hRTt8+pR6L4N2" crossorigin="anonymous" fetchpriority="high"></script><script src="/js/siteInit.js?v=0.4.17" type="module" fetchpriority="high" defer></script></body></html><!-- rebuild by hrmmi -->