{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://xuyong.cn",
    "items": [
        {
            "id": "http://xuyong.cn/posts/0.html",
            "url": "http://xuyong.cn/posts/0.html",
            "title": "",
            "date_published": "2025-04-10T13:39:44.243Z",
            "content_html": "<h3 id=\"一键永久激活window-office教程\"><a class=\"anchor\" href=\"#一键永久激活window-office教程\">#</a> 一键永久激活 Window、office 教程</h3>\n<p>1、按下 Win 键 + R，调出运行对话框，输入 powershell 并回车，启动命令提示符窗口。接着输入以下指令执行激活：</p>\n<pre><code>irm https://get.activated.win | iex\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2UTxI\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2UTxI.png\" alt=\"pE2UTxI.png\" /></a></p>\n<p>该脚本包含四个功能：首个命令用于 Windows 系统永久激活，第二个用于 Office 永久激活，第三个将系统有效期延长至 2038 年，第四个则实现每 180 天自动循环激活。</p>\n<p><a href=\"https://imgse.com/i/pE2UHMt\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2UHMt.png\" alt=\"pE2UHMt.png\" /></a></p>\n<p>2. 我们再次使用 Windows 徽标 + R 快捷键打开运行框，输入 slmgr.vbs/xpr 就可以看到系统已经永久激活了。</p>\n<pre><code>slmgr.vbs /xpr\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2Uqqf\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2Uqqf.png\" alt=\"pE2Uqqf.png\" /></a></p>\n<p>以上，既然看到这里了，如果觉得不错，随手点个赞、打赏一下吧，⭐～谢谢你看我的文章，我们下次再见。</p>\n",
            "tags": []
        },
        {
            "id": "http://xuyong.cn/posts/3071070979.html",
            "url": "http://xuyong.cn/posts/3071070979.html",
            "title": "企业级私有仓库Harbor搭建",
            "date_published": "2025-04-10T13:32:09.000Z",
            "content_html": "",
            "tags": [
                "默认tags"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/985149017.html",
            "url": "http://xuyong.cn/posts/985149017.html",
            "title": "二进制高可用安装K8S集群",
            "date_published": "2025-04-10T12:58:40.000Z",
            "content_html": "<h2 id=\"二进制高可用安装k8s集群\"><a class=\"anchor\" href=\"#二进制高可用安装k8s集群\">#</a> 二进制高可用安装 K8s 集群</h2>\n<h4 id=\"1-基本配置\"><a class=\"anchor\" href=\"#1-基本配置\">#</a> 1. 基本配置</h4>\n<h5 id=\"11-基本环境配置\"><a class=\"anchor\" href=\"#11-基本环境配置\">#</a> 1.1 基本环境配置</h5>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP 地址</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master 节点 * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived 虚拟 IP（不占用机器）</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker 节点 * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* 配置信息 *</strong></em></th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>系统版本</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod 网段</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service 网段</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>所有节点</mark>更改主机名（其它节点按需修改）：</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>所有节点</mark>配置 hosts，修改 /etc/hosts 如下：</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>所有节点</mark>配置 yum 源：</p>\n<pre><code># 配置基础源\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>所有节点</mark>必备工具安装：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>所有节点</mark>关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>所有节点</mark>关闭 swap 分区：</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>所有节点</mark>安装 ntpdate：</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>所有节点</mark>同步时间并配置上海时区：</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# 加入到crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>所有节点</mark>配置 limit：</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>所有节点</mark>升级系统：</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 节点</mark>免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上</em></p>\n<p><mark>Master01 节点</mark>下载安装所有的源码文件：</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-内核配置\"><a class=\"anchor\" href=\"#12-内核配置\">#</a> 1.2 内核配置</h5>\n<p><mark>所有节点</mark>安装 ipvsadm：</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>所有节点</mark>配置 ipvs 模块：</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>所有节点</mark>创建 ipvs.conf，并配置开机自动加载：</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# 加入以下内容\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>所有节点</mark>然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>所有节点</mark>内核优化配置：</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>所有节点</mark>应用配置：</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>配置完内核后，重启机器，之后查看内核模块是否已自动加载：</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-高可用组件安装\"><a class=\"anchor\" href=\"#2-高可用组件安装\">#</a> 2. 高可用组件安装</h4>\n<p><em>注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装</em></p>\n<p><em>注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。</em></p>\n<p><mark>所有 Master 节点</mark>通过 yum 安装 HAProxy 和 KeepAlived：</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 HAProxy，需要注意黄色部分的 IP：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:8443       #HAProxy监听端口\n  bind 127.0.0.1:8443     #HAProxy监听端口\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IP地址\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IP地址\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IP地址\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 KeepAlived，需要注意黄色部分的配置。</p>\n<p><mark>Master01 节点</mark>的配置：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #网卡名称\n    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #网卡名称\n    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #网卡名称\n    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>所有 master 节点</mark>配置 KeepAlived 健康检查文件：</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>所有 master 节点</mark>配置健康检查文件添加执行权限：</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>所有 master 节点</mark>启动 haproxy 和 keepalived：</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的</p>\n<pre><code>所有节点测试VIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等</p>\n<ul>\n<li>所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld</li>\n<li>所有节点查看 selinux 状态，必须为 disable：getenforce</li>\n<li>master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy</li>\n<li>master 节点查看监听端口：netstat -lntp</li>\n</ul>\n<p>如果以上都没有问题，需要确认：</p>\n<ol>\n<li>\n<p>是否是公有云机器</p>\n</li>\n<li>\n<p>是否是私有云机器（类似 OpenStack）</p>\n</li>\n</ol>\n<p>上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询</p>\n<h4 id=\"3-runtime安装\"><a class=\"anchor\" href=\"#3-runtime安装\">#</a> 3. Runtime 安装</h4>\n<p>如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。</p>\n<h5 id=\"31-安装containerd\"><a class=\"anchor\" href=\"#31-安装containerd\">#</a> 3.1 安装 Containerd</h5>\n<p><mark>所有节点</mark>配置安装源：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>所有节点</mark>安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p>首先配置 Containerd 所需的模块（<mark>所有节点</mark>）：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载模块：</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>所有节点</mark>，配置 Containerd 所需的内核：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载内核：</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>生成 Containerd 的配置文件：</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>更改 Containerd 的 Cgroup 和 Pause 镜像配置：</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>启动 Containerd，并配置开机自启动：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>所有节点</mark>配置 crictl 客户端连接的运行时位置（可选）：</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-k8s及etcd安装\"><a class=\"anchor\" href=\"#4-k8s及etcd安装\">#</a> 4 . K8S 及 etcd 安装</h4>\n<p><mark>Master01</mark> 下载 kubernetes 安装包（1.32.3 需要更改为你看到的最新版本）：</p>\n<pre><code>[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz\n</code></pre>\n<p>最新版获取地址：<a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md\">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/</a></p>\n<p><mark>以下操作都在 master01 执行</mark></p>\n<p>下载 etcd 安装包：<a href=\"https://github.com/etcd-io/etcd/releases/\">https://github.com/etcd-io/etcd/releases/</a></p>\n<pre><code>[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz\n</code></pre>\n<p>解压 kubernetes 安装文件：</p>\n<pre><code>[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125;\n</code></pre>\n<p>解压 etcd 安装文件：</p>\n<pre><code>[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&#123;,ctl&#125;\n</code></pre>\n<p>版本查看：</p>\n<pre><code>[root@k8s-master01 ~]# kubelet --version\nKubernetes v1.32.3\n[root@k8s-master01 ~]# etcdctl version\netcdctl version: 3.5.16\nAPI version: 3.5\n</code></pre>\n<p>将组件发送到其他节点</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\nWorkNodes='k8s-node01 k8s-node02'\nfor NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done\nfor NODE in $WorkNodes; do     scp /usr/local/bin/kube&#123;let,-proxy&#125; $NODE:/usr/local/bin/ ; done\n</code></pre>\n<p><mark>Master01 节点</mark>切换到 1.32.x 分支（其他版本可以切换到其他分支，.x 即可，不需要更改为具体的小版本）：</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x\n</code></pre>\n<h4 id=\"5-生成证书\"><a class=\"anchor\" href=\"#5-生成证书\">#</a> 5 . 生成证书</h4>\n<p><em><mark>二进制安装最关键步骤，一步错误全盘皆输，一定要注意每个步骤都要是正确的</mark></em></p>\n<p><mark>Master01</mark> 下载生成证书工具（下载不成功可以去百度网盘）</p>\n<pre><code>wget &quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl\nwget &quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson\nchmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson\n</code></pre>\n<h5 id=\"51-etcd证书\"><a class=\"anchor\" href=\"#51-etcd证书\">#</a> 5.1 Etcd 证书</h5>\n<p><mark>所有 Master 节点</mark>创建 etcd 证书目录：</p>\n<pre><code>mkdir /etc/etcd/ssl -p\n</code></pre>\n<p><mark>所有节点</mark>创建 kubernetes 相关目录：</p>\n<pre><code>mkdir -p /etc/kubernetes/pki\n</code></pre>\n<p><mark>Master01 节点</mark>生成 etcd 证书</p>\n<p>生成证书的 CSR（证书签名请求文件，配置了一些域名、公司、单位）文件：</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\n# 生成etcd CA证书和CA证书的key\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca\n\n\ncfssl gencert \\\n   -ca=/etc/etcd/ssl/etcd-ca.pem \\\n   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\\n   -config=ca-config.json \\\n   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \\\n   -profile=kubernetes \\\n   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd\n\n执行结果\n[INFO] generate received request\n \t[INFO] received CSR\n     [INFO] generating key: rsa-2048\n     [INFO] encoded CSR\n     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033\n</code></pre>\n<p>将证书复制到其他 master 节点</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\n\nfor NODE in $MasterNodes; do\n     ssh $NODE &quot;mkdir -p /etc/etcd/ssl&quot;\n     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do\n       scp /etc/etcd/ssl/$&#123;FILE&#125; $NODE:/etc/etcd/ssl/$&#123;FILE&#125;\n     done\n done\n</code></pre>\n<h5 id=\"52-k8s组件证书\"><a class=\"anchor\" href=\"#52-k8s组件证书\">#</a> 5.2 K8s 组件证书</h5>\n<p><mark>Master01</mark> 生成 kubernetes CA 证书：</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca\n</code></pre>\n<h6 id=\"521-apiserver证书\"><a class=\"anchor\" href=\"#521-apiserver证书\">#</a> 5.2.1 APIServer 证书</h6>\n<p>注意：10.96.0. 是 k8s service 的网段，如果说需要更改 k8s service 网段，那就需要更改 10.96.0.1</p>\n<pre><code>cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver\n</code></pre>\n<p>生成 apiserver 的聚合证书：：</p>\n<pre><code>cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca \n\ncfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client\n</code></pre>\n<p>返回结果（忽略警告）：</p>\n<pre><code>2020/12/11 18:15:28 [INFO] generate received request\n2020/12/11 18:15:28 [INFO] received CSR\n2020/12/11 18:15:28 [INFO] generating key: rsa-2048\n\n2020/12/11 18:15:28 [INFO] encoded CSR\n2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845\n2020/12/11 18:15:28 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for\nwebsites. For more information see the Baseline Requirements for the Issuance and Management\nof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);\nspecifically, section 10.2.3 (&quot;Information Requirements&quot;).\n</code></pre>\n<h6 id=\"522-controllermanager\"><a class=\"anchor\" href=\"#522-controllermanager\">#</a> 5.2.2 ControllerManager</h6>\n<p>生成 controller-manage 的证书：</p>\n<pre><code class=\"language-\\\">cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager\n\n注意：修改黄色部分的IP地址\n# set-cluster：设置一个集群项，\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# 设置一个环境项，一个上下文\nkubectl config set-context system:kube-controller-manager@kubernetes \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# set-credentials 设置一个用户项\n\nkubectl config set-credentials system:kube-controller-manager \\\n     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\\n     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n\n# 使用某个环境当做默认环境\n\nkubectl config use-context system:kube-controller-manager@kubernetes \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n</code></pre>\n<h6 id=\"523-scheduler证书\"><a class=\"anchor\" href=\"#523-scheduler证书\">#</a> 5.2.3 Scheduler 证书</h6>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler\n\n注意：修改黄色部分的IP地址\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\n\nkubectl config set-credentials system:kube-scheduler \\\n     --client-certificate=/etc/kubernetes/pki/scheduler.pem \\\n     --client-key=/etc/kubernetes/pki/scheduler-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config set-context system:kube-scheduler@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-scheduler \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config use-context system:kube-scheduler@kubernetes \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n</code></pre>\n<h6 id=\"524-生成管理员证书\"><a class=\"anchor\" href=\"#524-生成管理员证书\">#</a> 5.2.4 生成管理员证书</h6>\n<p>Kubectl /etc/Kubernetes/admin.conf ~/.kube/config</p>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin\n\n注意：修改黄色部分的IP\n\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig\nkubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n</code></pre>\n<h6 id=\"525-创建serviceaccount证书\"><a class=\"anchor\" href=\"#525-创建serviceaccount证书\">#</a> 5.2.5 创建 ServiceAccount 证书</h6>\n<p>创建一对公钥，用来签发 ServiceAccount 的 Token：</p>\n<pre><code>openssl genrsa -out /etc/kubernetes/pki/sa.key 2048\n</code></pre>\n<p>返回结果：</p>\n<pre><code>Generating RSA private key, 2048 bit long modulus (2 primes)\n...................................................................................+++++\n...............+++++\ne is 65537 (0x010001)\n</code></pre>\n<pre><code> openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n</code></pre>\n<p>发送证书至其他节点：</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do \n  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do \n    scp /etc/kubernetes/pki/$&#123;FILE&#125; $NODE:/etc/kubernetes/pki/$&#123;FILE&#125;;\n  done; \n  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do \n    scp /etc/kubernetes/$&#123;FILE&#125; $NODE:/etc/kubernetes/$&#123;FILE&#125;;\n  done;\ndone\n</code></pre>\n<p>查看证书文件：</p>\n<pre><code>[root@k8s-master01 pki]# ls /etc/kubernetes/pki/\nadmin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem\nadmin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem\nadmin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr\n[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l\n23\n</code></pre>\n<h4 id=\"6-kubernetes组件配置\"><a class=\"anchor\" href=\"#6-kubernetes组件配置\">#</a> 6. Kubernetes 组件配置</h4>\n<h5 id=\"61-ecd配置\"><a class=\"anchor\" href=\"#61-ecd配置\">#</a> 6.1 Ecd 配置</h5>\n<p>Etcd 配置大致相同，注意修改每个 Master 节点的 etcd 配置的主机名和 IP 地址</p>\n<h6 id=\"611-master01\"><a class=\"anchor\" href=\"#611-master01\">#</a> 6.1.1 Master01</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master01'     # k8s-master01名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.71:2380'            # k8s-master01 IP\nlisten-client-urls: 'https://192.168.1.71:2379,http://127.0.0.1:2379'   # k8s-master01 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.71:2380'  # k8s-master01 IP\nadvertise-client-urls: 'https://192.168.1.71:2379'        # k8s-master01 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'     # k8s-master01、k8s-master02、k8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"612-master02\"><a class=\"anchor\" href=\"#612-master02\">#</a> 6.1.2 Master02</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\t\nname: 'k8s-master02'   # k8s-master02名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.72:2380'      # k8s-master02 IP\nlisten-client-urls: 'https://192.168.1.72:2379,http://127.0.0.1:2379'    # k8s-master02 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.72:2380'    # k8s-master02 IP\nadvertise-client-urls: 'https://192.168.1.72:2379'     # k8s-master02 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'             # k8s-master01、k8s-master02、k8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"613-master03\"><a class=\"anchor\" href=\"#613-master03\">#</a> 6.1.3 Master03</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master03'           # k8s-master03名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.73:2380'           # k8s-master03 IP\nlisten-client-urls: 'https://192.168.1.73:2379,http://127.0.0.1:2379'       # k8s-master03 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.73:2380'      # k8s-master03 IP\nadvertise-client-urls: 'https://192.168.1.73:2379'            # k8s-master03 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'                # k8s-master01、k8s-master02、k8s-master03 IP\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"614-启动etcd\"><a class=\"anchor\" href=\"#614-启动etcd\">#</a> 6.1.4 启动 Etcd</h6>\n<p><mark>所有 Master 节点</mark>创建 etcd service 并启动</p>\n<pre><code># vim /usr/lib/systemd/system/etcd.service\n[Unit]\nDescription=Etcd Service\nDocumentation=https://coreos.com/etcd/docs/latest/\nAfter=network.target\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml\nRestart=on-failure\nRestartSec=10\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nAlias=etcd3.service\n</code></pre>\n<p><mark>所有 Master 节点</mark>创建 etcd 的证书目录：</p>\n<pre><code>mkdir /etc/kubernetes/pki/etcd\nln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/\nsystemctl daemon-reload\nsystemctl enable --now etcd\n</code></pre>\n<p>查看 etcd 状态：</p>\n<pre><code>export ETCDCTL_API=3\netcdctl --endpoints=&quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n</code></pre>\n<h5 id=\"62-apiserver配置\"><a class=\"anchor\" href=\"#62-apiserver配置\">#</a> 6.2 APIServer 配置</h5>\n<h6 id=\"621-master01\"><a class=\"anchor\" href=\"#621-master01\">#</a> 6.2.1 Master01</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.71 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"622-master02\"><a class=\"anchor\" href=\"#622-master02\">#</a> 6.2.2 Master02</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.72 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"623-master03\"><a class=\"anchor\" href=\"#623-master03\">#</a> 6.2.3 Master03</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.73 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"624-启动apiserver\"><a class=\"anchor\" href=\"#624-启动apiserver\">#</a> 6.2.4 启动 apiserver</h6>\n<p><mark>所有 Master 节点</mark>开启 kube-apiserver：</p>\n<pre><code>systemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserver\n</code></pre>\n<p>检测 kube-server 状态：</p>\n<pre><code># systemctl status kube-apiserver\n\n● kube-apiserver.service – Kubernetes API Server\n   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)\n   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s ago \n</code></pre>\n<p>如果系统日志有这些提示可以忽略:</p>\n<pre><code>Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to “pick_first”\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;CONNECTING &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;READY &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = “transport is closing”\n</code></pre>\n<h5 id=\"63-controllermanage\"><a class=\"anchor\" href=\"#63-controllermanage\">#</a> 6.3 ControllerManage</h5>\n<p><mark>所有 Master 节点</mark>配置 kube-controller-manager service（所有 master 节点配置一样）</p>\n<p>注意：本文档使用的 k8s Pod 网段为 172.16.0.0/16，该网段不能和宿主机的网段、k8s Service 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\n      --v=2 \\\n      --root-ca-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\\n      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\\n      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --leader-elect=true \\\n      --use-service-account-credentials=true \\\n      --node-monitor-grace-period=40s \\\n      --node-monitor-period=5s \\\n      --controllers=*,bootstrapsigner,tokencleaner \\\n      --allocate-node-cidrs=true \\\n      --cluster-cidr=172.16.0.0/16 \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\\n      --node-cidr-mask-size=24\n      \nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>所有 Master 节点</mark>启动 kube-controller-manager</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service → /usr/lib/systemd/system/kube-controller-manager.service.\n</code></pre>\n<p>查看启动状态</p>\n<pre><code>[root@k8s-master01 pki]# systemctl  status kube-controller-manager\n● kube-controller-manager.service – Kubernetes Controller Manager\n   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)\n Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago\n     Docs: https://github.com/  ubernetes/  ubernetes\n Main PID: 7518 (kube-controller)\n</code></pre>\n<h5 id=\"64-scheduler\"><a class=\"anchor\" href=\"#64-scheduler\">#</a> 6.4 Scheduler</h5>\n<p>所有 Master 节点配置 kube-scheduler service（所有 master 节点配置一样）</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service \n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\n      --v=2 \\\n      --leader-elect=true \\\n      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>启动 scheduler：</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-scheduler\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service → /usr/lib/systemd/system/kube-scheduler.service.\n[root@k8s-master01 pki]# systemctl status kube-scheduler\n● kube-scheduler.service - Kubernetes Scheduler\n   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)\n   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago\n     Docs: https://github.com/kubernetes/kubernetes\n Main PID: 5815 (kube-scheduler)\n    Tasks: 9\n   Memory: 19.8M\n</code></pre>\n<h4 id=\"7-tls-bootstrapping配置\"><a class=\"anchor\" href=\"#7-tls-bootstrapping配置\">#</a> 7. TLS Bootstrapping 配置</h4>\n<p>只需要在<mark> Master01</mark> 创建 bootstrap</p>\n<p>注意： 修改黄色部分的 IP 地址</p>\n<pre><code>cd /root/k8s-ha-install/bootstrap\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n\n[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config\n</code></pre>\n<p>可以正常查询集群状态，才可以继续往下，否则不行，需要排查 k8s 组件是否有故障（只要有结果即可，如果返回不一样不影响）</p>\n<pre><code># kubectl get cs\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS    MESSAGE   ERROR\ncontroller-manager   Healthy   ok        \nscheduler            Healthy   ok        \netcd-0               Healthy   ok\n</code></pre>\n<p>创建 bootstrap 相关资源：</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml \nsecret/bootstrap-token-c8ad9c created\nclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created\nclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created\nclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created\n</code></pre>\n<h4 id=\"8-node节点配置\"><a class=\"anchor\" href=\"#8-node节点配置\">#</a> 8. Node 节点配置</h4>\n<h5 id=\"81-复制证书\"><a class=\"anchor\" href=\"#81-复制证书\">#</a> 8.1 复制证书</h5>\n<p><mark>Master01 节点</mark>复制证书至其他节点：</p>\n<pre><code>cd /etc/kubernetes/\n\nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do\n     ssh $NODE mkdir -p /etc/kubernetes/pki\n     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do\n       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&#123;FILE&#125;\n done\n done\n</code></pre>\n<p>执行结果：</p>\n<pre><code>ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    \n…\nbootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00\n</code></pre>\n<h5 id=\"82-kubelet配置\"><a class=\"anchor\" href=\"#82-kubelet配置\">#</a> 8.2 Kubelet 配置</h5>\n<p><mark>所有节点</mark>创建 Kubelet 配置目录</p>\n<pre><code>mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/\n</code></pre>\n<p><mark>所有节点</mark>配置 kubelet service</p>\n<pre><code>[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kubelet\n\nRestart=always\nStartLimitInterval=0\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>所有节点</mark>配置 kubelet service 的配置文件（也可以写到 kubelet.service）：</p>\n<pre><code># Runtime为Containerd\n# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf\n\n[Service]\nEnvironment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&quot;\nEnvironment=&quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot;\nEnvironment=&quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&quot;\nEnvironment=&quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' &quot;\nExecStart=\nExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS\n</code></pre>\n<p><mark>所有节点</mark>创建 kubelet 的配置文件</p>\n<p><em>注意：如果更改了 k8s 的 service 网段，需要更改 kubelet-conf.yml 的 clusterDNS: 配置，改成 k8s Service 网段的第十个地址，比如 10.96.0.10</em></p>\n<pre><code>[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml\n\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\ncgroupDriver: systemd\ncgroupsPerQOS: true\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerLogMaxFiles: 5\ncontainerLogMaxSize: 10Mi\ncontentType: application/vnd.kubernetes.protobuf\ncpuCFSQuota: true\ncpuManagerPolicy: none\ncpuManagerReconcilePeriod: 10s\nenableControllerAttachDetach: true\nenableDebuggingHandlers: true\nenforceNodeAllocatable:\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhairpinMode: promiscuous-bridge\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\niptablesDropBit: 15\niptablesMasqueradeBit: 14\nkubeAPIBurst: 10\nkubeAPIQPS: 5\nmakeIPTablesUtilChains: true\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nregistryBurst: 10\nregistryPullQPS: 5\nresolvConf: /etc/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 2m0s\nserializeImagePulls: true\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h0m0s\nsyncFrequency: 1m0s\nvolumeStatsAggPeriod: 1m0s\n</code></pre>\n<p>启动<mark>所有节点</mark> kubelet</p>\n<pre><code>systemctl daemon-reload\nsystemctl enable --now kubelet\n</code></pre>\n<p>此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<p><em>如果有很多报错日志，或者有大量看不懂的报错，说明 kubelet 的配置有误，需要检查 kubelet 配置</em></p>\n<p>Master01 查看集群状态 (Ready 或 NotReady 都正常)</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl get node\n</code></pre>\n<h5 id=\"83-kube-proxy配置\"><a class=\"anchor\" href=\"#83-kube-proxy配置\">#</a> 8.3 kube-proxy 配置</h5>\n<p><em>注意，如果不是高可用集群，192.168.1.70:8443 改为 master01 的地址，8443 改为 apiserver 的端口，默认是 6443</em></p>\n<p>生成 kube-proxy 的证书，以下操作只在<mark> Master01</mark> 执行</p>\n<pre><code>cd /root/k8s-ha-install/pki\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config set-credentials system:kube-proxy \\\n     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \\\n     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\nkubectl config set-context system:kube-proxy@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-proxy \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config use-context system:kube-proxy@kubernetes \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n</code></pre>\n<p>将 kubeconfig 发送至其他节点</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n\nfor NODE in k8s-node01 k8s-node02; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n</code></pre>\n<p><mark>所有节点</mark>添加 kube-proxy 的配置和 service 文件：</p>\n<pre><code>vim /usr/lib/systemd/system/kube-proxy.service\n\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\n  --config=/etc/kubernetes/kube-proxy.yaml \\\n  --v=2\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>如果更改了集群 Pod 的网段，需要更改 kube-proxy.yaml 的 clusterCIDR 为自己的 Pod 网段：</p>\n<pre><code>vim /etc/kubernetes/kube-proxy.yaml\n\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nclientConnection:\n  acceptContentTypes: &quot;&quot;\n  burst: 10\n  contentType: application/vnd.kubernetes.protobuf\n  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n  qps: 5\nclusterCIDR: 172.16.0.0/16 \nconfigSyncPeriod: 15m0s\nconntrack:\n  max: null\n  maxPerCore: 32768\n  min: 131072\n  tcpCloseWaitTimeout: 1h0m0s\n  tcpEstablishedTimeout: 24h0m0s\nenableProfiling: false\nhealthzBindAddress: 0.0.0.0:10256\nhostnameOverride: &quot;&quot;\niptables:\n  masqueradeAll: false\n  masqueradeBit: 14\n  minSyncPeriod: 0s\n  syncPeriod: 30s\nipvs:\n  masqueradeAll: true\n  minSyncPeriod: 5s\n  scheduler: &quot;rr&quot;\n  syncPeriod: 30s\nkind: KubeProxyConfiguration\nmetricsBindAddress: 127.0.0.1:10249\nmode: &quot;ipvs&quot;\nnodePortAddresses: null\noomScoreAdj: -999\nportRange: &quot;&quot;\nudpIdleTimeout: 250ms\n</code></pre>\n<p><mark>所有节点</mark>启动 kube-proxy</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload\n[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /usr/lib/systemd/system/kube-proxy.service.\n</code></pre>\n<p>此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<h4 id=\"9-calico组件的安装\"><a class=\"anchor\" href=\"#9-calico组件的安装\">#</a> 9. Calico 组件的安装</h4>\n<p>以下步骤只在 master01 执行：</p>\n<pre><code>cd /root/k8s-ha-install/calico/\n</code></pre>\n<p>更改 calico 的网段，主要需要将红色部分的网段，改为自己的 Pod 网段</p>\n<pre><code>sed -i &quot;s#POD_CIDR#172.16.0.0/16#g&quot; calico.yaml\n</code></pre>\n<p><em>检查网段是自己的 Pod 网段， grep &quot;IPV4POOL_CIDR&quot; calico.yaml  -A 1</em></p>\n<p>查看容器和节点状态：</p>\n<pre><code>[root@k8s-master01 calico]# kubectl get po -n kube-system\nNAME                                       READY   STATUS    RESTARTS      AGE\ncalico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s\ncalico-node-8fxqp                          1/1     Running   0             85s\ncalico-node-8nkfl                          1/1     Running   0             86s\ncalico-node-pmpf4                          1/1     Running   0             86s\ncalico-node-vnlk7                          1/1     Running   0             86s\ncalico-node-xpchb                          1/1     Running   0             85s\ncalico-typha-67c6dc57d6-259t8              1/1     Running   0             86s\n</code></pre>\n<p><em>如果容器状态异常可以使用 kubectl describe 或者 kubectl logs 查看容器的日志</em></p>\n<ol>\n<li>Kubectl logs -f POD_NAME -n kube-system</li>\n<li>Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system</li>\n</ol>\n<h4 id=\"10-安装coredns\"><a class=\"anchor\" href=\"#10-安装coredns\">#</a> 10. 安装 CoreDNS</h4>\n<pre><code>cd /root/k8s-ha-install/\n</code></pre>\n<p>如果更改了 k8s service 的网段需要将 coredns 的 serviceIP 改成 k8s service 网段的第十个 IP</p>\n<pre><code>COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk '&#123;print $3&#125;'`0\nsed -i &quot;s#KUBEDNS_SERVICE_IP#$&#123;COREDNS_SERVICE_IP&#125;#g&quot; CoreDNS/coredns.yaml\n</code></pre>\n<p>安装 coredns</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml \nserviceaccount/coredns created\nclusterrole.rbac.authorization.k8s.io/system:coredns created\nclusterrolebinding.rbac.authorization.k8s.io/system:coredns created\nconfigmap/coredns created\ndeployment.apps/coredns created\nservice/kube-dns created\n</code></pre>\n<h4 id=\"11-metrics部署\"><a class=\"anchor\" href=\"#11-metrics部署\">#</a> 11. Metrics 部署</h4>\n<p>在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。</p>\n<p>以下操作均在<mark> master01 节点</mark>执行，安装 metrics server:</p>\n<pre><code>cd /root/k8s-ha-install/metrics-server\nkubectl  create -f . \n\nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>等待 metrics server 启动然后查看状态：</p>\n<pre><code># kubectl  top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-master01   231m         5%     1620Mi          42%       \nk8s-master02   274m         6%     1203Mi          31%       \nk8s-master03   202m         5%     1251Mi          32%       \nk8s-node01     69m          1%     667Mi           17%       \nk8s-node02     73m          1%     650Mi           16%\n</code></pre>\n<p>如果有如下报错，可以等待 10 分钟后，再次查看：</p>\n<pre><code>Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)\n</code></pre>\n<h4 id=\"12-dashboard部署\"><a class=\"anchor\" href=\"#12-dashboard部署\">#</a> 12. Dashboard 部署</h4>\n<h5 id=\"121-安装dashboard\"><a class=\"anchor\" href=\"#121-安装dashboard\">#</a> 12.1 安装 Dashboard</h5>\n<p>Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"122-登录dashboard\"><a class=\"anchor\" href=\"#122-登录dashboard\">#</a> 12.2 登录 dashboard</h5>\n<p>在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>更改 dashboard 的 svc 为 NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）</em></p>\n<p>查看端口号：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：</p>\n<p>访问 Dashboard：<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>创建登录 Token：</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"14-containerd配置镜像加速\"><a class=\"anchor\" href=\"#14-containerd配置镜像加速\">#</a> 14. Containerd 配置镜像加速</h4>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>所有节点重新启动 Containerd：</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"15-docker配置镜像加速\"><a class=\"anchor\" href=\"#15-docker配置镜像加速\">#</a> 15. Docker 配置镜像加速</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>所有节点重新启动 Docker：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/2628187572.html",
            "url": "http://xuyong.cn/posts/2628187572.html",
            "title": "MySQL运维DBA应用与实践",
            "date_published": "2025-04-09T14:02:40.000Z",
            "content_html": "<h3 id=\"mysql运维dba应用与实践\"><a class=\"anchor\" href=\"#mysql运维dba应用与实践\">#</a> MySQL 运维 DBA 应用与实践</h3>\n<h4 id=\"1日志\"><a class=\"anchor\" href=\"#1日志\">#</a> 1. 日志</h4>\n<p>在任何一种数据库中，都会有各种各样的日志，这些日志记录了数据库运行的各个方面。可以帮助数据库管理员追踪数据库曾经发生的一些事情。</p>\n<p>对于 MySQL 数据库，提供了四种不同的日志帮助我们追踪。</p>\n<ul>\n<li>\n<p>错误日志</p>\n</li>\n<li>\n<p>二进制日志</p>\n</li>\n<li>\n<p>查询日志</p>\n</li>\n<li>\n<p>慢查询日志</p>\n</li>\n</ul>\n<h5 id=\"11-错误日志\"><a class=\"anchor\" href=\"#11-错误日志\">#</a> 1.1 错误日志</h5>\n<p>错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld (MySQL 服务) 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，建议首先查看此日志。</p>\n<p>该日志是默认开启的，默认存放目录 /var/log/，默认的日志文件名为 mysqld.log。查看日志位置；</p>\n<pre><code>mysql&gt; show variables like '%log_error%';\n+---------------------+---------------------+\n| Variable_name       | Value               |\n+---------------------+---------------------+\n| binlog_error_action | ABORT_SERVER        |\n| log_error           | /var/log/mysqld.log |\n| log_error_verbosity | 3                   |\n+---------------------+---------------------+\n</code></pre>\n<h5 id=\"12-二进制日志\"><a class=\"anchor\" href=\"#12-二进制日志\">#</a> 1.2 二进制日志</h5>\n<p>二进制日志 (BINLOG) 记录了所有的 DDL (数据定义语言) 语句和 DML (数据操纵语言) 语句，但不包括数据查询（SELECT、 SHOW）语句。</p>\n<p>作用:</p>\n<p>①. 灾难时的数据恢复；</p>\n<p>②. MySQL 的主从复制。</p>\n<p>在 MySQL5.7 版本中，默认二进制日志是关闭着的，涉及到的参数如下:</p>\n<h6 id=\"121-开启-bin-log记录\"><a class=\"anchor\" href=\"#121-开启-bin-log记录\">#</a> 1.2.1 开启 bin-log 记录</h6>\n<pre><code>1.1改修配置文件\n[root@db01 ~]# vim /etc/my.cnf\nserver-id=1\nlog-bin=mysql-bin\nmax_binlog_size=500M\nexpire_logs_days=15\n\n1.2查看是否开启binlog.\nmysql&gt; show variables like 'log_%';\n+----------------------------------------+--------------------------------+\n| Variable_name                          | Value                          |\n+----------------------------------------+--------------------------------+\n| log_bin                                | ON                             |\n| log_bin_basename                       | /var/lib/mysql/mysql-bin       |\n| log_bin_index                          | /var/lib/mysql/mysql-bin.index |\n| log_bin_trust_function_creators        | OFF                            |\n| log_bin_use_v1_row_events              | OFF                            |\n| log_builtin_as_identified_by_password  | OFF                            |\n| log_error                              | /var/log/mysqld.log            |\n| log_error_verbosity                    | 3                              |\n| log_output                             | FILE                           |\n| log_queries_not_using_indexes          | OFF                            |\n| log_slave_updates                      | OFF                            |\n| log_slow_admin_statements              | OFF                            |\n| log_slow_slave_statements              | OFF                            |\n| log_statements_unsafe_for_binlog       | ON                             |\n| log_syslog                             | OFF                            |\n| log_syslog_facility                    | daemon                         |\n| log_syslog_include_pid                 | ON                             |\n| log_syslog_tag                         |                                |\n| log_throttle_queries_not_using_indexes | 0                              |\n| log_timestamps                         | UTC                            |\n| log_warnings                           | 2                              |\n+----------------------------------------+--------------------------------+\n\n1.3查看binlog\nmysql&gt; show binary logs;\n+------------------+-----------+\n| Log_name         | File_size |\n+------------------+-----------+\n| mysql-bin.000001 |     36825 |\n| mysql-bin.000002 |    200464 |\n| mysql-bin.000003 |    419809 |\n+------------------+-----------+\n\n1.4查看binlog日志保存天数 \n# 0表示永久保留，expire_logs_days：保留指定日期范围内的binlog历史日志，上示例设置的15天内\nmysql&gt; show variables like 'expire_logs_days';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| expire_logs_days | 15    |\n+------------------+-------+\n1 row in set (0.00 sec)\n\n1.5查看binlog日志保存大小\n#max_binlog_size：bin log日志每达到设定大小后，会使用新的bin log日志。如mysql-bin.000002达到500M后，创建并使用mysql-bin.000003文件作为日志记录。\nmysql&gt; show variables like 'max_binlog_size';\n+-----------------+-----------+\n| Variable_name   | Value     |\n+-----------------+-----------+\n| max_binlog_size | 524288000 |\n+-----------------+-----------+\n\n1.6手动执行flush logs\n#将会new一个新文件用于记录binlog\nmysql&gt; flush logs;\n\n1.7手动清理binlog\n#将mysql-bin.000010之前的日志清理掉\nmysql&gt; purge binary logs to 'mysql-bin.000010';\nQuery OK, 0 rows affected (0.01 sec)\n\n#删除2022-04-21 18:08:00之前的binlog日志\nmysql&gt; purge binary logs before '2022-04-21 18:08:00';\n\n#清除全部binlog\nmysql&gt; reset master;\n</code></pre>\n<h6 id=\"122-日志格式\"><a class=\"anchor\" href=\"#122-日志格式\">#</a> <strong>1.2.2 日志格式</strong></h6>\n<p>MySQL 服务器中提供了多种格式来记录二进制日志，具体格式及特点如下：</p>\n<table>\n<thead>\n<tr>\n<th><strong>日志格式</strong></th>\n<th><strong>含义</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STATEMENT</td>\n<td>基于 SQL 语句的日志记录，记录的是 SQL 语句，对数据进行修改的 SQL 都会记录在日志文件中。</td>\n</tr>\n<tr>\n<td>ROW</td>\n<td>基于行的日志记录，记录的是每一行的数据变更。(默认)</td>\n</tr>\n<tr>\n<td>MIXED</td>\n<td>混合了 STATEMENT 和 ROW 两种格式，默认采用 STATEMENT, 在某些特殊情况下会自动切换为 ROW 进行记录。</td>\n</tr>\n</tbody>\n</table>\n<pre><code>mysql&gt; show variables like 'binlog_format';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n</code></pre>\n<p>由于日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 <code>mysqlbinlog</code>  来查看，具体语法:</p>\n<pre><code>mysqlbinlog [ 参数选项] logfilename\n参数选项:\n\t-d\t\t\t指定数据库名称，只列出指定的数据库相关操作。\n\t-o\t\t\t忽略掉日志中的前n行命令。\n\t-v\t\t\t将行事件(数据变更)重构为SQL语句\n\t-vv\t\t\t将行事件(数据变更)重构为SQL语句，并输出注释信息\n</code></pre>\n<pre><code>mysql&gt; use zh;\nDatabase changed\nmysql&gt; show tables;\n+----------------+\n| Tables_in_zh   |\n+----------------+\n| account        |\n| course         |\n| dept           |\n| emp            |\n| score          |\n| student        |\n| student_course |\n| tb_user        |\n| tb_user_edu    |\n| user           |\n| user1          |\n+----------------+\n11 rows in set (0.00 sec)\n\nmysql&gt;  update tb_user_edu set university = &quot;北京大学&quot;;\nQuery OK, 4 rows affected (0.00 sec)\nRows matched: 4  Changed: 4  Warnings: 0\n\n#二进制日志查看\n[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000001 \n</code></pre>\n<h6 id=\"123-修改binlog格式\"><a class=\"anchor\" href=\"#123-修改binlog格式\">#</a> 1.2.3 修改 binlog 格式</h6>\n<pre><code>[root@db01 ~]# vim /etc/my.cnf\n...\nbinlog_format=STATEMENT\n...\n[root@db01 ~]# systemctl restart mysqld\n\nmysql&gt;  update tb_user_edu set university = '清华大学';\n[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000002 \n...\nSET TIMESTAMP=1701440373/*!*/;\nupdate tb_user_edu set university = '清华大学'\n...\n</code></pre>\n<h5 id=\"13-查询日志\"><a class=\"anchor\" href=\"#13-查询日志\">#</a> 1.3 查询日志</h5>\n<p>查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的 SQL 语句。默认情况下，<strong>查询日志是未开启的</strong>。如果需要开启查询日志，可以设置以下配置︰</p>\n<pre><code>mysql&gt; show variables like '%general%';\n+------------------+-------------------------+\n| Variable_name    | Value                   |\n+------------------+-------------------------+\n| general_log      | OFF                     |\n| general_log_file | /var/lib/mysql/db01.log |\n+------------------+-------------------------+\n2 rows in set (0.00 sec)\n\n#开启查询日志功能\n[root@db01 ~]# cat /etc/my.cnf\ngeneral_log=1\ngeneral_log_file=/var/lib/mysql/mysql_query.log \n[root@db01 ~]# systemctl restart mysqld\n\n[root@db01 ~]# tail -f /var/lib/mysql/mysql_query.log \n2023-12-01T14:31:28.554384Z\t    2 Field List\tstudent \n2023-12-01T14:31:28.554743Z\t    2 Field List\tstudent_course \n2023-12-01T14:31:35.737041Z\t    2 Query\tshow variables like '%general%'\n2023-12-01T14:31:37.345179Z\t    2 Query\tshow variables like '%general%'\n2023-12-01T14:32:17.593471Z\t    2 Query\tSELECT DATABASE()\n2023-12-01T14:32:17.593651Z\t    2 Init DB\tzh\n2023-12-01T14:32:25.249258Z\t    2 Query\tselect * from emp\n</code></pre>\n<h5 id=\"14-慢查询日志\"><a class=\"anchor\" href=\"#14-慢查询日志\">#</a> 1.4 慢查询日志</h5>\n<p>慢查询<a href=\"https://so.csdn.net/so/search?q=%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95&amp;spm=1001.2101.3001.7020\">日志记录</a>了所有执行时间超过参数 <code>long_ query_time</code>  设置值并且扫描记录数不小于 <code>min_examined_row_limit</code>  的所有的 SQL 语句的日志，默认未开启。<strong> <code>long_query_time</code>  默认为 10 秒，最小为 0，精度可以到微秒。</strong></p>\n<pre><code>[root@db01 ~]# vim /etc/my.cnf\n#慢查询日志\nslow_query_log=on\n##执行时间参数\nlong_query_time=2\n# 若没有指定，默认名字为hostname_slow.log\nslow_query_log_file = /var/lib/mysql/slow-query.log\n[root@db01 ~]# systemctl restart mysqld\n\n#制造慢查询并执行\nmysql&gt; select sleep(3);\n[root@db01 ~]# tail -f /var/lib/mysql/slow-query.log \n/usr/sbin/mysqld, Version: 5.7.43-log (MySQL Community Server (GPL)). started with:\nTcp port: 0  Unix socket: /var/lib/mysql/mysql.sock\nTime                 Id Command    Argument\n# Time: 2023-12-01T14:47:57.763735Z\n# User@Host: root[root] @ localhost []  Id:     2\n# Query_time: 3.001229  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0\nuse zh;\nSET timestamp=1701442077;\nselect sleep(3);\n</code></pre>\n<p>默认情况下，不会记录管理语句，也不会记录不使用索引进行查找的查询。可以使用 <code>log_slow_admin_statements</code>  和更改此行为 <code>log_queries_not_using_indexes</code> , 如下所述。</p>\n<pre><code>#记录执行较慢的管理语句\nlog_slow_admin_statements = 1\n#记录执行较慢的未使用索引的语句\nlog_queries_not_using_indexes = 1\n</code></pre>\n<h4 id=\"2-主从复制\"><a class=\"anchor\" href=\"#2-主从复制\">#</a> 2. 主从复制</h4>\n<h5 id=\"21-主从复制的概述\"><a class=\"anchor\" href=\"#21-主从复制的概述\">#</a> 2.1 主从复制的概述</h5>\n<p>主从复制是指将<strong>主数据库的 DDL 和 DML 操作</strong>通过<strong>二进制日志</strong>传到<strong>从库服务器</strong>中，然后在从库上对这些日志重新执行 (也叫重做) ，从而使得从库和主库的数据保持同步。</p>\n<p><a href=\"https://imgse.com/i/pEgO0Mj\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgO0Mj.png\" alt=\"pEgO0Mj.png\" /></a></p>\n<p>MySQL 支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库， 实现链状复制。</p>\n<p>MySQL 复制的有点主要包含以下三个方面：</p>\n<ol>\n<li>主库出现问题，可以快速切换到从库提供服务；</li>\n<li>实现读写分离，降低主库的访问压力；（如果增删改对主库 查询对从库）</li>\n<li>可以在从库中执行备份，以避免备份期间影响主库服务。</li>\n</ol>\n<h5 id=\"22-主从复制的原理\"><a class=\"anchor\" href=\"#22-主从复制的原理\">#</a> 2.2 主从复制的原理</h5>\n<p><a href=\"https://imgse.com/i/pEgOdzQ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgOdzQ.png\" alt=\"pEgOdzQ.png\" /></a></p>\n<p>从上图来看，复制分成三步：</p>\n<ol>\n<li>Master 主库在事务提交时，会把数据变更记录在二进制日志文件 Binlog 中。</li>\n<li>从库 IO 线程读取主库的二进制日志文件 Binlog，写入到从库的中继日志 Relay Log。</li>\n<li>slave 重做中继日志中的事件，SQL 线程将改变反映它自己的数据。</li>\n</ol>\n<h5 id=\"23-主从复制的搭建\"><a class=\"anchor\" href=\"#23-主从复制的搭建\">#</a> 2.3 主从复制的搭建</h5>\n<p><strong>主从复制的搭建步骤</strong>：</p>\n<ol>\n<li>准备主从复制服务器环境</li>\n<li>完成主库配置</li>\n<li>完成从库配置</li>\n</ol>\n<h6 id=\"231-服务器准备\"><a class=\"anchor\" href=\"#231-服务器准备\">#</a> 2.3.1 服务器准备</h6>\n<p><a href=\"https://imgse.com/i/pEgODLn\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgODLn.png\" alt=\"pEgODLn.png\" /></a></p>\n<h6 id=\"232-主库配置\"><a class=\"anchor\" href=\"#232-主库配置\">#</a> 2.3.2 主库配置</h6>\n<p><strong>#1. 安装 MySQL</strong></p>\n<pre><code>#1、关闭防火墙、selinux、环境配置\n[root@db01 ~]# hostnamectl set-hostname db01\n[root@db01 ~]# systemctl stop firewalld\n[root@db01 ~]# systemctl disable firewalld\n[root@db01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@db01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@db01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@db01 ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@db01 ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@db01 ~]# ntpdate time2.aliyun.com\n[root@db01 ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@db01 ~]# mkdir /soft /data /scripts /backup\n\n#2、安装Mysql5.7\n[root@db01 ~]# yum install -y mysql-community-server\n[root@db01 ~]# systemctl start mysqld &amp;&amp; systemctl enable mysqld\n\n[root@db01 ~]# mysql -uroot -p$(awk '/temporary password/&#123;print $NF&#125;' /var/log/mysqld.log)\nmysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'passwd';\nmysql&gt; grant all on *.* to 'root'@'192.168.1.%' identified by 'passwd';\n\n#3、允许root用户在任何地方进行远程登录，并具有所有库任何操作权限，具体操作如下：\nmysql -u root -p&quot;youpass&quot;\nmysql&gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'passwd' WITH GRANT OPTION;\nFLUSH PRIVILEGES;\n\n#4.配置主库\n[root@db01 ~]# vim /etc/my.cnf\nserver-id=1                #mysql服务ID，保证整个集群环境中唯一， 取值范围: 1 - 2^&#123;32&#125;-1\nlog-bin=mysql-bin          #启动二进制日志\nread-only=0                #是否只读,1代表只读, 0代表读写\n#binlog-ignore-db=mysql    #忽略的数据，指不需要同步的数据库\n#binlog-do-db=db01         #指定同步的数据库\n[root@db01 ~]# systemctl restart mysqld\n\n#5.创建repl用户，并设置密码，该用户可在任意主机连接该MySQL服务\nmysql&gt; grant replication slave on *.* to 'repl'@'%' identified by 'passwd';\n\n#6.查看master位置点\nmysql&gt; show master status;        \n+------------------+----------+--------------+------------------+-------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n+------------------+----------+--------------+------------------+-------------------+\n| mysql-bin.000006 |      889 |              |                  |                   |\n+------------------+----------+--------------+------------------+-------------------+\n1 row in set (0.00 sec)\n</code></pre>\n<h6 id=\"233-从库配置\"><a class=\"anchor\" href=\"#233-从库配置\">#</a> 2.3.3 从库配置</h6>\n<table>\n<thead>\n<tr>\n<th>参数名</th>\n<th>含义</th>\n<th><strong>8.0.23 之前</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SOURCE_HOST</td>\n<td>主库 IP 地址</td>\n<td>MASTER_HOST</td>\n</tr>\n<tr>\n<td>SOURCE_USER</td>\n<td>连接主库的用户名</td>\n<td>MASTER_USER</td>\n</tr>\n<tr>\n<td>SOURCE_PASSWORD</td>\n<td>连接主库的密码</td>\n<td>MASTER_PASSWORD</td>\n</tr>\n<tr>\n<td>SOURCE_LOG FILE</td>\n<td>binlog 日志文件名</td>\n<td>MASTER LOG_FILE</td>\n</tr>\n<tr>\n<td>SOURCE_LOG POS</td>\n<td>binlog 日志文件位置</td>\n<td>MASTER_LOG_POS</td>\n</tr>\n</tbody>\n</table>\n<pre><code>#1.配置从库\n[root@db02 ~]# vim /etc/my.cnf\nserver-id=2           #mysql服务ID\nread-only=1           #是否只读,1代表只读, 0代表读写\n[root@db02 ~]# systemctl restart mysqld\n\n#2..配置从服务器，连接主服务器\nmysql&gt; change master to master_host='192.168.40.150',master_user='repl',master_password='passwd',master_log_file='mysql-bin.000006',master_log_pos=889;\n\n#3.开启从库\nmysql&gt; start slave;\nQuery OK, 0 rows affected (0.00 sec)\n\n#4.检查主从复制状态\nmysql&gt; show slave status\\G\n*************************** 1. row ***************************\n               Slave_IO_State: Waiting for master to send event\n                  Master_Host: 192.168.40.150\n                  Master_User: repl\n                  Master_Port: 3306\n                Connect_Retry: 60\n              Master_Log_File: mysql-bin.000006\n          Read_Master_Log_Pos: 889\n               Relay_Log_File: db02-relay-bin.000002\n                Relay_Log_Pos: 320\n        Relay_Master_Log_File: mysql-bin.000006\n             Slave_IO_Running: Yes\n            Slave_SQL_Running: Yes\n              Replicate_Do_DB: \n          Replicate_Ignore_DB: \n           Replicate_Do_Table: \n       Replicate_Ignore_Table: \n      Replicate_Wild_Do_Table: \n  Replicate_Wild_Ignore_Table: \n                   Last_Errno: 0\n                   Last_Error: \n                 Skip_Counter: 0\n          Exec_Master_Log_Pos: 889\n              Relay_Log_Space: 526\n              Until_Condition: None\n               Until_Log_File: \n                Until_Log_Pos: 0\n           Master_SSL_Allowed: No\n           Master_SSL_CA_File: \n           Master_SSL_CA_Path: \n              Master_SSL_Cert: \n            Master_SSL_Cipher: \n               Master_SSL_Key: \n        Seconds_Behind_Master: 0\nMaster_SSL_Verify_Server_Cert: No\n                Last_IO_Errno: 0\n                Last_IO_Error: \n               Last_SQL_Errno: 0\n               Last_SQL_Error: \n  Replicate_Ignore_Server_Ids: \n             Master_Server_Id: 1\n                  Master_UUID: 9b911bea-43e6-11ee-b239-000c29074f5d\n             Master_Info_File: /var/lib/mysql/master.info\n                    SQL_Delay: 0\n          SQL_Remaining_Delay: NULL\n      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates\n           Master_Retry_Count: 86400\n                  Master_Bind: \n      Last_IO_Error_Timestamp: \n     Last_SQL_Error_Timestamp: \n               Master_SSL_Crl: \n           Master_SSL_Crlpath: \n           Retrieved_Gtid_Set: \n            Executed_Gtid_Set: \n                Auto_Position: 0\n         Replicate_Rewrite_DB: \n                 Channel_Name: \n           Master_TLS_Version: \n1 row in set (0.00 sec)\n</code></pre>\n<h4 id=\"3-分库分表\"><a class=\"anchor\" href=\"#3-分库分表\">#</a> 3. <a href=\"https://so.csdn.net/so/search?q=%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8&amp;spm=1001.2101.3001.7020\">分库分表</a></h4>\n<h5 id=\"31-分库分表介绍\"><a class=\"anchor\" href=\"#31-分库分表介绍\">#</a> 3.1 分库分表介绍</h5>\n<h6 id=\"311-现在的问题\"><a class=\"anchor\" href=\"#311-现在的问题\">#</a> 3.1.1 现在的问题</h6>\n<p><strong>单数据库</strong></p>\n<p>所有数据都是存放在一个<a href=\"https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%BA%93%E6%96%87%E4%BB%B6&amp;spm=1001.2101.3001.7020\">数据库文件</a>里的，经过常年累月，内存不足了怎么办？</p>\n<p><a href=\"https://imgse.com/i/pEgOyd0\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgOyd0.png\" alt=\"pEgOyd0.png\" /></a></p>\n<p>随着互联网及移动互联网的发展，应用系统的数据量也是成指数式增长，若采用单数据库进行数据存储，存在以下性能瓶颈：</p>\n<p>IO 瓶颈：热点数据太多，数据库缓存不足，产生大量磁盘 IO，效率较低。请求数据太多，带宽不够，网络 IO 瓶颈。<br />\nCPU 瓶颈： 排序、分组、连接查询、聚合统计等 SQL 会耗费大量的 CPU 资源，请求数太多，CPU 出现瓶颈。</p>\n<p><a href=\"https://imgse.com/i/pEgO6oV\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgO6oV.png\" alt=\"pEgO6oV.png\" /></a></p>\n<p><strong>分库分表的中心思想：<br />\n将数据分散存储，使得单一数据库 / 表的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的。</strong></p>\n<h6 id=\"312-拆分策略\"><a class=\"anchor\" href=\"#312-拆分策略\">#</a> 3.1.2 拆分策略</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/e1f3b20cb856474eaec049096d9fbdd9.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"313-垂直拆分策略\"><a class=\"anchor\" href=\"#313-垂直拆分策略\">#</a> 3.1.3 垂直拆分策略</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/4912fad133d84e638b3f0ba0a6827238.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p>特点:</p>\n<ol>\n<li>每个库的表结构都不一样。</li>\n<li>每个库的数据也不一样 。</li>\n<li>所有，库的并集是全量数据。</li>\n</ol>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/8d6d88eb551f415a9edc9dcf79352140.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p>特点:</p>\n<ol>\n<li>每个表的结构都不一样。</li>\n<li>每个表的数据也术一样，一般通过一列 (主键 / 外键) 关联。</li>\n<li>所有表的并集是全量数据。</li>\n</ol>\n<h6 id=\"314-水平拆分策略\"><a class=\"anchor\" href=\"#314-水平拆分策略\">#</a> 3.1.4 水平拆分策略</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/d1f4ddf5d43f49538395ff16be70410f.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p>水平分库：以 “字段” 为依据，改为以 “行（记录）” 为依据。讲一个库的数据拆分到多个库</p>\n<p>特点：</p>\n<ol>\n<li>每个库的表结构都一样。</li>\n<li>每个库的数据都不一样。</li>\n<li>所有库的并集是全量数据。</li>\n</ol>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/a0f66e9cebc14bcabd67e4d038d2e1d1.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p>特点：</p>\n<ol>\n<li>每个表的表结构都一样 。</li>\n<li>每个表的数据都不一样 。</li>\n<li>所有表的并集是全量数据。</li>\n</ol>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/1506ad73a5fb408988fec987ba0b6282.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<ul>\n<li>shardingJDBC：基于 AOP 原理，在应用程序中对本地执行的 SQL 进行拦截，解析、改写、路由处理。需要自行编码配置实现，只支持 java 语言，性能较高。</li>\n<li>MyCat：数据库分库分表中间件，不用调整代码即可实现分库分表，支持多种语言，性能不及前者。</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/32abcd34fd5742cb91944c1402918500.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h5 id=\"32-mycat概述\"><a class=\"anchor\" href=\"#32-mycat概述\">#</a> 3.2 Mycat 概述</h5>\n<p>Mycat 是开源的、活跃的、基于 Java 语言编写的<strong> MySQL 数据库中间件</strong>。可以像使用 mysql 一样来使用 mycat，对于开发人员来说根本感觉不到 mycat 的存在。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/ee2bc7bf90354c73bcb89ca8aa0fa141.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p>优势：</p>\n<ul>\n<li>性能可靠稳定</li>\n<li>强大的技术团队</li>\n<li>体系完善</li>\n<li>社区活跃</li>\n</ul>\n<p>Mycat 是采用 java 语言开发的开源的数据库中间件，支持 Windows 和 Linux 运行环境，下面介绍 MyCat 的 Linux 中的环境搭建。 我们需要在准备好的服务器中安装如下软件。</p>\n<table>\n<thead>\n<tr>\n<th>服务器</th>\n<th>安装软件</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>192.168.40.213</td>\n<td>JDK、Mycat</td>\n<td>MyCat 中间件服务器</td>\n</tr>\n<tr>\n<td>192.168.40.210</td>\n<td>MySQL</td>\n<td>分片服务器</td>\n</tr>\n<tr>\n<td>192.168.40.211</td>\n<td>MySQL</td>\n<td>分片服务器</td>\n</tr>\n<tr>\n<td>192.168.40.212</td>\n<td>MySQL</td>\n<td>分片服务器</td>\n</tr>\n</tbody>\n</table>\n<p>JDK 安装</p>\n<pre><code>#解压jdk\n[root@mycat ~]# tar xf jdk-8u371-linux-x64.tar.gz -C /usr/local\n[root@mycat ~]# ln -s /usr/local/jdk1.8.0_371/ /usr/local/jdk\n\n# 添加环境变量\n[root@mycat ~]# vim /etc/profile.d/jdk.sh \nexport JAVA_HOME=/usr/local/jdk\nexport PATH=$PATH:$JAVA_HOME/bin\nexport JRE_HOME=$JAVA_HOME/jre \nexport CLASSPATH=$JAVA_HOME/lib/:$JRE_HOME/lib/\n\n[root@mycat ~]# source /etc/profile\n[root@mycat ~]# java -version\n</code></pre>\n<p>Mycat 安装</p>\n<pre><code>[root@mycat ~]# tar xf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz -C /usr/local/\n[root@mycat ~]# ll /usr/local/mycat/\ntotal 12\ndrwxr-xr-x 2 root root  190 Dec  2 22:15 bin\ndrwxrwxrwx 2 root root    6 Mar  1  2016 catlet\ndrwxrwxrwx 4 root root 4096 Dec  2 22:15 conf\ndrwxr-xr-x 2 root root 4096 Dec  2 22:15 lib\ndrwxrwxrwx 2 root root    6 Oct 28  2016 logs\n-rwxrwxrwx 1 root root  217 Oct 28  2016 version.txt\n\n#上传jar包\n[root@mycat ~]# rz /usr/local/mycat/lib/mysql-connector-java-8.0.25.jar\n[root@mycat lib]# chmod 777 mysql-connector-java-8.0.25.jar \n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/1271af61dd794d77bf062037a7bd9207.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/9e8d0d07b6984b58a4205da0e0aad689.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"321-mycat入门\"><a class=\"anchor\" href=\"#321-mycat入门\">#</a> 3.2.1 Mycat 入门</h6>\n<p>由于 tb_gorder 表中数据量很大，磁盘 IO 及容量都到达了瓶颈，现在需要对 tb_order 表进行数据分片，分为三个数据节点，每一个节点主机位于不同的服务器上，具体的结构，参考下图：</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/7a17499312664b0dbe44f2f712ddf566.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/883c1a5153274f409fe1235f9348dc5d.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"322-mycat配置\"><a class=\"anchor\" href=\"#322-mycat配置\">#</a> 3.2.2 Mycat 配置</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/f1965885c11542f8a0006e9be3b6f6a1.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;DB01&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;TB_ORDER&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot; /&gt;\n\t&lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;db01&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;db01&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;db01&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/9c7db64a028149c697b4e3183764fb9d.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<pre><code>[root@mycat mycat]# cat /usr/local/mycat/conf/server.xml \n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!-- - - Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); \n\t- you may not use this file except in compliance with the License. - You \n\tmay obtain a copy of the License at - - http://www.apache.org/licenses/LICENSE-2.0 \n\t- - Unless required by applicable law or agreed to in writing, software - \n\tdistributed under the License is distributed on an &quot;AS IS&quot; BASIS, - WITHOUT \n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. - See the \n\tLicense for the specific language governing permissions and - limitations \n\tunder the License. --&gt;\n&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt;\n&lt;mycat:server xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;system&gt;\n\t&lt;property name=&quot;useSqlStat&quot;&gt;0&lt;/property&gt;  &lt;!-- 1为开启实时统计、0为关闭 --&gt;\n\t&lt;property name=&quot;useGlobleTableCheck&quot;&gt;0&lt;/property&gt;  &lt;!-- 1为开启全加班一致性检测、0为关闭 --&gt;\n\n\t\t&lt;property name=&quot;sequnceHandlerType&quot;&gt;2&lt;/property&gt;\n      &lt;!--  &lt;property name=&quot;useCompression&quot;&gt;1&lt;/property&gt;--&gt; &lt;!--1为开启mysql压缩协议--&gt;\n        &lt;!--  &lt;property name=&quot;fakeMySQLVersion&quot;&gt;5.6.20&lt;/property&gt;--&gt; &lt;!--设置模拟的MySQL版本号--&gt;\n\t&lt;!-- &lt;property name=&quot;processorBufferChunk&quot;&gt;40960&lt;/property&gt; --&gt;\n\t&lt;!-- \n\t&lt;property name=&quot;processors&quot;&gt;1&lt;/property&gt; \n\t&lt;property name=&quot;processorExecutor&quot;&gt;32&lt;/property&gt; \n\t --&gt;\n\t\t&lt;!--默认为type 0: DirectByteBufferPool | type 1 ByteBufferArena--&gt;\n\t\t&lt;property name=&quot;processorBufferPoolType&quot;&gt;0&lt;/property&gt;\n\t\t&lt;!--默认是65535 64K 用于sql解析时最大文本长度 --&gt;\n\t\t&lt;!--&lt;property name=&quot;maxStringLiteralLength&quot;&gt;65535&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;sequnceHandlerType&quot;&gt;0&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;backSocketNoDelay&quot;&gt;1&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;frontSocketNoDelay&quot;&gt;1&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;processorExecutor&quot;&gt;16&lt;/property&gt;--&gt;\n\t\t&lt;!--\n\t\t\t&lt;property name=&quot;serverPort&quot;&gt;8066&lt;/property&gt; &lt;property name=&quot;managerPort&quot;&gt;9066&lt;/property&gt; \n\t\t\t&lt;property name=&quot;idleTimeout&quot;&gt;300000&lt;/property&gt; &lt;property name=&quot;bindIp&quot;&gt;0.0.0.0&lt;/property&gt; \n\t\t\t&lt;property name=&quot;frontWriteQueueSize&quot;&gt;4096&lt;/property&gt; &lt;property name=&quot;processors&quot;&gt;32&lt;/property&gt; --&gt;\n\t\t&lt;!--分布式事务开关，0为不过滤分布式事务，1为过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤），2为不过滤分布式事务,但是记录分布式事务日志--&gt;\n\t\t&lt;property name=&quot;handleDistributedTransactions&quot;&gt;0&lt;/property&gt;\n\t\t\n\t\t\t&lt;!--\n\t\t\toff heap for merge/order/group/limit      1开启   0关闭\n\t\t--&gt;\n\t\t&lt;property name=&quot;useOffHeapForMerge&quot;&gt;1&lt;/property&gt;\n\n\t\t&lt;!--\n\t\t\t单位为m\n\t\t--&gt;\n\t\t&lt;property name=&quot;memoryPageSize&quot;&gt;1m&lt;/property&gt;\n\n\t\t&lt;!--\n\t\t\t单位为k\n\t\t--&gt;\n\t\t&lt;property name=&quot;spillsFileBufferSize&quot;&gt;1k&lt;/property&gt;\n\n\t\t&lt;property name=&quot;useStreamOutput&quot;&gt;0&lt;/property&gt;\n\n\t\t&lt;!--\n\t\t\t单位为m\n\t\t--&gt;\n\t\t&lt;property name=&quot;systemReserveMemorySize&quot;&gt;384m&lt;/property&gt;\n\n\n\t\t&lt;!--是否采用zookeeper协调切换  --&gt;\n\t\t&lt;property name=&quot;useZKSwitch&quot;&gt;true&lt;/property&gt;\n\n\n\t&lt;/system&gt;\n\t\n\t&lt;!-- 全局SQL防火墙设置 --&gt;\n\t&lt;!-- \n\t&lt;firewall&gt; \n\t   &lt;whitehost&gt;\n\t      &lt;host host=&quot;127.0.0.1&quot; user=&quot;mycat&quot;/&gt;\n\t      &lt;host host=&quot;127.0.0.2&quot; user=&quot;mycat&quot;/&gt;\n\t   &lt;/whitehost&gt;\n       &lt;blacklist check=&quot;false&quot;&gt;\n       &lt;/blacklist&gt;\n\t&lt;/firewall&gt;\n\t--&gt;\n\t\n\t&lt;user name=&quot;root&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;DB01&lt;/property&gt;\n\t\t\n\t\t&lt;!-- 表级 DML 权限设置 --&gt;\n\t\t&lt;!-- \t\t\n\t\t&lt;privileges check=&quot;false&quot;&gt;\n\t\t\t&lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt;\n\t\t\t\t&lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt;\n\t\t\t\t&lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt;\n\t\t\t&lt;/schema&gt;\n\t\t&lt;/privileges&gt;\t\t\n\t\t --&gt;\n\t&lt;/user&gt;\n\n\t&lt;user name=&quot;user&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;DB01&lt;/property&gt;\n\t\t&lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;\n\t&lt;/user&gt;\n\n&lt;/mycat:server&gt;\n</code></pre>\n<h6 id=\"323-mycat启动\"><a class=\"anchor\" href=\"#323-mycat启动\">#</a> 3.2.3 Mycat 启动</h6>\n<pre><code>#1.启动mycat\n[root@mycat mycat]# ./bin/mycat restart\n\n#2.wrapper.log日志中常见错误\nERROR | wrapper | 2021/1/10 13:31:05 | Startup failed: Timed out waiting for signal from JVM.\nERROR | wrapper | 2021/1/10 13:31:05 | JVM did not exit on request, terminated\n\n#3.启动Mycat超时,前往wrapper.conf配置超时策略\n[root@mycat mycat]# vim /usr/local/mycat/conf/wrapper.conf\n...\nwrapper.startup.timeout=300     //添加此行，超时时间300秒\nwrapper.ping.timeout=120\n\n#4.查看mycat是否启动\n[root@mycat mycat]# tail -f logs/wrapper.log\n...\nINFO   | jvm 1    | 2023/12/02 22:53:44 | MyCAT Server startup successfully. see logs in logs/mycat.log\n[root@mycat mycat]# netstat -lntp|grep 8066\ntcp6       0      0 :::8066                 :::*                    LISTEN      18028/java\n</code></pre>\n<h6 id=\"324-分片测试\"><a class=\"anchor\" href=\"#324-分片测试\">#</a> 3.2.4 分片测试</h6>\n<pre><code>[root@db3 ~]#  mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql: [Warning] Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 3\nServer version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)\n\nCopyright (c) 2000, 2023, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| DB01     |\n+----------+\n1 row in set (0.00 sec)\n\nmysql&gt; use DB01;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql&gt; show tables;\n+----------------+\n| Tables in DB01 |\n+----------------+\n| tb_order       |\n+----------------+\n1 row in set (0.00 sec)\nmysql&gt; CREATE TABLE TB_ORDER(\n    -&gt; id BIGINT(20) NOT NULL,\n    -&gt; title VARCHAR(100) NOT NULL,\n    -&gt; PRIMARY KEY (id)\n    -&gt; )ENGINE=INNODB DEFAULT CHARSET=utf8;\nQuery OK, 0 rows affected (0.04 sec)\n OK!\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(1,'guods1');\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(2,'guods2');\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(3,'guods3');\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(4,'guods4');\nmysql&gt; select * from TB_ORDER;\n+------+--------+\n| id   | title  |\n+------+--------+\n|    1 | guods1 |\n|    2 | guods2 |\n|    3 | guods3 |\n|    4 | guods4 |\n+------+--------+\n4 rows in set (0.03 sec)\n</code></pre>\n<p><strong>数据写入到 db1 中，因为 mycat 分片规则为 0-50000000 存入节点 1,5000001-10000000 存入节点 2,10000001-15000000 存入节点 3，15000001 以上无法插入数据，需要增加数据节点。</strong></p>\n<pre><code>[root@mycat mycat]# vim conf/rule.xml\n...\n        &lt;tableRule name=&quot;auto-sharding-long&quot;&gt;\n                &lt;rule&gt;\n                        &lt;columns&gt;id&lt;/columns&gt;\n                        &lt;algorithm&gt;rang-long&lt;/algorithm&gt;\n                &lt;/rule&gt;\n        &lt;/tableRule&gt;\n\n....\n       &lt;function name=&quot;rang-long&quot;\n                class=&quot;io.mycat.route.function.AutoPartitionByLong&quot;&gt;\n                &lt;property name=&quot;mapFile&quot;&gt;autopartition-long.txt&lt;/property&gt;\n        &lt;/function&gt;\n\n...\n\n[root@mycat mycat]# cat conf/autopartition-long.txt\n# range start-end ,data node index\n# K=1000,M=10000.\n0-500M=0\n500M-1000M=1\n\n#5000001-10000000存入节点2 \nmysql&gt; INSERT INTO TB_ORDER(id,title) VALUES(5000001,'guods5000001');\nQuery OK, 1 row affected (0.01 sec)\n OK!\n \n#10000001-15000000存入节点3 \nmysql&gt; INSERT INTO TB_ORDER(id,title) VALUES(10000001,'guods10000001');\nQuery OK, 1 row affected (0.00 sec)\n OK!\n\n#15000001以上无法插入数据，需要增加数据节点\nmysql&gt; INSERT INTO TB_ORDER(id,title) VALUES(15000001,'guods15000001');\nERROR 1064 (HY000): can't find any valid datanode :TB_ORDER -&gt; ID -&gt; 15000001\n</code></pre>\n<h5 id=\"33-mycat配置\"><a class=\"anchor\" href=\"#33-mycat配置\">#</a> 3.3 Mycat 配置</h5>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/5419360b032f4ac689bae6cc3f116b31.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"331-schema标签\"><a class=\"anchor\" href=\"#331-schema标签\">#</a> 3.3.1 Schema 标签</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/46a044f4e39b492f901f75149fc93a83.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p>schema 标签用于定义 MyCat 实例中的逻辑库，一个 MyCat 实例中，可以有多个逻辑库，可以通过 schema 标签来划分不同的逻辑库。MyCat 中的逻辑库的概念，等同于 MySQL 中的 database 概念，需要操作某个逻辑库下的表时也需要切换逻辑库 (use xxx)。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/70c92df034f1430db113f402a64e60a1.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/faf24e9c66a140f7a44b3eb02181cddc.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"332-datanode标签\"><a class=\"anchor\" href=\"#332-datanode标签\">#</a> 3.3.2 Datanode 标签</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/b06d787c5cdb4a369c684ee97e11f937.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"333-datahost标签\"><a class=\"anchor\" href=\"#333-datahost标签\">#</a> 3.3.3 Datahost 标签</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/39158b4dc09d40bb867926e331a877bb.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"334-rulexml\"><a class=\"anchor\" href=\"#334-rulexml\">#</a> 3.3.4 rule.xml</h6>\n<p>rule.xml 中定义所有拆分表的规则，在使用过程中可以灵活的使用分片算法，或者对同一个分片算法使用不同的参数，它让分片过程可配置化。主要包含两类标签： <code>tableRule</code> 、 <code>Function</code> 。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/eb947d54cc5b4a849f2cafde330a882e.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"335-serverxml\"><a class=\"anchor\" href=\"#335-serverxml\">#</a> 3.3.5 server.xml</h6>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/d975eb41a0224765b0ebfa749ec41e7c.png#pic_center\" alt=\"在这里插入图片描述\" /></p>\n<h5 id=\"34-mycat分片\"><a class=\"anchor\" href=\"#34-mycat分片\">#</a> 3.4 Mycat 分片</h5>\n<h6 id=\"341-分库分表-mycat分片-垂直分库\"><a class=\"anchor\" href=\"#341-分库分表-mycat分片-垂直分库\">#</a> 3.4.1 分库分表 - MyCat 分片 - 垂直分库</h6>\n<p>场景：在业务系统中，涉及以下表结构，但是由于用户与订单每天都会产生大量的数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分，原有的数据库表如下。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/e59c1067e38f401b915698c7b480ee2b.png\" alt=\"在这里插入图片描述\" /></p>\n<p><strong>ps: 分库不需要指定 rule，涉及分表需要使用 rule；</strong></p>\n<p><strong>环境准备</strong></p>\n<p>①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。<br />\n②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。<br />\n③三台服务器关闭防火墙或者开放对应的端口。<br />\n④分别在三台 MySQL 中创建数据库 shopping。<br />\n<img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/956d2339ccf1416ca76f9092339887f8.png\" alt=\"在这里插入图片描述\" /></p>\n<p><strong>schema.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t&lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>server.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/server.xml \n...\n\t&lt;user name=&quot;root&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING&lt;/property&gt;\n\t\t\n\t\t&lt;!-- 表级 DML 权限设置 --&gt;\n\t\t&lt;!-- \t\t\n\t\t&lt;privileges check=&quot;false&quot;&gt;\n\t\t\t&lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt;\n\t\t\t\t&lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt;\n\t\t\t\t&lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt;\n\t\t\t&lt;/schema&gt;\n\t\t&lt;/privileges&gt;\t\t\n\t\t --&gt;\n\t&lt;/user&gt;\n\n\t&lt;user name=&quot;user&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING&lt;/property&gt;\n\t\t&lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;\n\t&lt;/user&gt;\n\n&lt;/mycat:server&gt;\n</code></pre>\n<p><strong>分库分表 - MyCat 分片 - 垂直分库 - 测试</strong></p>\n<p><strong>垂直分库 - 测试</strong></p>\n<pre><code>#1.重启mycat\n[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log \n...\nINFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log\ncreate database shopping default charset utf8mb4;\n\n#2.在3台节点创建shopping数据库\nmysql&gt; create database shopping default charset utf8mb4;\nmysql&gt; create database shopping default charset utf8mb4;\nmysql&gt; create database shopping default charset utf8mb4;\n\n#3.登入mycat\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| SHOPPING |\n+----------+\n1 row in set (0.01 sec)\n\n#4.查看逻辑库\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| SHOPPING |\n+----------+\n1 row in set (0.01 sec)\n\n#5.切换到SHOPPING数据库\nmysql&gt; use SHOPPING;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\n\n#6.查看逻辑表\nmysql&gt; show tables;\n+--------------------+\n| Tables in SHOPPING |\n+--------------------+\n| tb_areas_city      |\n| tb_areas_provinces |\n| tb_areas_region    |\n| tb_goods_base      |\n| tb_goods_brand     |\n| tb_goods_cat       |\n| tb_goods_desc      |\n| tb_goods_item      |\n| tb_order_item      |\n| tb_order_master    |\n| tb_order_pay_log   |\n| tb_user            |\n| tb_user_address    |\n+--------------------+\n13 rows in set (0.00 sec)\n\n#7.上传shopping-table.sql表结构文件与shopping-insert.sql数据文件\n\n#8.执行shopping-table.sql文件\nmysql&gt; source /root/shopping-table.sql\n\n#9.执行shopping-insert.sql文件\nmysql&gt; source /root/shopping-insert.sql\n\n#10.查看三个数据库可以发现（根据schema.xml配置文件的配置进行了实现）\n①192.168.40.210的数据库中存放了 tb_goods_base、tb_goods_brand、tb_goods_cat、tb_goods_desc、tb_goods_item这五张表\n②192.168.40.211的数据库中存放了 tb_order_item、tb_order_master、tb_order_pay_log这三张表；\n③192.168.40.212的数据库中存放了 tb_user、tb_user_address、tb_areas_provinces、tb_areas_city、tb_areas_region这五张表\n</code></pre>\n<p><strong>exam1: 查询用户的收件人及收件人地址信息 (包含省、市、区)。</strong></p>\n<pre><code>mysql&gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;\n+-----------+-----------+-----------+-----------+-----------+--------------------+\n| user_id   | contact   | province  | city      | area      | address            |\n+-----------+-----------+-----------+-----------+-----------+--------------------+\n| deng      | 叶问      | 北京市    | 市辖区    | 西城区    | 咏春武馆总部       |\n| java00001 | 李佳红    | 北京市    | 市辖区    | 崇文区    | 修正大厦           |\n| deng      | 李小龙    | 北京市    | 市辖区    | 崇文区    | 永春武馆           |\n| zhaoliu   | 赵三      | 北京市    | 市辖区    | 宣武区    | 西直门             |\n| java00001 | 李嘉诚    | 北京市    | 市辖区    | 朝阳区    | 金燕龙办公楼       |\n| java00001 | 李佳星    | 北京市    | 市辖区    | 朝阳区    | 中腾大厦           |\n+-----------+-----------+-----------+-----------+-----------+--------------------+\n</code></pre>\n<p><em><strong>ps: 此查询语句只涉及了一个分片所以查询成功</strong></em></p>\n<p><strong>exam2: 查询每一笔订单及订单的收件地址信息 (包含省、市、区)。</strong></p>\n<pre><code>mysql&gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;\nERROR 1064 (HY000): invalid route in sql, multi tables found but datanode has no intersection  sql:SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid\n</code></pre>\n<p><em><strong>ps: 此查询语句涉及多个分片所以查询报错，为了解决这个问题需要进行全局表配置</strong></em></p>\n<p><strong>全局表配置</strong></p>\n<p>对于省、市、区 / 县表 tb_areas_provinces，tb_areas_city，tb_areas_region，是属于数据字典表，在多个业务模块中都可能会遇到，可以将其设置为全局表，利于业务操作。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/53feb8089b8f407d85736dd0b1726f13.png\" alt=\"在这里插入图片描述\" /></p>\n<p><strong>1. 修改 MyCat—schema.xml 文件配置</strong></p>\n<p><strong>schema.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\n\t\t&lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot;/&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot;/&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t&lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>2. 全局表测试</strong></p>\n<pre><code>#1.删除3个节点上原有表\n\n#2.重启mycat\n[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log \n...\nINFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log\ncreate database shopping default charset utf8mb4;\n\n#3.执行shopping-table.sql文件\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql&gt; source /root/shopping-table.sql\n\n#4.执行shopping-insert.sql文件\nmysql&gt; source /root/shopping-insert.sql\n\n#5 exam1:查询用户的收件人及收件人地址信息(包含省、市、区)。\nmysql&gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;\n\n#6 exam2:查询每一笔订单及订单的收件地址信息(包含省、市、区)\nmysql&gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;\n</code></pre>\n<h6 id=\"342-分库分表-mycat分片-水平分表\"><a class=\"anchor\" href=\"#342-分库分表-mycat分片-水平分表\">#</a> 3.4.2 分库分表 - MyCat 分片 - 水平分表</h6>\n<ul>\n<li><strong>水平分表</strong></li>\n</ul>\n<p><strong>场景</strong>：在业务系统中，有一张表（日志表），业务系统每天都会产生大量的日志数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/50c6b8bfbf2d4a8f86c457c9d7db0cde.png\" alt=\"在这里插入图片描述\" /></p>\n<p><strong>准备环境：</strong></p>\n<p>①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。<br />\n②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。<br />\n③三台服务器关闭防火墙或者开放对应的端口。<br />\n④分别在三台 MySQL 中创建数据库 itcast。<br />\n<img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/3791fd8009f949f9b5a752400370a2ec.png\" alt=\"在这里插入图片描述\" /></p>\n<p><strong>1. 三台 MySQL 中创建数据库 itcast</strong></p>\n<pre><code>mysql&gt; create database itcast default charset utf8mb4;\nmysql&gt; create database itcast default charset utf8mb4;\nmysql&gt; create database itcast default charset utf8mb4;\n</code></pre>\n<p><strong>2.MyCat—server.xml 文件配置</strong></p>\n<p><strong>server.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\n                &lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t&lt;/schema&gt;\n\n        &lt;schema name=&quot;ITCAST&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n        \t&lt;table name=&quot;tb_log&quot; dataNode=&quot;dn4,dn5,dn6&quot; primaryKey=&quot;id&quot; rule=&quot;mod-long&quot; /&gt;\n        &lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\n\t&lt;dataNode name=&quot;dn4&quot; dataHost=&quot;dhost1&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn5&quot; dataHost=&quot;dhost2&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn6&quot; dataHost=&quot;dhost3&quot; database=&quot;itcast&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>3.MyCat—server.xml 文件配置</strong></p>\n<p><strong>server.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/server.xml \n...\n\t&lt;user name=&quot;root&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING,ITCAST&lt;/property&gt;\n\t\t\n\t\t&lt;!-- 表级 DML 权限设置 --&gt;\n\t\t&lt;!-- \t\t\n\t\t&lt;privileges check=&quot;false&quot;&gt;\n\t\t\t&lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt;\n\t\t\t\t&lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt;\n\t\t\t\t&lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt;\n\t\t\t&lt;/schema&gt;\n\t\t&lt;/privileges&gt;\t\t\n\t\t --&gt;\n\t&lt;/user&gt;\n\n\t&lt;user name=&quot;user&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING,ITCAST&lt;/property&gt;\n\t\t&lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;\n\t&lt;/user&gt;\n\n&lt;/mycat:server&gt;\n</code></pre>\n<p><strong>4.MyCat 启动</strong></p>\n<pre><code>#1.重启mycat\n[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log \n...\nINFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log\ncreate database shopping default charset utf8mb4;\n\n#2.登入mycat\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| ITCAST   |\n| SHOPPING |\n+----------+\n2 rows in set (0.00 sec)\n\nmysql&gt; use ITCAST;\nmysql&gt; show tables;\n+------------------+\n| Tables in ITCAST |\n+------------------+\n| tb_log           |\n+------------------+\n\n#3.创建表结构及数据导入\nmysql&gt; CREATE TABLE tb_log (\n    -&gt;   id bigint(20) NOT NULL COMMENT 'ID',\n    -&gt;   model_name varchar(200) DEFAULT NULL COMMENT '模块名',\n    -&gt;   model_value varchar(200) DEFAULT NULL COMMENT '模块值',\n    -&gt;   return_value varchar(200) DEFAULT NULL COMMENT '返回值',\n    -&gt;   return_class varchar(200) DEFAULT NULL COMMENT '返回值类型',\n    -&gt;   operate_user varchar(20) DEFAULT NULL COMMENT '操作用户',\n    -&gt;   operate_time varchar(20) DEFAULT NULL COMMENT '操作时间',\n    -&gt;   param_and_value varchar(500) DEFAULT NULL COMMENT '请求参数名及参数值',\n    -&gt;   operate_class varchar(200) DEFAULT NULL COMMENT '操作类',\n    -&gt;   operate_method varchar(200) DEFAULT NULL COMMENT '操作方法',\n    -&gt;   cost_time bigint(20) DEFAULT NULL COMMENT '执行方法耗时, 单位 ms',\n    -&gt;   source int(1) DEFAULT NULL COMMENT '来源 : 1 PC , 2 Android , 3 IOS',\n    -&gt;   PRIMARY KEY (id)\n    -&gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\nQuery OK, 0 rows affected (0.09 sec)\n OK!\n查看三个数据库可以发现表和表结构都有了\n\n#4.添加数据\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('1','user','insert','success','java.lang.String','10001','2022-01-06 18:12:28','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','insert','10',1);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('2','user','insert','success','java.lang.String','10001','2022-01-06 18:12:27','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','insert','23',1);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('3','user','update','success','java.lang.String','10001','2022-01-06 18:16:45','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','update','34',1);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('4','user','update','success','java.lang.String','10001','2022-01-06 18:16:45','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','update','13',2);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('5','user','insert','success','java.lang.String','10001','2022-01-06 18:30:31','&#123;\\&quot;age\\&quot;:\\&quot;200\\&quot;,\\&quot;name\\&quot;:\\&quot;TomCat\\&quot;,\\&quot;gender\\&quot;:\\&quot;0\\&quot;&#125;','cn.itcast.controller.UserController','insert','29',3);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('6','user','find','success','java.lang.String','10001','2022-01-06 18:30:31','&#123;\\&quot;age\\&quot;:\\&quot;200\\&quot;,\\&quot;name\\&quot;:\\&quot;TomCat\\&quot;,\\&quot;gender\\&quot;:\\&quot;0\\&quot;&#125;','cn.itcast.controller.UserController','find','29',2);\n\n查看三个数据库内的tb_log表发现有数据了，数据的分布规则是 id模以3的结果为0的数据分布在第一个节点，id模以3的结果为1的数据分布在第二个节点，id模以3的结果为2的数据分布在第三个节点\n</code></pre>\n<h5 id=\"33-分库分表-分片规则\"><a class=\"anchor\" href=\"#33-分库分表-分片规则\">#</a> 3.3 分库分表 - 分片规则</h5>\n<h6 id=\"331-分库分表-分片规则-范围分片\"><a class=\"anchor\" href=\"#331-分库分表-分片规则-范围分片\">#</a> 3.3.1 分库分表 - 分片规则 - 范围分片</h6>\n<p><strong>范围分片</strong>：根据指定的字段及其配置的范围与数据节点的对应情况，来决定该数据属于哪一个分片。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/03a9b7c4bb2e46318de134cf7d88499d.png\" alt=\"在这里插入图片描述\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/80ec2db3bffc4e29835565145c89157d.png\" alt=\"在这里插入图片描述\" /></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/autopartition-long.txt\n# range start-end ,data node index\n# K=1000,M=10000.\n0-500M=0\n500M-1000M=1\n1000M-1500M=2\n</code></pre>\n<h6 id=\"332-分库分表-分片规则-取模分片\"><a class=\"anchor\" href=\"#332-分库分表-分片规则-取模分片\">#</a> 3.3.2 分库分表 - 分片规则 - 取模分片</h6>\n<p><strong>取模分片</strong>：根据指定的字段值与节点数量进行求模运算，根据运算结果，来决定该数据属于哪一个分片。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/c863d1cdde204446b992374ed2802d49.png\" alt=\"在这里插入图片描述\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/e714e7a5524d4c6f92fca19565f711ba.png\" alt=\"在这里插入图片描述\" /></p>\n<h6 id=\"333-分库分表-分片规则-一致性hash算法\"><a class=\"anchor\" href=\"#333-分库分表-分片规则-一致性hash算法\">#</a> 3.3.3 分库分表 - 分片规则 - 一致性 hash 算法</h6>\n<p><strong>一致性 hash 算法</strong>：所谓一致性哈希，相同的哈希因子计算值总是被划分到相同的分区表中，不会因为分区节点的增加而改变原来数据的分区位置。</p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/02cd32046b4d447db16be17d418105a9.png\" alt=\"在这里插入图片描述\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://img-blog.csdnimg.cn/eb546ce3d10946bfb4819545cc2f0aba.png\" alt=\"在这里插入图片描述\" /></p>\n<p><strong>一致性 hash 测试</strong></p>\n<p>schema.xml 配置</p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\n                &lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t&lt;/schema&gt;\n\n        &lt;schema name=&quot;ITCAST&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n        \t&lt;table name=&quot;tb_log&quot; dataNode=&quot;dn4,dn5,dn6&quot; primaryKey=&quot;id&quot; rule=&quot;mod-long&quot; /&gt;\n        \t&lt;table name=&quot;tb_order&quot; dataNode=&quot;dn4,dn5,dn6&quot; primaryKey=&quot;id&quot; rule=&quot;sharding-by-murmur&quot; /&gt;\n        &lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\n\t&lt;dataNode name=&quot;dn4&quot; dataHost=&quot;dhost1&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn5&quot; dataHost=&quot;dhost2&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn6&quot; dataHost=&quot;dhost3&quot; database=&quot;itcast&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>rule.xml 配置</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/rule.xml \n...\n\t&lt;function name=&quot;murmur&quot;\n\t\tclass=&quot;io.mycat.route.function.PartitionByMurmurHash&quot;&gt;\n\t\t&lt;property name=&quot;seed&quot;&gt;0&lt;/property&gt;&lt;!-- 默认是0 --&gt;\n\t\t&lt;property name=&quot;count&quot;&gt;3&lt;/property&gt;&lt;!-- 要分片的数据库节点数量，必须指定，否则没法分片 --&gt;\n\t\t&lt;property name=&quot;virtualBucketTimes&quot;&gt;160&lt;/property&gt;&lt;!-- 一个实际的数据库节点被映射为这么多虚拟节点，默认是160倍，也就是虚拟节点数是物理节点数的160倍 --&gt;\n\t\t&lt;!-- &lt;property name=&quot;weightMapFile&quot;&gt;weightMapFile&lt;/property&gt; 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写，以从0开始到count-1的整数值也就是节点索引为key，以节点权重值为值。所有权重值必须是正整数，否则以1代替 --&gt;\n\t\t&lt;!-- &lt;property name=&quot;bucketMapPath&quot;&gt;/etc/mycat/bucketMapPath&lt;/property&gt; \n\t\t\t用于测试时观察各物理节点与虚拟节点的分布情况，如果指定了这个属性，会把虚拟节点的murmur hash值与物理节点的映射按行输出到这个文件，没有默认值，如果不指定，就不会输出任何东西 --&gt;\n\t&lt;/function&gt;\n...\n</code></pre>\n<p><strong>重启 mycat 并插入数据测试</strong></p>\n<pre><code>[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log\n...\nINFO   | jvm 1    | 2023/12/03 22:17:47 | MyCAT Server startup successfully. see logs in logs/mycat.log\n\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nServer version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)\n\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| ITCAST   |\n| SHOPPING |\n+----------+\n2 rows in set (0.00 sec)\n\nmysql&gt; use ITCAST;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql&gt; show tables;\n+------------------+\n| Tables in ITCAST |\n+------------------+\n| tb_log           |\n| tb_order         |\n+------------------+\n2 rows in set (0.00 sec)\n\n#创建表结构\ncreate table tb_order(\n    id  varchar(100) not null primary key,\n    money   int null,\n    content varchar(200) null\n);\n\n#插入数据\nINSERT INTO tb_order (id, money, content) VALUES ('b92fdaaf-6fc4-11ec-b831-482ae33c4a2d', 10, 'b92fdaf8-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b93482b6-6fc4-11ec-b831-482ae33c4a2d', 20, 'b93482d5-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b937e246-6fc4-11ec-b831-482ae33c4a2d', 50, 'b937e25d-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b93be2dd-6fc4-11ec-b831-482ae33c4a2d', 100, 'b93be2f9-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b93f2d68-6fc4-11ec-b831-482ae33c4a2d', 130, 'b93f2d7d-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9451b98-6fc4-11ec-b831-482ae33c4a2d', 30, 'b9451bcc-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9488ec1-6fc4-11ec-b831-482ae33c4a2d', 560, 'b9488edb-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b94be6e6-6fc4-11ec-b831-482ae33c4a2d', 10, 'b94be6ff-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b94ee10d-6fc4-11ec-b831-482ae33c4a2d', 123, 'b94ee12c-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b952492a-6fc4-11ec-b831-482ae33c4a2d', 145, 'b9524945-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b95553ac-6fc4-11ec-b831-482ae33c4a2d', 543, 'b95553c8-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9581cdd-6fc4-11ec-b831-482ae33c4a2d', 17, 'b9581cfa-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b95afc0f-6fc4-11ec-b831-482ae33c4a2d', 18, 'b95afc2a-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b95daa99-6fc4-11ec-b831-482ae33c4a2d', 134, 'b95daab2-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9667e3c-6fc4-11ec-b831-482ae33c4a2d', 156, 'b9667e60-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b96ab489-6fc4-11ec-b831-482ae33c4a2d', 175, 'b96ab4a5-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b96e2942-6fc4-11ec-b831-482ae33c4a2d', 180, 'b96e295b-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b97092ec-6fc4-11ec-b831-482ae33c4a2d', 123, 'b9709306-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b973727a-6fc4-11ec-b831-482ae33c4a2d', 230, 'b9737293-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b978840f-6fc4-11ec-b831-482ae33c4a2d', 560, 'b978843c-6fc4-11ec-b831-482ae33c4a2d');\n</code></pre>\n<p>PS：数据按一致性 hash 分布在不同节点</p>\n",
            "tags": [
                "MySQL"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/2771271649.html",
            "url": "http://xuyong.cn/posts/2771271649.html",
            "title": "云原生K8s安全专家CKS认证考题详解",
            "date_published": "2025-04-09T13:38:39.000Z",
            "content_html": "<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"抱歉, 这个密码看着不太对, 请再试试。\" data-whm=\"抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-xray\">\n      <input class=\"hbe hbe-input-field hbe-input-field-xray\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-xray\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-xray\">您好, 这里需要输入密码。</span>\n      </label>\n      <svg class=\"hbe hbe-graphic hbe-graphic-xray\" width=\"300%\" height=\"100%\" viewBox=\"0 0 1200 60\" preserveAspectRatio=\"none\">\n        <path d=\"M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0\"></path>\n        <path d=\"M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0\"></path>\n      </svg>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/1414180692.html",
            "url": "http://xuyong.cn/posts/1414180692.html",
            "title": "Redis集群（主从+哨兵）模式",
            "date_published": "2025-04-09T11:50:06.000Z",
            "content_html": "<h3 id=\"redis集群主从哨兵模式\"><a class=\"anchor\" href=\"#redis集群主从哨兵模式\">#</a> Redis 集群（主从 + 哨兵）模式</h3>\n<h3 id=\"一-什么是redis主从复制\"><a class=\"anchor\" href=\"#一-什么是redis主从复制\">#</a> 一、什么是 redis 主从复制？</h3>\n<p>主从复制，是指将一台 Redis 服务器的数据，复制到其他的 Redis 服务器。前者称为主节点 (master)，后者称为从节点 (slave), 数据的复制是单向的，只能由主节点到从节点。master 以写为主，slave 以读为主。</p>\n<p><a href=\"https://imgse.com/i/pEgTlKx\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgTlKx.png\" alt=\"pEgTlKx.png\" /></a></p>\n<h3 id=\"二-主从复制的作用\"><a class=\"anchor\" href=\"#二-主从复制的作用\">#</a> 二、主从复制的作用</h3>\n<p>数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。<br />\n故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。<br />\n负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。<br />\n读写分离：用于实现读写分离，主库写、从库读，读写分离不仅可以提高服务器的负载能力，同时可根据需求的变化，改变从库的数量；<br />\n高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是 Redis 高可用的基础。</p>\n<h3 id=\"三-实现主从复制\"><a class=\"anchor\" href=\"#三-实现主从复制\">#</a> 三、实现主从复制</h3>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP</th>\n<th>角色</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>redis01</td>\n<td>192.168.40.101</td>\n<td>master</td>\n</tr>\n<tr>\n<td>redis02</td>\n<td>192.168.40.102</td>\n<td>slave</td>\n</tr>\n<tr>\n<td>redis03</td>\n<td>192.168.40.103</td>\n<td>slave</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"31-关闭防火墙-selinux\"><a class=\"anchor\" href=\"#31-关闭防火墙-selinux\">#</a> 3.1 关闭防火墙、selinux</h4>\n<pre><code>[root@master01 ~]# hostnamectl set-hostname redis01\n[root@redis01 ~]# systemctl stop firewalld\n[root@redis01 ~]# systemctl disable firewalld\n[root@redis01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@redis01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@redis01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@redis01 ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@redis01 ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@redis01 ~]# ntpdate time2.aliyun.com\n[root@redis01 ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/null\n[root@redis01 ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h4 id=\"32-安装redis\"><a class=\"anchor\" href=\"#32-安装redis\">#</a> 3.2 安装 redis</h4>\n<pre><code>[root@redis01 ~]# yum install gcc-c++ -y\n[root@redis01 soft]# wget https://download.redis.io/releases/redis-6.2.11.tar.gz\n[root@redis01 soft]# tar xf redis-6.2.11.tar.gz \n[root@redis01 soft]# ln -s /soft/redis-6.2.11 /soft/redis\n[root@redis01 soft]# cd /soft/redis\n[root@redis01 redis]# make            #执行make编译\n[root@redis01 redis]# make install    #将 src下的许多可执行文件复制到/usr/local/bin 目录下\n[root@redis01 redis]# redis-server /soft/redis/redis.conf &amp;\n[root@redis01 redis]# netstat -lntp|grep redis\ntcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      69686/redis-server  \ntcp6       0      0 ::1:6379                :::*                    LISTEN      69686/redis-server     \n[root@redis01 redis]# redis-cli shutdown      #关闭Redis服务\n</code></pre>\n<h4 id=\"33-redis配置文件说明\"><a class=\"anchor\" href=\"#33-redis配置文件说明\">#</a> 3.3 redis 配置文件说明</h4>\n<pre><code>[root@db01 redis]# vim redis.conf \nbind 127.0.0.1      \t\t# 绑定的ip\nprotected-mode yes  \t\t# 保护模式\nport 6379           \t\t# 端口设置\ndaemonize yes               # 后台启动\nbind 127.0.0.1      \t\t# 绑定的ip\nprotected-mode yes  \t\t# 保护模式\nport 6379           \t\t# 端口设置\nloglevel notice     \t\t# 记录日志级别\nlogfile &quot;redis.log&quot;         # 日志的文件位置名\ndir ./               \t\t# 日志存储目录\ndatabases 16        \t\t# 数据库的数量，默认是 16 个数据库\nalways-show-logo yes \t\t# 是否总是显示LOGO\n\n# 如果900s内，如果至少有一个1 key进行了修改，我们及进行持久化操作\nsave 900 1\n# 如果300s内，如果至少10 key进行了修改，我们及进行持久化操作\nsave 300 10\n# 如果60s内，如果至少10000 key进行了修改，我们及进行持久化操作\nsave 60 10000\n# 我们之后学习持久化，会自己定义这个测试！\nstop-writes-on-bgsave-error yes   # 持久化如果出错，是否还需要继续工作！\nrdbcompression yes                # 是否压缩 rdb 文件，需要消耗一些cpu资源！\nrdbchecksum yes                   # 保存rdb文件的时候，进行错误的检查校验！\ndbfilename dump.rdb               # rdb 文件保存的名称！\ndir ./                            # rdb 文件保存的目录！\n\nslaveof 192.168.1.154 6379        # 配置主从复制\nrequirepass foobared              # 配置redis登录密码\n\nappendonly no    # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！\nappendfilename &quot;appendonly.aof&quot;   # 持久化的文件的名字\n# appendfsync always        # 每次修改都会 sync。消耗性能\nappendfsync everysec        # 每秒执行一次 sync，可能会丢失这1s的数据！\n# appendfsync no            # 不执行 sync，这个时候操作系统自己同步数据，速度最快！\nno-appendfsync-on-rewrite   #重写时是否可以运用appendsync，默认no，可以保证数据的安全性\n</code></pre>\n<h4 id=\"34-redis环境配置\"><a class=\"anchor\" href=\"#34-redis环境配置\">#</a> 3.4 redis 环境配置</h4>\n<p>#修改 maser 配置文件</p>\n<pre><code>vim redis.conf\nbind 192.168.40.101 #绑定本机ip地址\nport 6739          #绑定端口号\ndaemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no\npidfile /var/run/redis_6379.pid\nlogfile &quot;redis.log&quot;   #redis日志文件\nrequirepass Superman*2023  #本地redis密码\nmasterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到\nprotected-mode yes    #保护模式\n</code></pre>\n<p>#修改 slave01 配置文件</p>\n<pre><code>vim redis.conf\nbind 192.168.40.102 #绑定本机ip地址\nport 6739          #绑定端口号\ndaemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no\npidfile /var/run/redis_6379.pid\nlogfile &quot;redis.log&quot;   #redis日志文件\nreplicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置\nrequirepass Superman*2023  #本地redis密码\nmasterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到\nprotected-mode yes    #保护模式\n</code></pre>\n<p>#修改 slave02 配置文件</p>\n<pre><code>vim redis.conf\nbind 192.168.40.103 #绑定本机ip地址\nport 6739          #绑定端口号\ndaemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no\npidfile /var/run/redis_6379.pid\nlogfile &quot;redis.log&quot;   #redis日志文件\nreplicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置\nrequirepass Superman*2023  #本地redis密码\nmasterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到\nprotected-mode yes    #保护模式\n</code></pre>\n<h4 id=\"35-启动3台redis服务\"><a class=\"anchor\" href=\"#35-启动3台redis服务\">#</a> 3.5 启动 3 台 redis 服务</h4>\n<pre><code>#启动redis01\n[root@redis01 redis]# redis-server /soft/redis/redis.conf\n[root@redis0[root@redis01 redis]# redis-server /soft/redis/redis.conf redis]# netstat -lntp|grep redis\ntcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server \n\n#启动redis02\n[root@redis02 redis]# redis-server /soft/redis/redis.conf\n[root@redis02 redis]# netstat -lntp|grep redis\ntcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server\n\n启动redis03\n[root@redis03 redis]# redis-server /soft/redis/redis.conf\n[root@redis03 redis]# netstat -lntp|grep redis\ntcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server \n</code></pre>\n<h4 id=\"36-查看主从状态\"><a class=\"anchor\" href=\"#36-查看主从状态\">#</a> 3.6 查看主从状态</h4>\n<pre><code>#主节点\n[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023\n192.168.40.101:6379&gt; info replication\n# Replication\nrole:master\nconnected_slaves:2\nslave0:ip=192.168.40.102,port=6379,state=online,offset=616,lag=0\nslave1:ip=192.168.40.103,port=6379,state=online,offset=616,lag=0\nmaster_failover_state:no-failover\nmaster_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:616\nsecond_repl_offset:-1\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1\nrepl_backlog_histlen:616\n\n#从节点\n[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n192.168.40.103:6379&gt; info replication\n# Replication\nrole:slave\nmaster_host:192.168.40.101\nmaster_port:6379\nmaster_link_status:up\nmaster_last_io_seconds_ago:1\nmaster_sync_in_progress:0\nslave_read_repl_offset:812\nslave_repl_offset:812\nslave_priority:100\nslave_read_only:1\nreplica_announced:1\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:812\nsecond_repl_offset:-1\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:295\nrepl_backlog_histlen:518\n</code></pre>\n<h4 id=\"37-测试主从\"><a class=\"anchor\" href=\"#37-测试主从\">#</a> 3.7 测试主从</h4>\n<pre><code>[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n192.168.40.101:6379&gt; set k1 v1\nOK\n192.168.40.101:6379&gt; set k2 v2\nOK\n\n[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n192.168.40.103:6379&gt; get k1\n&quot;v1&quot;\n192.168.40.103:6379&gt; get k2\n&quot;v2&quot;\n</code></pre>\n<p><strong>注意:</strong><br />\n1、主机可以写，从机不能写，只能读。主机中的所有数据都会保存到从机中去。<br />\n2、主机断开连接，从机依旧连接到主机的，但是没有写操作，这个时候，主机如果回来了，从机依旧可以直接获取到主机写的信息！<br />\n3、如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从主机中获取值！<br />\n4、主从复制原理<br />\n Slave 启动成功连接到 master 后会发送一个 sync 同步命令<br />\n Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，并完成一次完全同步。<br />\n全量复制：slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。<br />\n增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave，完成同步，但是只要是重新连接 master，一次完全同步（全量复制）将被自动执行！ 主机的数据一定可以在从机中看到。</p>\n<h3 id=\"四-哨兵模式搭建\"><a class=\"anchor\" href=\"#四-哨兵模式搭建\">#</a> 四、哨兵模式搭建</h3>\n<p>1、什么是 redis 哨兵？<br />\nRedisSentinel 是 Redis 的高可用性解决方案，由一个或多个 Sentinel（哨兵）实例组成。它可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，它的主要功能如下：<br />\n监控 (Monitoring)：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。<br />\n通知 (Notification)：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。<br />\n故障迁移：当主服务器不能正常工作时，Sentinel 会自动进行故障迁移，也就是主从切换。<br />\n统一的配置：管理连接者询问 sentinel 取得主从的地址。</p>\n<p>2、哨兵原理是什么？<br />\nSentinel 使用的算法核心是 Raft 算法，主要用途就是用于分布式系统，系统容错，以及 Leader 选举，每个 Sentinel 都需要定期的执行以下任务：<br />\n每个 Sentinel 会自动发现其他 Sentinel 和从服务器，它以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。<br />\n如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。<br />\n如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有 Sentinel 要以每秒一次的频率确认主服务器的确进入了主观下线状态。<br />\n如果一个主服务器被标记为主观下线， 并且有足够数量的 Sentinel（至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断，那么这个主服务器被标记为客观下线。<br />\n在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO 命令。当一个主服务器 Sentinel 标记为客观下线时，Sentinel 向下线主服务器的所有从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。<br />\n当没有足够数量的 Sentinel 同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向 Sentinel 的 PING 命令返回有效回复时， 主服务器的主管下线状态就会被移除。</p>\n<p><a href=\"https://imgse.com/i/pEgT1r6\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgT1r6.png\" alt=\"pEgT1r6.png\" /></a></p>\n<h4 id=\"41-搭建哨兵\"><a class=\"anchor\" href=\"#41-搭建哨兵\">#</a> 4.1 搭建哨兵</h4>\n<p><em>在每台服务器上部署一个哨兵，配置方式如下:</em></p>\n<pre><code>[root@redis01 redis]# vim sentinel.conf\n#端口默认为26379。\nport 26379\n#关闭保护模式，可以外部访问。\nprotected-mode no\n#设置为后台启动。\ndaemonize yes\n#日志文件。\nlogfile &quot;/soft/redis/sentinel.log&quot;\n#指定服务器IP地址和端口，并且指定当有2台哨兵认为主机挂了，则对主机进行容灾切换。注意:三台哨兵这里的ip配置均为主节点ip 和端口\nsentinel monitor mymaster 192.168.40.101 6379 2\n#当在Redis实例中开启了requirepass，这里就需要提供密码。\nsentinel auth-pass mymaster psw66\n#这里设置了主机多少秒无响应，则认为挂了。\nsentinel down-after-milliseconds mymaster 3000\n#主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的\nsnetinel parallel-syncs mymaster 1\n#故障转移的超时时间，这里设置为三分钟。\nsentinel failover-timeout mymaster 180000\n</code></pre>\n<h4 id=\"42-启动三台服务器上的哨兵\"><a class=\"anchor\" href=\"#42-启动三台服务器上的哨兵\">#</a> 4.2 启动三台服务器上的哨兵</h4>\n<pre><code>#启动redis01的sentine\n[root@redis01 redis]# redis-sentinel /soft/redis/sentinel.conf\n[root@redis01 redis]#  netstat -lntp|grep redis\ntcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      33536/redis-sentine \ntcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server \ntcp6       0      0 :::26379                :::*                    LISTEN      33536/redis-sentine\n\n#启动redis02的sentine\n[root@redis02 redis]# redis-sentinel /soft/redis/sentinel.conf\n[root@redis02 redis]#  netstat -lntp|grep redis\ntcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      18757/redis-sentine \ntcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server  \ntcp6       0      0 :::26379                :::*                    LISTEN      18757/redis-sentine\n\n#启动redis03的sentine\n[root@redis03 redis]# redis-sentinel /soft/redis/sentinel.conf                     \n[root@redis03 redis]# netstat -lntp|grep redis\ntcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      19745/redis-sentine \ntcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server  \ntcp6       0      0 :::26379                :::*                    LISTEN      19745/redis-sentine\n</code></pre>\n<h4 id=\"43-连接客户端\"><a class=\"anchor\" href=\"#43-连接客户端\">#</a> 4.3 连接客户端</h4>\n<pre><code>[root@redis01 redis]# redis-cli -p 26379\n127.0.0.1:26379&gt;  info sentinel\n# Sentinel\nsentinel_masters:1\nsentinel_tilt:0\nsentinel_running_scripts:0\nsentinel_scripts_queue_length:0\nsentinel_simulate_failure_flags:0\nmaster0:name=mymaster,status=ok,address=192.168.40.101:6379,slaves=2,sentinels=3\n</code></pre>\n<h4 id=\"44-redis容灾切换\"><a class=\"anchor\" href=\"#44-redis容灾切换\">#</a> 4.4 redis 容灾切换</h4>\n<pre><code>#连接redis客户端\n[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 \n#验证密码\n192.168.40.101:6379&gt; auth Superman*2023\nOK\n#关闭redis服务\n192.168.40.101:6379&gt; shutdown\nnot connected&gt;\n#退出客户端\nnot connected&gt; exit\n</code></pre>\n<p>关闭主节点之后，我们去查看哨兵日志:</p>\n<pre><code>[root@redis01 ~]# tail -f /soft/redis/sentinel.log \n91936:X 14 Apr 2023 23:26:23.838 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n91936:X 14 Apr 2023 23:26:23.838 # Redis version=6.2.11, bits=64, commit=00000000, modified=0, pid=91936, just started\n91936:X 14 Apr 2023 23:26:23.838 # Configuration loaded\n91936:X 14 Apr 2023 23:26:23.838 * monotonic clock: POSIX clock_gettime\n91936:X 14 Apr 2023 23:26:23.839 * Running mode=sentinel, port=26379.\n91936:X 14 Apr 2023 23:26:23.839 # Sentinel ID is 835b4c8544fb250af5fd479f834ee369cc4f388e\n91936:X 14 Apr 2023 23:26:23.839 # +monitor master mymaster 192.168.40.101 6379 quorum 2\n\n\n\n91936:X 14 Apr 2023 23:31:25.329 # +sdown master mymaster 192.168.40.101 6379   #这里应该是发现主节点宕机\n91936:X 14 Apr 2023 23:31:25.359 # +new-epoch 5\n91936:X 14 Apr 2023 23:31:25.360 # +vote-for-leader ab43979285cb47b1b459aeb0ab91b63fa9d1a989 5\n91936:X 14 Apr 2023 23:31:25.401 # +odown master mymaster 192.168.40.101 6379 #quorum 3/2 两个哨兵都觉得主节点宕机了\n91936:X 14 Apr 2023 23:31:25.401 # Next failover delay: I will not start a failover before Fri Apr 14 23:37:25 2023\n91936:X 14 Apr 2023 23:31:26.468 # +config-update-from sentinel ab43979285cb47b1b459aeb0ab91b63fa9d1a989 192.168.40.102 26379 @ mymaster 192.168.40.101 6379\n91936:X 14 Apr 2023 23:31:26.468 # +switch-master mymaster 192.168.40.101 6379 192.168.40.103 6379 #通过投票选举40.103为新的主节点\n91936:X 14 Apr 2023 23:31:26.468 * +slave slave 192.168.40.102:6379 192.168.40.102 6379 @ mymaster 192.168.40.103 6379\n91936:X 14 Apr 2023 23:31:26.469 * +slave slave 192.168.40.101:6379 192.168.40.101 6379 @ mymaster 192.168.40.103 6379\n</code></pre>\n<p>下面我们去 40.103 下查看哨兵主从切换是否成功</p>\n<pre><code>[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103\n192.168.40.103:6379&gt; auth Superman*2023\nOK\n192.168.40.103:6379&gt; info replication\n# Replication\nrole:master   # 40.103变成主节点了\nconnected_slaves:1   # 下面的从机个数为1\nslave0:ip=192.168.40.102,port=6379,state=online,offset=108708,lag=1\nmaster_failover_state:no-failover\nmaster_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3\nmaster_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935\nmaster_repl_offset:108851\nsecond_repl_offset:59887\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1\nrepl_backlog_histlen:108851\n</code></pre>\n<p>重新连接挂掉的主节点</p>\n<pre><code>[root@redis01 redis]# redis-server redis.conf \n[root@redis01 redis]#  redis-cli -p 6379 -h 192.168.40.101\n192.168.40.101:6379&gt; auth Superman*2023\nOK\n192.168.40.101:6379&gt; info replication\n# Replication\nrole:slave          #主节点连接回来之后自动变成了从节点，并且成功连上了主机\nmaster_host:192.168.40.103\nmaster_port:6379\nmaster_link_status:up\nmaster_last_io_seconds_ago:1\nmaster_sync_in_progress:0\nslave_read_repl_offset:130607\nslave_repl_offset:130607\nslave_priority:100\nslave_read_only:1\nreplica_announced:1\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:130607\nsecond_repl_offset:-1\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:126982\nrepl_backlog_histlen:3626\n</code></pre>\n<p>再去主节点确认一下</p>\n<pre><code>192.168.40.103:6379&gt; info replication\n# Replication\nrole:master\nconnected_slaves:2   #两个从节点\nslave0:ip=192.168.40.102,port=6379,state=online,offset=147879,lag=1\nslave1:ip=192.168.40.101,port=6379,state=online,offset=147879,lag=1\nmaster_failover_state:no-failover\nmaster_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3\nmaster_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935\nmaster_repl_offset:148165\nsecond_repl_offset:59887\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1\nrepl_backlog_histlen:148165\n</code></pre>\n<p>五、哨兵模式的优缺点<br />\n 1. 优点</p>\n<p>哨兵集群，基于主从复制模式，所有的主从配置优点，它全有</p>\n<p>主从可以切换，故障可以转移，系统的可用性就会更好</p>\n<p>哨兵模式就是主从模式的升级，手动到自动，更加健壮！</p>\n<p>2. 缺点</p>\n<p>Redis 不好在线扩容，集群容量一旦到达上限，在线扩容就十分麻烦</p>\n<p>哨兵模式的配置繁琐</p>\n<p>3. 哨兵模式的配置文件详解</p>\n<pre><code># Example sentinel.conf\n# 哨兵sentinel实例运行的端口 默认26379\nport 26379\n \n# 哨兵sentinel的工作目录\ndir /tmp\n \n# 哨兵sentinel监控的redis主节点的 ip port\n# master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。\n# quorum 配置多少个sentinel哨兵统一认为master主节点失联 那么这时客观上认为主节点失联了\n# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;\nsentinel monitor mymaster 127.0.0.1 6379 2\n  \n# 当在Redis实例中开启了requirepass foobared 授权密码这样所有连接Redis实例的客户端都要提供 密码\n# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码\n# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;\nsentinel auth-pass mymaster MySUPER--secret-0123passw0rd\n \n# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时哨兵主观上认为主节点下线 默认30秒\n# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;\nsentinel down-after-milliseconds mymaster 30000\n \n# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长， 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。\n# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;\nsentinel parallel-syncs mymaster 1\n \n# 故障转移的超时时间 failover-timeout 可以用在以下这些方面：\n#1. 同一个sentinel对同一个master两次failover之间的间隔时间。\n#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那 里同步数据时。\n#3.当想要取消一个正在进行的failover所需要的时间。\n#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时， slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了 # 默认三分钟\n# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt; bilibili：\nsentinel failover-timeout mymaster 180000\n \n# SCRIPTS EXECUTION\n#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知 相关人员。\n#对于脚本的运行结果有以下规则：\n#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10\n#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。\n#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。\n#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。\n#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等）， 将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信 息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配 置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无 法正常启动成功。\n#通知脚本\n# shell编程\n# sentinel notification-script &lt;master-name&gt; &lt;script-path&gt; sentinel\nnotification-script mymaster /var/redis/notify.sh\n \n# 客户端重新配置主节点参数脚本\n# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已 经发生改变的信息。\n# 以下参数将会在调用脚本时传给脚本:\n# &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt; # 目前&lt;state&gt;总是“failover”,\n# &lt;role&gt;是“leader”或者“observer”中的一个。\n# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通 信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。\n# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt; sentinel client-reconfig-\nscript mymaster /var/redis/reconfig.sh\n</code></pre>\n<p><em>再去看一下 redis 的配置文件和哨兵的配置文件，你会惊讶的发现，里边的配置文件已经被改过来了。</em></p>\n<pre><code>cat redis.con\n...\nreplicaof 192.168.40.103 6379\n</code></pre>\n",
            "tags": [
                "Redis"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/3166738000.html",
            "url": "http://xuyong.cn/posts/3166738000.html",
            "title": "Kubeadm高可用安装K8s集群",
            "date_published": "2025-04-09T10:28:34.000Z",
            "content_html": "<h2 id=\"kubeadm高可用安装k8s集群\"><a class=\"anchor\" href=\"#kubeadm高可用安装k8s集群\">#</a> Kubeadm 高可用安装 K8s 集群</h2>\n<h4 id=\"1-基本配置\"><a class=\"anchor\" href=\"#1-基本配置\">#</a> 1. 基本配置</h4>\n<h5 id=\"11-基本环境配置\"><a class=\"anchor\" href=\"#11-基本环境配置\">#</a> 1.1 基本环境配置</h5>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP 地址</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master 节点 * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived 虚拟 IP（不占用机器）</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker 节点 * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* 配置信息 *</strong></em></th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>系统版本</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod 网段</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service 网段</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>所有节点</mark>更改主机名（其它节点按需修改）：</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>所有节点</mark>配置 hosts，修改 /etc/hosts 如下：</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>所有节点</mark>配置 yum 源：</p>\n<pre><code># 配置基础源\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>所有节点</mark>必备工具安装：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>所有节点</mark>关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>所有节点</mark>关闭 swap 分区：</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>所有节点</mark>安装 ntpdate：</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>所有节点</mark>同步时间并配置上海时区：</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# 加入到crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>所有节点</mark>配置 limit：</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>所有节点</mark>升级系统：</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 节点</mark>免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上</em></p>\n<p><mark>Master01 节点</mark>下载安装所有的源码文件：</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-内核配置\"><a class=\"anchor\" href=\"#12-内核配置\">#</a> 1.2 内核配置</h5>\n<p><mark>所有节点</mark>安装 ipvsadm：</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>所有节点</mark>配置 ipvs 模块：</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>所有节点</mark>创建 ipvs.conf，并配置开机自动加载：</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# 加入以下内容\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>所有节点</mark>然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>所有节点</mark>内核优化配置：</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>所有节点</mark>应用配置：</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>配置完内核后，重启机器，之后查看内核模块是否已自动加载：</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-高可用组件安装\"><a class=\"anchor\" href=\"#2-高可用组件安装\">#</a> 2. 高可用组件安装</h4>\n<p><em>注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装</em></p>\n<p><em>注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。</em></p>\n<p><mark>所有 Master 节点</mark>通过 yum 安装 HAProxy 和 KeepAlived：</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 HAProxy，需要注意黄色部分的 IP：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:16443       #HAProxy监听端口\n  bind 127.0.0.1:16443     #HAProxy监听端口\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IP地址\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IP地址\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IP地址\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 KeepAlived，需要注意黄色部分的配置。</p>\n<p><mark>Master01 节点</mark>的配置：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #网卡名称\n    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #网卡名称\n    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #网卡名称\n    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>所有 master 节点</mark>配置 KeepAlived 健康检查文件：</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>所有 master 节点</mark>配置健康检查文件添加执行权限：</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>所有 master 节点</mark>启动 haproxy 和 keepalived：</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的</p>\n<pre><code>所有节点测试VIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等</p>\n<ul>\n<li>所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld</li>\n<li>所有节点查看 selinux 状态，必须为 disable：getenforce</li>\n<li>master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy</li>\n<li>master 节点查看监听端口：netstat -lntp</li>\n</ul>\n<p>如果以上都没有问题，需要确认：</p>\n<ol>\n<li>\n<p>是否是公有云机器</p>\n</li>\n<li>\n<p>是否是私有云机器（类似 OpenStack）</p>\n</li>\n</ol>\n<p>上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询</p>\n<h4 id=\"3-runtime安装\"><a class=\"anchor\" href=\"#3-runtime安装\">#</a> 3. Runtime 安装</h4>\n<p>如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。</p>\n<h5 id=\"31-安装containerd\"><a class=\"anchor\" href=\"#31-安装containerd\">#</a> 3.1 安装 Containerd</h5>\n<p><mark>所有节点</mark>配置安装源：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>所有节点</mark>安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p>首先配置 Containerd 所需的模块（<mark>所有节点</mark>）：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载模块：</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>所有节点</mark>，配置 Containerd 所需的内核：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载内核：</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>生成 Containerd 的配置文件：</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>更改 Containerd 的 Cgroup 和 Pause 镜像配置：</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>启动 Containerd，并配置开机自启动：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>所有节点</mark>配置 crictl 客户端连接的运行时位置（可选）：</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-安装kubernetes组件\"><a class=\"anchor\" href=\"#4-安装kubernetes组件\">#</a> 4 . 安装 Kubernetes 组件</h4>\n<p><mark>所有节点</mark>配置源（注意更改版本号）：</p>\n<pre><code>cat &lt;&lt;EOF | tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key\nEOF\n</code></pre>\n<p>首先在<mark> Master01 节点</mark>查看最新的 Kubernetes 版本是多少：</p>\n<pre><code># yum list kubeadm.x86_64 --showduplicates | sort -r\n</code></pre>\n<p><mark>所有节点</mark>安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：</p>\n<pre><code># yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y\n</code></pre>\n<p><mark>所有节点</mark>设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now kubelet\n</code></pre>\n<p><em>此时 kubelet 是起不来的，日志会有报错不影响！</em></p>\n<h4 id=\"5-集群初始化\"><a class=\"anchor\" href=\"#5-集群初始化\">#</a> 5 . 集群初始化</h4>\n<p>以下操作在<mark> master01</mark>（注意黄色部分）：</p>\n<pre><code>vim kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 7t2weq.bjbawausm0jaxury\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.1.71\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  name: k8s-master01\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n---\napiServer:\n  certSANs:\n  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443\ncontrollerManager: &#123;&#125;\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突\n  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突\nscheduler: &#123;&#125;\n</code></pre>\n<p><mark>master01 节点</mark>更新 kubeadm 文件：</p>\n<pre><code>kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml\n</code></pre>\n<p>将 new.yaml 文件复制到<mark>其他 master 节点</mark>:</p>\n<pre><code>for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done\n</code></pre>\n<p>之后<mark>所有 Master 节点</mark>提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：</p>\n<pre><code>kubeadm config images pull --config /root/new.yaml \n</code></pre>\n<p>正确的反馈信息如下（<em><strong>* 版本可能不一样 *</strong></em>）：</p>\n<pre><code>[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml \n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0\n</code></pre>\n<p><mark>Master01 节点</mark>初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：</p>\n<pre><code>kubeadm init --config /root/new.yaml  --upload-certs\n</code></pre>\n<p>初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：</p>\n<pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n# 不要复制文档当中的，要去使用节点生成的\n  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94\n</code></pre>\n<p><mark>Master01 节点</mark>配置环境变量，用于访问 Kubernetes 集群：</p>\n<pre><code>cat &lt;&lt;EOF &gt;&gt; /root/.bashrc\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nEOF\nsource /root/.bashrc\n</code></pre>\n<p><mark>Master01 节点</mark>查看节点状态：（显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE   VERSION\nk8s-master01   NotReady   control-plane   24s   v1.32.3\n</code></pre>\n<p>采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：</p>\n<pre><code class=\"language-\\\"># kubectl get pods -n kube-system\n</code></pre>\n<h5 id=\"51-初始化失败排查\"><a class=\"anchor\" href=\"#51-初始化失败排查\">#</a> 5.1 初始化失败排查</h5>\n<p>如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：</p>\n<pre><code>kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube\n</code></pre>\n<p>如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：</p>\n<pre><code>tail -f /var/log/messages | grep -v &quot;not found&quot;\n</code></pre>\n<p>经常出错的原因：</p>\n<ol>\n<li>Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对</li>\n<li>new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443</li>\n<li>new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突</li>\n<li>VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错</li>\n</ol>\n<h5 id=\"52-高可用master\"><a class=\"anchor\" href=\"#52-高可用master\">#</a> 5.2 高可用 Master</h5>\n<p><strong>其他 master</strong> 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n</code></pre>\n<p>查看当前状态：（如果显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\n</code></pre>\n<h5 id=\"53-token过期处理\"><a class=\"anchor\" href=\"#53-token过期处理\">#</a> 5.3 Token 过期处理</h5>\n<p>注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。</p>\n<p>Token 过期后生成新的 token：</p>\n<pre><code>kubeadm token create --print-join-command\n</code></pre>\n<p>Master 需要生成 --certificate-key：</p>\n<pre><code>kubeadm init phase upload-certs  --upload-certs\n</code></pre>\n<h4 id=\"6-node节点的配置\"><a class=\"anchor\" href=\"#6-node节点的配置\">#</a> 6. Node 节点的配置</h4>\n<p>Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5\n</code></pre>\n<p>所有节点初始化完成后，查看集群状态（NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\nk8s-node01     NotReady   &lt;none&gt;          13s     v1.32.3\nk8s-node02     NotReady   &lt;none&gt;          10s     v1.32.3\n</code></pre>\n<h4 id=\"7-calico组件的安装\"><a class=\"anchor\" href=\"#7-calico组件的安装\">#</a> 7. Calico 组件的安装</h4>\n<p><mark>所有节点</mark>禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：</p>\n<pre><code>cat &gt;&gt;/etc/NetworkManager/conf.d/calico.conf&lt;&lt;EOF\n[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali\nEOF\nsystemctl daemon-reload\nsystemctl restart NetworkManager\n</code></pre>\n<p>以下步骤只在<mark> master01</mark> 执行（.x 不需要更改）：</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x &amp;&amp; cd calico/\n</code></pre>\n<p>修改 Pod 网段：</p>\n<pre><code>POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '&#123;print $NF&#125;'`\n\nsed -i &quot;s#POD_CIDR#$&#123;POD_SUBNET&#125;#g&quot; calico.yaml\nkubectl apply -f calico.yaml\n</code></pre>\n<p>查看容器和节点状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -n kube-system\nNAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h\ncalico-node-7mzmb                          1/1     Running   0          24h\ncalico-node-ljqnl                          1/1     Running   0          24h\ncalico-node-njqlb                          1/1     Running   0          24h\ncalico-node-ph4m4                          1/1     Running   0          24h\ncalico-node-rx8rl                          1/1     Running   0          24h\ncoredns-76fccbbb6b-76559                   1/1     Running   0          24h\ncoredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h\netcd-k8s-master01                          1/1     Running   0          24h\netcd-k8s-master02                          1/1     Running   0          24h\netcd-k8s-master03                          1/1     Running   0          24h\nkube-apiserver-k8s-master01                1/1     Running   0          24h\nkube-apiserver-k8s-master02                1/1     Running   0          24h\nkube-apiserver-k8s-master03                1/1     Running   0          24h\nkube-controller-manager-k8s-master01       1/1     Running   0          24h\nkube-controller-manager-k8s-master02       1/1     Running   0          24h\nkube-controller-manager-k8s-master03       1/1     Running   0          24h\nkube-proxy-9dtz4                           1/1     Running   0          24h\nkube-proxy-jh7rl                           1/1     Running   0          24h\nkube-proxy-jvvwt                           1/1     Running   0          24h\nkube-proxy-sh89l                           1/1     Running   0          24h\nkube-proxy-t2j49                           1/1     Running   0          24h\nkube-scheduler-k8s-master01                1/1     Running   0          24h\nkube-scheduler-k8s-master02                1/1     Running   0          24h\nkube-scheduler-k8s-master03                1/1     Running   0          24h\nmetrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h\n</code></pre>\n<p>此时节点全部变为 Ready 状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get nodes\nNAME           STATUS   ROLES           AGE   VERSION\nk8s-master01   Ready    control-plane   24h   v1.32.3\nk8s-master02   Ready    control-plane   24h   v1.32.3\nk8s-master03   Ready    control-plane   24h   v1.32.3\nk8s-node01     Ready    &lt;none&gt;          24h   v1.32.3\nk8s-node02     Ready    &lt;none&gt;          24h   v1.32.3\n</code></pre>\n<h4 id=\"8-metrics部署\"><a class=\"anchor\" href=\"#8-metrics部署\">#</a> 8. Metrics 部署</h4>\n<p>在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。</p>\n<p>将<mark> Master01 节点</mark>的 front-proxy-ca.crt 复制到所有 Node 节点</p>\n<pre><code>scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt\n\nscp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt\n</code></pre>\n<p>以下操作均在<mark> master01 节点</mark>执行:</p>\n<p>安装 metrics server</p>\n<pre><code>cd /root/k8s-ha-install/kubeadm-metrics-server\n\n# kubectl  create -f comp.yaml \nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>查看状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server\nNAME                              READY   STATUS    RESTARTS   AGE\nmetrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h\n</code></pre>\n<p>等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl top node\nNAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \nk8s-master01   132m         3%       932Mi           5%          \nk8s-master02   131m         3%       845Mi           5%          \nk8s-master03   148m         3%       912Mi           5%          \nk8s-node01     54m          1%       600Mi           3%          \nk8s-node02     49m          1%       602Mi           3%          \n[root@k8s-master01 ~]#  kubectl top po -A\nNAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   \ningress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            \ningress-nginx          ingress-nginx-controller-r978m               1m           104Mi           \nkrm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            \nkrm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             \nkrm                    nginx-574cf48959-vcfjs                       0m           2Mi             \nkube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            \nkube-system            calico-node-7mzmb                            16m          176Mi           \nkube-system            calico-node-ljqnl                            15m          182Mi           \nkube-system            calico-node-njqlb                            19m          180Mi           \nkube-system            calico-node-ph4m4                            15m          178Mi           \nkube-system            calico-node-rx8rl                            17m          180Mi           \nkube-system            coredns-76fccbbb6b-76559                     2m           16Mi            \nkube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            \nkube-system            etcd-k8s-master01                            22m          86Mi            \nkube-system            etcd-k8s-master02                            27m          84Mi            \nkube-system            etcd-k8s-master03                            22m          84Mi            \nkube-system            kube-apiserver-k8s-master01                  22m          267Mi           \nkube-system            kube-apiserver-k8s-master02                  20m          242Mi           \nkube-system            kube-apiserver-k8s-master03                  18m          241Mi           \nkube-system            kube-controller-manager-k8s-master01         6m           69Mi            \nkube-system            kube-controller-manager-k8s-master02         2m           21Mi            \nkube-system            kube-controller-manager-k8s-master03         1m           19Mi            \nkube-system            kube-proxy-9dtz4                             11m          30Mi            \nkube-system            kube-proxy-jh7rl                             1m           27Mi            \nkube-system            kube-proxy-jvvwt                             17m          29Mi            \nkube-system            kube-proxy-sh89l                             1m           29Mi            \nkube-system            kube-proxy-t2j49                             16m          29Mi            \nkube-system            kube-scheduler-k8s-master01                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master02                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master03                  6m           25Mi            \nkube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            \nkubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            \nkubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  \n</code></pre>\n<h4 id=\"9-dashboard部署\"><a class=\"anchor\" href=\"#9-dashboard部署\">#</a> 9. Dashboard 部署</h4>\n<h5 id=\"91-安装dashboard\"><a class=\"anchor\" href=\"#91-安装dashboard\">#</a> 9.1 安装 Dashboard</h5>\n<p>Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"92-登录dashboard\"><a class=\"anchor\" href=\"#92-登录dashboard\">#</a> 9.2 登录 dashboard</h5>\n<p>在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>更改 dashboard 的 svc 为 NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）</em></p>\n<p>查看端口号：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：</p>\n<p>访问 Dashboard：<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>创建登录 Token：</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"10必看一些必须的配置更改\"><a class=\"anchor\" href=\"#10必看一些必须的配置更改\">#</a> 10.【必看】一些必须的配置更改</h4>\n<p>将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：</p>\n<p>在 master01 节点执行：</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\nmode: ipvs\n</code></pre>\n<p>更新 Kube-Proxy 的 Pod：</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>验证 Kube-Proxy 模式:</p>\n<pre><code>[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode\nipvs\n</code></pre>\n<h4 id=\"11必看注意事项\"><a class=\"anchor\" href=\"#11必看注意事项\">#</a> 11.【必看】注意事项</h4>\n<p>注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。</p>\n<p>启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。</p>\n<p>其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。</p>\n<p>kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\n</code></pre>\n<p>进行更改，更改完成后，可以通过 patch 重启 kube-proxy</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-\n</code></pre>\n<h4 id=\"12-containerd配置镜像加速\"><a class=\"anchor\" href=\"#12-containerd配置镜像加速\">#</a> 12. Containerd 配置镜像加速</h4>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>所有节点重新启动 Containerd：</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"13-docker配置镜像加速\"><a class=\"anchor\" href=\"#13-docker配置镜像加速\">#</a> 13. Docker 配置镜像加速</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>所有节点重新启动 Docker：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/1922841233.html",
            "url": "http://xuyong.cn/posts/1922841233.html",
            "title": "Rsync服务实践",
            "date_published": "2025-03-30T12:45:48.000Z",
            "content_html": "<h3 id=\"ursync服务实践u\"><a class=\"anchor\" href=\"#ursync服务实践u\">#</a> <u>Rsync 服务实践</u></h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"1rsync服务端\"><a class=\"anchor\" href=\"#1rsync服务端\">#</a> 1.rsync 服务端</h4>\n<h5 id=\"11-关闭防火墙-selinux\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync\"><a class=\"anchor\" href=\"#12-安装rsync\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件\"><a class=\"anchor\" href=\"#13-修改配置文件\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-rsync客户端\"><a class=\"anchor\" href=\"#2-rsync客户端\">#</a> 2. rsync 客户端</h4>\n<h5 id=\"21-安装rsync\"><a class=\"anchor\" href=\"#21-安装rsync\">#</a> 2.1 安装 rsync</h5>\n<pre><code>[root@db01 ~]# yum install nfs-utils -y\n</code></pre>\n<h5 id=\"22-配置传输密码\"><a class=\"anchor\" href=\"#22-配置传输密码\">#</a> 2.2 配置传输密码</h5>\n<p>方法 1：将密码写入文件</p>\n<pre><code>[root@db01 ~]#  echo 'your passwd' &gt; /etc/rsync.pass\n[root@db01 ~]# cat /etc/rsync.pass \nyour passwd\n[root@db01 ~]# chmod 600 /etc/rsync.pass\n--测试收发数据：\n[root@db01 ~]# rsync -avz --password-file=/etc/rsync.pass /root/test rsync_backup@192.168.40.101::backup_file\nsending incremental file list\n\nsent 47 bytes  received 20 bytes  134.00 bytes/sec\ntotal size is 0  speedup is 0.00\n</code></pre>\n<p>方法 2：使用密码环境变量 RSYNC_PASSWORD</p>\n<pre><code>[root@db01 ~]# export RSYNC_PASSWORD='your passwd'\n--测试收发数据：\n[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file\nsending incremental file list\n\nsent 47 bytes  received 20 bytes  134.00 bytes/sec\ntotal size is 0  speedup is 0.00\n</code></pre>\n<h3 id=\"ursync企业级备份案例u\"><a class=\"anchor\" href=\"#ursync企业级备份案例u\">#</a> <u>Rsync 企业级备份案例</u></h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<p><strong>客户端需求</strong></p>\n<ul>\n<li>客户端每天凌晨 3 点备份 MySQL 至 /backup 下以 &quot;主机名_IP 地址_当前时间命名&quot; 的目录中</li>\n<li>客户端推送 /backup 目录下数据备份目录至 Rsync 备份服务器</li>\n<li>客户端只保留最近七天的备份数据，避免浪费磁盘空间</li>\n</ul>\n<p><strong>服务端需求</strong></p>\n<ul>\n<li>服务端部署 rsync 服务，用于接收用户的备份数据</li>\n<li>服务端每天校验客户端推送过来的数据是否完整，并将结果以邮件的方式发送给管理员</li>\n<li>服务端仅保留 6 个月的备份数据</li>\n</ul>\n<p><strong>注意</strong>：所有服务器的备份目录均为 /backup，所有脚本存放目录均为 /scripts。</p>\n<h4 id=\"1-服务端部署rsync服务\"><a class=\"anchor\" href=\"#1-服务端部署rsync服务\">#</a> <strong>1. 服务端部署 rsync 服务</strong></h4>\n<h5 id=\"11-关闭防火墙-selinux-2\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux-2\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync-2\"><a class=\"anchor\" href=\"#12-安装rsync-2\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件-2\"><a class=\"anchor\" href=\"#13-修改配置文件-2\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限-2\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限-2\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启-2\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启-2\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-rsync客户端-2\"><a class=\"anchor\" href=\"#2-rsync客户端-2\">#</a> 2. rsync 客户端</h4>\n<h5 id=\"21-安装rsync-2\"><a class=\"anchor\" href=\"#21-安装rsync-2\">#</a> 2.1 安装 rsync</h5>\n<pre><code>[root@db01 ~]# yum install nfs-utils -y\n</code></pre>\n<h5 id=\"22-测试客户端备份数据并推送至rsync服务器\"><a class=\"anchor\" href=\"#22-测试客户端备份数据并推送至rsync服务器\">#</a> 2.2 测试客户端备份数据并推送至 rsync 服务器</h5>\n<pre><code>[root@db01 ~]# export RSYNC_PASSWORD='your passwd'\n[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file\n</code></pre>\n<h5 id=\"23-客户端备份数据并推送至rsync服务器\"><a class=\"anchor\" href=\"#23-客户端备份数据并推送至rsync服务器\">#</a> <strong>2.3 客户端备份数据并推送至 rsync 服务器</strong></h5>\n<pre><code>[root@db01 ~]# mkdir /scripts\n[root@db01 ~]# cat /scripts/mysql_backup.sh \n#!/bin/bash\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\n\n#1、定义变量\nHost=$(hostname)\nIp=$(ifconfig ens192 | awk 'NR==2&#123;print $2&#125;')\nDate=$(date +%F)\nBackupDir=/backup/mysql\nDest=$&#123;BackupDir&#125;/$&#123;Host&#125;_$&#123;Ip&#125;_$&#123;Date&#125;\nFILE_NAME=mysql_backup_`date '+%Y%m%d%H%M%S'`;\nOLDBINLOG=/var/lib/mysql/oldbinlog\n\n#2、创建备份目录\nif [ ! -d $Dest ];then\n  mkdir -p $Dest\nfi\n\n#3、备份目录\n/usr/bin/mysqldump -u'root' -p'your passwd' nf_flms &gt; $Dest/nf-flms_$&#123;FILE_NAME&#125;.sql\ntar -czvf $Dest/$&#123;FILE_NAME&#125;.tar.gz $Dest/nf-flms_$&#123;FILE_NAME&#125;.sql\nrm -rf $Dest/*$&#123;FILE_NAME&#125;.sql\necho &quot;Your database backup successfully&quot;\n\n#4、校验\nmd5sum $Dest/* &gt;$Dest/backup_check_$Date\n\n#5、将备份目录推动到rsync服务端\nRsync_Ip=192.168.1.145\nRsync_user=rsync_backup\nRsync_Module=backup_mysql\nexport RSYNC_PASSWORD=your passwd\nrsync -avz $Dest $Rsync_user@$Rsync_Ip::$Rsync_Module\n\n#6、删除15天备份目录\nfind $Dest -type d -mtime +15 | xargs rm -rf\necho &quot;remove file  successfully&quot;\n\n[root@db01 ~]# chmod +x /scripts/etc_backup.sh\n[root@db01 ~]# crontab -e\n00 03 * * * /bin/bash /scripts/mysql_backup.sh &amp;&gt; /dev/null\n</code></pre>\n<h5 id=\"24-服务端校验数据并将结果以邮件发送给管理员\"><a class=\"anchor\" href=\"#24-服务端校验数据并将结果以邮件发送给管理员\">#</a> <strong>2.4 服务端校验数据并将结果以邮件发送给管理员</strong></h5>\n<h6 id=\"241-配置邮件服务\"><a class=\"anchor\" href=\"#241-配置邮件服务\">#</a> 2.4.1 配置邮件服务</h6>\n<pre><code>[root@backup ~]# yum -y install mailx\n[root@backup ~]# cat /etc/mail.rc      #最后一行插入\nset from=373370405@qq.com\nset smtp=smtps://smtp.qq.com:465\nset smtp-auth-user=373370405@qq.com\nset smtp-auth-password=**********   # 发件邮箱的授权码\nset smtp-auth=login\nset ssl-verify=ignore\nset nss-config-dir=/etc/pki/nssdb\n</code></pre>\n<h6 id=\"242-发送邮件测试\"><a class=\"anchor\" href=\"#242-发送邮件测试\">#</a> 2.4.2 发送邮件测试</h6>\n<pre><code>[root@backup ~]#  echo Hello World | mail -s test 373370405@qq.com &amp;&gt; /dev/null\n</code></pre>\n<h6 id=\"243-配置脚本校验数据并将结果发送给管理员\"><a class=\"anchor\" href=\"#243-配置脚本校验数据并将结果发送给管理员\">#</a> 2.4.3 配置脚本校验数据并将结果发送给管理员</h6>\n<pre><code>[root@backup mysql]# cat /scripts/check_backup.sh \n#!/bin/bash\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\n\n#1、定义变量\nPath=/backup/mysql\nDate=$(date +%F)\n\n#2、查看flag文件，并对对文件进行校验,然后将校验的结果保存至result_时间\nfind $Path -type f -name &quot;backup_check_$&#123;Date&#125;*&quot;|xargs md5sum -c &gt;$Path/result_$&#123;Date&#125;\n\n#3、将校验结果发送邮件给管理员\nmail -s &quot;Mysql Backup&quot; 373370405@qq.com &lt;$Path/result_$&#123;Date&#125; &amp;&gt; /dev/null\n\n#4、删除超过7天的校验结果文件，删除超过180天的备份数据文件\nfind $Path -type f -name &quot;result*&quot; -mtime +7 | xargs rm -rf\nfind $Path -type f -mtime +180 | xargs rm -rf\n</code></pre>\n<h6 id=\"244-写计划任务\"><a class=\"anchor\" href=\"#244-写计划任务\">#</a> <strong>2.4.4 写计划任务</strong></h6>\n<pre><code>[root@backup ~]# chmod +x /scripts/check_backup.sh \n[root@db01 ~]# crontab -e\n00 06 * * * /bin/bash /scripts/mysql_backup.sh &amp;&gt; /dev/null\n</code></pre>\n<h3 id=\"rsyncsersync实现数据实时同步\"><a class=\"anchor\" href=\"#rsyncsersync实现数据实时同步\">#</a> Rsync+sersync 实现数据实时同步</h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"1rsync服务端-2\"><a class=\"anchor\" href=\"#1rsync服务端-2\">#</a> 1.rsync 服务端</h4>\n<h5 id=\"11-关闭防火墙-selinux-3\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux-3\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync-3\"><a class=\"anchor\" href=\"#12-安装rsync-3\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件-3\"><a class=\"anchor\" href=\"#13-修改配置文件-3\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限-3\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限-3\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启-3\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启-3\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-客户端安装sersync\"><a class=\"anchor\" href=\"#2-客户端安装sersync\">#</a> 2. 客户端安装 sersync</h4>\n<p><strong>2.1 安装 sercync 依赖</strong></p>\n<pre><code>[root@nfs ~]# yum install -y inotify-tools rsync\n</code></pre>\n<p><strong>2.2 安装 sercync</strong></p>\n<pre><code>[root@nfs ~]# mkdir -p /soft\n[root@nfs ~]# cd /soft/\n[root@nfs ~]# wget https://down.whsir.com/downloads/sersync2.5.4_64bit_binary_stable_final.tar.gz\n[root@nfs soft]# tar -xf sersync2.5.4_64bit_binary_stable_final.tar.gz\n[root@nfs soft]# mv GNU-Linux-x86 /usr/local/sersync\n</code></pre>\n<h5 id=\"23-修改配置文件\"><a class=\"anchor\" href=\"#23-修改配置文件\">#</a> 2.3 <strong>修改配置文件</strong></h5>\n<pre><code>[root@nfs soft]# cd /usr/local/sersync/\n[root@nfs sersync]# cp confxml.xml confxml.xml.bak\n[root@nfs sersync]# vim confxml.xml\n...\n5    &lt;fileSystem xfs=&quot;true&quot;/&gt;    #第5行 false改为true\n13          &lt;delete start=&quot;true&quot;/&gt; #第13-20行 false改为true,#说明：监控以上变化推送\n14        &lt;createFolder start=&quot;true&quot;/&gt;\n15        &lt;createFile start=&quot;false&quot;/&gt;\n16        &lt;closeWrite start=&quot;true&quot;/&gt;\n17        &lt;moveFrom start=&quot;true&quot;/&gt;\n18        &lt;moveTo start=&quot;true&quot;/&gt;\n19        &lt;attrib start=&quot;true&quot;/&gt;\n20        &lt;modify start=&quot;true&quot;/&gt;\n24        &lt;localpath watch=&quot;/data&quot;&gt;      #监控的本地目录\n25             &lt;remote ip=&quot;192.168.1.145&quot; name=&quot;backup_file&quot;/&gt;  #rsync服务端IP和模块名backup_file\n30      &lt;commonParams params=&quot;-avz&quot;/&gt;  #rsync命令选项\n31      &lt;auth start=&quot;true&quot; users=&quot;rsync_backup&quot; passwordfile=&quot;/etc/rsync.passwd&quot;/&gt; #rsync认证信息\n...\n</code></pre>\n<h5 id=\"24-生成密码文件\"><a class=\"anchor\" href=\"#24-生成密码文件\">#</a> 2.4 生成密码文件</h5>\n<pre><code>[root@nfs sersync]# echo 'your passwd' &gt; /etc/rsync.passwd\n[root@nfs sersync]# chmod 600 /etc/rsync.passwd\n</code></pre>\n<h5 id=\"25-启动sersync\"><a class=\"anchor\" href=\"#25-启动sersync\">#</a> 2.5 启动 sersync</h5>\n<pre><code>[root@nfs sersync]# ln -s /usr/local/sersync/sersync2 /usr/bin/\n[root@nfs sersync]# sersync2 -dro /usr/local/sersync/confxml.xml     #针对配置文件confxml.xml启动sersync\n</code></pre>\n<p><strong>2.5 设置 sersync 开机自启</strong></p>\n<pre><code>[root@qzj_nfs sersync]# vim /etc/rc.d/rc.local   \n/usr/local/sersync/sersync2 -d -r -o  /usr/local/sersync/confxml.xml  #在最后添加一行\n[root@qzj_nfs sersync]# chmod +x /etc/rc.d/rc.local\n</code></pre>\n<p><strong>2.6 测试</strong></p>\n<p><em>在客户端 /data 目录增删改目录文件，rsync 服务端数据存放目录变化</em></p>\n<pre><code>[root@backup backup]# watch ls\n</code></pre>\n<p><strong>2.7 添加脚本监控 sersync 是否正常运行</strong></p>\n<pre><code>[root@nfs sersync]# cat /scripts/check_sersync.sh \n#!/bin/sh\nsersync=&quot;/usr/local/sersync/sersync2&quot;\nconfxml=&quot;/usr/local/sersync/confxml.xml&quot;\nstatus=$(ps aux |grep 'sersync2'|grep -v 'grep'|wc -l)\nif [ $status -eq 0 ];\nthen\n$sersync -d -r -o $confxml &amp;\nelse\nexit 0;\nfi\n\n[root@nfs sersync]# chmod +x /scripts/check_sersync.sh\n[root@nfs sersync]# crontab -l\n*/5 * * * * /usr/bin/sh /scripts/check_sersync.sh &amp;&gt; /dev/null\n</code></pre>\n<p><em><strong>补充： 多实例情况</strong></em><br />\n 1、配置多个 confxml.xml 文件（比如：www、bbs、blog.... 等等）<br />\n2、修改端口、同步路径、模块名称<br />\n 3、根据不同的需求同步对应的实例文件<br />\n /usr/local/sersync/sersync2 -dro /usr/local/sersync/www_confxml.xml<br />\n/usr/local/sersync/sersync2 -dro /usr/local/sersync/bbs_confxml.xml</p>\n",
            "tags": [
                "rsync"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/3071070978.html",
            "url": "http://xuyong.cn/posts/3071070978.html",
            "title": "企业级私有仓库Harbor搭建",
            "date_published": "2025-03-30T08:17:00.000Z",
            "content_html": "<h3 id=\"企业级私有仓库harbor\"><a class=\"anchor\" href=\"#企业级私有仓库harbor\">#</a> 企业级私有仓库 Harbor</h3>\n<p>企业部署 Kuberetes 集群环境之后，我们就可以将原来在传统虚拟机上运行的业务，迁移到 kubernetes 上，让 Kubernetes 通过容器的方式来管理。而一旦我们需要将传统业务使用容器的方式运行起来，就需要构建很多镜像，那么这些镜像就需要有一个专门的位置存储起来，为我们提供镜像上传和镜像下载等功能。但我们不能使用阿里云或者 Dockerhub 等仓库，首先拉取速度比较慢，其次镜像的安全性无法保证，所以就需要部署一个私有的镜像仓库来管理这些容器镜像。同时该仓库还需要提供高可用功能，确保随时都能上传和下载可用的容器镜像。</p>\n<h4 id=\"1-关闭防火墙-selinux-环境配置\"><a class=\"anchor\" href=\"#1-关闭防火墙-selinux-环境配置\">#</a> 1、关闭防火墙、Selinux、环境配置</h4>\n<pre><code>[root@harbor ~]# sudo mkdir -p /etc/docker\n[root@harbor ~]# hostnamectl set-hostname harbor\n[root@harbor ~]# systemctl stop firewalld\n[root@harbor ~]# systemctl disable firewalld\n[root@harbor ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@harbor ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate -y\n[root@harbor ~]# yum update -y\n[root@harbor ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h4 id=\"2-docker安装\"><a class=\"anchor\" href=\"#2-docker安装\">#</a> 2、Docker 安装</h4>\n<pre><code>[root@harbor ~]# yum install -y yum-utils device-mapper-persistent-data lvm2\n[root@harbor ~]# curl -o /etc/yum.repos.d/docker-ce.repo  https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n[root@harbor ~]# yum list docker-ce --showduplicates |sort -r \n[root@harbor ~]# yum install docker-ce docker-compose -y\n</code></pre>\n<h4 id=\"3-配置docker加速\"><a class=\"anchor\" href=\"#3-配置docker加速\">#</a> 3、配置 Docker 加速</h4>\n<pre><code>[root@harbor ~]# sudo mkdir -p /etc/docker\n[root@harbor ~]# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n[root@harbor ~]# systemctl enable docker --now\n</code></pre>\n<h4 id=\"4-安装harbor\"><a class=\"anchor\" href=\"#4-安装harbor\">#</a> 4、安装 Harbor</h4>\n<pre><code>[root@harbor ~]# cd /soft/\n[root@harbor ~]# wget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz\n[root@harbor soft]# tar xf harbor-offline-installer-v2.6.1.tgz\n[root@harbor soft]# cd harbor\n[root@harbor harbor]# vim harbor.yml\nhostname: 192.168.1.134\n...\n#https:\n#  # https port for harbor, default is 443\n#  port: 443\n#  # The path of cert and key files for nginx\n#  certificate: /your/certificate/path\n#  private_key: /your/private/key/path\n...\nharbor_admin_password: Harbor12345\n[root@harbor harbor]#  ./install.sh\n</code></pre>\n<h4 id=\"5-配置nginx负载均衡调度\"><a class=\"anchor\" href=\"#5-配置nginx负载均衡调度\">#</a> 5、配置 Nginx 负载均衡调度</h4>\n<pre><code>[root@lb ~]# vim s.hmallleasing.com.conf\nserver &#123;\n    listen 443 ssl;\n    server_name harbor.hmallleasing.com;\n    client_max_body_size 1G; \n    ssl_prefer_server_ciphers on;\n    ssl_certificate  /etc/nginx/sslkey/_.hmallleasing.com_chain.crt;\n    ssl_certificate_key  /etc/nginx/sslkey/_.hmallleasing.com_key.key;\n    location / &#123;\n        proxy_pass http://192.168.1.134;\n#      include proxy_params;\n#        proxy_set_header Host $http_host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        proxy_connect_timeout 30;\n        proxy_send_timeout 60;\n        proxy_read_timeout 60;\n        \n        proxy_buffering on;\n        proxy_buffer_size 32k;\n        proxy_buffers 4 128k;\n        proxy_temp_file_write_size 10240k;\t\t\n        proxy_max_temp_file_size 10240k;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n</code></pre>\n<h4 id=\"6-推送镜像至harbor\"><a class=\"anchor\" href=\"#6-推送镜像至harbor\">#</a> 6、推送镜像至 Harbor</h4>\n<pre><code>[root@harbor harbor]# docker tag beae173ccac6 harbor.hmallleasing.com/ops/busybox.v1\n[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1\n[root@harbor harbor]# docker login harbor.hmallleasing.com\n[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1\n</code></pre>\n<h4 id=\"7-harbor停止与启动\"><a class=\"anchor\" href=\"#7-harbor停止与启动\">#</a> 7、Harbor 停止与启动</h4>\n<pre><code>#停用Harbor\n[root@harbor harbor]# pwd\n/soft/harbor\n[root@harbor harbor]# docker-compose stop\n #启动Harbor\n[root@harbor harbor]# docker-compose up -d\n[root@harbor harbor]# docker-compose start\n</code></pre>\n",
            "tags": [
                "Harbor"
            ]
        }
    ]
}