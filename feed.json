{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://xuyong.cn",
    "items": [
        {
            "id": "http://xuyong.cn/posts/3166738000.html",
            "url": "http://xuyong.cn/posts/3166738000.html",
            "title": "Kubeadm高可用安装K8s集群",
            "date_published": "2025-04-09T10:28:34.000Z",
            "content_html": "<h2 id=\"kubeadm高可用安装k8s集群\"><a class=\"anchor\" href=\"#kubeadm高可用安装k8s集群\">#</a> Kubeadm 高可用安装 K8s 集群</h2>\n<h4 id=\"1-基本配置\"><a class=\"anchor\" href=\"#1-基本配置\">#</a> 1. 基本配置</h4>\n<h5 id=\"11-基本环境配置\"><a class=\"anchor\" href=\"#11-基本环境配置\">#</a> 1.1 基本环境配置</h5>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP 地址</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master 节点 * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived 虚拟 IP（不占用机器）</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker 节点 * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* 配置信息 *</strong></em></th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>系统版本</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod 网段</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service 网段</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>所有节点</mark>更改主机名（其它节点按需修改）：</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>所有节点</mark>配置 hosts，修改 /etc/hosts 如下：</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>所有节点</mark>配置 yum 源：</p>\n<pre><code># 配置基础源\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>所有节点</mark>必备工具安装：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>所有节点</mark>关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>所有节点</mark>关闭 swap 分区：</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>所有节点</mark>安装 ntpdate：</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>所有节点</mark>同步时间并配置上海时区：</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# 加入到crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>所有节点</mark>配置 limit：</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>所有节点</mark>升级系统：</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 节点</mark>免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上</em></p>\n<p><mark>Master01 节点</mark>下载安装所有的源码文件：</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-内核配置\"><a class=\"anchor\" href=\"#12-内核配置\">#</a> 1.2 内核配置</h5>\n<p><mark>所有节点</mark>安装 ipvsadm：</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>所有节点</mark>配置 ipvs 模块：</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>所有节点</mark>创建 ipvs.conf，并配置开机自动加载：</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# 加入以下内容\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>所有节点</mark>然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>所有节点</mark>内核优化配置：</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>所有节点</mark>应用配置：</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>配置完内核后，重启机器，之后查看内核模块是否已自动加载：</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-高可用组件安装\"><a class=\"anchor\" href=\"#2-高可用组件安装\">#</a> 2. 高可用组件安装</h4>\n<p><em>注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装</em></p>\n<p><em>注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。</em></p>\n<p><mark>所有 Master 节点</mark>通过 yum 安装 HAProxy 和 KeepAlived：</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 HAProxy，需要注意黄色部分的 IP：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:16443       #HAProxy监听端口\n  bind 127.0.0.1:16443     #HAProxy监听端口\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IP地址\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IP地址\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IP地址\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 KeepAlived，需要注意黄色部分的配置。</p>\n<p><mark>Master01 节点</mark>的配置：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #网卡名称\n    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #网卡名称\n    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #网卡名称\n    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>所有 master 节点</mark>配置 KeepAlived 健康检查文件：</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>所有 master 节点</mark>配置健康检查文件添加执行权限：</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>所有 master 节点</mark>启动 haproxy 和 keepalived：</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的</p>\n<pre><code>所有节点测试VIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等</p>\n<ul>\n<li>所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld</li>\n<li>所有节点查看 selinux 状态，必须为 disable：getenforce</li>\n<li>master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy</li>\n<li>master 节点查看监听端口：netstat -lntp</li>\n</ul>\n<p>如果以上都没有问题，需要确认：</p>\n<ol>\n<li>\n<p>是否是公有云机器</p>\n</li>\n<li>\n<p>是否是私有云机器（类似 OpenStack）</p>\n</li>\n</ol>\n<p>上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询</p>\n<h4 id=\"3-runtime安装\"><a class=\"anchor\" href=\"#3-runtime安装\">#</a> 3. Runtime 安装</h4>\n<p>如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。</p>\n<h5 id=\"31-安装containerd\"><a class=\"anchor\" href=\"#31-安装containerd\">#</a> 3.1 安装 Containerd</h5>\n<p><mark>所有节点</mark>配置安装源：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>所有节点</mark>安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p>首先配置 Containerd 所需的模块（<mark>所有节点</mark>）：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载模块：</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>所有节点</mark>，配置 Containerd 所需的内核：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载内核：</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>生成 Containerd 的配置文件：</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>更改 Containerd 的 Cgroup 和 Pause 镜像配置：</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>启动 Containerd，并配置开机自启动：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>所有节点</mark>配置 crictl 客户端连接的运行时位置（可选）：</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-安装kubernetes组件\"><a class=\"anchor\" href=\"#4-安装kubernetes组件\">#</a> 4 . 安装 Kubernetes 组件</h4>\n<p><mark>所有节点</mark>配置源（注意更改版本号）：</p>\n<pre><code>cat &lt;&lt;EOF | tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key\nEOF\n</code></pre>\n<p>首先在<mark> Master01 节点</mark>查看最新的 Kubernetes 版本是多少：</p>\n<pre><code># yum list kubeadm.x86_64 --showduplicates | sort -r\n</code></pre>\n<p><mark>所有节点</mark>安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：</p>\n<pre><code># yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y\n</code></pre>\n<p><mark>所有节点</mark>设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now kubelet\n</code></pre>\n<p><em>此时 kubelet 是起不来的，日志会有报错不影响！</em></p>\n<h4 id=\"5-集群初始化\"><a class=\"anchor\" href=\"#5-集群初始化\">#</a> 5 . 集群初始化</h4>\n<p>以下操作在<mark> master01</mark>（注意黄色部分）：</p>\n<pre><code>vim kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 7t2weq.bjbawausm0jaxury\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.1.71\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  name: k8s-master01\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n---\napiServer:\n  certSANs:\n  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443\ncontrollerManager: &#123;&#125;\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突\n  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突\nscheduler: &#123;&#125;\n</code></pre>\n<p><mark>master01 节点</mark>更新 kubeadm 文件：</p>\n<pre><code>kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml\n</code></pre>\n<p>将 new.yaml 文件复制到<mark>其他 master 节点</mark>:</p>\n<pre><code>for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done\n</code></pre>\n<p>之后<mark>所有 Master 节点</mark>提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：</p>\n<pre><code>kubeadm config images pull --config /root/new.yaml \n</code></pre>\n<p>正确的反馈信息如下（<em><strong>* 版本可能不一样 *</strong></em>）：</p>\n<pre><code>[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml \n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0\n</code></pre>\n<p><mark>Master01 节点</mark>初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：</p>\n<pre><code>kubeadm init --config /root/new.yaml  --upload-certs\n</code></pre>\n<p>初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：</p>\n<pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n# 不要复制文档当中的，要去使用节点生成的\n  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94\n</code></pre>\n<p><mark>Master01 节点</mark>配置环境变量，用于访问 Kubernetes 集群：</p>\n<pre><code>cat &lt;&lt;EOF &gt;&gt; /root/.bashrc\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nEOF\nsource /root/.bashrc\n</code></pre>\n<p><mark>Master01 节点</mark>查看节点状态：（显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE   VERSION\nk8s-master01   NotReady   control-plane   24s   v1.32.3\n</code></pre>\n<p>采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：</p>\n<pre><code class=\"language-\\\"># kubectl get pods -n kube-system\n</code></pre>\n<h5 id=\"51-初始化失败排查\"><a class=\"anchor\" href=\"#51-初始化失败排查\">#</a> 5.1 初始化失败排查</h5>\n<p>如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：</p>\n<pre><code>kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube\n</code></pre>\n<p>如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：</p>\n<pre><code>tail -f /var/log/messages | grep -v &quot;not found&quot;\n</code></pre>\n<p>经常出错的原因：</p>\n<ol>\n<li>Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对</li>\n<li>new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443</li>\n<li>new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突</li>\n<li>VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错</li>\n</ol>\n<h5 id=\"52-高可用master\"><a class=\"anchor\" href=\"#52-高可用master\">#</a> 5.2 高可用 Master</h5>\n<p><strong>其他 master</strong> 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n</code></pre>\n<p>查看当前状态：（如果显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\n</code></pre>\n<h5 id=\"53-token过期处理\"><a class=\"anchor\" href=\"#53-token过期处理\">#</a> 5.3 Token 过期处理</h5>\n<p>注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。</p>\n<p>Token 过期后生成新的 token：</p>\n<pre><code>kubeadm token create --print-join-command\n</code></pre>\n<p>Master 需要生成 --certificate-key：</p>\n<pre><code>kubeadm init phase upload-certs  --upload-certs\n</code></pre>\n<h4 id=\"6-node节点的配置\"><a class=\"anchor\" href=\"#6-node节点的配置\">#</a> 6. Node 节点的配置</h4>\n<p>Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5\n</code></pre>\n<p>所有节点初始化完成后，查看集群状态（NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\nk8s-node01     NotReady   &lt;none&gt;          13s     v1.32.3\nk8s-node02     NotReady   &lt;none&gt;          10s     v1.32.3\n</code></pre>\n<h4 id=\"7-calico组件的安装\"><a class=\"anchor\" href=\"#7-calico组件的安装\">#</a> 7. Calico 组件的安装</h4>\n<p><mark>所有节点</mark>禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：</p>\n<pre><code>cat &gt;&gt;/etc/NetworkManager/conf.d/calico.conf&lt;&lt;EOF\n[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali\nEOF\nsystemctl daemon-reload\nsystemctl restart NetworkManager\n</code></pre>\n<p>以下步骤只在<mark> master01</mark> 执行（.x 不需要更改）：</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x &amp;&amp; cd calico/\n</code></pre>\n<p>修改 Pod 网段：</p>\n<pre><code>POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '&#123;print $NF&#125;'`\n\nsed -i &quot;s#POD_CIDR#$&#123;POD_SUBNET&#125;#g&quot; calico.yaml\nkubectl apply -f calico.yaml\n</code></pre>\n<p>查看容器和节点状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -n kube-system\nNAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h\ncalico-node-7mzmb                          1/1     Running   0          24h\ncalico-node-ljqnl                          1/1     Running   0          24h\ncalico-node-njqlb                          1/1     Running   0          24h\ncalico-node-ph4m4                          1/1     Running   0          24h\ncalico-node-rx8rl                          1/1     Running   0          24h\ncoredns-76fccbbb6b-76559                   1/1     Running   0          24h\ncoredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h\netcd-k8s-master01                          1/1     Running   0          24h\netcd-k8s-master02                          1/1     Running   0          24h\netcd-k8s-master03                          1/1     Running   0          24h\nkube-apiserver-k8s-master01                1/1     Running   0          24h\nkube-apiserver-k8s-master02                1/1     Running   0          24h\nkube-apiserver-k8s-master03                1/1     Running   0          24h\nkube-controller-manager-k8s-master01       1/1     Running   0          24h\nkube-controller-manager-k8s-master02       1/1     Running   0          24h\nkube-controller-manager-k8s-master03       1/1     Running   0          24h\nkube-proxy-9dtz4                           1/1     Running   0          24h\nkube-proxy-jh7rl                           1/1     Running   0          24h\nkube-proxy-jvvwt                           1/1     Running   0          24h\nkube-proxy-sh89l                           1/1     Running   0          24h\nkube-proxy-t2j49                           1/1     Running   0          24h\nkube-scheduler-k8s-master01                1/1     Running   0          24h\nkube-scheduler-k8s-master02                1/1     Running   0          24h\nkube-scheduler-k8s-master03                1/1     Running   0          24h\nmetrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h\n</code></pre>\n<p>此时节点全部变为 Ready 状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get nodes\nNAME           STATUS   ROLES           AGE   VERSION\nk8s-master01   Ready    control-plane   24h   v1.32.3\nk8s-master02   Ready    control-plane   24h   v1.32.3\nk8s-master03   Ready    control-plane   24h   v1.32.3\nk8s-node01     Ready    &lt;none&gt;          24h   v1.32.3\nk8s-node02     Ready    &lt;none&gt;          24h   v1.32.3\n</code></pre>\n<h4 id=\"8-metrics部署\"><a class=\"anchor\" href=\"#8-metrics部署\">#</a> 8. Metrics 部署</h4>\n<p>在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。</p>\n<p>将<mark> Master01 节点</mark>的 front-proxy-ca.crt 复制到所有 Node 节点</p>\n<pre><code>scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt\n\nscp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt\n</code></pre>\n<p>以下操作均在<mark> master01 节点</mark>执行:</p>\n<p>安装 metrics server</p>\n<pre><code>cd /root/k8s-ha-install/kubeadm-metrics-server\n\n# kubectl  create -f comp.yaml \nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>查看状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server\nNAME                              READY   STATUS    RESTARTS   AGE\nmetrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h\n</code></pre>\n<p>等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl top node\nNAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \nk8s-master01   132m         3%       932Mi           5%          \nk8s-master02   131m         3%       845Mi           5%          \nk8s-master03   148m         3%       912Mi           5%          \nk8s-node01     54m          1%       600Mi           3%          \nk8s-node02     49m          1%       602Mi           3%          \n[root@k8s-master01 ~]#  kubectl top po -A\nNAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   \ningress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            \ningress-nginx          ingress-nginx-controller-r978m               1m           104Mi           \nkrm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            \nkrm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             \nkrm                    nginx-574cf48959-vcfjs                       0m           2Mi             \nkube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            \nkube-system            calico-node-7mzmb                            16m          176Mi           \nkube-system            calico-node-ljqnl                            15m          182Mi           \nkube-system            calico-node-njqlb                            19m          180Mi           \nkube-system            calico-node-ph4m4                            15m          178Mi           \nkube-system            calico-node-rx8rl                            17m          180Mi           \nkube-system            coredns-76fccbbb6b-76559                     2m           16Mi            \nkube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            \nkube-system            etcd-k8s-master01                            22m          86Mi            \nkube-system            etcd-k8s-master02                            27m          84Mi            \nkube-system            etcd-k8s-master03                            22m          84Mi            \nkube-system            kube-apiserver-k8s-master01                  22m          267Mi           \nkube-system            kube-apiserver-k8s-master02                  20m          242Mi           \nkube-system            kube-apiserver-k8s-master03                  18m          241Mi           \nkube-system            kube-controller-manager-k8s-master01         6m           69Mi            \nkube-system            kube-controller-manager-k8s-master02         2m           21Mi            \nkube-system            kube-controller-manager-k8s-master03         1m           19Mi            \nkube-system            kube-proxy-9dtz4                             11m          30Mi            \nkube-system            kube-proxy-jh7rl                             1m           27Mi            \nkube-system            kube-proxy-jvvwt                             17m          29Mi            \nkube-system            kube-proxy-sh89l                             1m           29Mi            \nkube-system            kube-proxy-t2j49                             16m          29Mi            \nkube-system            kube-scheduler-k8s-master01                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master02                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master03                  6m           25Mi            \nkube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            \nkubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            \nkubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  \n</code></pre>\n<h4 id=\"9-dashboard部署\"><a class=\"anchor\" href=\"#9-dashboard部署\">#</a> 9. Dashboard 部署</h4>\n<h5 id=\"91-安装dashboard\"><a class=\"anchor\" href=\"#91-安装dashboard\">#</a> 9.1 安装 Dashboard</h5>\n<p>Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"92-登录dashboard\"><a class=\"anchor\" href=\"#92-登录dashboard\">#</a> 9.2 登录 dashboard</h5>\n<p>在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>更改 dashboard 的 svc 为 NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）</em></p>\n<p>查看端口号：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：</p>\n<p>访问 Dashboard：<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>创建登录 Token：</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"10必看一些必须的配置更改\"><a class=\"anchor\" href=\"#10必看一些必须的配置更改\">#</a> 10.【必看】一些必须的配置更改</h4>\n<p>将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：</p>\n<p>在 master01 节点执行：</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\nmode: ipvs\n</code></pre>\n<p>更新 Kube-Proxy 的 Pod：</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>验证 Kube-Proxy 模式:</p>\n<pre><code>[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode\nipvs\n</code></pre>\n<h4 id=\"11必看注意事项\"><a class=\"anchor\" href=\"#11必看注意事项\">#</a> 11.【必看】注意事项</h4>\n<p>注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。</p>\n<p>启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。</p>\n<p>其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。</p>\n<p>kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\n</code></pre>\n<p>进行更改，更改完成后，可以通过 patch 重启 kube-proxy</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-\n</code></pre>\n<h4 id=\"12-containerd配置镜像加速\"><a class=\"anchor\" href=\"#12-containerd配置镜像加速\">#</a> 12. Containerd 配置镜像加速</h4>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>所有节点重新启动 Containerd：</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"13-docker配置镜像加速\"><a class=\"anchor\" href=\"#13-docker配置镜像加速\">#</a> 13. Docker 配置镜像加速</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>所有节点重新启动 Docker：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/1922841233.html",
            "url": "http://xuyong.cn/posts/1922841233.html",
            "title": "Rsync服务实践",
            "date_published": "2025-03-30T12:45:48.000Z",
            "content_html": "<h3 id=\"ursync服务实践u\"><a class=\"anchor\" href=\"#ursync服务实践u\">#</a> <u>Rsync 服务实践</u></h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"1rsync服务端\"><a class=\"anchor\" href=\"#1rsync服务端\">#</a> 1.rsync 服务端</h4>\n<h5 id=\"11-关闭防火墙-selinux\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync\"><a class=\"anchor\" href=\"#12-安装rsync\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件\"><a class=\"anchor\" href=\"#13-修改配置文件\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-rsync客户端\"><a class=\"anchor\" href=\"#2-rsync客户端\">#</a> 2. rsync 客户端</h4>\n<h5 id=\"21-安装rsync\"><a class=\"anchor\" href=\"#21-安装rsync\">#</a> 2.1 安装 rsync</h5>\n<pre><code>[root@db01 ~]# yum install nfs-utils -y\n</code></pre>\n<h5 id=\"22-配置传输密码\"><a class=\"anchor\" href=\"#22-配置传输密码\">#</a> 2.2 配置传输密码</h5>\n<p>方法 1：将密码写入文件</p>\n<pre><code>[root@db01 ~]#  echo 'your passwd' &gt; /etc/rsync.pass\n[root@db01 ~]# cat /etc/rsync.pass \nyour passwd\n[root@db01 ~]# chmod 600 /etc/rsync.pass\n--测试收发数据：\n[root@db01 ~]# rsync -avz --password-file=/etc/rsync.pass /root/test rsync_backup@192.168.40.101::backup_file\nsending incremental file list\n\nsent 47 bytes  received 20 bytes  134.00 bytes/sec\ntotal size is 0  speedup is 0.00\n</code></pre>\n<p>方法 2：使用密码环境变量 RSYNC_PASSWORD</p>\n<pre><code>[root@db01 ~]# export RSYNC_PASSWORD='your passwd'\n--测试收发数据：\n[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file\nsending incremental file list\n\nsent 47 bytes  received 20 bytes  134.00 bytes/sec\ntotal size is 0  speedup is 0.00\n</code></pre>\n<h3 id=\"ursync企业级备份案例u\"><a class=\"anchor\" href=\"#ursync企业级备份案例u\">#</a> <u>Rsync 企业级备份案例</u></h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<p><strong>客户端需求</strong></p>\n<ul>\n<li>客户端每天凌晨 3 点备份 MySQL 至 /backup 下以 &quot;主机名_IP 地址_当前时间命名&quot; 的目录中</li>\n<li>客户端推送 /backup 目录下数据备份目录至 Rsync 备份服务器</li>\n<li>客户端只保留最近七天的备份数据，避免浪费磁盘空间</li>\n</ul>\n<p><strong>服务端需求</strong></p>\n<ul>\n<li>服务端部署 rsync 服务，用于接收用户的备份数据</li>\n<li>服务端每天校验客户端推送过来的数据是否完整，并将结果以邮件的方式发送给管理员</li>\n<li>服务端仅保留 6 个月的备份数据</li>\n</ul>\n<p><strong>注意</strong>：所有服务器的备份目录均为 /backup，所有脚本存放目录均为 /scripts。</p>\n<h4 id=\"1-服务端部署rsync服务\"><a class=\"anchor\" href=\"#1-服务端部署rsync服务\">#</a> <strong>1. 服务端部署 rsync 服务</strong></h4>\n<h5 id=\"11-关闭防火墙-selinux-2\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux-2\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync-2\"><a class=\"anchor\" href=\"#12-安装rsync-2\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件-2\"><a class=\"anchor\" href=\"#13-修改配置文件-2\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限-2\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限-2\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启-2\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启-2\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-rsync客户端-2\"><a class=\"anchor\" href=\"#2-rsync客户端-2\">#</a> 2. rsync 客户端</h4>\n<h5 id=\"21-安装rsync-2\"><a class=\"anchor\" href=\"#21-安装rsync-2\">#</a> 2.1 安装 rsync</h5>\n<pre><code>[root@db01 ~]# yum install nfs-utils -y\n</code></pre>\n<h5 id=\"22-测试客户端备份数据并推送至rsync服务器\"><a class=\"anchor\" href=\"#22-测试客户端备份数据并推送至rsync服务器\">#</a> 2.2 测试客户端备份数据并推送至 rsync 服务器</h5>\n<pre><code>[root@db01 ~]# export RSYNC_PASSWORD='your passwd'\n[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file\n</code></pre>\n<h5 id=\"23-客户端备份数据并推送至rsync服务器\"><a class=\"anchor\" href=\"#23-客户端备份数据并推送至rsync服务器\">#</a> <strong>2.3 客户端备份数据并推送至 rsync 服务器</strong></h5>\n<pre><code>[root@db01 ~]# mkdir /scripts\n[root@db01 ~]# cat /scripts/mysql_backup.sh \n#!/bin/bash\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\n\n#1、定义变量\nHost=$(hostname)\nIp=$(ifconfig ens192 | awk 'NR==2&#123;print $2&#125;')\nDate=$(date +%F)\nBackupDir=/backup/mysql\nDest=$&#123;BackupDir&#125;/$&#123;Host&#125;_$&#123;Ip&#125;_$&#123;Date&#125;\nFILE_NAME=mysql_backup_`date '+%Y%m%d%H%M%S'`;\nOLDBINLOG=/var/lib/mysql/oldbinlog\n\n#2、创建备份目录\nif [ ! -d $Dest ];then\n  mkdir -p $Dest\nfi\n\n#3、备份目录\n/usr/bin/mysqldump -u'root' -p'your passwd' nf_flms &gt; $Dest/nf-flms_$&#123;FILE_NAME&#125;.sql\ntar -czvf $Dest/$&#123;FILE_NAME&#125;.tar.gz $Dest/nf-flms_$&#123;FILE_NAME&#125;.sql\nrm -rf $Dest/*$&#123;FILE_NAME&#125;.sql\necho &quot;Your database backup successfully&quot;\n\n#4、校验\nmd5sum $Dest/* &gt;$Dest/backup_check_$Date\n\n#5、将备份目录推动到rsync服务端\nRsync_Ip=192.168.1.145\nRsync_user=rsync_backup\nRsync_Module=backup_mysql\nexport RSYNC_PASSWORD=your passwd\nrsync -avz $Dest $Rsync_user@$Rsync_Ip::$Rsync_Module\n\n#6、删除15天备份目录\nfind $Dest -type d -mtime +15 | xargs rm -rf\necho &quot;remove file  successfully&quot;\n\n[root@db01 ~]# chmod +x /scripts/etc_backup.sh\n[root@db01 ~]# crontab -e\n00 03 * * * /bin/bash /scripts/mysql_backup.sh &amp;&gt; /dev/null\n</code></pre>\n<h5 id=\"24-服务端校验数据并将结果以邮件发送给管理员\"><a class=\"anchor\" href=\"#24-服务端校验数据并将结果以邮件发送给管理员\">#</a> <strong>2.4 服务端校验数据并将结果以邮件发送给管理员</strong></h5>\n<h6 id=\"241-配置邮件服务\"><a class=\"anchor\" href=\"#241-配置邮件服务\">#</a> 2.4.1 配置邮件服务</h6>\n<pre><code>[root@backup ~]# yum -y install mailx\n[root@backup ~]# cat /etc/mail.rc      #最后一行插入\nset from=373370405@qq.com\nset smtp=smtps://smtp.qq.com:465\nset smtp-auth-user=373370405@qq.com\nset smtp-auth-password=**********   # 发件邮箱的授权码\nset smtp-auth=login\nset ssl-verify=ignore\nset nss-config-dir=/etc/pki/nssdb\n</code></pre>\n<h6 id=\"242-发送邮件测试\"><a class=\"anchor\" href=\"#242-发送邮件测试\">#</a> 2.4.2 发送邮件测试</h6>\n<pre><code>[root@backup ~]#  echo Hello World | mail -s test 373370405@qq.com &amp;&gt; /dev/null\n</code></pre>\n<h6 id=\"243-配置脚本校验数据并将结果发送给管理员\"><a class=\"anchor\" href=\"#243-配置脚本校验数据并将结果发送给管理员\">#</a> 2.4.3 配置脚本校验数据并将结果发送给管理员</h6>\n<pre><code>[root@backup mysql]# cat /scripts/check_backup.sh \n#!/bin/bash\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\n\n#1、定义变量\nPath=/backup/mysql\nDate=$(date +%F)\n\n#2、查看flag文件，并对对文件进行校验,然后将校验的结果保存至result_时间\nfind $Path -type f -name &quot;backup_check_$&#123;Date&#125;*&quot;|xargs md5sum -c &gt;$Path/result_$&#123;Date&#125;\n\n#3、将校验结果发送邮件给管理员\nmail -s &quot;Mysql Backup&quot; 373370405@qq.com &lt;$Path/result_$&#123;Date&#125; &amp;&gt; /dev/null\n\n#4、删除超过7天的校验结果文件，删除超过180天的备份数据文件\nfind $Path -type f -name &quot;result*&quot; -mtime +7 | xargs rm -rf\nfind $Path -type f -mtime +180 | xargs rm -rf\n</code></pre>\n<h6 id=\"244-写计划任务\"><a class=\"anchor\" href=\"#244-写计划任务\">#</a> <strong>2.4.4 写计划任务</strong></h6>\n<pre><code>[root@backup ~]# chmod +x /scripts/check_backup.sh \n[root@db01 ~]# crontab -e\n00 06 * * * /bin/bash /scripts/mysql_backup.sh &amp;&gt; /dev/null\n</code></pre>\n<h3 id=\"rsyncsersync实现数据实时同步\"><a class=\"anchor\" href=\"#rsyncsersync实现数据实时同步\">#</a> Rsync+sersync 实现数据实时同步</h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"1rsync服务端-2\"><a class=\"anchor\" href=\"#1rsync服务端-2\">#</a> 1.rsync 服务端</h4>\n<h5 id=\"11-关闭防火墙-selinux-3\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux-3\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync-3\"><a class=\"anchor\" href=\"#12-安装rsync-3\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件-3\"><a class=\"anchor\" href=\"#13-修改配置文件-3\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限-3\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限-3\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启-3\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启-3\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-客户端安装sersync\"><a class=\"anchor\" href=\"#2-客户端安装sersync\">#</a> 2. 客户端安装 sersync</h4>\n<p><strong>2.1 安装 sercync 依赖</strong></p>\n<pre><code>[root@nfs ~]# yum install -y inotify-tools rsync\n</code></pre>\n<p><strong>2.2 安装 sercync</strong></p>\n<pre><code>[root@nfs ~]# mkdir -p /soft\n[root@nfs ~]# cd /soft/\n[root@nfs ~]# wget https://down.whsir.com/downloads/sersync2.5.4_64bit_binary_stable_final.tar.gz\n[root@nfs soft]# tar -xf sersync2.5.4_64bit_binary_stable_final.tar.gz\n[root@nfs soft]# mv GNU-Linux-x86 /usr/local/sersync\n</code></pre>\n<h5 id=\"23-修改配置文件\"><a class=\"anchor\" href=\"#23-修改配置文件\">#</a> 2.3 <strong>修改配置文件</strong></h5>\n<pre><code>[root@nfs soft]# cd /usr/local/sersync/\n[root@nfs sersync]# cp confxml.xml confxml.xml.bak\n[root@nfs sersync]# vim confxml.xml\n...\n5    &lt;fileSystem xfs=&quot;true&quot;/&gt;    #第5行 false改为true\n13          &lt;delete start=&quot;true&quot;/&gt; #第13-20行 false改为true,#说明：监控以上变化推送\n14        &lt;createFolder start=&quot;true&quot;/&gt;\n15        &lt;createFile start=&quot;false&quot;/&gt;\n16        &lt;closeWrite start=&quot;true&quot;/&gt;\n17        &lt;moveFrom start=&quot;true&quot;/&gt;\n18        &lt;moveTo start=&quot;true&quot;/&gt;\n19        &lt;attrib start=&quot;true&quot;/&gt;\n20        &lt;modify start=&quot;true&quot;/&gt;\n24        &lt;localpath watch=&quot;/data&quot;&gt;      #监控的本地目录\n25             &lt;remote ip=&quot;192.168.1.145&quot; name=&quot;backup_file&quot;/&gt;  #rsync服务端IP和模块名backup_file\n30      &lt;commonParams params=&quot;-avz&quot;/&gt;  #rsync命令选项\n31      &lt;auth start=&quot;true&quot; users=&quot;rsync_backup&quot; passwordfile=&quot;/etc/rsync.passwd&quot;/&gt; #rsync认证信息\n...\n</code></pre>\n<h5 id=\"24-生成密码文件\"><a class=\"anchor\" href=\"#24-生成密码文件\">#</a> 2.4 生成密码文件</h5>\n<pre><code>[root@nfs sersync]# echo 'your passwd' &gt; /etc/rsync.passwd\n[root@nfs sersync]# chmod 600 /etc/rsync.passwd\n</code></pre>\n<h5 id=\"25-启动sersync\"><a class=\"anchor\" href=\"#25-启动sersync\">#</a> 2.5 启动 sersync</h5>\n<pre><code>[root@nfs sersync]# ln -s /usr/local/sersync/sersync2 /usr/bin/\n[root@nfs sersync]# sersync2 -dro /usr/local/sersync/confxml.xml     #针对配置文件confxml.xml启动sersync\n</code></pre>\n<p><strong>2.5 设置 sersync 开机自启</strong></p>\n<pre><code>[root@qzj_nfs sersync]# vim /etc/rc.d/rc.local   \n/usr/local/sersync/sersync2 -d -r -o  /usr/local/sersync/confxml.xml  #在最后添加一行\n[root@qzj_nfs sersync]# chmod +x /etc/rc.d/rc.local\n</code></pre>\n<p><strong>2.6 测试</strong></p>\n<p><em>在客户端 /data 目录增删改目录文件，rsync 服务端数据存放目录变化</em></p>\n<pre><code>[root@backup backup]# watch ls\n</code></pre>\n<p><strong>2.7 添加脚本监控 sersync 是否正常运行</strong></p>\n<pre><code>[root@nfs sersync]# cat /scripts/check_sersync.sh \n#!/bin/sh\nsersync=&quot;/usr/local/sersync/sersync2&quot;\nconfxml=&quot;/usr/local/sersync/confxml.xml&quot;\nstatus=$(ps aux |grep 'sersync2'|grep -v 'grep'|wc -l)\nif [ $status -eq 0 ];\nthen\n$sersync -d -r -o $confxml &amp;\nelse\nexit 0;\nfi\n\n[root@nfs sersync]# chmod +x /scripts/check_sersync.sh\n[root@nfs sersync]# crontab -l\n*/5 * * * * /usr/bin/sh /scripts/check_sersync.sh &amp;&gt; /dev/null\n</code></pre>\n<p><em><strong>补充： 多实例情况</strong></em><br />\n 1、配置多个 confxml.xml 文件（比如：www、bbs、blog.... 等等）<br />\n2、修改端口、同步路径、模块名称<br />\n 3、根据不同的需求同步对应的实例文件<br />\n /usr/local/sersync/sersync2 -dro /usr/local/sersync/www_confxml.xml<br />\n/usr/local/sersync/sersync2 -dro /usr/local/sersync/bbs_confxml.xml</p>\n",
            "tags": [
                "rsync"
            ]
        },
        {
            "id": "http://xuyong.cn/posts/3071070978.html",
            "url": "http://xuyong.cn/posts/3071070978.html",
            "title": "企业级私有仓库Harbor搭建",
            "date_published": "2025-03-30T08:17:00.000Z",
            "content_html": "<h3 id=\"企业级私有仓库harbor\"><a class=\"anchor\" href=\"#企业级私有仓库harbor\">#</a> 企业级私有仓库 Harbor</h3>\n<p>企业部署 Kuberetes 集群环境之后，我们就可以将原来在传统虚拟机上运行的业务，迁移到 kubernetes 上，让 Kubernetes 通过容器的方式来管理。而一旦我们需要将传统业务使用容器的方式运行起来，就需要构建很多镜像，那么这些镜像就需要有一个专门的位置存储起来，为我们提供镜像上传和镜像下载等功能。但我们不能使用阿里云或者 Dockerhub 等仓库，首先拉取速度比较慢，其次镜像的安全性无法保证，所以就需要部署一个私有的镜像仓库来管理这些容器镜像。同时该仓库还需要提供高可用功能，确保随时都能上传和下载可用的容器镜像。</p>\n<h4 id=\"1-关闭防火墙-selinux-环境配置\"><a class=\"anchor\" href=\"#1-关闭防火墙-selinux-环境配置\">#</a> 1、关闭防火墙、Selinux、环境配置</h4>\n<pre><code>[root@harbor ~]# sudo mkdir -p /etc/docker\n[root@harbor ~]# hostnamectl set-hostname harbor\n[root@harbor ~]# systemctl stop firewalld\n[root@harbor ~]# systemctl disable firewalld\n[root@harbor ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@harbor ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate -y\n[root@harbor ~]# yum update -y\n[root@harbor ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h4 id=\"2-docker安装\"><a class=\"anchor\" href=\"#2-docker安装\">#</a> 2、Docker 安装</h4>\n<pre><code>[root@harbor ~]# yum install -y yum-utils device-mapper-persistent-data lvm2\n[root@harbor ~]# curl -o /etc/yum.repos.d/docker-ce.repo  https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n[root@harbor ~]# yum list docker-ce --showduplicates |sort -r \n[root@harbor ~]# yum install docker-ce docker-compose -y\n</code></pre>\n<h4 id=\"3-配置docker加速\"><a class=\"anchor\" href=\"#3-配置docker加速\">#</a> 3、配置 Docker 加速</h4>\n<pre><code>[root@harbor ~]# sudo mkdir -p /etc/docker\n[root@harbor ~]# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n[root@harbor ~]# systemctl enable docker --now\n</code></pre>\n<h4 id=\"4-安装harbor\"><a class=\"anchor\" href=\"#4-安装harbor\">#</a> 4、安装 Harbor</h4>\n<pre><code>[root@harbor ~]# cd /soft/\n[root@harbor ~]# wget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz\n[root@harbor soft]# tar xf harbor-offline-installer-v2.6.1.tgz\n[root@harbor soft]# cd harbor\n[root@harbor harbor]# vim harbor.yml\nhostname: 192.168.1.134\n...\n#https:\n#  # https port for harbor, default is 443\n#  port: 443\n#  # The path of cert and key files for nginx\n#  certificate: /your/certificate/path\n#  private_key: /your/private/key/path\n...\nharbor_admin_password: Harbor12345\n[root@harbor harbor]#  ./install.sh\n</code></pre>\n<h4 id=\"5-配置nginx负载均衡调度\"><a class=\"anchor\" href=\"#5-配置nginx负载均衡调度\">#</a> 5、配置 Nginx 负载均衡调度</h4>\n<pre><code>[root@lb ~]# vim s.hmallleasing.com.conf\nserver &#123;\n    listen 443 ssl;\n    server_name harbor.hmallleasing.com;\n    client_max_body_size 1G; \n    ssl_prefer_server_ciphers on;\n    ssl_certificate  /etc/nginx/sslkey/_.hmallleasing.com_chain.crt;\n    ssl_certificate_key  /etc/nginx/sslkey/_.hmallleasing.com_key.key;\n    location / &#123;\n        proxy_pass http://192.168.1.134;\n#      include proxy_params;\n#        proxy_set_header Host $http_host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        proxy_connect_timeout 30;\n        proxy_send_timeout 60;\n        proxy_read_timeout 60;\n        \n        proxy_buffering on;\n        proxy_buffer_size 32k;\n        proxy_buffers 4 128k;\n        proxy_temp_file_write_size 10240k;\t\t\n        proxy_max_temp_file_size 10240k;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n</code></pre>\n<h4 id=\"6-推送镜像至harbor\"><a class=\"anchor\" href=\"#6-推送镜像至harbor\">#</a> 6、推送镜像至 Harbor</h4>\n<pre><code>[root@harbor harbor]# docker tag beae173ccac6 harbor.hmallleasing.com/ops/busybox.v1\n[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1\n[root@harbor harbor]# docker login harbor.hmallleasing.com\n[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1\n</code></pre>\n<h4 id=\"7-harbor停止与启动\"><a class=\"anchor\" href=\"#7-harbor停止与启动\">#</a> 7、Harbor 停止与启动</h4>\n<pre><code>#停用Harbor\n[root@harbor harbor]# pwd\n/soft/harbor\n[root@harbor harbor]# docker-compose stop\n #启动Harbor\n[root@harbor harbor]# docker-compose up -d\n[root@harbor harbor]# docker-compose start\n</code></pre>\n",
            "tags": [
                "Harbor"
            ]
        }
    ]
}