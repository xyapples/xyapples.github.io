{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://ixuyong.cn",
    "items": [
        {
            "id": "http://ixuyong.cn/posts/170066797.html",
            "url": "http://ixuyong.cn/posts/170066797.html",
            "title": "消费租赁项目Kubernetes基于ELK日志分析与实践",
            "date_published": "2025-05-25T06:35:21.000Z",
            "content_html": "<h3 id=\"消费租赁项目kubernetes基于elk日志分析与实践\"><a class=\"anchor\" href=\"#消费租赁项目kubernetes基于elk日志分析与实践\">#</a> 消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</h3>\n<h6 id=\"snipaste_2025-05-25_13-43-46jpg\"><a class=\"anchor\" href=\"#snipaste_2025-05-25_13-43-46jpg\">#</a> <img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg\" alt=\"Snipaste_2025-05-25_13-43-46.jpg\" /></h6>\n<h4 id=\"1-elk创建namespace和secrets\"><a class=\"anchor\" href=\"#1-elk创建namespace和secrets\">#</a> 1. ELK 创建 Namespace 和 Secrets</h4>\n<pre><code># kubectl create ns logging\n# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd\n</code></pre>\n<h4 id=\"2-交付zookeeper集群至k8s\"><a class=\"anchor\" href=\"#2-交付zookeeper集群至k8s\">#</a> 2. 交付 Zookeeper 集群至 K8S</h4>\n<h5 id=\"21-制作zk集群镜像\"><a class=\"anchor\" href=\"#21-制作zk集群镜像\">#</a> 2.1 制作 ZK 集群镜像</h5>\n<h6 id=\"211-dockerfile\"><a class=\"anchor\" href=\"#211-dockerfile\">#</a> 2.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、拷贝Zookeeper压缩包和配置文件\nENV VERSION=3.8.4\nADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf\n\n# 2、对Zookeeper文件夹名称重新命名\nRUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper\n\n# 3、拷贝eentrpoint的启动脚本文件\nADD ./entrypoint.sh /entrypoint.sh\n\n# 4、暴露Zookeeper端口\nEXPOSE 2181 2888 3888\n\n# 5、执行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"212-zoocfg\"><a class=\"anchor\" href=\"#212-zoocfg\">#</a> 2.1.2 zoo.cfg</h6>\n<pre><code># cat zoo.cfg \n# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。\ntickTime=&#123;ZOOK_TICKTIME&#125;\n\n# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime\ninitLimit=&#123;ZOOK_INIT_LIMIT&#125;\n\n# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime\nsyncLimit=&#123;ZOOK_SYNC_LIMIT&#125;\n \n# 数据保存目录\ndataDir=&#123;ZOOK_DATA_DIR&#125;\n\n# 日志保存目录\ndataLogDir=&#123;ZOOK_LOG_DIR&#125;\n\n# 客户端连接端口\nclientPort=&#123;ZOOK_CLIENT_PORT&#125;\n\n# 客户端最大连接数。# 根据自己实际情况设置，默认为60个\nmaxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;\n\n# 客户端获取 zookeeper 服务的当前状态及相关信息\n4lw.commands.whitelist=*\n\n# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口\n</code></pre>\n<h6 id=\"213-entrypoint\"><a class=\"anchor\" href=\"#213-entrypoint\">#</a> 2.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n#设定变量\nZOOK_BIN_DIR=/zookeeper/bin\nZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg\n\n# 2、对配置文件中的字符串进行变量替换\nsed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;\n\n# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来\nfor server in $&#123;ZOOK_SERVERS&#125;\ndo\n\techo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;\ndone\n\n# 4、在datadir目录中创建myid的文件，并填入对应的编号\nZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))\necho $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid\n\n#5、前台运行Zookeeper\ncd $&#123;ZOOK_BIN_DIR&#125;\n./zkServer.sh start-foreground\n</code></pre>\n<h6 id=\"214-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#214-构建镜像并推送仓库\">#</a> 2.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .\n# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4\n</code></pre>\n<h5 id=\"22-迁移zookeeper至k8s\"><a class=\"anchor\" href=\"#22-迁移zookeeper至k8s\">#</a> 2.2  迁移 zookeeper 至 K8S</h5>\n<h6 id=\"221-zookeeper-headless\"><a class=\"anchor\" href=\"#221-zookeeper-headless\">#</a> 2.2.1 zookeeper-headless</h6>\n<pre><code># cat 01-zookeeper-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: zookeeper\n  ports:\n  - name: client\n    port: 2181\n    targetPort: 2181\n  - name: leader-follwer\n    port: 2888\n    targetPort: 2888\n  - name: selection\n    port: 3888\n    targetPort: 3888\n</code></pre>\n<h6 id=\"222-zookeeper-sts\"><a class=\"anchor\" href=\"#222-zookeeper-sts\">#</a> 2.2.2 zookeeper-sts</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper               \n  namespace: logging\nspec:\n  serviceName: &quot;zookeeper-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;zookeeper&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: zookeeper\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 2181\n        - name: leader-follwer\n          containerPort: 2888\n        - name: selection\n          containerPort: 3888\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: data\n        - name: data\n          mountPath: /logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"223-更新资源清单\"><a class=\"anchor\" href=\"#223-更新资源清单\">#</a> 2.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml \n[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml\n[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging\nNAME          READY   STATUS    RESTARTS   AGE\nzookeeper-0   1/1     Running   0          17m\nzookeeper-1   1/1     Running   0          14m\nzookeeper-2   1/1     Running   0          11m\n</code></pre>\n<h6 id=\"224-检查zookeeper集群状态\"><a class=\"anchor\" href=\"#224-检查zookeeper集群状态\">#</a> 2.2.4 检查 zookeeper 集群状态</h6>\n<pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: leader\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\n</code></pre>\n<h6 id=\"225-连接zookeeper集群\"><a class=\"anchor\" href=\"#225-连接zookeeper集群\">#</a> 2.2.5 连接 Zookeeper 集群</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh\n# /zookeeper/bin/zkCli.sh -server zookeeper-svc\n[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu\nCreated /hello\n[zk: zookeeper-svc(CONNECTED) 1] get /hello\noldxu\n</code></pre>\n<h4 id=\"3-交付kafka集群至k8s\"><a class=\"anchor\" href=\"#3-交付kafka集群至k8s\">#</a> 3. 交付 Kafka 集群至 K8S</h4>\n<h5 id=\"31-制作kafka集群镜像\"><a class=\"anchor\" href=\"#31-制作kafka集群镜像\">#</a> 3.1 制作 Kafka 集群镜像</h5>\n<h6 id=\"311-dockerfile\"><a class=\"anchor\" href=\"#311-dockerfile\">#</a> 3.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=2.12-2.2.0\nADD ./kafka_$&#123;VERSION&#125;.tgz /\nADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties\n\n# 3、修改kafka的名称\nRUN mv /kafka_$&#123;VERSION&#125; /kafka\n\n# 4、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 5、暴露kafka端口 9999是jmx的端口\nEXPOSE 9092 9999\n\n# 6、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"312-serverproperties\"><a class=\"anchor\" href=\"#312-serverproperties\">#</a> 3.1.2 server.properties</h6>\n<pre><code class=\"language-'\"># cat server.properties \n############################# Server Basics ############################# \n# broker的id，值为整数，且必须唯一，在一个集群中不能重复\nbroker.id=&#123;BROKER_ID&#125;\n\n############################# Socket Server Settings ############################# \n# kafka监听端口，默认9092\nlisteners=PLAINTEXT://&#123;LISTENERS&#125;:9092\n\n# 处理网络请求的线程数量，默认为3个\nnum.network.threads=3\n\n# 执行磁盘IO操作的线程数量，默认为8个 \nnum.io.threads=8\n\n# socket服务发送数据的缓冲区大小，默认100KB\nsocket.send.buffer.bytes=102400\n\n# socket服务接受数据的缓冲区大小，默认100KB\nsocket.receive.buffer.bytes=102400\n\n# socket服务所能接受的一个请求的最大大小，默认为100M\nsocket.request.max.bytes=104857600\n\n############################# Log Basics ############################# \n# kafka存储消息数据的目录\nlog.dirs=&#123;KAFKA_DATA_DIR&#125;\n\n# 每个topic默认的partition\nnum.partitions=1\n\n# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。\ndefault.replication.factor=3\n\n# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量\nnum.recovery.threads.per.data.dir=1\n\n############################# Log Flush Policy ############################# \n# 消息刷新到磁盘中的消息条数阈值\nlog.flush.interval.messages=10000\n\n# 消息刷新到磁盘中的最大时间间隔,1s\nlog.flush.interval.ms=1000\n\n############################# Log Retention Policy ############################# \n# 日志保留小时数，超时会自动删除，默认为7天\nlog.retention.hours=168\n\n# 日志保留大小，超出大小会自动删除，默认为1G\n#log.retention.bytes=1073741824\n\n# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件\nlog.segment.bytes=1073741824\n\n# 每隔多长时间检测数据是否达到删除条件,300s\nlog.retention.check.interval.ms=300000\n\n############################# Zookeeper ############################# \n# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开\nzookeeper.connect=&#123;ZOOK_SERVERS&#125;\n\n# 连接zookeeper的超时时间,6s\nzookeeper.connection.timeout.ms=6000\n</code></pre>\n<h6 id=\"313-entrypoint\"><a class=\"anchor\" href=\"#313-entrypoint\">#</a> 3.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 变量\nKAFKA_DIR=/kafka\nKAFKA_CONF=/kafka/config/server.properties\n\n# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一\nBROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))\nLISTENERS=$(hostname -i)\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;\n\n# 3、启动Kafka\ncd $&#123;KAFKA_DIR&#125;/bin\nsed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh\n./kafka-server-start.sh ../config/server.properties\n</code></pre>\n<h6 id=\"314-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#314-构建镜像并推送仓库\">#</a> 3.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2\n</code></pre>\n<h5 id=\"32-迁移kafka集群至k8s\"><a class=\"anchor\" href=\"#32-迁移kafka集群至k8s\">#</a> 3.2 迁移 Kafka 集群至 K8S</h5>\n<h6 id=\"321-kafka-headless\"><a class=\"anchor\" href=\"#321-kafka-headless\">#</a> 3.2.1 kafka-headless</h6>\n<pre><code># cat 01-kafka-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: kafka\n  ports:\n  - name: client\n    port: 9092\n    targetPort: 9092\n  - name: jmx\n    port: 9999\n    targetPort: 9999\n</code></pre>\n<h6 id=\"322-kafka-sts\"><a class=\"anchor\" href=\"#322-kafka-sts\">#</a> 3.2.2 kafka-sts</h6>\n<pre><code># cat 02-kafka-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  namespace: logging\nspec:\n  serviceName: &quot;kafka-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kafka\n  template:\n    metadata:\n      labels:\n        app: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;kafka&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kafka\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 9092\n        - name: jmxport\n          containerPort: 9999\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"323-更新资源清单\"><a class=\"anchor\" href=\"#323-更新资源清单\">#</a> 3.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml \n[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml\n[root@k8s-master01 02-kafka]# kubectl get pods -n logging \nNAME          READY   STATUS    RESTARTS       AGE\nkafka-0       1/1     Running   0              5m49s\nkafka-1       1/1     Running   0              4m43s\nkafka-2       1/1     Running   0              3m40s\n</code></pre>\n<h6 id=\"324-检查kafka集群\"><a class=\"anchor\" href=\"#324-检查kafka集群\">#</a> 3.2.4 检查 Kafka 集群</h6>\n<pre><code>1.创建一个topic\nroot@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu\n\n2.模拟消息发布\nroot@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu\n&gt;hello kubernetes\n&gt;hello world\n\n3.模拟消息订阅\nroot@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning\nhello kubernetes\nhello world\n</code></pre>\n",
            "tags": [
                "ELKStack"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/626047790.html",
            "url": "http://ixuyong.cn/posts/626047790.html",
            "title": "消费租赁系统微服务应用交付实践",
            "date_published": "2025-05-18T13:42:46.000Z",
            "content_html": "<h3 id=\"消费租赁系统微服务应用交付实践\"><a class=\"anchor\" href=\"#消费租赁系统微服务应用交付实践\">#</a> 消费租赁系统微服务应用交付实践</h3>\n<h4 id=\"一-部署中间件\"><a class=\"anchor\" href=\"#一-部署中间件\">#</a> 一、部署中间件</h4>\n<h5 id=\"11-部署mysql\"><a class=\"anchor\" href=\"#11-部署mysql\">#</a> 1.1 部署 MySQL</h5>\n<h6 id=\"111-mysql-configmap\"><a class=\"anchor\" href=\"#111-mysql-configmap\">#</a> 1.1.1 MySQL-ConfigMap</h6>\n<pre><code>[root@k8s-master01 01-nf-flms-mysql]# cat 01-mysql-cm.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql-cm\n  namespace: prod\ndata:\n  my.cnf: |-\n    [mysqld]\n    #performance setttings\n    lock_wait_timeout = 3600\n    open_files_limit = 65535\n    back_log = 1024\n    max_connections = 1024\n    max_connect_errors = 1000000\n    table_open_cache = 1024\n    table_definition_cache = 1024\n    thread_stack = 512K\n    sort_buffer_size = 4M\n    join_buffer_size = 4M\n    read_buffer_size = 8M\n    read_rnd_buffer_size = 4M\n    bulk_insert_buffer_size = 64M\n    thread_cache_size = 768\n    interactive_timeout = 600\n    wait_timeout = 600\n    tmp_table_size = 32M\n    max_heap_table_size = 32M\n    max_allowed_packet = 128M\n</code></pre>\n<h6 id=\"112-mysql-secret\"><a class=\"anchor\" href=\"#112-mysql-secret\">#</a> <strong>1.1.2 MySQL-Secret</strong></h6>\n<pre><code>[root@k8s-master01 01-nf-flms-mysql]# cat 02-mysql-secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysql-secret\n  namespace: prod\nstringData:\n  MYSQL_ROOT_PASSWORD: Superman*2023\ntype: Opaque\n</code></pre>\n<h6 id=\"113-mysql-statefulset\"><a class=\"anchor\" href=\"#113-mysql-statefulset\">#</a> <strong>1.1.3 MySQL-StatefulSet</strong></h6>\n<pre><code># cat 03-mysql-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql-nf-flms\n  namespace: prod\nspec:\n  serviceName: &quot;mysql-nf-flms-svc&quot;\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql\n      role: nf-flms\n  template:\n    metadata:\n      labels:\n        app: mysql\n        role: nf-flms\n    spec:\n      containers:\n      - name: db\n        image: mysql:8.0\n        args:\n        - &quot;--character-set-server=utf8&quot;\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: MYSQL_ROOT_PASSWORD\n        - name: MYSQL_DATABASE      #数据库名称\n          value: nf-flms        \n        ports:\n        - name: tcp-3306\n          containerPort: 3306\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4000Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql/\n        - name: config\n          mountPath: /etc/mysql/conf.d/my.cnf\n          subPath: my.cnf\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: config\n        configMap:\n          name: mysql-cm\n          items:\n            - key: my.cnf\n              path: my.cnf\n          defaultMode: 420\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      storageClassName: &quot;nfs-storage&quot;\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"114-mysql-service\"><a class=\"anchor\" href=\"#114-mysql-service\">#</a> <strong>1.1.4 MySQL Service</strong></h6>\n<pre><code># cat 04-mysql-nf-flms-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-nf-flms-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n    role: nf-flms\n  ports:\n  - name: tcp-mysql-svc\n    protocol: TCP\n    port: 3306\n    targetPort: 3306\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: mysql-nf-flms-svc-balance\n  namespace: prod \nspec:\n  selector:\n    app: mysql\n    role: nf-flms\n  ports:\n  - name: tcp-mysql-balance\n    protocol: TCP\n    port: 3306\n    targetPort: 3306\n    nodePort: 32206\n  type: NodePort\n</code></pre>\n<h6 id=\"115-更新资源清单\"><a class=\"anchor\" href=\"#115-更新资源清单\">#</a> <strong>1.1.5 更新资源清单</strong></h6>\n<pre><code>[root@k8s-master01 01-nf-flms-mysql]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 01-nf-flms-mysql]# kubectl create ns prod\n[root@k8s-master01 01-nf-flms-mysql]# kubectl apply -f .\n</code></pre>\n<h6 id=\"116-导入数据库\"><a class=\"anchor\" href=\"#116-导入数据库\">#</a> <strong>1.1.6 导入数据库</strong></h6>\n<pre><code>[root@k8s-master01 03-nacos]# dig @10.96.0.10 mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local +short\n172.16.85.213\n[root@k8s-master01 01-nf-flms-mysql]# mysql -h 172.16.85.213 -uroot -p&quot;Superman*2023&quot; -B nf_flms &lt; /root/backup/qzj_db01_192.168.1.143_2023-03-30/nf-flms_mysql_backup_20230330030001.sql\n</code></pre>\n<h5 id=\"12-部署redis-single\"><a class=\"anchor\" href=\"#12-部署redis-single\">#</a> 1.2 部署 Redis-single</h5>\n<h6 id=\"121-redis-configmap\"><a class=\"anchor\" href=\"#121-redis-configmap\">#</a> 1.2.1 Redis-ConfigMap</h6>\n<pre><code># cat 01-redis-cm.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-conf\n  namespace: prod\ndata:\n  redis.conf: |\n    bind 0.0.0.0\n    appendonly yes\n    protected-mode no\n    dir /data\n    port 6379\n    requirepass Superman*2023\n</code></pre>\n<h6 id=\"122-redis-statefulset\"><a class=\"anchor\" href=\"#122-redis-statefulset\">#</a> 1.2.2 Redis-StatefulSet</h6>\n<pre><code># cat 02-redis-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis\n  namespace: prod\nspec:\n  serviceName: redis-svc\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:6.2.7\n        command:\n        - &quot;redis-server&quot;\n        args:\n        - &quot;/etc/redis/redis.conf&quot;\n        ports:\n        - name: redis-6379\n          containerPort: 6379\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 6379\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 6379\n        volumeMounts:\n        - name: config\n          mountPath: /etc/redis\n        - name: data\n          mountPath: /data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        resources:\n          limits:\n            cpu: '2'\n            memory: 4000Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n      volumes:\n      - name: config\n        configMap:\n          name: redis-conf\n          items:\n          - key: redis.conf\n            path: redis.conf\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 2Gi\n</code></pre>\n<h6 id=\"123-redis-service\"><a class=\"anchor\" href=\"#123-redis-service\">#</a> 1.2.3 Redis-Service</h6>\n<pre><code># cat 03-redis-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-svc\n  namespace: prod\n  labels:\n    app: redis\nspec:\n  ports:\n    - name: redis-6379\n      protocol: TCP\n      port: 6379\n      targetPort: 6379\n  selector:\n    app: redis\n  clusterIP: None\n</code></pre>\n<h6 id=\"124-更新资源清单\"><a class=\"anchor\" href=\"#124-更新资源清单\">#</a> 1.2.4 更新资源清单</h6>\n<pre><code>[root@k8s-master01 02-redis]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 02-redis]# kubectl apply -f .\n</code></pre>\n<h5 id=\"14-部署nacos集群\"><a class=\"anchor\" href=\"#14-部署nacos集群\">#</a> 1.4 部署 Nacos 集群</h5>\n<h6 id=\"141-部署nacos-mysql\"><a class=\"anchor\" href=\"#141-部署nacos-mysql\">#</a> 1.4.1 部署 Nacos-MySQL</h6>\n<pre><code># cat 01-mysql-nacos-sts-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-nacos-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n    role: nacos\n  ports:\n  - port: 3306\n    targetPort: 3306\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: mysql-nacos-balance\n  namespace: prod\nspec:\n  selector:\n    app: mysql\n    role: nacos\n  ports:\n  - name: tcp-mysql-balance\n    protocol: TCP\n    port: 3306\n    targetPort: 3306\n    nodePort: 31106\n  type: NodePort\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql-nacos\n  namespace: prod\nspec:\n  serviceName: &quot;mysql-nacos-svc&quot;\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql\n      role: nacos\n  template:\n    metadata:\n      labels:\n        app: mysql\n        role: nacos\n    spec:\n      containers:\n      - name: db\n        image: mysql:8.0\n        args:\n        - &quot;--character-set-server=utf8&quot;\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: Superman*2023\n        - name: MYSQL_DATABASE    #nacos库名称\n          value: nacos\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: '2'\n            memory: 4000Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql/\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"142-导入数据库\"><a class=\"anchor\" href=\"#142-导入数据库\">#</a> <strong>1.4.2 导入数据库</strong></h6>\n<p>nacos 下载地址：<a href=\"https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK\">https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK</a></p>\n<pre><code>[root@k8s-master01 03-nacos]#  sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 03-nacos]# kubectl apply -f 01-mysql-nacos-sts-svc.yaml\n[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-nacos-svc.prod.svc.cluster.local +short\n172.16.85.255\n[root@k8s-master01 03-nacos]# mysql -h 172.16.85.255  -uroot -p&quot;Superman*2023&quot; -B nacos &lt; nacos/conf/mysql-schema.sql\n</code></pre>\n<h6 id=\"143-部署nacos-configmap\"><a class=\"anchor\" href=\"#143-部署nacos-configmap\">#</a> 1.4.3 部署 Nacos-ConfigMap</h6>\n<pre><code># cat 02-nacos-configmap.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nacos-cm\n  namespace: prod\ndata:\n  mysql.host: &quot;mysql-nacos-svc.prod.svc.cluster.local&quot;\n  mysql.db.name: &quot;nacos&quot;   #nacos数据库名称\n  mysql.port: &quot;3306&quot;\n  mysql.user: &quot;root&quot;    #nacos数据库用户名\n  mysql.password: &quot;Superman*2023&quot;   #nacos数据库密码\n</code></pre>\n<h6 id=\"144-部署nacos-service-statefulset\"><a class=\"anchor\" href=\"#144-部署nacos-service-statefulset\">#</a> 1.4.4 部署 Nacos-Service-StatefulSet</h6>\n<pre><code># cat 03-nacos-sts-deploy-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: nacos-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: nacos\n  ports:\n  - name: server\n    port: 8848\n    targetPort: 8848\n  - name: client-rpc\n    port: 9848\n    targetPort: 9848\n  - name: raft-rpc\n    port: 9849\n    targetPort: 9849\n  - name: old-raft-rpc\n    port: 7848\n    targetPort: 7848\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nacos\n  namespace: prod\nspec:\n  serviceName: &quot;nacos-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nacos\n  template:\n    metadata:\n      labels:\n        app: nacos\n    spec:\n      affinity:                                                 # 避免Pod运行到同一个节点上了\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: app\n                    operator: In\n                    values: [&quot;nacos&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;  \n      initContainers:\n      - name: peer-finder-plugin-install\n        image: nacos/nacos-peer-finder-plugin:1.1\n        imagePullPolicy: IfNotPresent \n        volumeMounts:\n          - name: data\n            mountPath: /home/nacos/plugins/peer-finder\n            subPath: peer-finder\n      containers:\n      - name: nacos\n        image: nacos/nacos-server:v2.4.3\n        resources:\n          limits:\n            cpu: '2'\n            memory: 4Gi\n          requests:\n            cpu: &quot;100m&quot;\n            memory: &quot;1Gi&quot;\n        ports:\n        - name: client-port\n          containerPort: 8848\n        - name: client-rpc\n          containerPort: 9848\n        - name: raft-rpc\n          containerPort: 9849\n        - name: old-raft-rpc\n          containerPort: 7848\n        env:\n        - name: NACOS_REPLICAS\n          value: &quot;3&quot;\n        - name: SERVICE_NAME\n          value: &quot;nacos-svc&quot;\n        - name: DOMAIN_NAME\n          value: &quot;cluster.local&quot;\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: MYSQL_SERVICE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.host\n        - name: MYSQL_SERVICE_DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.db.name\n        - name: MYSQL_SERVICE_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.port\n        - name: MYSQL_SERVICE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.user\n        - name: MYSQL_SERVICE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.password\n        - name: SPRING_DATASOURCE_PLATFORM\n          value: &quot;mysql&quot;\n        - name: NACOS_SERVER_PORT\n          value: &quot;8848&quot;\n        - name: NACOS_APPLICATION_PORT\n          value: &quot;8848&quot;\n        - name: PREFER_HOST_MODE\n          value: &quot;hostname&quot;\n        - name: NACOS_AUTH_ENABLE\n          value: &quot;true&quot;\n        - name: NACOS_AUTH_IDENTITY_KEY\n          value: &quot;nacosAuthKey&quot;\n        - name: NACOS_AUTH_IDENTITY_VALUE\n          value: &quot;nacosSecurtyValue&quot;\n        - name: NACOS_AUTH_TOKEN\n          value: &quot;SecretKey012345678901234567890123456789012345678901234567890123456789&quot;\n        - name: NACOS_AUTH_TOKEN_EXPIRE_SECONDS\n          value: &quot;18000&quot;\n        volumeMounts:\n        - name: data\n          mountPath: /home/nacos/plugins/peer-finder\n          subPath: peer-finder\n        - name: data\n          mountPath: /home/nacos/data\n          subPath: data\n        - name: data\n          mountPath: /home/nacos/logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        storageClassName: &quot;nfs-storage&quot;\n        accessModes: [&quot;ReadWriteMany&quot;]\n        resources:\n          requests:\n            storage: 5Gi\n</code></pre>\n<h6 id=\"145-部署nacos-ingress\"><a class=\"anchor\" href=\"#145-部署nacos-ingress\">#</a> 1.4.5 部署 Nacos-Ingress</h6>\n<pre><code># cat 04-nacos-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nacos-ingress\n  namespace: prod\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: nacos.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nacos-svc\n            port:\n              number: 8848\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h6 id=\"146-更新资源清单\"><a class=\"anchor\" href=\"#146-更新资源清单\">#</a> <strong>1.4.6 更新资源清单</strong></h6>\n<pre><code>[root@k8s-master01 03-nacos]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 03-nacos]# kubectl apply -f .\n\n#检查cluster是否一致\n[root@k8s-master01 03-nacos]# for i in &#123;0..2&#125;; do echo nacos-$i; kubectl exec nacos-$i -c nacos -n prod -- cat conf/cluster.conf; donenacos-0\n#2025-05-21T10:43:12.668\nnacos-0.nacos-svc.prod.svc.cluster.local:8848\nnacos-1.nacos-svc.prod.svc.cluster.local:8848\nnacos-2.nacos-svc.prod.svc.cluster.local:8848\nnacos-1\n#2025-05-21T10:43:14.879\nnacos-0.nacos-svc.prod.svc.cluster.local:8848\nnacos-1.nacos-svc.prod.svc.cluster.local:8848\nnacos-2.nacos-svc.prod.svc.cluster.local:8848\nnacos-2\n#2025-05-21T10:43:17.299\nnacos-0.nacos-svc.prod.svc.cluster.local:8848\nnacos-1.nacos-svc.prod.svc.cluster.local:8848\nnacos-2.nacos-svc.prod.svc.cluster.local:8848\n</code></pre>\n<h6 id=\"147-web访问nacos\"><a class=\"anchor\" href=\"#147-web访问nacos\">#</a> <strong>1.4.7 Web 访问 nacos</strong></h6>\n<pre><code>Url：http://nacos.hmallleasing.com/nacos \nUser: nacos\nPasswd: nacos \n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/iNiUErY.jpeg\" alt=\"Snipaste_2025-05-18_21-13-44.jpg\" /></p>\n<h5 id=\"15-部署xxl-job\"><a class=\"anchor\" href=\"#15-部署xxl-job\">#</a> 1.5 部署 xxl-job</h5>\n<h6 id=\"151-部署xxl-job-mysql\"><a class=\"anchor\" href=\"#151-部署xxl-job-mysql\">#</a> 1.5.1 部署 xxl-job-MySQL</h6>\n<pre><code># cat 01-mysql-xxljob-sts-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-xxljob-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n    role: xxljob\n  ports:\n    - name: tcp-mysql-svc\n      protocol: TCP\n      port: 3306\n      targetPort: 3306\n---\napiVersion: v1\nkind: Service\napiVersion: v1\nmetadata:\n  name: mysql-xxljob-external\n  namespace: prod\nspec:\n  ports:\n    - name: tcp-mysql-external\n      protocol: TCP\n      port: 3306\n      targetPort: 3306\n      nodePort: 31206\n  selector:\n    app: mysql\n    role: xxljob\n  type: NodePort\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql-xxljob\n  namespace: prod\nspec:\n  serviceName: &quot;mysql-xxljob-svc&quot;\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql\n      role: xxljob\n  template:\n    metadata:\n      labels:\n        app: mysql\n        role: xxljob\n    spec:\n      containers:\n      - name: db\n        image: mysql:8.0\n        args:\n        - &quot;--character-set-server=utf8&quot;\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: Superman*2023\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4000Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql/\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"152-导入数据库\"><a class=\"anchor\" href=\"#152-导入数据库\">#</a> <strong>1.5.2 导入数据库</strong></h6>\n<p>xxljob 表结构下载地址：<a href=\"https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db\">https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db</a></p>\n<pre><code>[root@k8s-master01 05-xxl-job]# kubectl apply -f 01-mysql-xxljob-sts-svc.yaml\n[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-xxljob-svc.prod.svc.cluster.local +short\n172.16.85.250\n[root@k8s-master01 05-xxl-job]# mysql -h 172.16.85.250  -uroot -p&quot;Superman*2023&quot;  &lt; tables_xxl_job.sql\n</code></pre>\n<h6 id=\"153-部署xxl-job-service-deployment\"><a class=\"anchor\" href=\"#153-部署xxl-job-service-deployment\">#</a> 1.5.3 部署 xxl-job-Service-Deployment</h6>\n<pre><code># cat 02-xxljob-deploy-svc.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: xxl-job\n  namespace: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: xxl-job\n  template:\n    metadata:\n      labels:\n        app: xxl-job\n    spec:\n      containers:\n      - image: xuxueli/xxl-job-admin:3.1.0\n        name: xxl-job\n        ports:\n        - containerPort: 8080\n        env:\n        - name: PARAMS\n          value: &quot;--spring.datasource.url=jdbc:mysql://mysql-xxljob-svc.prod.svc.cluster.local:3306/xxl_job?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;serverTimezone=Asia/Shanghai --spring.datasource.username=root --spring.datasource.password=Superman*2023&quot;\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        resources:\n          limits:\n            cpu: '1'\n            memory: 2000Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: xxljob-svc\n  namespace: prod\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: xxl-job\n</code></pre>\n<h6 id=\"154-部署xxl-job-service\"><a class=\"anchor\" href=\"#154-部署xxl-job-service\">#</a> 1.5.4 部署 xxl-job-service</h6>\n<pre><code>[root@k8s-master01 05-xxl-job]# cat 04-xxljob-external.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: xxljob-balancer\n  namespace: prod\nspec:\n  type: NodePort\n  ports:\n    - name: xxljob-balancer\n      protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: xxl-job\n</code></pre>\n<h6 id=\"155-部署xxl-job-ingress\"><a class=\"anchor\" href=\"#155-部署xxl-job-ingress\">#</a> 1.5.5 部署 xxl-job-Ingress</h6>\n<pre><code>[root@k8s-master01 05-xxl-job]# cat 03-xxljob-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: xxljob-ingress\n  namespace: prod\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: xxljob.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: xxljob-svc\n            port:\n              number: 8080\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h6 id=\"156-更新资源清单\"><a class=\"anchor\" href=\"#156-更新资源清单\">#</a> 1.5.6 更新资源清单</h6>\n<pre><code>[root@k8s-master01 05-xxl-job]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 05-xxl-job]# kubectl apply -f .\n</code></pre>\n<h6 id=\"157-web访问xxl-job\"><a class=\"anchor\" href=\"#157-web访问xxl-job\">#</a> <strong>1.5.7 Web 访问 xxl-job</strong></h6>\n<pre><code>http://192.168.40.101:30904/xxl-job-admin/\nhttp://xxljob.hmallleasing.com/xxl-job-admin/ \nuser:admin    \npwd:1223456\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/A5NbU2Z.jpeg\" alt=\"Snipaste_2025-05-18_14-54-59.jpg\" /></p>\n<h5 id=\"16-部署rabbitmq集群\"><a class=\"anchor\" href=\"#16-部署rabbitmq集群\">#</a> 1.6 部署 rabbitmq 集群</h5>\n<h6 id=\"161-创建rbac权限\"><a class=\"anchor\" href=\"#161-创建rbac权限\">#</a> 1.6.1 创建 RBAC 权限</h6>\n<pre><code># cat 01-rabbitmq-rbac.yaml \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: rabbitmq-cluster\n  namespace: prod\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: rabbitmq-cluster\n  namespace: prod\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;endpoints&quot;]\n  verbs: [&quot;get&quot;]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: rabbitmq-cluster\n  namespace: prod\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: rabbitmq-cluster\nsubjects:\n- kind: ServiceAccount\n  name: rabbitmq-cluster\n  namespace: prod\n</code></pre>\n<h6 id=\"162-创建集群的secret\"><a class=\"anchor\" href=\"#162-创建集群的secret\">#</a> 1.6.2 创建集群的 Secret</h6>\n<pre><code># cat 02-rabbitmq-secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: rabbitmq-secret\n  namespace: prod\nstringData:\n  password: talent\n  url: amqp://RABBITMQ_USER:RABBITMQ_PASS@rmq-cluster-balancer\n  username: superman\ntype: Opaque\n</code></pre>\n<h6 id=\"163-创建configmap\"><a class=\"anchor\" href=\"#163-创建configmap\">#</a> 1.6.3 创建 ConfigMap</h6>\n<pre><code># cat 03-rabbitmq-cm.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rabbitmq-cluster-config\n  namespace: prod\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\ndata:\n    enabled_plugins: |\n      [rabbitmq_management,rabbitmq_peer_discovery_k8s].\n    rabbitmq.conf: |\n      loopback_users.guest = false\n      default_user = RABBITMQ_USER\n      default_pass = RABBITMQ_PASS\n      ## Cluster \n      cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s\n      cluster_formation.k8s.host = kubernetes.default.svc\n      #cluster_formation.k8s.host = kubernetes.default.svc.cluster.local\n      cluster_formation.k8s.address_type = hostname\n      #################################################\n      # prod is rabbitmq-cluster's namespace#\n      #################################################\n      cluster_formation.k8s.hostname_suffix = .rabbitmq-cluster.prod.svc.cluster.local\n      cluster_formation.node_cleanup.interval = 30\n      cluster_formation.node_cleanup.only_log_warning = true\n      cluster_partition_handling = autoheal\n      ## queue master locator\n      queue_master_locator = min-masters\n      cluster_formation.randomized_startup_delay_range.min = 0\n      cluster_formation.randomized_startup_delay_range.max = 2\n      # memory\n      vm_memory_high_watermark.absolute = 100MB\n      # disk\n      disk_free_limit.absolute = 2GB\n</code></pre>\n<p><em>注：配置文件 cluster_formation.k8s.host 设置为 kubernetes.default.svc.cluster.local，然后就是各种连不上，后来换上 kubernetes.default.svc 就可以了，不知道是不是 k8s 新版本的问题。</em></p>\n<h6 id=\"164-创建集群的svc\"><a class=\"anchor\" href=\"#164-创建集群的svc\">#</a> 1.6.4 创建集群的 svc</h6>\n<pre><code># cat 04-rabbitmq-cluster-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: rabbitmq-cluster\n  name: rabbitmq-cluster\n  namespace: prod\nspec:\n  clusterIP: None\n  ports:\n  - name: rmqport\n    port: 5672\n    targetPort: 5672\n  - name: http\n    port: 15672\n    protocol: TCP\n    targetPort: 15672\n  selector:\n    app: rabbitmq-cluster\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: rabbitmq-cluster-balancer\n  name: rabbitmq-cluster-balancer\n  namespace: prod\nspec:\n  ports:\n  - name: rmqport\n    port: 5672\n    targetPort: 5672\n  - name: http\n    port: 15672\n    protocol: TCP\n    targetPort: 15672\n  selector:\n    app: rabbitmq-cluster\n  type: NodePort\n</code></pre>\n<h6 id=\"165-创建statefulset\"><a class=\"anchor\" href=\"#165-创建statefulset\">#</a> 1.6.5 创建 StatefulSet</h6>\n<pre><code># cat 05-rabbitmq-cluster-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: rabbitmq-cluster\n  name: rabbitmq-cluster\n  namespace: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: rabbitmq-cluster\n  serviceName: rabbitmq-cluster\n  template:\n    metadata:\n      labels:\n        app: rabbitmq-cluster\n    spec:\n      affinity:                                                 # 避免Pod运行到同一个节点上了\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: app\n                    operator: In\n                    values: [&quot;rabbitmq-cluster&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;  \n      containers:\n      - args:\n        - -c\n        - cp -v /etc/rabbitmq/rabbitmq.conf $&#123;RABBITMQ_CONFIG_FILE&#125;; exec docker-entrypoint.sh rabbitmq-server\n        command:\n        - sh\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: rabbitmq-secret\n        - name: RABBITMQ_DEFAULT_PASS \n          valueFrom:\n            secretKeyRef:\n              key: password \n              name: rabbitmq-secret\n        - name: TZ\n          value: 'Asia/Shanghai'\n        - name: RABBITMQ_ERLANG_COOKIE\n          value: 'SWvCP0Hrqv43NG7GybHC95ntCJKoW8UyNFWnBEWG8TY='\n        - name: K8S_SERVICE_NAME\n          value: rabbitmq-cluster\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RABBITMQ_USE_LONGNAME\n          value: &quot;true&quot;\n        - name: RABBITMQ_NODENAME\n          value: rabbit@$(POD_NAME).$(K8S_SERVICE_NAME).$(POD_NAMESPACE).svc.cluster.local\n        - name: RABBITMQ_CONFIG_FILE\n          value: /var/lib/rabbitmq/rabbitmq.conf\n        image: rabbitmq:3.9-management\n        imagePullPolicy: IfNotPresent\n        name: rabbitmq\n        ports:\n        - containerPort: 15672\n          name: http\n          protocol: TCP\n        - containerPort: 5672\n          name: amqp\n          protocol: TCP\n        livenessProbe:\n          exec:\n            command: [&quot;rabbitmq-diagnostics&quot;, &quot;status&quot;]\n          initialDelaySeconds: 60\n          periodSeconds: 60\n          failureThreshold: 2\n          timeoutSeconds: 10\n        readinessProbe:\n          exec:\n            command: [&quot;rabbitmq-diagnostics&quot;, &quot;status&quot;]\n          failureThreshold: 2\n          initialDelaySeconds: 60\n          periodSeconds: 60\n          timeoutSeconds: 10\n        volumeMounts:\n        - mountPath: /etc/rabbitmq\n          name: config-volume\n          readOnly: false\n        - mountPath: /var/lib/rabbitmq\n          name: rabbitmq-storage\n          readOnly: false\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      serviceAccountName: rabbitmq-cluster\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: config-volume\n        configMap:\n          items:\n          - key: rabbitmq.conf\n            path: rabbitmq.conf\n          - key: enabled_plugins\n            path: enabled_plugins\n          name: rabbitmq-cluster-config\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: rabbitmq-storage\n    spec:\n      accessModes:\n      - ReadWriteMany\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"166-创建ingress\"><a class=\"anchor\" href=\"#166-创建ingress\">#</a> 1.6.6 创建 Ingress</h6>\n<pre><code>[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rabbitmq-ingress\n  namespace: prod\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: rabbitmq.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: rabbitmq-cluster\n            port:\n              number: 15672\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h6 id=\"167-更新资源清单\"><a class=\"anchor\" href=\"#167-更新资源清单\">#</a> <strong>1.6.7 更新资源清单</strong></h6>\n<pre><code>[root@k8s-master01 04-rabbitmq]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 04-rabbitmq]# kubectl apply -f .\n[root@k8s-master01 04-rabbitmq]# kubectl get pods -n prod\nNAME                 READY   STATUS    RESTARTS   AGE\nrabbitmq-cluster-0   1/1     Running   0          9m53s\nrabbitmq-cluster-1   1/1     Running   0          8m47s\nrabbitmq-cluster-2   1/1     Running   0          7m40s\n\n[root@k8s-master01 04-rabbitmq]# kubectl exec -it rabbitmq-cluster-0 -n prod -- /bin/bash\nroot@rabbitmq-cluster-0:/# rabbitmqctl cluster_status\nRABBITMQ_ERLANG_COOKIE env variable support is deprecated and will be REMOVED in a future version. Use the $HOME/.erlang.cookie file or the --erlang-cookie switch instead.\nCluster status of node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local ...\nBasics\n\nCluster name: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\n\nDisk Nodes\n\nrabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local\n\nRunning Nodes\n\nrabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local\n\nVersions\n\nrabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9\nrabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9\nrabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9\n\nMaintenance status\n\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance\n\nAlarms\n\nMemory alarm on node rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local\nMemory alarm on node rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local\nMemory alarm on node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\n\nNetwork Partitions\n\n(none)\n\nListeners\n\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\n\nFeature flags\n\nFlag: drop_unroutable_metric, state: enabled\nFlag: empty_basic_get_metric, state: enabled\nFlag: implicit_default_bindings, state: enabled\nFlag: maintenance_mode_status, state: enabled\nFlag: quorum_queue, state: enabled\nFlag: stream_queue, state: enabled\nFlag: user_limits, state: enabled\nFlag: virtual_host_metadata, state: enabled\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/XqURJbg.jpeg\" alt=\"Snipaste_2025-05-17_17-42-47.jpg\" /></p>\n<h6 id=\"168-web访问rabbitmq\"><a class=\"anchor\" href=\"#168-web访问rabbitmq\">#</a> <strong>1.6.8 Web 访问 rabbitmq</strong></h6>\n<pre><code>http://rabbitmq.hmallleasing.com/#/\nuser:superman\npwd:talent\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/7qKxUBf.jpeg\" alt=\"Snipaste_2025-05-17_17-14-54.jpg\" /></p>\n<h6 id=\"168-rabbitmq全部挂了无法重启解决方案\"><a class=\"anchor\" href=\"#168-rabbitmq全部挂了无法重启解决方案\">#</a> 1.6.8 rabbitMQ 全部挂了，无法重启解决方案</h6>\n<p>Kubernetes 环境中，遇到 RabbitMQ 集群无法启动的问题。原因是 RabbitMQ 所有实例均失效，需要在每个 Pod 对应的持久化存储路径下创建 force_load 文件来强制启动。通过获取 PV 存储路径，在指定目录创建该文件后，重新启动 RabbitMQ 服务，成功解决了集群启动问题</p>\n<pre><code>[root@k8s-node02 ~# cd /data/dev-rabbitmq-storage-rabbitmq-cluster-0-pvc-3abca920-3c68-44eb-b0fd-406a4358b153/mnesia/rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local\n[root@k8s-node02 rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local]# touch force_load\n</code></pre>\n<h5 id=\"17-部署rabbitmq-single\"><a class=\"anchor\" href=\"#17-部署rabbitmq-single\">#</a> 1.7 部署 rabbitmq-single</h5>\n<pre><code>[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-single.yaml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: rabbitmq-single-data\n  namespace: prod\nspec:\n  storageClassName: &quot;nfs-storage&quot;     # 明确指定使用哪个sc的供应商来创建pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi                      # 根据业务实际大小进行资源申请\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq-single-svc\n  namespace: prod\n  labels:\n    name: rabbitmq-single-svc\nspec:\n  ports:\n  - port: 5672 \n    protocol: TCP\n    name: web\n    targetPort: 5672\n  - name: http\n    port: 15672\n    protocol: TCP\n    targetPort: 15672\n  selector:\n    app: rabbitmq-single\n\n---\napiVersion: networking.k8s.io/v1 # k8s &gt;= 1.22 必须 v1\nkind: Ingress\nmetadata:\n  name: rabbitmq-single-ingress\n  namespace: prod\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: rabbitmq.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: rabbitmq-single-svc\n            port:\n              number: 15672\n        path: /\n        pathType: Prefix\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rabbitmq-single\n  namespace: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rabbitmq-single\n  template:\n    metadata:\n      labels:\n        app: rabbitmq-single\n    spec:\n      containers:\n      - name: rabbitmq-single\n        image: rabbitmq:3.9-management\n        ports:\n        - containerPort: 5672\n          name: web\n          protocol: TCP\n        - containerPort: 15672\n          name: http\n          protocol: TCP\n        env:\n        - name: RABBITMQ_DEFAULT_USER  # 自定义环境变量\n          value: admin\n        - name: RABBITMQ_DEFAULT_PASS\n          value: Superman*2025\n        resources:\n          requests:\n            memory: &quot;1Gi&quot;\n            cpu: &quot;500m&quot;\n        livenessProbe:\n          exec:\n            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: rabbitmq-storage\n          mountPath: /var/lib/rabbitmq\n      volumes:\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n      - name: rabbitmq-storage\n        persistentVolumeClaim:\n          claimName: rabbitmq-single-data\n</code></pre>\n<h4 id=\"二-部署微服务应用\"><a class=\"anchor\" href=\"#二-部署微服务应用\">#</a> 二、部署微服务应用</h4>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/1888662579.html",
            "url": "http://ixuyong.cn/posts/1888662579.html",
            "title": "Containerd常用命令",
            "date_published": "2025-05-14T12:29:07.000Z",
            "content_html": "<h3 id=\"containerd常用命令\"><a class=\"anchor\" href=\"#containerd常用命令\">#</a> Containerd 常用命令</h3>\n<h4 id=\"1-安装containerd\"><a class=\"anchor\" href=\"#1-安装containerd\">#</a> 1. 安装 Containerd</h4>\n<p><strong>1.1 配置安装源</strong></p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><strong>1.2 安装 docker-ce、containerd</strong></p>\n<pre><code>yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p><strong>1.3 配置 Containerd 所需的模块</strong></p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><strong>1.4 加载模块</strong></p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><strong>1.5 配置 Containerd 所需的内核</strong></p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><strong>1.6 加载内核</strong></p>\n<pre><code>sysctl --system\n</code></pre>\n<p><strong>1.7 生成 Containerd 的配置文件</strong></p>\n<pre><code>mkdir -p /etc/containerd\ncontainerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><strong>1.8 更改 Containerd 的 Cgroup 和 Pause 镜像</strong></p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><strong>1.9 启动 Containerd，并配置开机自启动</strong></p>\n<pre><code>systemctl daemon-reload\nsystemctl enable --now containerd\nsystemctl status  containerd \n</code></pre>\n<h4 id=\"2-containerd-配置镜像加速\"><a class=\"anchor\" href=\"#2-containerd-配置镜像加速\">#</a> 2. Containerd 配置镜像加速</h4>\n<p>打开 /etc/containerd/config.toml 文件，找到 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] 部分，添加所需的镜像源配置</p>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]\n  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n    endpoint = [\n      &quot;https://docker.io&quot;,\n      &quot;https://6qxc6b6n.mirror.aliyuncs.com&quot;,\n      &quot;https://docker.m.daocloud.io&quot;,\n      &quot;https://dockerproxy.com/&quot;\n    ]\n  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;gcr.io&quot;]\n    endpoint = [\n      &quot;https://gcr.m.daocloud.io&quot;,\n      &quot;https://gcr.nju.edu.cn&quot;,\n      &quot;https://gcr.dockerproxy.com&quot;\n    ]\n</code></pre>\n<p><strong>重新启动 Containerd</strong></p>\n<pre><code>systemctl daemon-reload\nsystemctl restart containerd\n</code></pre>\n<h4 id=\"3-containerd常用操作命令实践\"><a class=\"anchor\" href=\"#3-containerd常用操作命令实践\">#</a> 3. Containerd 常用操作命令实践</h4>\n<h5 id=\"31-查看containerd命名空间\"><a class=\"anchor\" href=\"#31-查看containerd命名空间\">#</a> <strong>3.1 查看 Containerd 命名空间</strong></h5>\n<p>namespace 来于指定类似于工作空间的隔离区域</p>\n<pre><code>[root@k8s-node02 ~]# ctr namespace ls \nNAME    LABELS \ndefault        \nk8s.io         \nmoby \n</code></pre>\n<h5 id=\"32-查看containerd镜像\"><a class=\"anchor\" href=\"#32-查看containerd镜像\">#</a> <strong>3.2 查看 Containerd 镜像</strong></h5>\n<p>因为没有指定 namespace，所以查看的是默认命名空间下的镜像</p>\n<pre><code>ctr images ls\n</code></pre>\n<p>查看指定命名空间 k8s.io 下的镜像</p>\n<pre><code>ctr -n k8s.io images ls\n</code></pre>\n<h5 id=\"33-拉取containerd镜像\"><a class=\"anchor\" href=\"#33-拉取containerd镜像\">#</a> <strong>3.3 拉取 Containerd 镜像</strong></h5>\n<p>拉取指定命名空间 k8s.io 镜像 pause-amd64:3.2</p>\n<pre><code> ctr -n k8s.io images pull registry.aliyuncs.com/google_containers/pause-amd64:3.2\n ctr -n k8s.io images pull docker.io/library/nginx:1.21\n</code></pre>\n<h5 id=\"34-删除containerd镜像\"><a class=\"anchor\" href=\"#34-删除containerd镜像\">#</a> <strong>3.4 删除 containerd 镜像</strong></h5>\n<pre><code>ctr -n k8s.io images rm registry.aliyuncs.com/google_containers/pause-amd64:3.2\n</code></pre>\n<h5 id=\"35-导出containerd镜像\"><a class=\"anchor\" href=\"#35-导出containerd镜像\">#</a> <strong>3.5 导出 Containerd 镜像</strong></h5>\n<pre><code>ctr -n k8s.io images export pause.tar.gz registry.aliyuncs.com/google_containers/pause-amd64:3.2\n</code></pre>\n<h5 id=\"36-导入containerd镜像\"><a class=\"anchor\" href=\"#36-导入containerd镜像\">#</a> <strong>3.6 导入 Containerd 镜像</strong></h5>\n<pre><code>ctr -n k8s.io image import pause.tar.gz\n</code></pre>\n<p><em>docker save -o 命令导出来的镜像可以用 ctr images import 导出，同理 ctr images export 导出来的镜像也可以有 docker load 还原。</em></p>\n<h5 id=\"37-标记containerd镜像\"><a class=\"anchor\" href=\"#37-标记containerd镜像\">#</a> <strong>3.7 标记 Containerd 镜像</strong></h5>\n<pre><code>ctr -n k8s.io images tag registry.aliyuncs.com/google_containers/pause-amd64:3.2 pause:3.2\n</code></pre>\n<h5 id=\"38-运行containerd容器\"><a class=\"anchor\" href=\"#38-运行containerd容器\">#</a> <strong>3.8 运行 Containerd 容器</strong></h5>\n<p>在后台运行一个 centos 镜像的容器，名称叫做 centos_k8s</p>\n<pre><code> ctr -n k8s.io  run -d  docker.io/library/nginx:1.21 web\n</code></pre>\n<h5 id=\"39-查看运行容器的task\"><a class=\"anchor\" href=\"#39-查看运行容器的task\">#</a> <strong>3.9 查看运行容器的 task</strong></h5>\n<pre><code>ctr -n k8s.io task ls\n</code></pre>\n<h5 id=\"310-启动指定容器task\"><a class=\"anchor\" href=\"#310-启动指定容器task\">#</a> 3.10 启动指定容器 task</h5>\n<pre><code>ctr -n k8s.io task start -d centos_k8s\n</code></pre>\n<h5 id=\"311-进入指定容器task\"><a class=\"anchor\" href=\"#311-进入指定容器task\">#</a> 3.11 进入指定容器 task</h5>\n<pre><code>ctr -n k8s.io task exec --exec-id 3118 -t web /bin/bash\n</code></pre>\n<h5 id=\"312-删除指定容器task\"><a class=\"anchor\" href=\"#312-删除指定容器task\">#</a> 3.12 删除指定容器 task</h5>\n<pre><code>ctr -n k8s.io task rm -f web\n</code></pre>\n<h5 id=\"313-停止指定容器task\"><a class=\"anchor\" href=\"#313-停止指定容器task\">#</a> 3.13 停止指定容器 task</h5>\n<pre><code>ctr -n k8s.io task kill --signal 9 centos_k8s\n</code></pre>\n<h5 id=\"314-查看容器\"><a class=\"anchor\" href=\"#314-查看容器\">#</a> 3.14 查看容器</h5>\n<pre><code>ctr -n k8s.io c list\n</code></pre>\n<h5 id=\"315-删除容器\"><a class=\"anchor\" href=\"#315-删除容器\">#</a> 3.15 删除容器</h5>\n<pre><code>ctr -n k8s.io c rm centos\n</code></pre>\n<p>删除容器以前需要将 task 删除，不然会报以下错误</p>\n<pre><code>[root@k8s-node02 ~]# ctr -n k8s.io c rm web \nERRO[0000] failed to delete container &quot;web&quot;              error=&quot;cannot delete a non stopped container: &#123;running 0 0001-01-01 00:00:00 +0000 UTC&#125;&quot;\nctr: cannot delete a non stopped container: &#123;running 0 0001-01-01 00:00:00 +0000 UTC&#125;\n</code></pre>\n<h4 id=\"4-docker与containerd常用命令对比\"><a class=\"anchor\" href=\"#4-docker与containerd常用命令对比\">#</a> 4. Docker 与 Containerd 常用命令对比</h4>\n<table>\n<thead>\n<tr>\n<th>说明</th>\n<th>docker 命令</th>\n<th>containerd 命令</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>查看本地镜像</td>\n<td>docker images</td>\n<td>ctr images ls</td>\n</tr>\n<tr>\n<td>拉取镜像</td>\n<td>docker pull imagename</td>\n<td>ctr images pull imagename</td>\n</tr>\n<tr>\n<td>推送镜像</td>\n<td>docker push imagename</td>\n<td>ctr images push imagename</td>\n</tr>\n<tr>\n<td>给镜像打标签</td>\n<td>docker tag imagename tagname</td>\n<td>ctr images tag imagename tagname</td>\n</tr>\n<tr>\n<td>导出镜像</td>\n<td>docker save filename imagename</td>\n<td>ctr images export filename imagename</td>\n</tr>\n<tr>\n<td>导入镜像</td>\n<td>docker load filename</td>\n<td>ctr image import filename</td>\n</tr>\n<tr>\n<td>运行并创建容器</td>\n<td>docker run [options] imagename commond</td>\n<td>ctr run [options]  imagenamecontainername</td>\n</tr>\n<tr>\n<td>进入容器</td>\n<td>docker exec [options] names commond</td>\n<td>ctr task exec [options]  names commond</td>\n</tr>\n<tr>\n<td>查看运行的容器</td>\n<td>docker ps</td>\n<td>ctr task list</td>\n</tr>\n<tr>\n<td>删除容器</td>\n<td>docker rm [options] names</td>\n<td>1.ctr task rm -f names 2. ctr c rm -f names</td>\n</tr>\n</tbody>\n</table>\n",
            "tags": [
                "Docker"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/1490514396.html",
            "url": "http://ixuyong.cn/posts/1490514396.html",
            "title": "Redis Cluster集群部署",
            "date_published": "2025-05-12T13:21:44.000Z",
            "content_html": "<h3 id=\"redis-cluster集群部署\"><a class=\"anchor\" href=\"#redis-cluster集群部署\">#</a> Redis Cluster 集群部署</h3>\n<h4 id=\"1-环境配置\"><a class=\"anchor\" href=\"#1-环境配置\">#</a> 1、环境配置</h4>\n<h5 id=\"11-关闭防火墙-selinux\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux\">#</a> 1.1 关闭防火墙、Selinux</h5>\n<pre><code>systemctl disable --now firewalld \nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\n</code></pre>\n<h5 id=\"11-配置yum源\"><a class=\"anchor\" href=\"#11-配置yum源\">#</a> 1.1 配置 yum 源</h5>\n<pre><code>#rocky linux配置\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/rocky-*.repo\nyum clean all &amp;&amp; yum makecache\nmkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"13-配置文件描述符\"><a class=\"anchor\" href=\"#13-配置文件描述符\">#</a> 1.3 配置文件描述符</h5>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<h5 id=\"14-系统内核参数调优\"><a class=\"anchor\" href=\"#14-系统内核参数调优\">#</a> 1.4 系统内核参数调优</h5>\n<pre><code># 修改/etc/sysctl.conf文件\ncat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOF\nvm.max_map_count = 262144\nvm.swappiness=1\n\nnet.ipv4.tcp_fin_timeout=2\nnet.ipv4.tcp_tw_reuse=1\n#net.ipv4.tcp_tw_recycle=1\nnet.ipv4.tcp_syncookies=1\nnet.ipv4.tcp_keepalive_time=600\nnet.ipv4.ip_local_port_range=4000 65000\nnet.ipv4.tcp_max_syn_backlog=16384\nnet.ipv4.route.gc_timeout=100\nnet.ipv4.tcp_max_tw_buckets= 5000\n\nnet.ipv4.tcp_syn_retries=1\nnet.ipv4.tcp_synack_retries=1\nnet.core.somaxconn=16384\nnet.core.netdev_max_backlog=16384\nnet.ipv4.tcp_max_orphans=16384\n\n# 设置最大内存共享段大小bytes\nkernel.shmmax=15461882265\nkernel.shmall=3774873\n# 修改消息队列长度\nkernel.msgmax=65535\nkernel.msgmnb=65535\nEOF\n</code></pre>\n<h5 id=\"15-修改默认限制内存\"><a class=\"anchor\" href=\"#15-修改默认限制内存\">#</a> 1.5 修改默认限制内存</h5>\n<pre><code>cat &gt;&gt;/etc/systemd/system.conf&lt;&lt; EOF\nDefaultLimitNOFILE=65536\nDefaultLimitNPROC=32000\nDefaultLimitMEMLOCK=infinity\nEOF\n</code></pre>\n<h5 id=\"16-执行命令生效状态\"><a class=\"anchor\" href=\"#16-执行命令生效状态\">#</a> 1.6 执行命令生效状态</h5>\n<pre><code>sysctl -p\n</code></pre>\n<h5 id=\"17-安装基础软件包\"><a class=\"anchor\" href=\"#17-安装基础软件包\">#</a> 1.7 安装基础软件包</h5>\n<pre><code>yum install wget jq psmisc vim unzip net-tools telnet tree yum-utils device-mapper-persistent-data \\\nlvm2 git ntpdate nfs-utils iotop httpd-tools dos2unix lrzsz -y\n</code></pre>\n<h5 id=\"18-升级系统\"><a class=\"anchor\" href=\"#18-升级系统\">#</a> 1.8 升级系统</h5>\n<pre><code>yum update -y --exclude=kernel* &amp;&amp; reboot\n</code></pre>\n<h4 id=\"2-redis-cluster部署\"><a class=\"anchor\" href=\"#2-redis-cluster部署\">#</a> 2、Redis cluster 部署</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">192.168.1.135</th>\n<th style=\"text-align:center\">192.168.1.136</th>\n<th style=\"text-align:center\">192.168.1.137</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">node1：7001</td>\n<td style=\"text-align:center\">node1：7001</td>\n<td style=\"text-align:center\">node1：7001</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">node2：7002</td>\n<td style=\"text-align:center\">node2：7002</td>\n<td style=\"text-align:center\">node2：7002</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">node3：7003</td>\n<td style=\"text-align:center\">node3：7003</td>\n<td style=\"text-align:center\">node3：7003</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"21-安装包下载\"><a class=\"anchor\" href=\"#21-安装包下载\">#</a> 2.1 安装包下载</h5>\n<pre><code>wget https://download.redis.io/releases/redis-7.2.1.tar.gz\n</code></pre>\n<h5 id=\"22-安装-redis\"><a class=\"anchor\" href=\"#22-安装-redis\">#</a> 2.2 安装 redis</h5>\n<pre><code>yum install gcc-c++ -y\nmkdir /soft\ntar -xzvf redis-7.2.1.tar.gz -C /soft\nln -s /soft/redis-7.2.1 /soft/redis\ncd /soft/redis\nmake\nmake install prefix=/soft/redis\n</code></pre>\n<h5 id=\"23-生成集群配置文件\"><a class=\"anchor\" href=\"#23-生成集群配置文件\">#</a> 2.3 生成集群配置文件</h5>\n<pre><code>mkdir -p /soft/redis/data/7001\nmkdir -p /soft/redis/data/7002\nmkdir -p /soft/redis/data/7003\nmkdir -p /soft/redis/log\ncd /soft/redis\n</code></pre>\n<p><strong>redis_7001.conf 配置文件</strong></p>\n<pre><code>cat &gt; /soft/redis/redis_7001.conf &lt;&lt;EOF\nprotected-mode yes\nport 7001\nrequirepass admin123\nmasterauth admin123\ncluster-enabled yes\ncluster-config-file nodes-7001.conf\ncluster-node-timeout 5000\nmaxmemory 2GB\nmaxmemory-policy volatile-lru\ntcp-backlog 511\ntimeout 0\ntcp-keepalive 300\ndaemonize yes\npidfile /soft/redis/data/redis7001.pid\nloglevel notice\nlogfile &quot;/soft/redis/log/redis7001.log&quot;\n#databases 16\nalways-show-logo no\nset-proc-title yes\nproc-title-template &quot;&#123;title&#125; &#123;listen-addr&#125; &#123;server-mode&#125;&quot;\nlocale-collate &quot;&quot;\nstop-writes-on-bgsave-error yes\nrdbcompression yes\nrdbchecksum yes\ndbfilename dump.rdb\nrdb-del-sync-files no\ndir /soft/redis/data/7001\nreplica-serve-stale-data yes\nreplica-read-only yes\nrepl-diskless-sync yes\nrepl-diskless-sync-delay 5\nrepl-diskless-sync-max-replicas 0\nrepl-diskless-load disabled\nrepl-disable-tcp-nodelay no\nreplica-priority 100\nacllog-max-len 128\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del no\nreplica-lazy-flush no\nlazyfree-lazy-user-del no\nlazyfree-lazy-user-flush no\noom-score-adj no\noom-score-adj-values 0 200 800\ndisable-thp yes\nappendonly no\nappendfilename &quot;appendonly.aof&quot;\nappenddirname &quot;appendonlydir&quot;\nappendfsync everysec\nno-appendfsync-on-rewrite no\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\naof-load-truncated yes\naof-use-rdb-preamble yes\naof-timestamp-enabled no\nslowlog-log-slower-than 10000\nslowlog-max-len 128\nlatency-monitor-threshold 0\nnotify-keyspace-events &quot;&quot;\nhash-max-listpack-entries 512\nhash-max-listpack-value 64\nlist-max-listpack-size -2\nlist-compress-depth 0\nset-max-intset-entries 512\nset-max-listpack-entries 128\nset-max-listpack-value 64\nzset-max-listpack-entries 128\nzset-max-listpack-value 64\nhll-sparse-max-bytes 3000\nstream-node-max-bytes 4096\nstream-node-max-entries 100\nactiverehashing yes\nclient-output-buffer-limit normal 0 0 0\nclient-output-buffer-limit replica 256mb 64mb 60\nclient-output-buffer-limit pubsub 32mb 8mb 60\nhz 10\ndynamic-hz yes\naof-rewrite-incremental-fsync yes\nrdb-save-incremental-fsync yes\njemalloc-bg-thread yes\nEOF\n</code></pre>\n<p><strong>redis_7002.conf 配置文件</strong></p>\n<pre><code>cat &gt; /soft/redis/redis_7002.conf &lt;&lt;EOF\nprotected-mode yes\nport 7002\nrequirepass admin123\nmasterauth admin123\ncluster-enabled yes\ncluster-config-file nodes-7002.conf\ncluster-node-timeout 5000\nmaxmemory 2GB\nmaxmemory-policy  volatile-lru\ntcp-backlog 511\ntimeout 0\ntcp-keepalive 300\ndaemonize yes\npidfile /soft/redis/data/redis7002.pid\nloglevel notice\nlogfile &quot;/soft/redis/log/redis7002.log&quot;\n#databases 16\nalways-show-logo no\nset-proc-title yes\nproc-title-template &quot;&#123;title&#125; &#123;listen-addr&#125; &#123;server-mode&#125;&quot;\nlocale-collate &quot;&quot;\nstop-writes-on-bgsave-error yes\nrdbcompression yes\nrdbchecksum yes\ndbfilename dump.rdb\nrdb-del-sync-files no\ndir /soft/redis/data/7002\nreplica-serve-stale-data yes\nreplica-read-only yes\nrepl-diskless-sync yes\nrepl-diskless-sync-delay 5\nrepl-diskless-sync-max-replicas 0\nrepl-diskless-load disabled\nrepl-disable-tcp-nodelay no\nreplica-priority 100\nacllog-max-len 128\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del no\nreplica-lazy-flush no\nlazyfree-lazy-user-del no\nlazyfree-lazy-user-flush no\noom-score-adj no\noom-score-adj-values 0 200 800\ndisable-thp yes\nappendonly no\nappendfilename &quot;appendonly.aof&quot;\nappenddirname &quot;appendonlydir&quot;\nappendfsync everysec\nno-appendfsync-on-rewrite no\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\naof-load-truncated yes\naof-use-rdb-preamble yes\naof-timestamp-enabled no\nslowlog-log-slower-than 10000\nslowlog-max-len 128\nlatency-monitor-threshold 0\nnotify-keyspace-events &quot;&quot;\nhash-max-listpack-entries 512\nhash-max-listpack-value 64\nlist-max-listpack-size -2\nlist-compress-depth 0\nset-max-intset-entries 512\nset-max-listpack-entries 128\nset-max-listpack-value 64\nzset-max-listpack-entries 128\nzset-max-listpack-value 64\nhll-sparse-max-bytes 3000\nstream-node-max-bytes 4096\nstream-node-max-entries 100\nactiverehashing yes\nclient-output-buffer-limit normal 0 0 0\nclient-output-buffer-limit replica 256mb 64mb 60\nclient-output-buffer-limit pubsub 32mb 8mb 60\nhz 10\ndynamic-hz yes\naof-rewrite-incremental-fsync yes\nrdb-save-incremental-fsync yes\njemalloc-bg-thread yes\nEOF\n</code></pre>\n<p><strong>redis_7003.conf 配置文件</strong></p>\n<pre><code>cat &gt; /soft/redis/redis_7003.conf &lt;&lt;EOF\nprotected-mode yes\nport 7003\nrequirepass admin123\nmasterauth admin123\ncluster-enabled yes\ncluster-config-file nodes-7003.conf\ncluster-node-timeout 5000\nmaxmemory 2GB\nmaxmemory-policy  volatile-lru\ntcp-backlog 511\ntimeout 0\ntcp-keepalive 300\ndaemonize yes\npidfile /soft/redis/data/redis7003.pid\nloglevel notice\nlogfile &quot;/soft/redis/log/redis7003.log&quot;\n#databases 16\nalways-show-logo no\nset-proc-title yes\nproc-title-template &quot;&#123;title&#125; &#123;listen-addr&#125; &#123;server-mode&#125;&quot;\nlocale-collate &quot;&quot;\nstop-writes-on-bgsave-error yes\nrdbcompression yes\nrdbchecksum yes\ndbfilename dump.rdb\nrdb-del-sync-files no\ndir /soft/redis/data/7003\nreplica-serve-stale-data yes\nreplica-read-only yes\nrepl-diskless-sync yes\nrepl-diskless-sync-delay 5\nrepl-diskless-sync-max-replicas 0\nrepl-diskless-load disabled\nrepl-disable-tcp-nodelay no\nreplica-priority 100\nacllog-max-len 128\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del no\nreplica-lazy-flush no\nlazyfree-lazy-user-del no\nlazyfree-lazy-user-flush no\noom-score-adj no\noom-score-adj-values 0 200 800\ndisable-thp yes\nappendonly no\nappendfilename &quot;appendonly.aof&quot;\nappenddirname &quot;appendonlydir&quot;\nappendfsync everysec\nno-appendfsync-on-rewrite no\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\naof-load-truncated yes\naof-use-rdb-preamble yes\naof-timestamp-enabled no\nslowlog-log-slower-than 10000\nslowlog-max-len 128\nlatency-monitor-threshold 0\nnotify-keyspace-events &quot;&quot;\nhash-max-listpack-entries 512\nhash-max-listpack-value 64\nlist-max-listpack-size -2\nlist-compress-depth 0\nset-max-intset-entries 512\nset-max-listpack-entries 128\nset-max-listpack-value 64\nzset-max-listpack-entries 128\nzset-max-listpack-value 64\nhll-sparse-max-bytes 3000\nstream-node-max-bytes 4096\nstream-node-max-entries 100\nactiverehashing yes\nclient-output-buffer-limit normal 0 0 0\nclient-output-buffer-limit replica 256mb 64mb 60\nclient-output-buffer-limit pubsub 32mb 8mb 60\nhz 10\ndynamic-hz yes\naof-rewrite-incremental-fsync yes\nrdb-save-incremental-fsync yes\njemalloc-bg-thread yes\nEOF\n</code></pre>\n<h5 id=\"24-redis开机自启\"><a class=\"anchor\" href=\"#24-redis开机自启\">#</a> 2.4 Redis 开机自启</h5>\n<p><strong>redis_7001.service</strong></p>\n<pre><code>cat &lt;&lt; &quot;EOF&quot; &gt; /usr/lib/systemd/system/redis_7001.service\n[Unit]\nDescription=Redis 7001 service\nDocumentation=https://redis.io/documentation\nWants=network-online.target\nAfter=network-online.target\n[Service]\nType=forking\nLimitNOFILE=10032\nUser=root\nGroup=root\nExecStart=/soft/redis/src/redis-server /soft/redis/redis_7001.conf\nPrivateTmp=true\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>\n<p><strong>redis_7002.service</strong></p>\n<pre><code>cat &lt;&lt; &quot;EOF&quot; &gt; /usr/lib/systemd/system/redis_7002.service\n[Unit]\nDescription=Redis 7002 service\nDocumentation=https://redis.io/documentation\nWants=network-online.target\nAfter=network-online.target\n[Service]\nType=forking\nLimitNOFILE=10032\nUser=root\nGroup=root\nExecStart=/soft/redis/src/redis-server /soft/redis/redis_7002.conf\nPrivateTmp=true\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>\n<p><strong>redis_7003.service</strong></p>\n<pre><code>cat &lt;&lt; &quot;EOF&quot; &gt; /usr/lib/systemd/system/redis_7003.service\n[Unit]\nDescription=Redis 7003 service\nDocumentation=https://redis.io/documentation\nWants=network-online.target\nAfter=network-online.target\n[Service]\nType=forking\nLimitNOFILE=10032\nUser=root\nGroup=root\nExecStart=/soft/redis/src/redis-server /soft/redis/redis_7003.conf\nPrivateTmp=true\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>\n<p><strong>设置开机自启</strong></p>\n<pre><code>chown -R root.root /soft/redis\nsystemctl daemon-reload\nsystemctl enable redis_7001.service\nsystemctl enable redis_7002.service\nsystemctl enable redis_7003.service\nsystemctl start redis_7001.service\nsystemctl start redis_7002.service\nsystemctl start redis_7003.service\nsystemctl status redis_7001.service\nsystemctl status redis_7002.service\nsystemctl status redis_7003.service\n</code></pre>\n<h5 id=\"25-启动redis集群服务\"><a class=\"anchor\" href=\"#25-启动redis集群服务\">#</a> 2.5 启动 redis 集群服务</h5>\n<p>--cluster-replicas 2 表示为集群中的每个主节点创建 2 个从节点</p>\n<pre><code>cd /soft/redis/src\n./redis-cli --cluster create \\  \n192.168.1.135:7001 192.168.1.135:7002 192.168.1.135:7003 \\  \n192.168.1.136:7001 192.168.1.136:7002 192.168.1.136:7003 \\ \n192.168.1.137:7001 192.168.1.137:7002 192.168.1.137:7003 \\ \n--cluster-replicas 2 -a admin123 \n</code></pre>\n<p>输入创建集群的命令后会出现以下提示，注意 Can I set the above configuration? (type 'yes' to accept): yes，该处请输入 yes</p>\n<pre><code>[root@qnyp_node01 src]# ./redis-cli --cluster create \\\n&gt; 192.168.1.135:7001 192.168.1.135:7002 192.168.1.135:7003 \\\n&gt; 192.168.1.136:7001 192.168.1.136:7002 192.168.1.136:7003 \\\n&gt; 192.168.1.137:7001 192.168.1.137:7002 192.168.1.137:7003 \\\n&gt; --cluster-replicas 2 -a admin123\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n&gt;&gt;&gt; Performing hash slots allocation on 9 nodes...\nMaster[0] -&gt; Slots 0 - 5460\nMaster[1] -&gt; Slots 5461 - 10922\nMaster[2] -&gt; Slots 10923 - 16383\nAdding replica 192.168.1.136:7002 to 192.168.1.135:7001\nAdding replica 192.168.1.137:7002 to 192.168.1.135:7001\nAdding replica 192.168.1.135:7003 to 192.168.1.136:7001\nAdding replica 192.168.1.137:7003 to 192.168.1.136:7001\nAdding replica 192.168.1.136:7003 to 192.168.1.137:7001\nAdding replica 192.168.1.135:7002 to 192.168.1.137:7001\nM: 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 192.168.1.135:7001\n   slots:[0-5460] (5461 slots) master\nS: 4508bee0c33784e0d5be25b64e4c7e677cd9d396 192.168.1.135:7002\n   replicates f9133541e2175958117753ef4e206ea43a21f07c\nS: a0e13083fcc1d6e96398f3bb2ea5581b7a64e05e 192.168.1.135:7003\n   replicates 06ea827f8d328d9d776c9643109317b0100727a6\nM: 06ea827f8d328d9d776c9643109317b0100727a6 192.168.1.136:7001\n   slots:[5461-10922] (5462 slots) master\nS: 1d1b9817e39ee8987a3518f62a9b91c3ab666eff 192.168.1.136:7002\n   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7\nS: a73c099dcc63f5d46a11d0f61c91270ef61290ff 192.168.1.136:7003\n   replicates f9133541e2175958117753ef4e206ea43a21f07c\nM: f9133541e2175958117753ef4e206ea43a21f07c 192.168.1.137:7001\n   slots:[10923-16383] (5461 slots) master\nS: 626dc659bb1059ec40039869241f7de88a49cd87 192.168.1.137:7002\n   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7\nS: 622f73cd06c5658f8d02056925ac708750f12c1a 192.168.1.137:7003\n   replicates 06ea827f8d328d9d776c9643109317b0100727a6\nCan I set the above configuration? (type 'yes' to accept):\n</code></pre>\n<p>输完 yes 后，会出现如下提示，[OK] All 16384 slots covered. 说明成功啦</p>\n<pre><code>Can I set the above configuration? (type 'yes' to accept): yes\n&gt;&gt;&gt; Nodes configuration updated\n&gt;&gt;&gt; Assign a different config epoch to each node\n&gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster\nWaiting for the cluster to join\n..\n&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.135:7001)\nM: 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 192.168.1.135:7001\n   slots:[0-5460] (5461 slots) master\n   2 additional replica(s)\nM: 06ea827f8d328d9d776c9643109317b0100727a6 192.168.1.136:7001\n   slots:[5461-10922] (5462 slots) master\n   2 additional replica(s)\nS: 622f73cd06c5658f8d02056925ac708750f12c1a 192.168.1.137:7003\n   slots: (0 slots) slave\n   replicates 06ea827f8d328d9d776c9643109317b0100727a6\nS: 1d1b9817e39ee8987a3518f62a9b91c3ab666eff 192.168.1.136:7002\n   slots: (0 slots) slave\n   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7\nS: a73c099dcc63f5d46a11d0f61c91270ef61290ff 192.168.1.136:7003\n   slots: (0 slots) slave\n   replicates f9133541e2175958117753ef4e206ea43a21f07c\nS: a0e13083fcc1d6e96398f3bb2ea5581b7a64e05e 192.168.1.135:7003\n   slots: (0 slots) slave\n   replicates 06ea827f8d328d9d776c9643109317b0100727a6\nM: f9133541e2175958117753ef4e206ea43a21f07c 192.168.1.137:7001\n   slots:[10923-16383] (5461 slots) master\n   2 additional replica(s)\nS: 626dc659bb1059ec40039869241f7de88a49cd87 192.168.1.137:7002\n   slots: (0 slots) slave\n   replicates 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7\nS: 4508bee0c33784e0d5be25b64e4c7e677cd9d396 192.168.1.135:7002\n   slots: (0 slots) slave\n   replicates f9133541e2175958117753ef4e206ea43a21f07c\n[OK] All nodes agree about slots configuration.\n&gt;&gt;&gt; Check for open slots...\n&gt;&gt;&gt; Check slots coverage...\n[OK] All 16384 slots covered.\n</code></pre>\n<h5 id=\"26-访问reids集群并验证\"><a class=\"anchor\" href=\"#26-访问reids集群并验证\">#</a> 2.6 访问 reids 集群并验证</h5>\n<pre><code>cd /data/redis/src\n./redis-cli -h 192.168.1.135 -p 7001 -c -a admin123\n#列出当前节点的信息：cluster info\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n192.168.1.135:7001&gt; cluster info\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:9\ncluster_size:3\ncluster_current_epoch:9\ncluster_my_epoch:1\ncluster_stats_messages_ping_sent:344\ncluster_stats_messages_pong_sent:354\ncluster_stats_messages_sent:698\ncluster_stats_messages_ping_received:346\ncluster_stats_messages_pong_received:344\ncluster_stats_messages_meet_received:8\ncluster_stats_messages_received:698\ntotal_cluster_links_buffer_limit_exceeded:0\n列出集群的节点的信息：cluster nodes\n192.168.1.135:7001&gt; cluster nodes\n06ea827f8d328d9d776c9643109317b0100727a6 192.168.1.136:7001@17001 master - 0 1747034145581 4 connected 5461-10922\n622f73cd06c5658f8d02056925ac708750f12c1a 192.168.1.137:7003@17003 slave 06ea827f8d328d9d776c9643109317b0100727a6 0 1747034145581 4 connected\n1d1b9817e39ee8987a3518f62a9b91c3ab666eff 192.168.1.136:7002@17002 slave 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 0 1747034145581 1 connected\na73c099dcc63f5d46a11d0f61c91270ef61290ff 192.168.1.136:7003@17003 slave f9133541e2175958117753ef4e206ea43a21f07c 0 1747034145581 7 connected\na0e13083fcc1d6e96398f3bb2ea5581b7a64e05e 192.168.1.135:7003@17003 slave 06ea827f8d328d9d776c9643109317b0100727a6 0 1747034144077 4 connected\nf9133541e2175958117753ef4e206ea43a21f07c 192.168.1.137:7001@17001 master - 0 1747034145079 7 connected 10923-16383\n626dc659bb1059ec40039869241f7de88a49cd87 192.168.1.137:7002@17002 slave 928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 0 1747034144000 1 connected\n928637d72deb6a2e7935f8a7bb5ebd9455cf64a7 192.168.1.135:7001@17001 myself,master - 0 1747034144000 1 connected 0-5460\n4508bee0c33784e0d5be25b64e4c7e677cd9d396 192.168.1.135:7002@17002 slave f9133541e2175958117753ef4e206ea43a21f07c 0 1747034144578 7 connected\n</code></pre>\n",
            "tags": [
                "Redis"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/170573601.html",
            "url": "http://ixuyong.cn/posts/170573601.html",
            "title": "K8s服务发布Ingress",
            "date_published": "2025-04-26T08:52:06.000Z",
            "content_html": "<h4 id=\"1-ingress-nginx-controller-安装\"><a class=\"anchor\" href=\"#1-ingress-nginx-controller-安装\">#</a> 1. Ingress Nginx Controller 安装</h4>\n<table>\n<thead>\n<tr>\n<th>Supported</th>\n<th>Ingress-NGINX version</th>\n<th>k8s supported version</th>\n<th>Alpine Version</th>\n<th>Nginx Version</th>\n<th>Helm Chart Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>🔄</td>\n<td><strong>v1.12.1</strong></td>\n<td>1.32, 1.31, 1.30, 1.29, 1.28</td>\n<td>3.21.3</td>\n<td>1.25.5</td>\n<td>4.12.1</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.12.0</strong></td>\n<td>1.32, 1.31, 1.30, 1.29, 1.28</td>\n<td>3.21.0</td>\n<td>1.25.5</td>\n<td>4.12.0</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.12.0-beta.0</strong></td>\n<td>1.32, 1.31, 1.30, 1.29, 1.28</td>\n<td>3.20.3</td>\n<td>1.25.5</td>\n<td>4.12.0-beta.0</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.11.5</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.21.3</td>\n<td>1.25.5</td>\n<td>4.11.5</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.11.4</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.21.0</td>\n<td>1.25.5</td>\n<td>4.11.4</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.11.3</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.3</td>\n<td>1.25.5</td>\n<td>4.11.3</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.11.2</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.11.2</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.11.1</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.11.1</td>\n</tr>\n<tr>\n<td>🔄</td>\n<td><strong>v1.11.0</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.11.0</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.6</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.21.0</td>\n<td>1.25.5</td>\n<td>4.10.6</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.5</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.3</td>\n<td>1.25.5</td>\n<td>4.10.5</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.4</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.10.4</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.3</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.10.3</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.2</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.10.2</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.1</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.19.1</td>\n<td>1.25.3</td>\n<td>4.10.1</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.0</strong></td>\n<td>1.29, 1.28, 1.27, 1.26</td>\n<td>3.19.1</td>\n<td>1.25.3</td>\n<td>4.10.0</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.6</td>\n<td>1.29, 1.28, 1.27, 1.26, 1.25</td>\n<td>3.19.0</td>\n<td>1.21.6</td>\n<td>4.9.1</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.5</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.9.0</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.4</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.8.3</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.3</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.8.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.1</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.8.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.0</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.2</td>\n<td>1.21.6</td>\n<td>4.8.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.8.4</td>\n<td>1.27, 1.26, 1.25, 1.24</td>\n<td>3.18.2</td>\n<td>1.21.6</td>\n<td>4.7.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.7.1</td>\n<td>1.27, 1.26, 1.25, 1.24</td>\n<td>3.17.2</td>\n<td>1.21.6</td>\n<td>4.6.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.6.4</td>\n<td>1.26, 1.25, 1.24, 1.23</td>\n<td>3.17.0</td>\n<td>1.21.6</td>\n<td>4.5.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.5.1</td>\n<td>1.25, 1.24, 1.23</td>\n<td>3.16.2</td>\n<td>1.21.6</td>\n<td>4.4.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.4.0</td>\n<td>1.25, 1.24, 1.23, 1.22</td>\n<td>3.16.2</td>\n<td>1.19.10†</td>\n<td>4.3.0</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.3.1</td>\n<td>1.24, 1.23, 1.22, 1.21, 1.20</td>\n<td>3.16.2</td>\n<td>1.19.10†</td>\n<td>4.2.5</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"11-helm安装ingress-nginx-controller\"><a class=\"anchor\" href=\"#11-helm安装ingress-nginx-controller\">#</a> 1.1 Helm 安装 Ingress Nginx Controller</h5>\n<ol>\n<li>安装 Helm</li>\n</ol>\n<pre><code># wget https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz\n# tar xf helm-v3.6.3-linux-amd64.tar.gz\n# mv linux-amd64/helm /usr/local/bin/helm\n# helm version\n</code></pre>\n<ol start=\"2\">\n<li>下载 Ingress Nginx Controller 安装包</li>\n</ol>\n<pre><code>官方文档：https://github.com/kubernetes/ingress-nginx/tree/helm-chart-4.8.2         #根据自己k8s版本下载\n# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n# helm repo update\n# helm repo list\n# helm pull ingress-nginx/ingress-nginx --version 4.8.2\n</code></pre>\n<ol start=\"3\">\n<li>配置 Ingress Nginx Controller</li>\n</ol>\n<pre><code># tar xf ingress-nginx-4.8.2.tgz\n# cd ingress-nginx\n# vim values.yaml\n...\n 16 controller:\n 17   name: controller\n 18   enableAnnotationValidations: false\n 19   image:\n 20     ## Keep false as default for now!\n 21     chroot: false\n 22     registry: registry.cn-hangzhou.aliyuncs.com\n 23     image: kubernetes_public/ingress-nginx-controller\n 24     ## for backwards compatibility consider setting the full image url via the repository value below\n 25     ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml wil    l fail\n 26     ## repository:\n 27     tag: &quot;v1.9.3&quot;\n 28     #digest: sha256:8fd21d59428507671ce0fb47f818b1d859c92d2ad07bb7c947268d433030ba98\n...\n 42   # -- Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configurat    ion/configmap/\n 43   config:\n 44     allow-snippet-annotations: true          #开启server snippet的配置\n...\n 67   dnsPolicy: ClusterFirstWithHostNet\n...\n 88   hostNetwork: true\n...\n107   ingressClassResource:\n108     # -- Name of the ingressClass\n109     name: nginx\n110     # -- Is this ingressClass enabled or not\n111     enabled: true\n112     # -- Is this the default ingressClass for the cluster\n113     default: true\n...\n184   kind: DaemonSet\n...\n287   nodeSelector:\n288     kubernetes.io/os: linux\n289     ingress: &quot;true&quot;\n...\n638       image:\n639         registry: registry.cn-hangzhou.aliyuncs.com\n640         image: kubernetes_public/kube-webhook-certgen\n641         ## for backwards compatibility consider setting the full image url via the repository value below\n642         ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml     will fail\n643         ## repository:\n644         tag: v20231011-8b53cabe0\n645         #digest: sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\n...\n</code></pre>\n<p>4. 给需要部署 ingress 的节点上打标签</p>\n<pre><code># kubectl label node k8s-node02 ingress=true\n# kubectl label node k8s-node01 ingress=true\n# kubectl create ns ingress-nginx\n# helm install ingress-nginx -n ingress-nginx .     #安装\n# helm upgrade ingress-nginx -n ingress-nginx .     #更新\n# kubectl get pods -n ingress-nginx \nNAME                             READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-7nfqn   1/1     Running   0          27s\ningress-nginx-controller-k4p2n   1/1     Running   0          17m\ningress-nginx-controller-kw5jk   1/1     Running   0          24s\n</code></pre>\n<h5 id=\"12-bare-metal安装ingress-nginx-controller\"><a class=\"anchor\" href=\"#12-bare-metal安装ingress-nginx-controller\">#</a> 1.2 Bare metal 安装 Ingress Nginx Controller</h5>\n<ol>\n<li>下载 Ingress 部署文件，链接地址：<a href=\"https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters\">https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters</a></li>\n</ol>\n<pre><code>[root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.1/deploy/static/provider/baremetal/deploy.yaml\n</code></pre>\n<ol start=\"2\">\n<li>配置 Ingress</li>\n</ol>\n<pre><code>[root@k8s-master01 ingress-master]# cat deploy.yaml \napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  name: ingress-nginx\n---\napiVersion: v1\nautomountServiceAccountToken: true\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\n  namespace: ingress-nginx\n---\napiVersion: v1\nautomountServiceAccountToken: true\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\n  namespace: ingress-nginx\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - namespaces\n  verbs:\n  - get\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - configmaps\n  - pods\n  - secrets\n  - endpoints\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses/status\n  verbs:\n  - update\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingressclasses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resourceNames:\n  - ingress-nginx-leader\n  resources:\n  - leases\n  verbs:\n  - get\n  - update\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - create\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - events\n  verbs:\n  - create\n  - patch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - list\n  - watch\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - secrets\n  verbs:\n  - get\n  - create\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - configmaps\n  - endpoints\n  - nodes\n  - pods\n  - secrets\n  - namespaces\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - nodes\n  verbs:\n  - get\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - events\n  verbs:\n  - create\n  - patch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses/status\n  verbs:\n  - update\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingressclasses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - list\n  - watch\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\nrules:\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - validatingwebhookconfigurations\n  verbs:\n  - get\n  - update\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\n  namespace: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ingress-nginx\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ingress-nginx-admission\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ingress-nginx\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ingress-nginx-admission\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\n---\napiVersion: v1\ndata: null\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - appProtocol: http\n    name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - appProtocol: https\n    name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  #type: NodePort\n  type: ClusterIP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller-admission\n  namespace: ingress-nginx\nspec:\n  ports:\n  - appProtocol: https\n    name: https-webhook\n    port: 443\n    targetPort: webhook\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  type: ClusterIP\n---\napiVersion: apps/v1\n#kind: Deployment\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/name: ingress-nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/version: 1.12.1\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --election-id=ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/ingress-nginx-controller-v1.12.1:v1.12.1 \n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: controller\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        - containerPort: 8443\n          name: webhook\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsGroup: 82\n          runAsNonRoot: true\n          runAsUser: 101\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /usr/local/certificates/\n          name: webhook-cert\n          readOnly: true\n      hostNetwork: true                         # 与节点共享网络名称空间\n      #dnsPolicy: ClusterFirst\n      dnsPolicy: ClusterFirstWithHostNet        # dns 策略\n      nodeSelector:                             # 节点选择器\n        kubernetes.io/os: linux\n        ingress: &quot;true&quot;\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: ingress-nginx-admission\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission-create\n  namespace: ingress-nginx\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-webhook\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/version: 1.12.1\n      name: ingress-nginx-admission-create\n    spec:\n      containers:\n      - args:\n        - create\n        - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 \n        imagePullPolicy: IfNotPresent\n        name: create\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      nodeSelector:\n        kubernetes.io/os: linux\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission-patch\n  namespace: ingress-nginx\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-webhook\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/version: 1.12.1\n      name: ingress-nginx-admission-patch\n    spec:\n      containers:\n      - args:\n        - patch\n        - --webhook-name=ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 \n        imagePullPolicy: IfNotPresent\n        name: patch\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      nodeSelector:\n        kubernetes.io/os: linux\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n---\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: nginx\nspec:\n  controller: k8s.io/ingress-nginx\n---\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\nwebhooks:\n- admissionReviewVersions:\n  - v1\n  clientConfig:\n    service:\n      name: ingress-nginx-controller-admission\n      namespace: ingress-nginx\n      path: /networking/v1/ingresses\n      port: 443\n  failurePolicy: Fail\n  matchPolicy: Equivalent\n  name: validate.nginx.ingress.kubernetes.io\n  rules:\n  - apiGroups:\n    - networking.k8s.io\n    apiVersions:\n    - v1\n    operations:\n    - CREATE\n    - UPDATE\n    resources:\n    - ingresses\n  sideEffects: None\n</code></pre>\n<ul>\n<li>type: ClusterIP                                              #service 类型改为 ClusterIP</li>\n<li>hostNetwork: true                                      # 与节点共享网络名称空间</li>\n<li>dnsPolicy: ClusterFirstWithHostNet        # dns 策略</li>\n<li>nodeSelector:                                             # 节点选择器</li>\n<li>kind: DaemonSet                                        # 资源类型 DaemonSet</li>\n</ul>\n<ol start=\"3\">\n<li>在指定节点部署 Ingress-Controller</li>\n</ol>\n<pre><code>[root@k8s-master01 ingress-master]# kubectl apply -f deploy.yaml -n ingress-nginx\n\n[root@k8s-master01 ingress-master]# kubectl label node k8s-node01 ingress=true\n[root@k8s-master01 ingress-master]# kubectl label node k8s-node02 ingress=true\n[root@k8s-master01 ingress-master]# kubectl label node k8s-master03 ingress-     #取消节点部署\n\n[root@k8s-master01 ingress-master]# kubectl get pods -n ingress-nginx \nNAME                                   READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create-zp6mh   0/1     Completed   0          12m\ningress-nginx-admission-patch-f2bpd    0/1     Completed   0          12m\ningress-nginx-controller-rgtkc         1/1     Running     0          3m59s\ningress-nginx-controller-trmn8         1/1     Running     0          3m59s\n</code></pre>\n<h4 id=\"2-ingress-nginx-入门使用\"><a class=\"anchor\" href=\"#2-ingress-nginx-入门使用\">#</a> 2. Ingress Nginx 入门使用</h4>\n<pre><code># cat web-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: test.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h4 id=\"3-ingress-nginx-域名重定向-redirect\"><a class=\"anchor\" href=\"#3-ingress-nginx-域名重定向-redirect\">#</a> 3. Ingress Nginx 域名重定向 Redirect</h4>\n<pre><code># cat redirect-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: redirect-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/permanent-redirect: https://www.baidu.com\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: redirect.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h4 id=\"4-ingress-nginx-前后端分离-rewrite\"><a class=\"anchor\" href=\"#4-ingress-nginx-前后端分离-rewrite\">#</a> 4. Ingress Nginx 前后端分离 Rewrite</h4>\n<pre><code># cat rewrite-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rewrite-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: rewrite.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /api(/|$)(.*)\n        pathType: ImplementationSpecif\n</code></pre>\n<h4 id=\"5-ingress-nginx-错误代码重定向\"><a class=\"anchor\" href=\"#5-ingress-nginx-错误代码重定向\">#</a> 5. Ingress Nginx 错误代码重定向</h4>\n<pre><code>\n</code></pre>\n<h4 id=\"6-ingress-nginx-ssl\"><a class=\"anchor\" href=\"#6-ingress-nginx-ssl\">#</a> 6. Ingress Nginx SSL</h4>\n<pre><code>1.生成证书\n# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.cert -subj &quot;/CN=s.hmallleasing.com/O=tls.hmallleasing.com&quot;\n\n2.创建证书\n# kubectl create secret tls tls.hmallleasig.com --key tls.key --cert tls.cert\n\n3.ingress配置\n# kubectl create secret tls tls.hmallleasig.com --cert=tls.crt --key=tls.key\n# cat tls-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #禁用https强制跳转\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: tls.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n  tls:                  #https\n  - hosts:\n    - tls.hmallleasing.com\n    secretName: &quot;tls.hmallleasig.com&quot;\t\n</code></pre>\n<h4 id=\"7-ingress-nginx-匹配请求头\"><a class=\"anchor\" href=\"#7-ingress-nginx-匹配请求头\">#</a> 7. Ingress Nginx 匹配请求头</h4>\n<pre><code>1.部署移动端应用\n# kubectl create deploy phone --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:phone\n# kubectl expose deploy phone --port 80\n# vim m-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: m-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: m.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: phone\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\n2.部署PC端应用\t\t\n# kubectl create deploy laptop --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:laptop\t\n# kubectl expose deploy laptop --port 80\n# vim laptop-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/server-snippet: |\n      set $agentflag 0;\n          if ($http_user_agent ~* &quot;(Android|iPhone|Windows Phone|UC|Kindle)&quot; )&#123;\n              set $agentflag 1;\n          &#125;\n          if ( $agentflag = 1 ) &#123;\n              return 301 http://m.hmallleaing.com;\n          &#125;\n  name: laptop-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: laptop\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\t\n</code></pre>\n<h4 id=\"8ingress-nginx-基本认证\"><a class=\"anchor\" href=\"#8ingress-nginx-基本认证\">#</a> 8.Ingress Nginx 基本认证</h4>\n<pre><code># yum install httpd -y\n# htpasswd -c auth superman\n# cat auth \nsuperman:$apr1$AC1pc3dK$RJyWnyDJFNKY6twneGVrA1\t\t\n\n# kubectl create secret generic basic-auth --from-file=auth\n# cat basic-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: basic-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/auth-type: basic  # 认证类型\n    nginx.ingress.kubernetes.io/auth-secret: basic-auth  # 包含用户和密码的 secret 资源名称\n    nginx.ingress.kubernetes.io/auth-realm: 'Please User password'  # 要显示的信息\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: basic.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h4 id=\"9-ingress-nginx-黑白名单\"><a class=\"anchor\" href=\"#9-ingress-nginx-黑白名单\">#</a> 9. Ingress Nginx 黑 / 白名单</h4>\n<pre><code>写法一：\n# cat white-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: white-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/whitelist-source-range: &quot;192.168.40.101&quot;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: white.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\t\n\n写法二：\t\t\n[root@k8s-master01 ingress]# cat white-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: white-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/whitelist-source-range: &quot;192.168.40.0/24&quot;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: white.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\n\n写法三：\n# cat white-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: white-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/server-snippet: |\n      allow 192.168.40.0/24;\n      deny all;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: white.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\t\t\n\n#Master01测试\t\t\n# curl -H &quot;Host:white.hmallleasing.com&quot; http://192.168.40.103 -I\nHTTP/1.1 200 OK\nDate: Sat, 14 Oct 2023 13:12:03 GMT\nContent-Type: text/html\nContent-Length: 612\nConnection: keep-alive\nLast-Modified: Tue, 16 Apr 2019 13:08:19 GMT\nETag: &quot;5cb5d3c3-264&quot;\nAccept-Ranges: bytes\t\t\n\n#Master02测试\n# curl -H &quot;Host:white.hmallleasing.com&quot; http://192.168.40.103 -I\nHTTP/1.1 403 Forbidden\nDate: Sat, 14 Oct 2023 13:13:34 GMT\nContent-Type: text/html\nContent-Length: 146\nConnection: keep-alive\n</code></pre>\n<h4 id=\"10-ingress-nginx-速率限制\"><a class=\"anchor\" href=\"#10-ingress-nginx-速率限制\">#</a> 10. Ingress Nginx 速率限制</h4>\n<pre><code>[root@k8s-master01 ingress]# cat limit-rate-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rate-limit-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/limit-rps: &quot;50&quot;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: rate-limit.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\n# ab -c 20 -n 1000 http://rate-limit.hmallleasing.com/ |grep request\nComplete requests:      1000\nFailed requests:        724\nTime per request:       10.301 [ms] (mean)\nTime per request:       0.515 [ms] (mean, across all concurrent requests)\nPercentage of the requests served within a certain time (ms)\n</code></pre>\n<h4 id=\"11使用-nginx-实现灰度金丝雀发布\"><a class=\"anchor\" href=\"#11使用-nginx-实现灰度金丝雀发布\">#</a> 11. 使用 Nginx 实现灰度 / 金丝雀发布</h4>\n<pre><code>1.创建 v1 版本\n# kubectl create deploy canary-v1 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v1\t\n# kubectl expose deploy canary-v1 --port 8080\n# cat canary-v1-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: canary-v1-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: canary.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: canary-v1\n            port:\n              number: 8080\n        path: /\n        pathType: ImplementationSpecific\n\t\t\n# curl -H &quot;Host:canary.hmallleasing.com&quot; http://192.168.40.103 \t\n\n2.创建 v2 版本\n# kubectl create deploy canary-v2 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v2\n# kubectl expose deploy canary-v2 --port 8080\n# cat canary-v2-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: canary-v2-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: &quot;true&quot;    #启动灰度发布\n    nginx.ingress.kubernetes.io/canary-weight: &quot;20&quot;  #基于权重,50%流量调度到这个灰度的版本上\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: canary.hmallleasing.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: canary-v2\n            port:\n              number: 8080\n\n#测试灰度发布\n[root@k8s-master01 ingress]# cat canary.sh \n#!/bin/bash\n\nwhile true\ndo\n\tcurl -H &quot;Host:canary.hmallleasing.com&quot; http://192.168.40.103\n\tsleep 0.5\ndone\n</code></pre>\n<h4 id=\"12-kubernetes-dashboard配置证书\"><a class=\"anchor\" href=\"#12-kubernetes-dashboard配置证书\">#</a> 12. kubernetes-dashboard 配置证书</h4>\n<pre><code>1.创建证书\nkubectl create secret tls kubernetes-dashboard-certs --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n kubernetes-dashboard\n\n2.修改kubernetes-dashboard资源清单\nkubectl edit deployment -n kubernetes-dashboard kubernetes-dashboard\n...\n      - args:\n        - --auto-generate-certificates=false\n        - --tls-key-file=_.hmallleasing.com_key.key\n        - --tls-cert-file=_.hmallleasing.com_chain.crt\n        - --token-ttl=21600\n        - --authentication-mode=basic,token\n        - --namespace=kubernetes-dashboard\n...\n\n3.创建ingress\n#cat dashboard-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;    \n    nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;    \nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: dashboard.hmallleasing.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n\n# kubectl apply -f dashboard-ingress.yaml \n</code></pre>\n<h4 id=\"13-入口lb配置\"><a class=\"anchor\" href=\"#13-入口lb配置\">#</a> 13. 入口 LB 配置</h4>\n<pre><code>[root@lb nginx]# cat /etc/nginx/conf.d/ingress.conf \nupstream ingress &#123;\n\tserver 192.168.40.103:80 max_conns=2000 max_fails=2 fail_timeout=5s;\n\tserver 192.168.40.104:80 max_conns=2000 max_fails=2 fail_timeout=5s;\n\tserver 192.168.40.105:80 max_conns=2000 max_fails=2 fail_timeout=5s;\n&#125;\n\nserver &#123;\n    listen 443 ssl;\n    server_name test.hmallleasing.com;\n    client_max_body_size 1G; \n    ssl_prefer_server_ciphers on;\n    ssl_certificate  /etc/nginx/sslkey/*.hmallleasing.com_chain.crt;\n    ssl_certificate_key  /etc/nginx/sslkey/*.hmallleasing.com_key.key;\n\n    location / &#123;\n        proxy_pass http://ingress;\n        include proxy_params;\n\t    proxy_next_upstream error timeout http_500 http_502 http_503 http_504;\n\t    proxy_next_upstream_tries 2;\n\t    proxy_next_upstream_timeout 3s;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name test.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n\n[root@lb ~]# mkdir /etc/nginx/sslkey -p\n\n\n[root@lb ~]# cat proxy_params \nproxy_http_version 1.1;\nproxy_set_header Connectin &quot;&quot;;\n\nproxy_set_header Host $http_host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\nproxy_connect_timeout 60;\nproxy_send_timeout 120;\nproxy_read_timeout 120;\n\nproxy_buffering on;\nproxy_buffer_size 32k;\nproxy_buffers 4 128k;\nproxy_temp_file_write_size 10240k;\nproxy_max_temp_file_size 10240k;\n</code></pre>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3364424907.html",
            "url": "http://ixuyong.cn/posts/3364424907.html",
            "title": "阿里云+Github构建镜像仓库",
            "date_published": "2025-04-26T08:20:14.000Z",
            "content_html": "<h3 id=\"阿里云github构建镜像仓库解决-k8sgcrio访问\"><a class=\"anchor\" href=\"#阿里云github构建镜像仓库解决-k8sgcrio访问\">#</a> 阿里云 + github 构建镜像仓库解决 k8s.gcr.io 访问</h3>\n<p><a href=\"http://xn--k8s-xi9d897o.gcr.io/\">由于 k8s.gcr.io/</a> 镜像仓库位于国外，国内使用 kubeadm 构建 docker 集群时无法访问相应的 docker 镜像。</p>\n<h4 id=\"1-登录github创建仓库\"><a class=\"anchor\" href=\"#1-登录github创建仓库\">#</a> <strong>1.</strong> 登录 Github 创建仓库</h4>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/vgZkKBC.png\" alt=\"1.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/VnJlhBE.png\" alt=\"2.png\" /></p>\n<h4 id=\"2-创建dockerfile\"><a class=\"anchor\" href=\"#2-创建dockerfile\">#</a> <strong>2.</strong> 创建 Dockerfile</h4>\n<p>仓库下面创建一个 Dockerfile，以 ingress-nginx-controller 为例下的 dockerfile 内容如下：</p>\n<pre><code>[root@manager ~]# mkdir ingress-nginx-controller\n[root@manager ~]# cd ingress-nginx-controller/\n[root@manager ingress-nginx-controller]# cat Dockerfile \nFROM registry.k8s.io/ingress-nginx/controller:v1.12.1 \n</code></pre>\n<h4 id=\"3-ssh免密登录github\"><a class=\"anchor\" href=\"#3-ssh免密登录github\">#</a> 3. SSH 免密登录 GitHub</h4>\n<pre><code>[root@manager ingress-nginx-controller]# ssh-keygen\n[root@manager ingress-nginx-controller]# cat ~/.ssh/id_rsa.pub\n</code></pre>\n<p>进入<em> GitHub</em> 的个人设置，找到【<em>SSH and GPG keys</em>】，然后点击新增 SSH Key，进入如下界面，<em>title</em> 输入你对于当前<em> SSH key</em> 的备注，下面的<em> key</em> 就粘贴上一步生成的<em> id_rsa.pub</em> 内的内容</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/3djSHRS.png\" alt=\"3.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/8gVcVu4.png\" alt=\"5.png\" /></p>\n<h4 id=\"4-推送dockerfile至github\"><a class=\"anchor\" href=\"#4-推送dockerfile至github\">#</a> 4. 推送 Dockerfile 至 Github</h4>\n<pre><code>[root@manager ingress-nginx-controller]# yum install git -y\n[root@manager ingress-nginx-controller]# git --version\n[root@manager ingress-nginx-controller]# git config --global user.email &quot;373370405@qq.com&quot;\n[root@manager ingress-nginx-controller]# git config --global color.ui true\n[root@manager ingress-nginx-controller]# git init\n[root@manager ingress-nginx-controller]# git add .\n[root@manager ingress-nginx-controller]# git commit -m &quot;first commit&quot;\n[root@manager ingress-nginx-controller]# git branch -M main\n[root@manager ingress-nginx-controller]# git remote add origin git@github.com:xyapples/ingress-nginx-controller.git   #添加远程仓库\n[root@manager ingress-nginx-controller]# git remote -v\n[root@manager ingress-nginx-controller]# git push -u origin main\n[root@manager ingress-nginx-controller]# git remote remove origin  #移除远程仓库\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/N3t49eX.png\" alt=\"6.png\" /></p>\n<h4 id=\"5-登录阿里云创建镜像仓库\"><a class=\"anchor\" href=\"#5-登录阿里云创建镜像仓库\">#</a> <strong>5.</strong> 登录阿里云创建镜像仓库</h4>\n<p>登录阿里云镜像：<a href=\"https://cr.console.aliyun.com/%EF%BC%8C%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%EF%BC%9A\">https://cr.console.aliyun.com/，创建镜像仓库：</a></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/1zFqa35.png\" alt=\"7.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/PhzoeeT.png\" alt=\"1.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/iYAbB0x.png\" alt=\"2.png\" /></p>\n<h4 id=\"6-构建镜像\"><a class=\"anchor\" href=\"#6-构建镜像\">#</a> 6. 构建镜像</h4>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/KD3DI7J.png\" alt=\"3.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/WiwNBRK.png\" alt=\"4.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/155pUrE.png\" alt=\"7.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/155pUrE.png\" alt=\"7.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/AU1371X.png\" alt=\"8.png\" /></p>\n",
            "tags": [
                "Docker"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3030097036.html",
            "url": "http://ixuyong.cn/posts/3030097036.html",
            "title": "K8S云原生存储Rook-Ceph",
            "date_published": "2025-04-24T13:43:19.000Z",
            "content_html": "<h3 id=\"k8s云原生存储rook-ceph\"><a class=\"anchor\" href=\"#k8s云原生存储rook-ceph\">#</a> K8S 云原生存储 Rook-Ceph</h3>\n<h4 id=\"1-storageclass动态存储\"><a class=\"anchor\" href=\"#1-storageclass动态存储\">#</a> 1. StorageClass 动态存储</h4>\n<p>StorageClass：存储类，由 K8s 管理员创建，用于动态 PV 的管理，可以链接至不同的后端存储，比如 Ceph、GlusterFS 等。之后对存储的请求可以指向 StorageClass，然后 StorageClass 会自动的创建、删除 PV。</p>\n<p>实现方式：</p>\n<ul>\n<li>in-tree: 内置于 K8s 核心代码，对于存储的管理，都需要编写相应的代码。</li>\n<li>out-of-tree：由存储厂商提供一个驱动（CSI 或 Flex Volume），安装到 K8s 集群，然后 StorageClass 只需要配置该驱动即可，驱动器会代替 StorageClass 管理存储。</li>\n</ul>\n<p>StorageClass 官网介绍：<a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes/\">https://kubernetes.io/docs/concepts/storage/storage-classes/</a></p>\n<h4 id=\"2-云原生存储rook\"><a class=\"anchor\" href=\"#2-云原生存储rook\">#</a> 2. 云原生存储 Rook</h4>\n<p>Rook 是一个自我管理的分布式存储编排系统，它本身并不是存储系统，在存储和 k8s 之前搭建了一个桥梁，使存储系统的搭建或者维护变得特别简单，Rook 将分布式存储系统转变为自我管理、自我扩展、自我修复的存储服务。它让一些存储的操作，比如部署、配置、扩容、升级、迁移、灾难恢复、监视和资源管理变得自动化，无需人工处理。并且 Rook 支持 CSI，可以利用 CSI 做一些 PVC 的快照、扩容、克隆等操作。</p>\n<p>Rook 官网介绍：<a href=\"https://rook.io/\">https://rook.io/</a></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/CK4Gn1u.jpeg\" alt=\"Snipaste_2025-05-07_20-15-59.jpg\" /></p>\n<h4 id=\"3-rook-安装\"><a class=\"anchor\" href=\"#3-rook-安装\">#</a> 3. Rook 安装</h4>\n<p>环境准备</p>\n<ul>\n<li>K8s 集群至少五个节点，每个节点的内存不低于 5G，CPU 不低于 2 核</li>\n<li>所有节点时间同步</li>\n<li>至少有三个存储节点，并且每个节点至少有一个裸盘，k8s-master03、k8s-node01、k8s-node02 增加裸盘</li>\n</ul>\n<h5 id=\"31-下载-rook-安装文件\"><a class=\"anchor\" href=\"#31-下载-rook-安装文件\">#</a> 3.1 下载 Rook 安装文件</h5>\n<pre><code>[root@k8s-master01 ~]# git clone --single-branch --branch v1.17.2 https://github.com/rook/rook.git\n</code></pre>\n<h5 id=\"32-配置更改\"><a class=\"anchor\" href=\"#32-配置更改\">#</a> 3.2 配置更改</h5>\n<pre><code>[root@k8s-master01 ~]# cd rook/deploy/examples\n[root@k8s-master01 ~]# vim operator.yaml\n  ROOK_CSI_CEPH_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephcsi:v3.14.0&quot;\n  ROOK_CSI_REGISTRAR_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-node-driver-registrar:v2.13.0&quot;\n  ROOK_CSI_RESIZER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-resizer:v1.13.1&quot;\n  ROOK_CSI_PROVISIONER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-provisioner:v5.1.0&quot;\n  ROOK_CSI_SNAPSHOTTER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-snapshotter:v8.2.0&quot;\n  ROOK_CSI_ATTACHER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-attacher:v4.8.0&quot;\n\n#ROOK_ENABLE_DISCOVERY_DAEMON 改成 true 即可\nROOK_ENABLE_DISCOVERY_DAEMON: &quot;true&quot;\n</code></pre>\n<h5 id=\"33-部署-rook\"><a class=\"anchor\" href=\"#33-部署-rook\">#</a> 3.3 部署 rook</h5>\n<pre><code>[root@k8s-master01 ceph]# kubectl create -f crds.yaml -f common.yaml -f operator.yaml\n[root@k8s-master01 examples]# kubectl get pods -n rook-ceph\nNAME                                  READY   STATUS    RESTARTS   AGE\nrook-ceph-operator-84ff77778b-7ww2w   1/1     Running   0          91m\nrook-discover-6j68f                   1/1     Running   0          82m\nrook-discover-9w4kt                   1/1     Running   0          82m\nrook-discover-h2zfm                   1/1     Running   0          82m\nrook-discover-hsz8b                   1/1     Running   0          19m\nrook-discover-rj4t7                   1/1     Running   0          82m\n</code></pre>\n<h4 id=\"4创建-ceph-集群\"><a class=\"anchor\" href=\"#4创建-ceph-集群\">#</a> 4. 创建 Ceph 集群</h4>\n<h5 id=\"41-配置更改\"><a class=\"anchor\" href=\"#41-配置更改\">#</a> 4.1 配置更改</h5>\n<pre><code>[root@k8s-master01 examples]# vim cluster.yaml\n...\n    image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephv19.2.2:v19.2.2\n...\n  skipUpgradeChecks: true     #改为true，跳过升级\n....\n  dashboard:\n    enabled: true\n    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)\n    # urlPrefix: /ceph-dashboard\n    # serve the dashboard at the given port.\n    # port: 8443\n    # serve the dashboard using SSL\n    ssl: false          #改为false\n...\n  storage: # cluster level storage configuration and selection\n    useAllNodes: false      #改为false,不使用所有的节点当osd\n    useAllDevices: false    #改为false,不使用所有的磁盘当osd\n...\n    #     deviceFilter: &quot;^sd.&quot;\n    nodes:\n    - name: &quot;k8s-master03&quot;\n      devices:\n      - name: &quot;sdb&quot;\n    - name: &quot;k8s-node01&quot;\n      devices:\n      - name: &quot;sdb&quot;\n    - name: &quot;k8s-node02&quot;\n      devices:\n      - name: &quot;sdb&quot;\n...\n</code></pre>\n<p>注意：新版必须采用裸盘，即未格式化的磁盘。其中 k8s-master03、 k8s-node01、  k8s-node02 有新加的一个磁盘，可以通过 lsblk -f 查看新添加的磁盘名称。建议最少三个节点，否则后面的试验可能会出现问题</p>\n<h5 id=\"42-创建-ceph-集群\"><a class=\"anchor\" href=\"#42-创建-ceph-集群\">#</a> 4.2 创建 Ceph 集群</h5>\n<pre><code>[root@k8s-master01 examples]# kubectl create -f cluster.yaml\n[root@k8s-master01 examples]# kubectl get pods -n rook-ceph\nNAME                                                     READY   STATUS      RESTARTS        AGE\ncsi-cephfsplugin-5nmnl                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-6b6ct                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-8xlnl                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-fh9w5                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-mslst                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-provisioner-59bd447c6d-5zwj2            6/6     Running     0               61s\ncsi-cephfsplugin-provisioner-59bd447c6d-7t2kg            6/6     Running     2 (20s ago)     61s\ncsi-rbdplugin-5gvmp                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-dzcs4                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-n82b5                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-provisioner-6856fb8b86-86hw8               6/6     Running     0               19s\ncsi-rbdplugin-provisioner-6856fb8b86-lj9s4               6/6     Running     0               19s\ncsi-rbdplugin-vh8j2                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-xfgwr                                      3/3     Running     1 (60m ago)     62m\nrook-ceph-crashcollector-k8s-master01-bbc78d496-bzjk8    1/1     Running     0               8m26s\nrook-ceph-crashcollector-k8s-master03-765ff964bb-95wmt   1/1     Running     0               28m\nrook-ceph-crashcollector-k8s-node01-7cf4c4b6b6-r4n84     1/1     Running     0               20m\nrook-ceph-crashcollector-k8s-node02-f887f8cf9-jz2l8      1/1     Running     0               28m\nrook-ceph-detect-version-nsrwj                           0/1     Init:0/1    0               3s\nrook-ceph-exporter-k8s-master01-5cd4577b79-ckd4m         1/1     Running     0               8m26s\nrook-ceph-exporter-k8s-master03-75f4cf6f7-hc9zb          1/1     Running     0               28m\nrook-ceph-exporter-k8s-node01-96fc7cf49-d2r24            1/1     Running     0               20m\nrook-ceph-exporter-k8s-node02-777b9f555b-7j6cz           1/1     Running     0               27m\nrook-ceph-mgr-a-6f46b4b945-q6cjb                         3/3     Running     3 (14m ago)     35m\nrook-ceph-mgr-b-5d4cc5465b-8dfh6                         3/3     Running     0               35m\nrook-ceph-mon-a-7c7b7555c7-nlhwg                         2/2     Running     2 (6m14s ago)   51m\nrook-ceph-mon-c-559bcf95fd-cl62w                         2/2     Running     0               8m27s\nrook-ceph-mon-d-7dbc6b8f5c-8264t                         2/2     Running     0               28m\nrook-ceph-operator-645478ff5b-jdcrp                      1/1     Running     0               102m\nrook-ceph-osd-0-6d9cf78f76-4zhx8                         2/2     Running     0               12m\nrook-ceph-osd-1-88c78bbcb-cn48c                          2/2     Running     0               5m15s\nrook-ceph-osd-2-b464c9fc6-458hv                          2/2     Running     0               4m29s\nrook-ceph-osd-prepare-k8s-master03-pwnrc                 0/1     Completed   0               86s\nrook-ceph-osd-prepare-k8s-node01-xxp2j                   0/1     Completed   0               83s\nrook-ceph-osd-prepare-k8s-node02-8nz7x                   0/1     Completed   0               78s\nrook-discover-jzmkr                                      1/1     Running     0               91m\nrook-discover-k7pxt                                      1/1     Running     0               91m\nrook-discover-vqjh5                                      1/1     Running     0               91m\nrook-discover-wk8jq                                      1/1     Running     0               91m\nrook-discover-x8rsn                                      1/1     Running     0               91m\n\n[root@k8s-master01 examples]# kubectl get cephcluster -n rook-ceph\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH        EXTERNAL   FSID\nrook-ceph   /var/lib/rook     3          63m   Ready   Cluster created successfully   HEALTH_WARN              ca429602-66f4-4a1e-9d5c-a5773a0f594f\n</code></pre>\n<h5 id=\"43-安装-ceph-snapshot-控制器\"><a class=\"anchor\" href=\"#43-安装-ceph-snapshot-控制器\">#</a> 4.3 安装 ceph snapshot 控制器</h5>\n<pre><code>[root@k8s-master01 ~]# cd /root/k8s-ha-install/\n[root@k8s-master01 k8s-ha-install]# git checkout manual-installation-v1.32.x\n[root@k8s-master01 k8s-ha-install]# kubectl create -f snapshotter/ -n kube-system\n[root@k8s-master01 k8s-ha-install]# kubectl get po -n kube-system -l app=snapshot-controller\nNAME                    READY   STATUS    RESTARTS   AGE\nsnapshot-controller-0   1/1     Running   0          67s\n</code></pre>\n<h4 id=\"5-安装-ceph-客户端工具\"><a class=\"anchor\" href=\"#5-安装-ceph-客户端工具\">#</a> 5. 安装 ceph 客户端工具</h4>\n<pre><code>[root@k8s-master01 k8s-ha-install]# cd /root/rook/deploy/examples/\n[root@k8s-master01 examples]# kubectl create -f toolbox.yaml -n rook-ceph\n[root@k8s-master01 examples]# kubectl get po -n rook-ceph -l app=rook-ceph-tools\nNAME                               READY   STATUS    RESTARTS   AGE\nrook-ceph-tools-7b75b967db-sqddk   1/1     Running   0          8s\n[root@k8s-master01 examples]# kubectl exec -it rook-ceph-tools-7b75b967db-sqddk -n rook-ceph -- bash\nbash-5.1$ ceph status\n  cluster:\n    id:     87b85368-9487-4967-a4e4-5970d2e0ec94\n    health: HEALTH_WARN\n            1 mgr modules have recently crashed\n \n  services:\n    mon: 3 daemons, quorum b,c (age 12s), out of quorum: a\n    mgr: a(active, since 7m), standbys: b\n    osd: 3 osds: 3 up (since 8m), 3 in (since 3h)\n \n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   82 MiB used, 60 GiB / 60 GiB avail\n    pgs: \n\t\nbash-4.4$  ceph osd status\nID  HOST           USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      \n 0  k8s-master03  20.6M  19.9G      0        0       0        0   exists,up  \n 1  k8s-node01    20.6M  19.9G      0        0       0        0   exists,up  \n 2  k8s-node02    20.6M  19.9G      0        0       0        0   exists,up \n\nbash-4.4$ ceph df\n--- RAW STORAGE ---\nCLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED\nhdd    60 GiB  60 GiB  62 MiB    62 MiB       0.10\nTOTAL  60 GiB  60 GiB  62 MiB    62 MiB       0.10\n\n--- POOLS ---\nPOOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL\n.mgr   1    1  449 KiB        2  1.3 MiB      0     19 GiB\n</code></pre>\n<h4 id=\"6-ceph-dashboard\"><a class=\"anchor\" href=\"#6-ceph-dashboard\">#</a> 6. Ceph dashboard</h4>\n<h5 id=\"61-暴露服务\"><a class=\"anchor\" href=\"#61-暴露服务\">#</a> 6.1 暴露服务</h5>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc -n rook-ceph\nNAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nrook-ceph-mgr             ClusterIP   10.96.54.15     &lt;none&gt;        9283/TCP            133m\nrook-ceph-mgr-dashboard   ClusterIP   10.96.97.117    &lt;none&gt;        7000/TCP            133m        #暴露ingresss也可\nrook-ceph-mon-a           ClusterIP   10.96.125.216   &lt;none&gt;        6789/TCP,3300/TCP   170m\nrook-ceph-mon-b           ClusterIP   10.96.34.183    &lt;none&gt;        6789/TCP,3300/TCP   133m\nrook-ceph-mon-c           ClusterIP   10.96.232.252   &lt;none&gt;        6789/TCP,3300/TCP   133m\n\n\n[root@k8s-master01 examples]# kubectl create -f dashboard-external-http.yaml           #暴露nodeport\n[root@k8s-master01 examples]# kubectl get svc -n rook-ceph rook-ceph-mgr-dashboard-external-http\nNAME                                     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\nrook-ceph-mgr-dashboard-external-https   NodePort   10.96.11.120   &lt;none&gt;        8443:32611/TCP   45s\n</code></pre>\n<h5 id=\"62-配置ingress访问ceph\"><a class=\"anchor\" href=\"#62-配置ingress访问ceph\">#</a> 6.2 配置 ingress 访问 ceph</h5>\n<pre><code>[root@k8s-master01 examples]# cat dashboard-ingress-https.yaml \n#\n# This example is for Kubernetes running an nginx-ingress\n# and an ACME (e.g. Let's Encrypt) certificate service\n#\n# The nginx-ingress annotations support the dashboard\n# running using HTTPS with a self-signed certificate\n#\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rook-ceph-mgr-dashboard\n  namespace: rook-ceph # namespace:cluster\n#  annotations:\n#    kubernetes.io/ingress.class: &quot;nginx&quot;\n#    kubernetes.io/tls-acme: &quot;true&quot;\n#    nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;\n#    nginx.ingress.kubernetes.io/server-snippet: |\n#      proxy_ssl_verify off;\n\nspec:\n  ingressClassName: &quot;nginx&quot;\n#  tls:\n#    - hosts:\n#        - rook-ceph.hmallleasing.com\n#      secretName: rook-ceph.example.com\n  rules:\n    - host: rook-ceph.hmallleasing.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: rook-ceph-mgr-dashboard\n                port:\n                  name: http-dashboard\n</code></pre>\n<h5 id=\"63-登录\"><a class=\"anchor\" href=\"#63-登录\">#</a> 6.3 登录</h5>\n<pre><code>http://192.168.40.100:32611\n用户名：admin\n密码：kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;&#123;['data']['password']&#125;&quot; | base64 --decode &amp;&amp; echo\n</code></pre>\n<h4 id=\"7-ceph-块存储的使用\"><a class=\"anchor\" href=\"#7-ceph-块存储的使用\">#</a> 7. Ceph 块存储的使用</h4>\n<p>块存储一般用于一个 Pod 挂载一块存储使用，相当于一个服务器新挂了一个盘，只给一个应用使用。</p>\n<h5 id=\"71-创建-storageclass-和-ceph-的存储池\"><a class=\"anchor\" href=\"#71-创建-storageclass-和-ceph-的存储池\">#</a> 7.1 创建 StorageClass 和 ceph 的存储池</h5>\n<pre><code>[root@k8s-master01 examples]# kubectl get csidriver\nNAME                            ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE\nrook-ceph.cephfs.csi.ceph.com   true             false            false             &lt;unset&gt;         false               Persistent   15h       #文件存储csi\nrook-ceph.rbd.csi.ceph.com      true             false            false             &lt;unset&gt;         false               Persistent   15h       #块存储csi\n\n[root@k8s-master01 ~]# cd /root/rook/deploy/examples/\n[root@k8s-master01 examples]# vim csi/rbd/storageclass.yaml\n...\napiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph # namespace:cluster\nspec:\n  failureDomain: host\n  replicated:\n    size: 3                #数据保存几份，测试环境可以将副本数设置成了 2（不能设置为 1），生产环境最少为 3，且要小于等于 osd 的数量\n...\nallowVolumeExpansion: true     #是否可以扩容\nreclaimPolicy: Delete          #pv回收策略\n\n[root@k8s-master01 examples]# kubectl create -f csi/rbd/storageclass.yaml -n rook-ceph\n\n[root@k8s-master01 examples]# kubectl get cephblockpool -n rook-ceph\nNAME          PHASE\nreplicapool   Ready\n[root@k8s-master01 examples]# kubectl get sc\nNAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nnfs-storage       nfzl.com/nfs                 Delete          Immediate           false                  16h\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   37s\n</code></pre>\n<h5 id=\"72-挂载测试\"><a class=\"anchor\" href=\"#72-挂载测试\">#</a> 7.2 挂载测试</h5>\n<pre><code>[root@k8s-master01 ~]# cat ceph-block-pvc.yaml        #创建PVC\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ceph-block-pvc\nspec:\n  storageClassName: &quot;rook-ceph-block&quot;     # 明确指定使用哪个sc的供应商来创建pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi                      # 根据业务实际大小进行资源申请\n\n[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc.yaml \n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nceph-block-pvc   Bound    pvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            rook-ceph-block   3s\n\n[root@k8s-master01 ~]# kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS      REASON   AGE\npvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            Delete           Bound    default/ceph-block-pvc   rook-ceph-block\n\t  \n[root@k8s-master01 ~]# cat ceph-block-pvc-pod.yaml    #挂载PVC测试 \napiVersion: v1\nkind: Pod\nmetadata:\n  name: ceph-block-pvc-pod\nspec:\n  containers:\n  - name: ceph-block-pvc-pod\n    image: nginx\n    volumeMounts:\n    - name: nginx-page\n      mountPath: /usr/share/nginx/html\n  volumes:\n  - name: nginx-page\n    persistentVolumeClaim:      \n      claimName: ceph-block-pv\n\n[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-pod.yaml\n</code></pre>\n<h5 id=\"73-statefulset-volumeclaimtemplates\"><a class=\"anchor\" href=\"#73-statefulset-volumeclaimtemplates\">#</a> 7.3 StatefulSet volumeClaimTemplates</h5>\n<pre><code>[root@k8s-master01 ~]# cat ceph-block-pvc-sts.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n\n  - port: 80\n    name: web\n      clusterIP: None\n      selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx # 必须匹配 .spec.template.metadata.labels\n  serviceName: &quot;nginx&quot;\n  replicas: 3 # 默认值是 1\n  template:\n    metadata:\n      labels:\n        app: nginx # 必须匹配 .spec.selector.matchLabels\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.20\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n    name: www\n    spec:\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      storageClassName: &quot;rook-ceph-block&quot;\n      resources:\n        requests:\n          storage: 1Gi\n    \t  \n[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-sts.yaml \n\n[root@k8s-master01 ~]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS       AGE\nweb-0                                     1/1     Running   0              4m19s\nweb-1                                     1/1     Running   0              4m10s\nweb-2                                     1/1     Running   0              2m21s\n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nwww-web-0   Bound    pvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            rook-ceph-block   4m23s\nwww-web-1   Bound    pvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            rook-ceph-block   4m14s\nwww-web-2   Bound    pvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            rook-ceph-block   2m25s\n\n[root@k8s-master01 ~]# kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS      REASON   AGE\npvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            Delete           Bound    default/www-web-0   rook-ceph-block            4m25s\npvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            Delete           Bound    default/www-web-2   rook-ceph-block            2m27s\npvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            Delete           Bound    default/www-web-1   rook-ceph-block            4m16s\n</code></pre>\n<h4 id=\"8-共享文件系统的使用\"><a class=\"anchor\" href=\"#8-共享文件系统的使用\">#</a> 8. 共享文件系统的使用</h4>\n<p>共享文件系统一般用于多个 Pod 共享一个存储</p>\n<h5 id=\"81-创建共享类型的文件系统\"><a class=\"anchor\" href=\"#81-创建共享类型的文件系统\">#</a> 8.1 创建共享类型的文件系统</h5>\n<pre><code>[root@k8s-master01 ~]# cd /root/rook/deploy/examples/\n[root@k8s-master01 examples]# kubectl apply -f filesystem.yaml\n[root@k8s-master01 examples]# kubectl get pod -l app=rook-ceph-mds -n rook-ceph\nNAME                                    READY   STATUS    RESTARTS   AGE\nrook-ceph-mds-myfs-a-7d76cb5988-9nz9p   2/2     Running   0          36s\nrook-ceph-mds-myfs-b-76ff7c784c-vs8nm   2/2     Running   0          33s\n</code></pre>\n<h5 id=\"82-创建共享类型文件系统的-storageclass\"><a class=\"anchor\" href=\"#82-创建共享类型文件系统的-storageclass\">#</a> 8.2 创建共享类型文件系统的 StorageClass</h5>\n<pre><code>[root@k8s-master01 examples]# cd csi/cephfs\n[root@k8s-master01 cephfs]# kubectl create -f storageclass.yaml\n[root@k8s-master01 cephfs]# kubectl get sc\nNAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nnfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  17h\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   82m\nrook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   13s\n</code></pre>\n<h5 id=\"83-挂载测试\"><a class=\"anchor\" href=\"#83-挂载测试\">#</a> 8.3 挂载测试</h5>\n<pre><code>[root@k8s-master01 ~]# cat cephfs-pvc-deploy.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  selector:\n    app: nginx\n  type: ClusterIP\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: nginx-share-pvc\nspec:\n  storageClassName: rook-cephfs \n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi\n---\napiVersion: apps/v1\nkind: Deployment \nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx # has to match .spec.template.metadata.labels\n  replicas: 3 # by default is 1\n  template:\n    metadata:\n      labels:\n        app: nginx # has to match .spec.selector.matchLabels\n    spec:\n      containers:\n      - name: nginx\n        image: nginx \n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n      volumes:\n        - name: www\n          persistentVolumeClaim:\n            claimName: nginx-share-pvc\n\t\t\t\n[root@k8s-master01 ~]# kubectl apply -f cephfs-pvc-deploy.yaml\n[root@k8s-master01 ~]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS        AGE\ncluster-test-84dfc9c68b-5q4ng             1/1     Running   84 (4m2s ago)   16d\nnfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (123m ago)    18h\nweb-6c59f8559-g5xzb                       1/1     Running   0               46s\nweb-6c59f8559-ns77q                       1/1     Running   0               46s\nweb-6c59f8559-qxb5f                       1/1     Running   0               46s\n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    52s\n\n[root@k8s-master01 ~]# kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE\npvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             53s\n\n[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash\nroot@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# echo &quot;hello cephfs&quot; &gt;&gt; index.html\n\n[root@k8s-master01 ~]# kubectl get svc\nNAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\nkubernetes           ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP    16d\nmysql-svc-external   ClusterIP   None          &lt;none&gt;        3306/TCP   9d\nnginx                ClusterIP   10.96.58.17   &lt;none&gt;        80/TCP     4m34s\n[root@k8s-master01 ~]# curl 10.96.58.17\nhello cephfs\n</code></pre>\n<h4 id=\"9pvc-扩容\"><a class=\"anchor\" href=\"#9pvc-扩容\">#</a> 9.PVC 扩容</h4>\n<pre><code>[root@k8s-master01 ~]# kubectl get sc\nNAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nnfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  18h\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   104m     #true允许扩容\nrook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   22m      #true允许扩容\n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    13m\n[root@k8s-master01 ~]# kubectl edit pvc nginx-share-pvc\n...\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi         #更改pvc大小\n  storageClassName: rook-cephfs\n...\n\n[root@k8s-master01 ~]# kubectl get pvc       #查看PVC是否扩容\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    15m\n\n[root@k8s-master01 ~]# kubectl get pv         #查看PV是否扩容\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE\npvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             15m\n\n[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash       #进入容器，查看pod是否扩容  \nroot@web-6c59f8559-g5xzb:/# df -h \nFilesystem                                                                                                                                             Size  Used Avail Use% Mounted on\noverlay                                                                                                                                                 17G   13G  4.1G  76% /\ntmpfs                                                                                                                                                   64M     0   64M   0% /dev\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/sda3                                                                                                                                               17G   13G  4.1G  76% /etc/hosts\nshm                                                                                                                                                     64M     0   64M   0% /dev/shm\n10.96.121.140:6789,10.96.131.130:6789,10.96.62.64:6789:/volumes/csi/csi-vol-3b645a11-58f4-475a-9404-5d84964f5291/e4bdf743-eb18-42c8-b04f-41964f76de4f  5.0G     0  5.0G   0% /usr/share/nginx/html\ntmpfs                                                                                                                                                  3.8G   12K  3.8G   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/asound\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/acpi\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/scsi\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/firmware\n</code></pre>\n<h4 id=\"10-pvc-快照\"><a class=\"anchor\" href=\"#10-pvc-快照\">#</a> 10. PVC 快照</h4>\n<h5 id=\"101-文件共享类型快照\"><a class=\"anchor\" href=\"#101-文件共享类型快照\">#</a> 10.1 文件共享类型快照</h5>\n<pre><code>[root@k8s-master01 ~]# cd rook/deploy/examples\n[root@k8s-master01 examples]# kubectl create -f csi/cephfs/snapshotclass.yaml \n\n[root@k8s-master01 examples]# kubectl get volumesnapshotclass\nNAME                         DRIVER                          DELETIONPOLICY   AGE\ncsi-cephfsplugin-snapclass   rook-ceph.cephfs.csi.ceph.com   Delete           25s\n\n\n#拍摄快照\t\n[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash         #pvc新增数据\nroot@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# touch &#123;1..10&#125;\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls\n1  10  2  3  4\t5  6  7  8  9  index.html\n\n[root@k8s-master01 examples]# kubectl get pvc       #查看pvs并对nginx-share-pvc拍摄快照\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h23m\n\n[root@k8s-master01 examples]# cat csi/cephfs/snapshot.yaml         #拍摄快照\n---\n# 1.17 &lt;= K8s &lt;= v1.19\n# apiVersion: snapshot.storage.k8s.io/v1beta1\n# K8s &gt;= v1.20\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: cephfs-pvc-snapshot\nspec:\n  volumeSnapshotClassName: csi-cephfsplugin-snapclass\n  source:\n    persistentVolumeClaimName: nginx-share-pvc         #基于那个PVC拍摄快照\n\t\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/snapshot.yaml\n[root@k8s-master01 examples]# kubectl get volumesnapshot\nNAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE\ncephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   4m6s           4m8s\n\n#删除pvc数据\n[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash\nroot@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls\n1  10  2  3  4\t5  6  7  8  9  index.html\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# rm -rf &#123;1..10&#125;\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls\nindex.html\n\n#pvc回滚数据\n[root@k8s-master01 examples]# kubectl get volumesnapshot\nNAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE\ncephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   7m39s          7m41s\n\t\n[root@k8s-master01 examples]# cat csi/cephfs/pvc-restore.yaml \n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc-restore\nspec:\n  storageClassName: rook-cephfs       #创建pv的storageclass名称相同\n  dataSource:\n    name: cephfs-pvc-snapshot         #volumesnapshot数据源\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi      #大小等于snapshot大小\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-restore.yaml\n\n[root@k8s-master01 examples]# kubectl get pvc\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ncephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    54s          \nnginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h50m\n\n#挂载PVC测试数据是否恢复\n[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml \n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n    - name: web-server\n      image: nginx\n      volumeMounts:\n        - name: mypvc\n          mountPath: /var/lib/www/html\n  volumes:\n    - name: mypvc\n      persistentVolumeClaim:\n        claimName: cephfs-pvc-restore        #挂载恢复pvc\n        readOnly: false\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml\n[root@k8s-master01 examples]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS        AGE\ncluster-test-84dfc9c68b-5q4ng             1/1     Running   88 (57m ago)    16d\ncsicephfs-demo-pod                        1/1     Running   0               24s\nnfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (6h57m ago)   23h\nweb-6c59f8559-g5xzb                       1/1     Running   0               4h54m\nweb-6c59f8559-ns77q                       1/1     Running   0               4h54m\nweb-6c59f8559-qxb5f                       1/1     Running   0               4h54m\n[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash\nroot@csicephfs-demo-pod:/# ls /var/lib/www/html/               #s删除数据已经恢复\n1  10  2  3  4\t5  6  7  8  9  index.html\n</code></pre>\n<h5 id=\"102-pvc-克隆\"><a class=\"anchor\" href=\"#102-pvc-克隆\">#</a> 10.2 PVC 克隆</h5>\n<pre><code>[root@k8s-master01 examples]# kubectl get pvc\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ncephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    11m\nnginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h1m\n\n\n[root@k8s-master01 examples]# cat csi/cephfs/pvc-clone.yaml \n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc-clone\nspec:\n  storageClassName: rook-cephfs      # pvc 的 storageClass 名称\n  dataSource:\n    name: nginx-share-pvc          #克隆的PVC名称\n    kind: PersistentVolumeClaim\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi                 #大小等于所克隆的PVC大小\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-clone.yaml\n\n[root@k8s-master01 examples]# kubectl get pvc\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ncephfs-pvc-clone     Bound    pvc-0a19b65e-cb5e-4379-a7f7-e0783fcf8ddf   5Gi        RWX            rook-cephfs    22s\ncephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    15m\nnginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h4m\n\n#挂载克隆PVC测试\n[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml \n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n    - name: web-server\n      image: nginx\n      volumeMounts:\n        - name: mypvc\n          mountPath: /var/lib/www/html\n  volumes:\n    - name: mypvc\n      persistentVolumeClaim:\n        claimName: cephfs-pvc-clone      #挂载克隆的pvc\n        readOnly: false\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml          \n[root@k8s-master01 examples]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS         AGE\ncluster-test-84dfc9c68b-5q4ng             1/1     Running   89 (9m54s ago)   16d\ncsicephfs-demo-pod                        1/1     Running   0                17s\nnfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (7h9m ago)     23h\nweb-6c59f8559-g5xzb                       1/1     Running   0                5h6m\nweb-6c59f8559-ns77q                       1/1     Running   0                5h6m\nweb-6c59f8559-qxb5f                       1/1     Running   0                5h6m\n\n[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash\nroot@csicephfs-demo-pod:/# cat /var/lib/www/html/index.html \nhello cephfs\n</code></pre>\n<h4 id=\"11-测试数据清理\"><a class=\"anchor\" href=\"#11-测试数据清理\">#</a> 11. 测试数据清理</h4>\n<pre><code>参考文档：https://rook.io/docs/rook/v1.11/Getting-Started/ceph-teardown/#delete-the-cephcluster-crd\n[root@k8s-master01 ~]# kubectl delete deploy web\n\n[root@k8s-master01 ~]# kubectl delete pods csicephfs-demo-pod\n\n[root@k8s-master01 ~]# kubectl delete pvc --all\n[root@k8s-master01 ~]# kubectl get pvc\nNo resources found in default namespace.\n[root@k8s-master01 ~]# kubectl get pv\nNo resources found\n\n\n[root@k8s-master01 ~]# kubectl get volumesnapshot\nNAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE\ncephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   61m            61m\n[root@k8s-master01 ~]# kubectl delete volumesnapshot cephfs-pvc-snapshot\nvolumesnapshot.snapshot.storage.k8s.io &quot;cephfs-pvc-snapshot&quot; deleted\n\nkubectl delete -n rook-ceph cephblockpool replicapool\nkubectl delete -n rook-ceph cephfilesystem myfs\n\nkubectl delete storageclass rook-ceph-block\nkubectl delete storageclass rook-cephfs\nkubectl delete -f csi/cephfs/kube-registry.yaml\nkubectl delete storageclass csi-cephfs\n\nkubectl -n rook-ceph delete cephcluster rook-ceph\n\nkubectl delete -f operator.yaml\nkubectl delete -f common.yaml\nkubectl delete -f crds.yaml\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3890389502.html",
            "url": "http://ixuyong.cn/posts/3890389502.html",
            "title": "K8S持久化存储NFS+StorageClass",
            "date_published": "2025-04-23T12:08:26.000Z",
            "content_html": "<h3 id=\"k8s持久化存储nfsstorageclass\"><a class=\"anchor\" href=\"#k8s持久化存储nfsstorageclass\">#</a> K8S 持久化存储 NFS+StorageClass</h3>\n<h4 id=\"1-搭建nfs服务器\"><a class=\"anchor\" href=\"#1-搭建nfs服务器\">#</a> 1. 搭建 NFS 服务器</h4>\n<pre><code>#所有K8S节点安装nfs-utils\n[root@k8s-node02 ~]# yum install nfs-utils -y    \n\n#K8S-node02节点配置nfs服务\n[root@k8s-node02 ~]# mkdir /data/nfs -p\n[root@k8s-node02 ~]# cat /etc/exports\n/data/nfs 192.168.1.0/24(rw,no_root_squash)\n[root@k8s-node02 ~]# exportfs -arv   #NFS配置生效 \n[root@k8s-node02 ~]# systemctl start nfs-server &amp;&amp; systemctl enable nfs-server &amp;&amp; systemctl status nfs-server\n</code></pre>\n<h4 id=\"2-创建rbac\"><a class=\"anchor\" href=\"#2-创建rbac\">#</a> 2.  创建 RBAC</h4>\n<pre><code>[root@k8s-node02 ~]# cat 01-rbac.yaml \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: nfs-client-provisioner-runner\nrules:\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;nodes&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;persistentvolumes&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;persistentvolumeclaims&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]\n  - apiGroups: [&quot;storage.k8s.io&quot;]\n    resources: [&quot;storageclasses&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;events&quot;]\n    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: run-nfs-client-provisioner\nsubjects:\n  - kind: ServiceAccount\n    name: nfs-client-provisioner\n    # replace with namespace where provisioner is deployed\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: nfs-client-provisioner-runner\n  apiGroup: rbac.authorization.k8s.io\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: leader-locking-nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\nrules:\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;endpoints&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: leader-locking-nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\nsubjects:\n  - kind: ServiceAccount\n    name: nfs-client-provisioner\n    # replace with namespace where provisioner is deployed\n    namespace: default\nroleRef:\n  kind: Role\n  name: leader-locking-nfs-client-provisioner\n  apiGroup: rbac.authorization.k8s.io\n  \n  \n[root@k8s-master01 ~]# kubectl apply -f 01-rbac.yaml \nserviceaccount/nfs-client-provisioner created\nclusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created\nclusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created\nrole.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created\nrolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created\n</code></pre>\n<h4 id=\"3-创建nfs-provisioner\"><a class=\"anchor\" href=\"#3-创建nfs-provisioner\">#</a> 3. 创建 nfs-provisioner</h4>\n<pre><code>[root@k8s-master01 ~]# cat 02-nfs-provisioner.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nfs-client-provisioner\n  labels:\n    app: nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-client-provisioner\n  template:\n    metadata:\n      labels:\n        app: nfs-client-provisioner\n    spec:\n      serviceAccountName: nfs-client-provisioner\n      containers:\n        - name: nfs-client-provisioner\n          image: registry.cn-hangzhou.aliyuncs.com/old_xu/nfs-subdir-external-provisioner:v4.0.2\n          volumeMounts:\n            - name: nfs-client-root\n              mountPath: /persistentvolumes\n          env:\n            - name: PROVISIONER_NAME\t# nfs-provisioner的名称，后续storageClass要与该名称一致\n              value: nfzl.com/nfs\n            - name: NFS_SERVER\t\t# NFS服务的IP地址\n              value: 192.168.1.75\n            - name: NFS_PATH\t\t# NFS服务共享的路径\n              value: /data/nfs\n      volumes:\n        - name: nfs-client-root\n          nfs:\n            server: 192.168.1.75\n            path: /data/nfs\n\n[root@k8s-master01 ~]# kubectl apply -f 02-nfs-provisioner.yaml \n[root@k8s-master01 ~]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS   AGE\nnfs-client-provisioner-6bcc4587f8-zp8qc   1/1     Running   0          17s\n</code></pre>\n<h4 id=\"4-创建storageclass\"><a class=\"anchor\" href=\"#4-创建storageclass\">#</a> 4. 创建 StorageClass</h4>\n<pre><code>[root@k8s-master01 ~]# cat 03-storageClass.yaml \napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-storage \t# pvc申请时需明确指定的storageClass名称\nprovisioner: nfzl.com/nfs        # 供应商名称，必须和上面创建的&quot;PROVISIONER_NAME&quot;变量值致\nparameters:\n  archiveOnDelete: &quot;false&quot;     # 如果值为false，删除PVC后也会删除目录内容, &quot;true&quot;则会对数据进行保留\n</code></pre>\n<h4 id=\"5-创建pvc\"><a class=\"anchor\" href=\"#5-创建pvc\">#</a> 5. 创建 PVC</h4>\n<pre><code>[root@k8s-master01 ~]# cat 04-nginx-pvc.yaml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sc-pvc-001\nspec:\n  storageClassName: &quot;nfs-storage&quot;     # 明确指定使用哪个sc的供应商来创建pv\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi                      # 根据业务实际大小进行资源申请\n      \n[root@k8s-master01 ~]# kubectl apply -f 04-nginx-pvc.yaml \n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/fgpaP15.png\" alt=\"1.png\" /></p>\n<h4 id=\"6-挂载pvc测试\"><a class=\"anchor\" href=\"#6-挂载pvc测试\">#</a> 6. 挂载 PVC 测试</h4>\n<pre><code>[root@k8s-master01 ~]# cat 05-nginx-pod.yaml \napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-sc-001\nspec:\n  containers:\n  - name: nginx-sc-001\n    image: nginx\n    volumeMounts:\n    - name: nginx-page\n      mountPath: /usr/share/nginx/html\n  volumes:\n  - name: nginx-page\n    persistentVolumeClaim:      \n      claimName: sc-pvc-001\n\n[root@k8s-master01 ~]# kubectl apply -f 05-nginx-pod.yaml\n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                                      READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nnginx-sc-001                              1/1     Running   0          15s   172.16.85.244   k8s-node01   &lt;none&gt;           &lt;none&gt;\n\n[root@k8s-master01 ~]# curl 172.16.85.244\nhello world\n</code></pre>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/722512536.html",
            "url": "http://ixuyong.cn/posts/722512536.html",
            "title": "K8s细粒度权限控制RBAC",
            "date_published": "2025-04-23T12:04:03.000Z",
            "content_html": "<h3 id=\"k8s细粒度权限控制rbac\"><a class=\"anchor\" href=\"#k8s细粒度权限控制rbac\">#</a> K8s 细粒度权限控制 RBAC</h3>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/KCZPPkv.jpeg\" alt=\"rbac.jpg\" /></p>\n<h4 id=\"1-创建不同权限的clusterrole\"><a class=\"anchor\" href=\"#1-创建不同权限的clusterrole\">#</a> 1. 创建不同权限的 clusterrole</h4>\n<h5 id=\"11-命令空间只读namespace-readonly\"><a class=\"anchor\" href=\"#11-命令空间只读namespace-readonly\">#</a> 1.1 命令空间只读 namespace-readonly</h5>\n<pre><code># cat namespace-readonly.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: namespace-readonly\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - metrics.k8s.io\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>\n<h5 id=\"12-资源查看resource-readonly\"><a class=\"anchor\" href=\"#12-资源查看resource-readonly\">#</a> 1.2 资源查看 resource-readonly</h5>\n<pre><code># cat resource-readonly.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: resource-readonly\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - configmaps\n  - endpoints\n  - persistentvolumeclaims\n  - pods\n  - replicationcontrollers\n  - replicationcontrollers/scale\n  - serviceaccounts\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - bindings\n  - events\n  - limitranges\n  - namespaces/status\n  - pods/log\n  - pods/status\n  - replicationcontrollers/status\n  - resourcequotas\n  - resourcequotas/status\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - controllerrevisions\n  - daemonsets\n  - deployments\n  - deployments/scale\n  - replicasets\n  - replicasets/scale\n  - statefulsets\n  - statefulsets/scale\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - deployments/scale\n  - ingresses\n  - networkpolicies\n  - replicasets\n  - replicasets/scale\n  - replicationcontrollers/scale\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - metrics.k8s.io\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>\n<h5 id=\"13-pod日志查看\"><a class=\"anchor\" href=\"#13-pod日志查看\">#</a> 1.3 pod 日志查看</h5>\n<pre><code># cat pod-log.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-log\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods\n  - pods/log\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>\n<h5 id=\"14-pod删除\"><a class=\"anchor\" href=\"#14-pod删除\">#</a> 1.4 Pod 删除</h5>\n<pre><code># cat pod-delete.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-delete\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - delete\n</code></pre>\n<h5 id=\"15-pod执行\"><a class=\"anchor\" href=\"#15-pod执行\">#</a> 1.5 Pod 执行</h5>\n<pre><code># cat pod-exec.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-exec\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods/exec\n  verbs:\n  - create\n</code></pre>\n<h5 id=\"16-创建不同权限的clusterrole\"><a class=\"anchor\" href=\"#16-创建不同权限的clusterrole\">#</a> 1.6 创建不同权限的 clusterrole</h5>\n<pre><code>[root@k8s-master01 rbac]# kubectl apply -f .\n</code></pre>\n<h4 id=\"2-创建serviceaccount\"><a class=\"anchor\" href=\"#2-创建serviceaccount\">#</a> 2. 创建 serviceaccount</h4>\n<pre><code># kubectl create ns kube-users\n\n# kubectl create sa test -n kube-users   \n# kubectl create sa dev -n kube-users    \n# kubectl create sa ops -n kube-users    \n\n# kubectl create token test -n kube-users\n# kubectl create token dev -n kube-users\n# kubectl create token ops -n kube-users\n</code></pre>\n<h4 id=\"3-创建clusterrolebinding\"><a class=\"anchor\" href=\"#3-创建clusterrolebinding\">#</a> 3. 创建 ClusterRoleBinding</h4>\n<h5 id=\"31-绑定全局命名空间查看权限\"><a class=\"anchor\" href=\"#31-绑定全局命名空间查看权限\">#</a> 3.1 绑定全局命名空间查看权限</h5>\n<pre><code># cat clusterrolebinding-namespace-readonly.yaml \napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: clusterrolebinding-namespace-readonly \nsubjects:\n- kind: Group\n  name: system:serviceaccounts:kube-users\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: namespace-readonly\n  apiGroup: rbac.authorization.k8s.io\n  \n# kubectl apply -f clusterrolebinding-namespace-readonly.yaml\n</code></pre>\n<h5 id=\"32-绑定日志查看权限\"><a class=\"anchor\" href=\"#32-绑定日志查看权限\">#</a> 3.2 绑定日志查看权限</h5>\n<pre><code># kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<h5 id=\"33-绑定资源查看权限\"><a class=\"anchor\" href=\"#33-绑定资源查看权限\">#</a> 3.3 绑定资源查看权限</h5>\n<pre><code># kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<h5 id=\"34-绑定pod执行权限\"><a class=\"anchor\" href=\"#34-绑定pod执行权限\">#</a> 3.4 绑定 Pod 执行权限</h5>\n<pre><code># kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<h5 id=\"35-绑定pod删除权限\"><a class=\"anchor\" href=\"#35-绑定pod删除权限\">#</a> 3.5 绑定 Pod 删除权限</h5>\n<pre><code># kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/176412055.html",
            "url": "http://ixuyong.cn/posts/176412055.html",
            "title": "K8s准入控制ResourceQuota、LimitRange、QoS服务质量",
            "date_published": "2025-04-23T11:55:19.000Z",
            "content_html": "<h3 id=\"k8s准入控制resourcequota-limitrange-qos服务质量\"><a class=\"anchor\" href=\"#k8s准入控制resourcequota-limitrange-qos服务质量\">#</a> K8s 准入控制 ResourceQuota、LimitRange、QoS 服务质量</h3>\n<h4 id=\"1-resourcequota配置解析\"><a class=\"anchor\" href=\"#1-resourcequota配置解析\">#</a> 1. ResourceQuota 配置解析</h4>\n<p>ResourceQuotas 实现资源配额，避免过度创建资源，针对 namespace 进行限制。cpu 内存则是根据 pod 配置的 resources 总额进行限制，如果没有配置 resources 参数则无法限制。</p>\n<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: resourcequota-test\n  namespace: test\n  labels:\n    app: resourcequota\nspec:\n  hard:\n    pods: 3\n    requests.cpu: 3\n    requests.memory: 512Mi\n    limits.cpu: 8\n    limits.memory: 16Gi\n    configmaps: 201\n    requests.storage: 40Gi\n    persistentvolumeclaims: 20\n    replicationcontrollers: 20\n    secrets: 20\n    services: 50\n    services.loadbalancers: &quot;2&quot;\n    services.nodeports: &quot;10&quot;\n</code></pre>\n<ul>\n<li>pods：限制最多启动 Pod 的个数</li>\n<li>requests.cpu：限制最高 CPU 请求数</li>\n<li>requests.memory：限制最高内存的请求数</li>\n<li>limits.cpu：限制最高 CPU 的 limit 上限</li>\n<li>limits.memory：限制最高内存的 limit 上限</li>\n<li>services：限制 services 数量</li>\n<li>services.nodeports：限制 services 中 nodeport 类型 service 数量</li>\n<li>services.loadbalancers：限制 services 中 loadbalancers 类型 service 数量</li>\n</ul>\n<h5 id=\"11-resourcequota配置示例\"><a class=\"anchor\" href=\"#11-resourcequota配置示例\">#</a> 1.1 ResourceQuota 配置示例</h5>\n<pre><code>#1.限制test命名空间pods数量量为3、configmap数量为2\n[root@k8s-master01 resourcequota]# cat rq-test.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: resourcequota-test\n  namespace: test\n  labels:\n    app: resourcequota\nspec:\n  hard:\n    pods: 3\n#    requests.cpu: 3\n#    requests.memory: 512Mi\n#    limits.cpu: 8\n#    limits.memory: 16Gi\n    configmaps: 2\n#    requests.storage: 40Gi\n#    persistentvolumeclaims: 20\n#    replicationcontrollers: 20\n#    secrets: 20\n#    services: 50\n#    services.loadbalancers: &quot;2&quot;\n#    services.nodeports: &quot;10&quot;\n\n#2.test命名空间已创建configmap数量为1,限制数量为2\n[root@k8s-master01 resourcequota]# kubectl get resourcequota -n test\nNAME                 AGE   REQUEST                      LIMIT\nresourcequota-test   61s   configmaps: 1/2, pods: 0/3  \n\n#3.test命名空间创建第2个configmap时正常，创建第3个configmap时报错\n[root@k8s-master01 resourcequota]# kubectl create cm rq-cm1 -n test --from-literal=key1=value1\n[root@k8s-master01 resourcequota]# kubectl create cm rq-cm2 -n test --from-literal=key2=value2\nerror: failed to create configmap: configmaps &quot;rq-cm2&quot; is forbidden: exceeded quota: resourcequota-test, requested: configmaps=1, used: configmaps=2, limited: configmaps=2\n</code></pre>\n<h4 id=\"2-limitrange配置解析\"><a class=\"anchor\" href=\"#2-limitrange配置解析\">#</a> 2. LimitRange 配置解析</h4>\n<p>虽然 ResourceQuota 可以实现资源配额，可以限制某个命名空间内存和 CPU，但是如果创建的 Pod 都没有配置 resources 参数则无法限制。如果配置 LimitRange，Pod 没有配置 resources 情况下，创建的 Pod 会根据 LimitRange 配置自动添加 CPU 内存配置，并且可以限制 resources 参数最大配置和最小配置，LimitRange 针对 Pod 进行限制。</p>\n<pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #限制CPU内存默认limits配置\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #限制CPU内存默认request配置\n      cpu: 0.5\n      memory: 256Mi\n    max:                #限制CPU内存最大配置 \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #限制CPU内存最小配置\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #限制pvc大小\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi\n</code></pre>\n<ul>\n<li>default：默认 limits 配置</li>\n<li>defaultRequest：默认 requests 配置</li>\n</ul>\n<h5 id=\"21-配置默认的requests和limits\"><a class=\"anchor\" href=\"#21-配置默认的requests和limits\">#</a> 2.1 配置默认的 requests 和 limits</h5>\n<p>Pod 没有配置 resources 情况下，创建的 Pod 会根据 LimitRange 配置自动添加 CPU 内存配置。</p>\n<pre><code>#1.创建LimitRange\n[root@k8s-master01 resourcequota]# cat limitrange.yaml \napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #限制CPU内存默认limits配置\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #限制CPU内存默认request配置\n      cpu: 0.5\n      memory: 256Mi\n    max:                #限制CPU内存最大配置 \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #限制CPU内存最小配置\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #限制pvc大小\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi  \n      \n[root@k8s-master01 resourcequota]# kubectl apply -f limitrange.yaml\n[root@k8s-master01 resourcequota]# kubectl get limitrange -n test\nNAME                  CREATED AT\ncpu-mem-limit-range   2025-04-23T07:55:03Z\n\n#2.创建deployment, 查看是否会根据LimitRange自动添加CPU内存配置\n[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-limirange\n  labels:\n    app: deploy-limirange\n  namespace: test\nspec:\n  selector:\n    matchLabels:\n      app: deploy-limirange\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: deploy-limirange\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: deploy-limirange\n          image: nginx\n          imagePullPolicy: IfNotPresent\n\n[root@k8s-master01 resourcequota]# kubectl get pod -n test\nNAME                                READY   STATUS    RESTARTS   AGE\ndeploy-limirange-854c9545ff-grpxr   1/1     Running   0          39s\n[root@k8s-master01 resourcequota]# kubectl get pod -n test -oyaml\n...\n  spec:\n    containers:\n    - image: nginx\n      imagePullPolicy: IfNotPresent\n      name: deploy-limirange\n      resources:\n        limits:\n          cpu: &quot;1&quot;\n          memory: 512Mi\n        requests:\n          cpu: 500m\n          memory: 256Mi\n...\n</code></pre>\n<h5 id=\"22-限制requests和limits范围\"><a class=\"anchor\" href=\"#22-限制requests和limits范围\">#</a> 2.2 限制 requests 和 limits 范围</h5>\n<pre><code>#1.创建LimitRange\n[root@k8s-master01 resourcequota]# cat limitrange.yaml \napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #限制CPU内存默认limits配置\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #限制CPU内存默认request配置\n      cpu: 0.5\n      memory: 256Mi\n    max:                #限制CPU内存最大配置 \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #限制CPU内存最小配置\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #限制pvc大小\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi  \n\n#2.创建deployment, CPU内存limits和requests高于/低于LimitRangeCPU内存max、min配置\n[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-limirange\n  labels:\n    app: deploy-limirange\n  namespace: test\nspec:\n  selector:\n    matchLabels:\n      app: deploy-limirange\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: deploy-limirange\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: deploy-limirange\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 8096Mi\n              cpu: 5\n            requests:\n              memory: 64Mi\n              cpu: 10m\n\n#3.由于创建deployment, CPU内存limits和requests高于/低于LimitRangeCPU内存max、min配置，pod没有创建\n[root@k8s-master01 resourcequota]# kubectl create -f deploy-limitrange.yaml \n\n[root@k8s-master01 resourcequota]# kubectl get deploy deploy-limirange -n test\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\ndeploy-limirange   0/1     0            0           2m7s\n[root@k8s-master01 resourcequota]# kubectl get pods -n test\n\n[root@k8s-master01 resourcequota]# kubectl describe rs deploy-limirange-54c5d69b4b -n test\nName:           deploy-limirange-54c5d69b4b\nNamespace:      test\nSelector:       app=deploy-limirange,pod-template-hash=54c5d69b4b\nLabels:         app=deploy-limirange\n                pod-template-hash=54c5d69b4b\nAnnotations:    deployment.kubernetes.io/desired-replicas: 1\n                deployment.kubernetes.io/max-replicas: 2\n                deployment.kubernetes.io/revision: 1\nControlled By:  Deployment/deploy-limirange\nReplicas:       0 current / 1 desired\nPods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=deploy-limirange\n           pod-template-hash=54c5d69b4b\n  Containers:\n   deploy-limirange:\n    Image:      nginx\n    Port:       &lt;none&gt;\n    Host Port:  &lt;none&gt;\n    Limits:\n      cpu:     5\n      memory:  8096Mi\n    Requests:\n      cpu:         10m\n      memory:      64Mi\n    Environment:   &lt;none&gt;\n    Mounts:        &lt;none&gt;\n  Volumes:         &lt;none&gt;\n  Node-Selectors:  &lt;none&gt;\n  Tolerations:     &lt;none&gt;\nConditions:\n  Type             Status  Reason\n  ----             ------  ------\n  ReplicaFailure   True    FailedCreate\nEvents:\n  Type     Reason        Age                 From                   Message\n  ----     ------        ----                ----                   -------\n  Warning  FailedCreate  3m8s                replicaset-controller  Error creating: pods &quot;deploy-limirange-54c5d69b4b-zxhzk&quot; is forbidden: [minimum cpu usage per Container is 100m, but request is 10m, minimum memory usage per Container is 100Mi, but request is 64Mi, maximum cpu usage per Container is 4, but limit is 5, maximum memory usage per Container is 4Gi, but limit is 8096Mi]\n</code></pre>\n<h5 id=\"23-限制存储空间大小\"><a class=\"anchor\" href=\"#23-限制存储空间大小\">#</a> 2.3 限制存储空间大小</h5>\n<pre><code>#1.创建LimitRange\n[root@k8s-master01 resourcequota]# cat limitrange.yaml \napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #限制CPU内存默认limits配置\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #限制CPU内存默认request配置\n      cpu: 0.5\n      memory: 256Mi\n    max:                #限制CPU内存最大配置 \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #限制CPU内存最小配置\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #限制pvc大小\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi  \n  \n#2.由于创建的pvc大于2G，所以报错  \n[root@k8s-master01 ~]# cat pvc.yaml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sc-pvc-001\nspec:\n  storageClassName: &quot;nfs-storage&quot;     # 明确指定使用哪个sc的供应商来创建pv\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 3Gi                      # 根据业务实际大小进行资源申请  \n[root@k8s-master01 ~]# kubectl create -f pvc.yaml -n test\nError from server (Forbidden): error when creating &quot;pvc.yaml&quot;: persistentvolumeclaims &quot;sc-pvc-001&quot; is forbidden: maximum storage usage per PersistentVolumeClaim is 2Gi, but request is 3Gi\n</code></pre>\n<h4 id=\"3-服务质量-qos\"><a class=\"anchor\" href=\"#3-服务质量-qos\">#</a> 3. 服务质量 QoS</h4>\n<ul>\n<li>Guaranteed：最高服务质量，当宿主机内存不够时，会先 kill 掉 QoS 为 BestEffort 和 Burstable 的 Pod，如果内存还是不够，才会 kill 掉 QoS 为 Guaranteed，该级别 Pod 的资源占用量一般比较明确，即 requests 的 cpu 和 memory 和 limits 的 cpu 和 memory 配置的一致。</li>\n<li>Burstable： 服务质量低于 Guaranteed，当宿主机内存不够时，会先 kill 掉 QoS 为 BestEffort 的 Pod，如果内存还是不够之后就会 kill 掉 QoS 级别为 Burstable 的 Pod，用来保证 QoS 质量为 Guaranteed 的 Pod，该级别 Pod 一般知道最小资源使用量，但是当机器资源充足时，还是想尽可能的使用更多的资源，即 limits 字段的 cpu 和 memory 大于 requests 的 cpu 和 memory 的配置。</li>\n<li>BestEffort：尽力而为，当宿主机内存不够时，首先 kill 的就是该 QoS 的 Pod，用以保证 Burstable 和 Guaranteed 级别的 Pod 正常运行。</li>\n</ul>\n<h5 id=\"31-实现qos为guaranteed的pod\"><a class=\"anchor\" href=\"#31-实现qos为guaranteed的pod\">#</a> 3.1 实现 QoS 为 Guaranteed 的 Pod</h5>\n<ol>\n<li>\n<p>Pod 中的每个容器必须指定 limits.memory 和 requests.memory，并且两者需要相等；</p>\n</li>\n<li>\n<p>Pod 中的每个容器必须指定 limits.cpu 和 limits.memory，并且两者需要相等。</p>\n</li>\n</ol>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 1024Mi\n              cpu: 1\n</code></pre>\n<h5 id=\"32-实现qos为burstable的pod\"><a class=\"anchor\" href=\"#32-实现qos为burstable的pod\">#</a> 3.2 实现 QoS 为 Burstable 的 Pod</h5>\n<ol>\n<li>\n<p>Pod 不符合 Guaranteed 的配置要求；</p>\n</li>\n<li>\n<p>Pod 中至少有一个容器配置了 requests.cpu 或 requests.memory。</p>\n</li>\n</ol>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<h5 id=\"33-实现qos为besteffort的pod\"><a class=\"anchor\" href=\"#33-实现qos为besteffort的pod\">#</a> 3.3 实现 QoS 为 BestEffort 的 Pod</h5>\n<ol>\n<li>不设置 resources 参数</li>\n</ol>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/312010518.html",
            "url": "http://ixuyong.cn/posts/312010518.html",
            "title": "K8s亲和力Affinity",
            "date_published": "2025-04-20T09:59:58.000Z",
            "content_html": "<h3 id=\"k8s亲和力affinity\"><a class=\"anchor\" href=\"#k8s亲和力affinity\">#</a> K8s 亲和力 Affinity</h3>\n<p>Pod 和节点之间的关系：</p>\n<ul>\n<li>某些 Pod 优先选择有 ssd=true 标签的节点，如果没有在考虑部署到其它节点；</li>\n<li>某些 Pod 需要部署在 ssd=true 和 type=physical 的节点上，但是优先部署在 ssd=true 的节点上。</li>\n</ul>\n<p>Pod 和 Pod 之间的关系：</p>\n<ul>\n<li>同一个应用的 Pod 不同的副本或者同一个项目的应用尽量或必须不部署在同一个节点或者符合某个标签的一类节点上或者不同的区域；</li>\n<li>相互依赖的两个 Pod 尽量或必须部署在同一个节点上或者同一个域内。</li>\n</ul>\n<h4 id=\"1-affinity分类\"><a class=\"anchor\" href=\"#1-affinity分类\">#</a> 1. Affinity 分类</h4>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/hTd0wmD.png\" alt=\"1.png\" /></p>\n<h4 id=\"2-节点亲和力配置详解\"><a class=\"anchor\" href=\"#2-节点亲和力配置详解\">#</a> 2. 节点亲和力配置详解</h4>\n<h5 id=\"21-硬亲和力required\"><a class=\"anchor\" href=\"#21-硬亲和力required\">#</a> 2.1 硬亲和力 required</h5>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: kubernetes.io/hostname\n                    operator: In\n                    values:\n                      - k8s-node01\n                      - k8s-node02\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<ul>\n<li>requiredDuringSchedulingIgnoredDuringExecution：硬亲和力配置</li>\n<li>nodeSelectorTerms：节点选择器配置，可以配置多个 matchExpressions（满足其一即可）</li>\n<li>matchExpressions：matchExpressions 下可以配置多个 key、values（都需要满足），其中 values 可以配置多个（满足其一即可）</li>\n<li>operator：\n<ul>\n<li>IN 相当于 key = value 的形式，<strong>NotIn 相当于 key!=value 的形式 (反亲和力)</strong></li>\n<li>Exists: 节点存在 label 的 key 为指定的值即可，不能配置 values 字段</li>\n<li>DoesNotExist: 节点不存在 label 的 key 为指定的值即可，不能配置 values 字段</li>\n<li>Gt：大于 value 指定的值</li>\n<li>Lt：小于 value 指定的值</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"22-软亲和力preferred\"><a class=\"anchor\" href=\"#22-软亲和力preferred\">#</a> 2.2 软亲和力 preferred</h5>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 6\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              preference:\n                matchExpressions:\n                  - key: ssd\n                    operator: In\n                    values:\n                      - 'true'\n            - weight: 50\n              preference:\n                matchExpressions:\n                  - key: kubernetes.io/hostname\n                    operator: In\n                    values:\n                      - k8s-master01\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<ul>\n<li>preferredDuringSchedulingIgnoredDuringExecution：软亲和力配置</li>\n<li>weight：软亲和力的权重，权重越高优先级越大，范围 1-100</li>\n<li>matchExpressions：matchExpressions 下可以配置多个 key、values（都需要满足），其中 values 可以配置多个（满足其一即可）</li>\n<li>operator：\n<ul>\n<li>IN 相当于 key = value 的形式，<strong>NotIn 相当于 key!=value 的形式 (反亲和力)</strong></li>\n<li>Exists: 节点存在 label 的 key 为指定的值即可，不能配置 values 字段</li>\n<li>DoesNotExist: 节点不存在 label 的 key 为指定的值即可，不能配置 values 字段</li>\n<li>Gt：大于 value 指定的值</li>\n<li>Lt：小于 value 指定的值</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"3-pod亲和力详解\"><a class=\"anchor\" href=\"#3-pod亲和力详解\">#</a> 3. Pod 亲和力详解</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:              \n        podAntiAffinity:   #pod硬反亲和力\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - nginx-deploy\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:       #pod软反亲和力\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - nginx-deploy\n              namespaces:     #和哪个命名空间的Pod进行匹配，为空为当前命名空间\n              - default\n              topologyKey: kubernetes.io/hostname\n</code></pre>\n<ul>\n<li>\n<p>labelSelector：Pod 选择器配置，可以配置多个</p>\n</li>\n<li>\n<p>matchExpressions：matchExpressions 下可以配置多个 key、values（都需要满足），其中 values 可以配置多个（满足其一即可）</p>\n</li>\n<li>\n<p>topologyKey：匹配的拓扑域的 key，也就是节点上 label 的 key，key 和 value 相同的为同一个域，可以用于标注不同的机房和地区</p>\n</li>\n<li>\n<p>Namespaces: 和哪个命名空间的 Pod 进行匹配，为空为当前命名空间</p>\n</li>\n<li>\n<p>operator：配置和节点亲和力一致，但是没有 Gt 和 Lt</p>\n<ul>\n<li>\n<p>IN 相当于 key = value 的形式；</p>\n</li>\n<li>\n<p>Exists: 节点存在 label 的 key 为指定的值即可，不能配置 values 字段；</p>\n</li>\n<li>\n<p>DoesNotExist: 节点不存在 label 的 key 为指定的值即可，不能配置 values 字段</p>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"4-节点亲和力配置示例\"><a class=\"anchor\" href=\"#4-节点亲和力配置示例\">#</a> 4. 节点亲和力配置示例</h4>\n<p>Pod 尽量部署在 ssd=true 和 type=physical 的节点上，但是优先部署在 ssd=true 的节点上，不能部署 label 为 gpu=true 的节点。</p>\n<pre><code>[root@k8s-master01 ~]# kubectl label nodes k8s-node01 ssd=true\n[root@k8s-master01 ~]# kubectl label nodes k8s-master01 ssd=true\n[root@k8s-master01 ~]# kubectl label nodes k8s-master01 gpu=true\n[root@k8s-master01 ~]# kubectl label nodes k8s-node02 type=physical\n\n[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n      annotations:\n        app: nginx-deploy\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              preference:\n                matchExpressions:\n                  - key: ssd\n                    operator: In\n                    values:\n                      - 'true'\n                  - key: gpu\n                    operator: NotIn\n                    values:\n                      - 'true'\n            - weight: 50\n              preference:\n                matchExpressions:\n                  - key: type\n                    operator: In\n                    values:\n                      - physical\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n\n\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nnginx-deploy-7d65fbdf-2b4jr   1/1     Running   0          5s    172.16.85.236   k8s-node01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-jjzwr   1/1     Running   0          5s    172.16.58.251   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-kx5lm   1/1     Running   0          5s    172.16.85.237   k8s-node01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-lrmcg   1/1     Running   0          5s    172.16.85.238   k8s-node01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-n6mlp   1/1     Running   0          5s    172.16.58.250   k8s-node02   &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<h4 id=\"5-pod亲和力-反亲和力配置示例\"><a class=\"anchor\" href=\"#5-pod亲和力-反亲和力配置示例\">#</a> 5. Pod 亲和力、反亲和力配置示例</h4>\n<h5 id=\"51-pod反亲和力required\"><a class=\"anchor\" href=\"#51-pod反亲和力required\">#</a> 5.1 Pod 反亲和力 required</h5>\n<p>同一个应用部署在不同的宿主机</p>\n<pre><code>#1.节点存在污点pod无法调度至该节点\n# kubectl describe nodes|grep -i taint\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\n\n#2.pod反亲和力required\n# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: app\n                    operator: In\n                    values:\n                      - nginx-deploy\n              topologyKey: kubernetes.io/hostname\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n\n#3.部署deployment\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-deploy-5787887b6f-4654b   1/1     Running   0          4s    172.16.85.234    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4s    172.16.122.152   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-fdkft   1/1     Running   0          4s    172.16.58.247    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4s    172.16.32.152    k8s-master01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4s    172.16.195.14    k8s-master03   &lt;none&gt;           &lt;none&gt;\n\n#4.将副本扩成6个，由于K8s集群只有5个节点，即5个topologyKey（拓扑域），每个域只能有一个副本，所以有一个pod会pending\n[root@k8s-master01 ~]# kubectl scale deploy nginx-deploy --replicas=6 \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                            READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-deploy-5787887b6f-4654b   1/1     Running   0          4m44s   172.16.85.234    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4m44s   172.16.122.152   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-fdkft   1/1     Running   0          4m44s   172.16.58.247    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4m44s   172.16.32.152    k8s-master01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4m44s   172.16.195.14    k8s-master03   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-sztm7   0/1     Pending   0          9s      &lt;none&gt;           &lt;none&gt;         &lt;none&gt;           &lt;none&gt;\n\n[root@k8s-master01 ~]# kubectl describe pods nginx-deploy-5787887b6f-sztm7\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  102s  default-scheduler  0/5 nodes are available: 5 node(s) didn't match pod anti-affinity rules. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.\n</code></pre>\n<p><strong>将副本扩成 6 个，有一个会 pending 状态，原因 K8s 集群只有 5 个节点，即 5 个 topologyKey（拓扑域），每个拓扑域只能有一个副本，所以有一个 pod 会 pending。</strong></p>\n<p><strong>topologyKey：匹配的拓扑域的 key，也就是节点上 label 的 key，key 和 value 相同的为同一个域，可以用于标注不同的机房和地区</strong>。</p>\n<h5 id=\"52-pod反亲和力preferred\"><a class=\"anchor\" href=\"#52-pod反亲和力preferred\">#</a> 5.2 Pod 反亲和力 preferred</h5>\n<p>同一个应用尽量部署在不同的宿主机</p>\n<pre><code>#1.节点存在污点pod无法调度至该节点\n# kubectl describe nodes|grep -i taint\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\n\n#2.pod反亲和力preferred\n# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 6\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - podAffinityTerm:\n                labelSelector:\n                  matchExpressions:\n                    - key: app\n                      operator: In\n                      values:\n                        - nginx-deploy\n                topologyKey: kubernetes.io/hostname\n              weight: 100\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n\n#3.部署deployment\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-deploy-7c47567b79-97qs5   1/1     Running   0          6s    172.16.122.153   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-g49h4   1/1     Running   0          6s    172.16.85.235    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-g5n2s   1/1     Running   0          6s    172.16.58.248    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-g5v5b   1/1     Running   0          6s    172.16.195.15    k8s-master03   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-pjwws   1/1     Running   0          6s    172.16.58.249    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-q2hn5   1/1     Running   0          6s    172.16.32.153    k8s-master01   &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<h5 id=\"53-pod亲和力required\"><a class=\"anchor\" href=\"#53-pod亲和力required\">#</a> 5.3 Pod 亲和力 required</h5>\n<p>同一个应用必须部署在同一个宿主机</p>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 8\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:              \n        podAffinity:   #pod硬亲和力\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - nginx-deploy\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n      volumes:\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nnginx-deploy-dbcc4d65c-2sthn   1/1     Running   0          12s   172.16.58.255   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-78nxf   1/1     Running   0          12s   172.16.58.197   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-82ssq   1/1     Running   0          12s   172.16.58.194   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-986cb   1/1     Running   0          12s   172.16.58.254   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-9rnt7   1/1     Running   0          12s   172.16.58.252   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-knm8q   1/1     Running   0          12s   172.16.58.195   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-kx56f   1/1     Running   0          12s   172.16.58.253   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-sqlhf   1/1     Running   0          12s   172.16.58.198   k8s-node02   &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<h5 id=\"54-pod亲和力preferre\"><a class=\"anchor\" href=\"#54-pod亲和力preferre\">#</a> 5.4 Pod 亲和力 preferre</h5>\n<p>同一个应用尽量部署在同一个宿主机</p>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 20\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:              \n        podAffinity:       #pod软亲和力\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - nginx-deploy\n              namespaces:     #和哪个命名空间的Pod进行匹配，为空为当前命名空间\n              - default\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n      volumes:\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3254599477.html",
            "url": "http://ixuyong.cn/posts/3254599477.html",
            "title": "K8s容忍和污点",
            "date_published": "2025-04-20T07:51:58.000Z",
            "content_html": "<h3 id=\"k8s容忍和污点\"><a class=\"anchor\" href=\"#k8s容忍和污点\">#</a> K8s 容忍和污点</h3>\n<p>Taint 指定服务器上打上污点，让不能容忍这个污点的 Pod 不能部署在打了污点的服务器上。Toleration 是让 Pod 容忍节点上配置的污点，可以让一些需要特殊配置的 Pod 能够调用到具有污点和特殊配置的节点上。</p>\n<h4 id=\"1-taint配置解析\"><a class=\"anchor\" href=\"#1-taint配置解析\">#</a> 1. Taint 配置解析</h4>\n<pre><code>#1.Taint语法\n# kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT\n\n#2.创建Taint示例\n# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule\n\n#3.查看污点\n# kubectl describe node k8s-node01 | grep Taints -A 10\n\n#4.删除污点\n# kubectl taint nodes k8s-node01 ssd-                   #基于Key删除\n# kubectl taint nodes k8s-node01 ssd:PreferNoSchedule-  #基于Key+Effect删除\n\n#5.修改污点（Key和Effect相同）\n# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule --overwrite\n</code></pre>\n<p>EFFECT 排斥等级：</p>\n<ul>\n<li>NoSchedule：禁止调度到该节点，已经在该节点上的 Pod 不受影响</li>\n<li>NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后）</li>\n<li>PreferNoSchedule：尽量避免将 Pod 调度到指定的节点上，如果没有更合适的节点，可以部署到该节点</li>\n</ul>\n<h4 id=\"2toleration配置解析\"><a class=\"anchor\" href=\"#2toleration配置解析\">#</a> 2.Toleration 配置解析</h4>\n<pre><code>#1.完全匹配\ntolerations:\n- key: &quot;taintKey&quot;\n  operator: &quot;Equal&quot;\n  value: &quot;taintValue&quot;\n  effect: &quot;NoSchedule\n \n#2.不完全匹配 \ntolerations:\n- key: &quot;taintKey&quot;\n  operator: &quot;Exists&quot;\n  effect: &quot;NoSchedule&quot;\n  \n#3.大范围匹配（不推荐key为内置Taint，会导致节点故障pod无法漂移）\ntolerations:\n- key: &quot;taintKey&quot;\n  operator: &quot;Exists\n  \n#4.容忍时间配置\ntolerations:\n- key: &quot;key1&quot;\n  operator: &quot;Equal&quot;\n  value: &quot;value1&quot;\n  effect: &quot;NoExecute&quot;\n  tolerationSeconds: 3600\n</code></pre>\n<h4 id=\"3-taint-toleration配置示例\"><a class=\"anchor\" href=\"#3-taint-toleration配置示例\">#</a> 3. Taint、Toleration 配置示例</h4>\n<p>有一个 K8s 节点是纯 SSD 硬盘的节点，现需要只有一些需要高性能存储的 Pod 才能调度到该节点上。</p>\n<pre><code>#1.给节点打上污点和标签\n# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule\n# kubectl label node k8s-node01 ssd=true\n\n#2.配置Toleration：\n# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n      nodeSelector:\n        ssd: 'true'\n      tolerations:\n        - key: ssd\n          operator: Exists\n          effect: NoSchedule\n</code></pre>\n<h4 id=\"4-k8s内置污点\"><a class=\"anchor\" href=\"#4-k8s内置污点\">#</a> 4. K8s 内置污点</h4>\n<ul>\n<li><a href=\"http://node.kubernetes.io/not-ready%EF%BC%9A%E8%8A%82%E7%82%B9%E6%9C%AA%E5%87%86%E5%A4%87%E5%A5%BD%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81Ready%E7%9A%84%E5%80%BC%E4%B8%BAFalse%E3%80%82\">node.kubernetes.io/not-ready：节点未准备好，相当于节点状态 Ready 的值为 False。</a></li>\n<li><a href=\"http://node.kubernetes.io/unreachable%EF%BC%9ANode\">node.kubernetes.io/unreachable：Node</a> Controller 访问不到节点，相当于节点状态 Ready 的值为 Unknown。</li>\n<li><a href=\"http://node.kubernetes.io/out-of-disk%EF%BC%9A%E8%8A%82%E7%82%B9%E7%A3%81%E7%9B%98%E8%80%97%E5%B0%BD%E3%80%82\">node.kubernetes.io/out-of-disk：节点磁盘耗尽。</a></li>\n<li><a href=\"http://node.kubernetes.io/memory-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E5%86%85%E5%AD%98%E5%8E%8B%E5%8A%9B%E3%80%82\">node.kubernetes.io/memory-pressure：节点存在内存压力。</a></li>\n<li><a href=\"http://node.kubernetes.io/disk-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E7%A3%81%E7%9B%98%E5%8E%8B%E5%8A%9B%E3%80%82\">node.kubernetes.io/disk-pressure：节点存在磁盘压力。</a></li>\n<li><a href=\"http://node.kubernetes.io/network-unavailable%EF%BC%9A%E8%8A%82%E7%82%B9%E7%BD%91%E7%BB%9C%E4%B8%8D%E5%8F%AF%E8%BE%BE%E3%80%82\">node.kubernetes.io/network-unavailable：节点网络不可达。</a></li>\n<li><a href=\"http://node.kubernetes.io/unschedulable%EF%BC%9A%E8%8A%82%E7%82%B9%E4%B8%8D%E5%8F%AF%E8%B0%83%E5%BA%A6%E3%80%82\">node.kubernetes.io/unschedulable：节点不可调度。</a></li>\n<li><a href=\"http://node.cloudprovider.kubernetes.io/uninitialized%EF%BC%9A%E5%A6%82%E6%9E%9CKubelet%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8C%87%E5%AE%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%A4%96%E9%83%A8%E7%9A%84cloudprovider%EF%BC%8C%E5%AE%83%E5%B0%86%E7%BB%99%E5%BD%93%E5%89%8D%E8%8A%82%E7%82%B9%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AATaint%E5%B0%86%E5%85%B6%E6%A0%87%E8%AE%B0%E4%B8%BA%E4%B8%8D%E5%8F%AF%E7%94%A8%E3%80%82%E5%9C%A8cloud-controller-manager%E7%9A%84%E4%B8%80%E4%B8%AAcontroller%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%E5%90%8E%EF%BC%8CKubelet%E5%B0%86%E5%88%A0%E9%99%A4%E8%BF%99%E4%B8%AATaint%E3%80%82\">node.cloudprovider.kubernetes.io/uninitialized：如果 Kubelet 启动时指定了一个外部的 cloudprovider，它将给当前节点添加一个 Taint 将其标记为不可用。在 cloud-controller-manager 的一个 controller 初始化这个节点后，Kubelet 将删除这个 Taint。</a></li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/vO7kURL.png\" alt=\"1.png\" /></p>\n<p>Deployment 创建后 K8s 默认为 Pod 添加容忍，当 Pod 所在的节点宕机，300 秒后 pod 会漂移，默认容忍时间 300 秒。</p>\n<h4 id=\"5节点宕机快速恢复业务应用\"><a class=\"anchor\" href=\"#5节点宕机快速恢复业务应用\">#</a> 5. 节点宕机快速恢复业务应用</h4>\n<p>节点不健康，180 秒后再驱逐（默认是 300 秒）</p>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n      tolerations:\n        - key: node.kubernetes.io/unreachable\n          operator: Exists\n          effect: NoExecute\n          tolerationSeconds: 180\n        - key: node.kubernetes.io/not-ready\n          operator: Exists\n          effect: NoExecute\n          tolerationSeconds: 180\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3142072607.html",
            "url": "http://ixuyong.cn/posts/3142072607.html",
            "title": "K8s初始化容器、临时容器",
            "date_published": "2025-04-19T13:07:20.000Z",
            "content_html": "<h3 id=\"k8s初始化容器-临时容器\"><a class=\"anchor\" href=\"#k8s初始化容器-临时容器\">#</a> K8s 初始化容器、临时容器</h3>\n<h4 id=\"1-初始化容器\"><a class=\"anchor\" href=\"#1-初始化容器\">#</a> 1. 初始化容器</h4>\n<h5 id=\"1-1-初始化容器的用途\"><a class=\"anchor\" href=\"#1-1-初始化容器的用途\">#</a> 1. 1 初始化容器的用途</h5>\n<p>初始化容器主要是在主应用启动之前，做一些初始化的操作，比如创建文件、修改内核参数、等待依赖程序启动或其他需要在主程序启动之前需要做的工作。</p>\n<ul>\n<li>Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码；</li>\n<li>Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低；</li>\n<li>Init 容器可以以 root 身份运行，执行一些高权限命令；</li>\n<li>Init 容器相关操作执行完成以后即退出，不会给业务容器带来安全隐患。</li>\n</ul>\n<h5 id=\"12-初始化容器和poststart区别\"><a class=\"anchor\" href=\"#12-初始化容器和poststart区别\">#</a> 1.2 初始化容器和 PostStart 区别</h5>\n<p>PostStart：依赖主应用的环境，而且并不一定先于 Command 运行。</p>\n<p>InitContainer：不依赖主应用的环境，可以有更高的权限和更多的工具，一定会在主应用启动之前完成</p>\n<h5 id=\"13-初始化容器和普通容器的区别\"><a class=\"anchor\" href=\"#13-初始化容器和普通容器的区别\">#</a> 1.3 初始化容器和普通容器的区别</h5>\n<p>Init 容器与普通的容器非常像，除了如下几点：</p>\n<ul>\n<li>\n<p>第一个 Init 容器运行成功后才会运行下一个 Init 容器；</p>\n</li>\n<li>\n<p>所有的 Init 容器运行成功后才会运行主容器；</p>\n</li>\n<li>\n<p>如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止，但是 Pod 对应的 restartPolicy 值为 Never，Kubernetes 不会重新启动 Pod。</p>\n</li>\n<li>\n<p>Init 容器不支持 lifecycle、livenessProbe、readinessProbe 和 startupProbe</p>\n</li>\n</ul>\n<h5 id=\"14-初始化容器示例\"><a class=\"anchor\" href=\"#14-初始化容器示例\">#</a> 1.4 初始化容器示例</h5>\n<pre><code>[root@k8s-master01 ~]# cat init.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;echo hello kubernetes&gt;/usr/share/nginx/html/index.html&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: share-volume\n          mountPath: /usr/share/nginx/html\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: share-volume\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: share-volume\n        emptyDir: &#123;&#125;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n\n[root@k8s-master01 ~]# kubectl create -f init.yaml\n\n[root@k8s-master01 ~]# curl 172.16.32.145\nhello kubernetes\n</code></pre>\n<h4 id=\"2-临时容器\"><a class=\"anchor\" href=\"#2-临时容器\">#</a> 2. 临时容器</h4>\n<h5 id=\"21-注入临时容器到pod\"><a class=\"anchor\" href=\"#21-注入临时容器到pod\">#</a> 2.1 注入临时容器到 Pod</h5>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods\nNAME                            READY   STATUS    RESTARTS      AGE\nnginx-deploy-7dd6cd4b44-ktw5k   1/1     Running   1             16h\nnginx-deploy-7dd6cd4b44-mjcgq   1/1     Running   1 (28m ago)   16h\nnginx-deploy-7dd6cd4b44-wdm6p   1/1     Running   1 (28m ago)   16h\n\n#1.进入容器发现pod没有ps和netstat命令\n[root@k8s-master01 ~]# kubectl exec -it nginx-deploy-7dd6cd4b44-ktw5k  -- bash\nroot@nginx-deploy-7dd6cd4b44-ktw5k:/# ps aux\nroot@nginx-deploy-7dd6cd4b44-ktw5k:/# netstat -lntp\n\n#2.注入临时容器至该Pod\n[root@k8s-master01 ~]# kubectl debug nginx-deploy-7dd6cd4b44-wdm6p -ti --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools\n</code></pre>\n<h5 id=\"21-注入临时容器到节点\"><a class=\"anchor\" href=\"#21-注入临时容器到节点\">#</a> 2.1 注入临时容器到节点</h5>\n<pre><code>kubectl debug node k8s-node01 -it --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3833778957.html",
            "url": "http://ixuyong.cn/posts/3833778957.html",
            "title": "K8s计划任务Job、Cronjob",
            "date_published": "2025-04-19T13:00:21.000Z",
            "content_html": "<h3 id=\"k8s计划任务job-cronjob\"><a class=\"anchor\" href=\"#k8s计划任务job-cronjob\">#</a> K8s 计划任务 Job、Cronjob</h3>\n<h4 id=\"1-job配置参数详解\"><a class=\"anchor\" href=\"#1-job配置参数详解\">#</a> 1. Job 配置参数详解</h4>\n<pre><code># cat job.yaml \napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    job-name: echo\n  name: echo\n  namespace: default\nspec:\n  #suspend: true # 1.21+\n  #ttlSecondsAfterFinished: 100\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    spec:\n      containers:\n      - name: echo\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - echo &quot;Hello Job&quot;\n      restartPolicy: Never\n      \n[root@k8s-master01 ~]# kubectl get jobs\nNAME   STATUS     COMPLETIONS   DURATION   AGE\necho   Complete   1/1           70s        2m5s\n\n[root@k8s-master01 ~]# kubectl get pods\nNAME          READY   STATUS      RESTARTS      AGE\necho-564c8    0/1     Completed   0             2m10s\n\n[root@k8s-master01 ~]# kubectl logs echo-564c8\nHello Job\n</code></pre>\n<ul>\n<li>backoffLimit:：如果任务执行失败，失败多少次后不再执行</li>\n<li>completions：有多少个 Pod 执行成功，认为任务是成功的，默认为空和 parallelism 数值一样</li>\n<li>parallelism：并行执行任务的数量，如果 parallelism 数值大于 completions 数值，只会创建 completions 的数量；如果 completions 是 4，并发是 3，第一次会创建 3 个 Pod 执行任务，第二次只会创建一个 Pod 执行任务</li>\n<li>ttlSecondsAfterFinished：Job 在执行结束之后（状态为 completed 或 Failed）自动清理。设置为 0 表示执行结束立即删除，不设置则不会清除，需要开启 TTLAfterFinished 特性</li>\n</ul>\n<h4 id=\"2-cronjob配置参数详解\"><a class=\"anchor\" href=\"#2-cronjob配置参数详解\">#</a> 2. CronJob 配置参数详解</h4>\n<pre><code># cat cronjob.yaml \napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  concurrencyPolicy: Allow   #允许同时运行多个任务\n  failedJobsHistoryLimit: 10  #保留多少失败的任务\n  successfulJobsHistoryLimit: 10  #保留多少已完成的任务\n  #suspend: true             #如果true则取消周期性执行任务\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            command:\n            - sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure \n          \n[root@k8s-master01 ~]# kubectl get  cj\nNAME    SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nhello   */1 * * * *   &lt;none&gt;     False     0        6s              81s\n\n[root@k8s-master01 ~]# kubectl get  jobs\nNAME             STATUS     COMPLETIONS   DURATION   AGE\nhello-29084454   Complete   1/1           4s         72s\nhello-29084455   Complete   1/1           5s         12s\n\n[root@k8s-master01 ~]# kubectl get  pods\nNAME                   READY   STATUS      RESTARTS   AGE\nhello-29084454-hwv7p   0/1     Completed   0          78s\nhello-29084455-vf99w   0/1     Completed   0          18s\n\n[root@k8s-master01 ~]# kubectl logs -f hello-29084455-vf99w\nSat Apr 19 12:55:02 UTC 2025\nHello from the Kubernetes cluster\n</code></pre>\n<ul>\n<li>apiVersion: batch/v1beta1   #1.21+ batch/v1</li>\n<li>schedule：调度周期，和 Linux 一致，分别是分时日月周。</li>\n<li>restartPolicy：重启策略，和 Pod 一致。</li>\n<li>concurrencyPolicy：并发调度策略。可选参数如下：\n<ul>\n<li>Allow：允许同时运行多个任务。</li>\n<li>Forbid：不允许并发运行，如果之前的任务尚未完成，新的任务不会被创建。</li>\n<li>Replace：如果之前的任务尚未完成，新的任务会替换的之前的任务。</li>\n</ul>\n</li>\n<li>suspend：如果设置为 true，则暂停后续的任务，默认为 false。</li>\n<li>successfulJobsHistoryLimit：保留多少已完成的任务，按需配置。</li>\n<li>failedJobsHistoryLimit：保留多少失败的任务。</li>\n</ul>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/169153047.html",
            "url": "http://ixuyong.cn/posts/169153047.html",
            "title": "K8s持久化存储",
            "date_published": "2025-04-18T14:25:17.000Z",
            "content_html": "<h3 id=\"k8s持久化存储\"><a class=\"anchor\" href=\"#k8s持久化存储\">#</a> K8s 持久化存储</h3>\n<h4 id=\"1-volume\"><a class=\"anchor\" href=\"#1-volume\">#</a> 1. Volume</h4>\n<p>Container（容器）中的磁盘文件是短暂的，当容器崩溃时，kubelet 会重新启动容器，Container 会以最干净的状态启动，最初的文件将丢失。另外，当一个 Pod 运行多个 Container 时，各个容器可能需要共享一些文件。Kubernetes Volume 可以解决这两个问题。</p>\n<ul>\n<li>一些需要持久化数据的程序才会用到 Volumes，或者一些需要共享数据的容器需要 volumes。</li>\n<li>日志收集的需求需要在应用程序的容器里面加一个 sidecar，这个容器是一个收集日志的容器，比如 filebeat，它通过 volumes 共享应用程序的日志文件目录。</li>\n</ul>\n<h5 id=\"11-emptydir实现数据共享\"><a class=\"anchor\" href=\"#11-emptydir实现数据共享\">#</a> 1.1 EmptyDir 实现数据共享</h5>\n<p>和上述 volume 不同的是，如果删除 Pod，emptyDir 卷中的数据也将被删除，一般 emptyDir 卷用于 Pod 中的不同 Container 共享数据。</p>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n        - name: share-volume\n          emptyDir: &#123;&#125;\n      containers:\n        - name: nginx\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n            - name: share-volume\n              mountPath: /opt\n        - name: nginx2\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          command:\n            - sh\n            - '-c'\n            - sleep 3600\n          volumeMounts:\n            - name: share-volume\n              mountPath: /mnt\n</code></pre>\n<h5 id=\"12-volumes-hostpath挂载宿主机路径\"><a class=\"anchor\" href=\"#12-volumes-hostpath挂载宿主机路径\">#</a> 1.2 Volumes HostPath 挂载宿主机路径</h5>\n<p>hostPath 卷可将节点上的文件或目录挂载到 Pod 上，用于 Pod 自定义日志输出或访问 Docker 内部的容器等。</p>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n      - name: share-volume\n        emptyDir: &#123;&#125;\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      containers:\n        - name: nginx\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: share-volume\n            mountPath: /opt\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n        - name: nginx2\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          command:\n            - sh\n            - '-c'\n            - sleep 3600\n          volumeMounts:\n          - name: share-volume\n            mountPath: /mnt\n</code></pre>\n<p>hostPath 卷常用的 type（类型）如下：</p>\n<ul>\n<li>type 为空字符串：默认选项，意味着挂载 hostPath 卷之前不会执行任何检查。</li>\n<li>DirectoryOrCreate：如果给定的 path 不存在任何东西，那么将根据需要创建一个权限为 0755 的空目录，和 Kubelet 具有相同的组和权限。</li>\n<li>Directory：目录必须存在于给定的路径下。</li>\n<li>FileOrCreate：如果给定的路径不存储任何内容，则会根据需要创建一个空文件，权限设置为 0644，和 Kubelet 具有相同的组和所有权。</li>\n<li>File：文件，必须存在于给定路径中。</li>\n<li>Socket：UNIX 套接字，必须存在于给定路径中。</li>\n<li>CharDevice：字符设备，必须存在于给定路径中。</li>\n<li>BlockDevice：块设备，必须存在于给定路径中。</li>\n</ul>\n<h5 id=\"13-挂载nfs至容器\"><a class=\"anchor\" href=\"#13-挂载nfs至容器\">#</a> 1.3 挂载 NFS 至容器</h5>\n<pre><code>#1.安装nfs\n# yum install nfs-utils -y       \n# mkdir /data/nfs -p\n# vim /etc/exports \n/data 192.168.1.0/24(rw,no_root_squash)\n# exportfs -arv   \n# systemctl start nfs-server &amp;&amp; systemctl enable nfs-server &amp;&amp; systemctl status nfs-server \n\n#2.测试客户端挂载\n# showmount -e 192.168.1.75\n# mount -t nfs 192.168.1.75:/data/nfs /mnt\n\n#3.Deploy挂载NFS\n[root@k8s-master01 ~]# cat nginx-deploy-nfs.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n      annotations:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n      - name: nfs-volume\n        nfs:\n          server: 192.168.1.75\n          path: /data/nfs\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: nfs-volume\n            mountPath: /usr/share/nginx/html\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n</code></pre>\n<h4 id=\"2-pv-pvc\"><a class=\"anchor\" href=\"#2-pv-pvc\">#</a> 2. PV、PVC</h4>\n<p>PersistentVolume：简称 PV，是由 Kubernetes 管理员设置的存储，可以配置 Ceph、NFS、GlusterFS 等常用存储配置，相对于 Volume 配置，提供了更多的功能，比如生命周期的管理、大小的限制。PV 分为静态和动态。</p>\n<p>PersistentVolumeClaim：简称 PVC，是对存储 PV 的请求，表示需要什么类型的 PV，需要存储的技术人员只需要配置 PVC 即可使用存储，或者 Volume 配置 PVC 的名称即可。</p>\n<h5 id=\"21-pv回收策略\"><a class=\"anchor\" href=\"#21-pv回收策略\">#</a> 2.1 PV 回收策略</h5>\n<ul>\n<li>Retain：保留，该策略允许手动回收资源，当删除 PVC 时，PV 仍然存在，PV 被视为已释放，管理员可以手动回收卷。</li>\n<li>Recycle：回收，如果 Volume 插件支持，Recycle 策略会对卷执行 rm -rf 清理该 PV，并使其可用于下一个新的 PVC，但是本策略将来会被弃用，目前只有 NFS 和 HostPath 支持该策略。</li>\n<li>Delete：删除，如果 Volume 插件支持，删除 PVC 时会同时删除 PV，动态卷默认为 Delete，目前支持 Delete 的存储后端包括 AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder 等。</li>\n<li>可以通过 persistentVolumeReclaimPolicy: Recycle 字段配置</li>\n</ul>\n<h5 id=\"22-pv访问策略\"><a class=\"anchor\" href=\"#22-pv访问策略\">#</a> 2.2 PV 访问策略</h5>\n<ul>\n<li>ReadWriteOnce：可以被单节点以读写模式挂载，命令行中可以被缩写为 RWO。</li>\n<li>ReadOnlyMany：可以被多个节点以只读模式挂载，命令行中可以被缩写为 ROX。</li>\n<li>ReadWriteMany：可以被多个节点以读写模式挂载，命令行中可以被缩写为 RWX。</li>\n<li>ReadWriteOncePod ：只允许被单个 Pod 访问，需要 K8s 1.22 + 以上版本，并且是 CSI 创建的 PV 才可使用，缩写为 RWOP</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Volume Plugin</th>\n<th style=\"text-align:center\">ReadWriteOnce</th>\n<th style=\"text-align:center\">ReadOnlyMany</th>\n<th style=\"text-align:center\">ReadWriteMany</th>\n<th style=\"text-align:center\">ReadWriteOncePod</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">AzureFile</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CephFS</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CSI</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FC</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FlexVolume</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">HostPath</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">iSCSI</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">NFS</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">RBD</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">VsphereVolume</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">- (works when Pods are collocated)</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">PortworxVolume</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"23-存储分类\"><a class=\"anchor\" href=\"#23-存储分类\">#</a> 2.3 存储分类</h5>\n<ul>\n<li>文件存储：一些数据可能需要被多个节点使用，比如用户的头像、用户上传的文件等，实现方式：NFS、NAS、FTP、CephFS 等。</li>\n<li>块存储：一些数据只能被一个节点使用，或者是需要将一块裸盘整个挂载使用，比如数据库、Redis 等，实现方式：Ceph、GlusterFS、公有云。</li>\n<li>对象存储：由程序代码直接实现的一种存储方式，云原生应用无状态化常用的实现方式，实现方式：一般是符合 S3 协议的云存储，比如 AWS 的 S3 存储、Minio、七牛云等。</li>\n</ul>\n<h5 id=\"24-pv配置示例nfs\"><a class=\"anchor\" href=\"#24-pv配置示例nfs\">#</a> 2.4 PV 配置示例 NFS</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv1\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: nfs-slow\n  nfs:\n    path: /data/pv1\n    server: 192.168.1.75\n</code></pre>\n<p>capacity：容量配置</p>\n<p>volumeMode：卷的模式，目前支持 Filesystem（文件系统） 和 Block（块），其中 Block 类型需要后端存储支持，默认为文件系统</p>\n<p>accessModes：该 PV 的访问模式</p>\n<p>storageClassName：PV 的类，一个特定类型的 PV 只能绑定到特定类别的 PVC；</p>\n<p>persistentVolumeReclaimPolicy：回收策略</p>\n<p>mountOptions：非必须，新版本中已弃用</p>\n<p>nfs：NFS 服务配置，包括以下两个选项</p>\n<ul>\n<li>path：NFS 上的共享目录</li>\n<li>server：NFS 的 IP 地址</li>\n</ul>\n<h5 id=\"25-pv配置示例hostpath\"><a class=\"anchor\" href=\"#25-pv配置示例hostpath\">#</a> 2.5 PV 配置示例 HostPath</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: hostpath\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: hostpath\n  hostPath:\n    path: &quot;/mnt/data&quot;\n</code></pre>\n<p>hostPath：hostPath 服务配置</p>\n<ul>\n<li>path：宿主机路径</li>\n</ul>\n<h5 id=\"26-pv的状态\"><a class=\"anchor\" href=\"#26-pv的状态\">#</a> 2.6 PV 的状态</h5>\n<ul>\n<li>Available：可用，没有被 PVC 绑定的空闲资源。</li>\n<li>Bound：已绑定，已经被 PVC 绑定。</li>\n<li>Released：已释放，PVC 被删除，但是资源还未被重新使用。</li>\n<li>Failed：失败，自动回收失败。</li>\n</ul>\n<h5 id=\"27-pvc绑定pv\"><a class=\"anchor\" href=\"#27-pvc绑定pv\">#</a> 2.7 PVC 绑定 PV</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  storageClassName: nfs-slow\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi      \n</code></pre>\n<ul>\n<li>PVC 的空间申请大小≤PV 的大小</li>\n<li>PVC 的 StorageClassName 和 PV 的一致</li>\n<li>PVC 的 accessModes 和 PV 的一致</li>\n</ul>\n<h5 id=\"28-depoyment挂载pvc\"><a class=\"anchor\" href=\"#28-depoyment挂载pvc\">#</a> 2.8 Depoyment 挂载 PVC</h5>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      volumes:\n      - name: nfs-pvc-storage  #volume名称\n        persistentVolumeClaim:\n          claimName: nfs-pvc   #PVC名称\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n         - name: nfs-pvc-storage\n          mountPath: /usr/share/nginx/html\n</code></pre>\n<p>挂载 PVC 的 Pod 一直处于 Pending：</p>\n<ul>\n<li>PVC 没有创建成功或 PVC 不存在</li>\n<li>PVC 和 Pod 不在同一个 Namespace</li>\n</ul>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3992668367.html",
            "url": "http://ixuyong.cn/posts/3992668367.html",
            "title": "K8s配置管理Configmap",
            "date_published": "2025-04-14T13:47:47.000Z",
            "content_html": "<h3 id=\"k8s配置管理configmap\"><a class=\"anchor\" href=\"#k8s配置管理configmap\">#</a> K8s 配置管理 Configmap</h3>\n<h4 id=\"1-configmap\"><a class=\"anchor\" href=\"#1-configmap\">#</a> 1. Configmap</h4>\n<h5 id=\"1-1-基于from-env-file创建configmap\"><a class=\"anchor\" href=\"#1-1-基于from-env-file创建configmap\">#</a> 1. 1 基于 from-env-file 创建 Configmap</h5>\n<pre><code># cat cm_env.conf \npodname=nf-flms-system\npodip=192.168.1.100\nenv=prod\nnacosaddr=nacos.svc.cluster.local\n\n#kubectl create cm cmenv --from-env-file=./cm_env.conf \n</code></pre>\n<h5 id=\"12-基于from-literal创建configmap\"><a class=\"anchor\" href=\"#12-基于from-literal创建configmap\">#</a> 1.2 基于 from-literal 创建 Configmap</h5>\n<pre><code># kubectl create cm cmliteral --from-literal=level=INFO --from-literal=passwd=Superman*2023\n</code></pre>\n<h5 id=\"13-基于from-file创建configmap\"><a class=\"anchor\" href=\"#13-基于from-file创建configmap\">#</a> 1.3 基于 from-file 创建 Configmap</h5>\n<pre><code># cat s.hmallleasing.com.conf \nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    client_max_body_size 1G; \n    location / &#123;\n        proxy_pass http://192.168.1.134;\n        proxy_set_header Host $http_host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        proxy_connect_timeout 30;\n        proxy_send_timeout 60;\n        proxy_read_timeout 60;\n        \n        proxy_buffering on;\n        proxy_buffer_size 32k;\n        proxy_buffers 4 128k;\n        proxy_temp_file_write_size 10240k;\t\t\n        proxy_max_temp_file_size 10240k;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n\n# kubectl create cm nginxconfig --from-file=./s.hmallleasing.com.conf\n</code></pre>\n<h5 id=\"14-deployment挂载configmap示例\"><a class=\"anchor\" href=\"#14-deployment挂载configmap示例\">#</a> 1.4 Deployment 挂载 configmap 示例</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字\n</code></pre>\n<h5 id=\"15-重命名挂载的configmaq-key的名称\"><a class=\"anchor\" href=\"#15-重命名挂载的configmaq-key的名称\">#</a> 1.5 重命名挂载的 configmaq key 的名称</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字\n          items:                # 重命名挂载的configmaq key的名称为nginx.conf\n          - key: s.hmallleasing.com.conf  \n            path: nginx.conf\n \n#查看挂载的configmaq key的名称重命名为nginx.conf\n[root@k8s-master01 cm]# kubectl get pods\nNAME                           READY   STATUS    RESTARTS   AGE\nnginx-deploy-bc476bc56-flln4   1/1     Running   0          10h\nnginx-deploy-bc476bc56-jhsh6   1/1     Running   0          10h\nnginx-deploy-bc476bc56-splv9   1/1     Running   0          10h\n[root@k8s-master01 cm]# kubectl exec -it nginx-deploy-bc476bc56-flln4 -- bash\nroot@nginx-deploy-bc476bc56-flln4:/# ls /etc/nginx/conf.d/\nnginx.conf\n</code></pre>\n<h5 id=\"16-修改挂载的configmaq-权限\"><a class=\"anchor\" href=\"#16-修改挂载的configmaq-权限\">#</a> 1.6 修改挂载的 configmaq 权限</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字\n          items:                # 重命名挂载的configmaq key的名称为nginx.conf\n          - key: s.hmallleasing.com.conf  \n            path: nginx.conf\n            mode: 0644        # 配置挂载权限，针对单个key生效\n          defaultMode: 0666   # 配置挂载权限，针对整个key生效\n    \n#查看挂载权限\nroot@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/nginx.conf \nlrwxrwxrwx 1 root root 17 Apr 16 13:37 /etc/nginx/conf.d/nginx.conf -&gt; ..data/nginx.conf\nroot@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/..data/nginx.conf \n-rw-rw-rw- 1 root root 722 Apr 16 13:37 /etc/nginx/conf.d/..data/nginx.conf\n</code></pre>\n<h5 id=\"17-subpath解决挂载覆盖问题\"><a class=\"anchor\" href=\"#17-subpath解决挂载覆盖问题\">#</a> 1.7 subpath 解决挂载覆盖问题</h5>\n<pre><code>#1.创建configmap\n[root@k8s-master01 cm]# cat nginx.conf \n\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents &#123;\n    worker_connections  512;\n&#125;\n\n\nhttp &#123;\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '\n                      '$status $body_bytes_sent &quot;$http_referer&quot; '\n                      '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n&#125;\n\n[root@k8s-master01 cm]# kubectl create cm nginx-config --from-file=./nginx.conf\n\n#subpath解决挂载覆盖问题\n[root@k8s-master01 study]# cat cm-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # ①批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # ②自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # ③挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:\n        - name: config\n          mountPath: &quot;/etc/nginx/nginx.conf&quot;   #只挂在nginx.conf一个文件,不覆盖目录\n          subPath: nginx.conf      \n      volumes:\n      - name: config\n        configMap:\n          name: nginx-config      # 提供你想要挂载的ConfigMap的名字\n</code></pre>\n<h4 id=\"2-secret\"><a class=\"anchor\" href=\"#2-secret\">#</a> 2. Secret</h4>\n<h5 id=\"21-secret拉取私有仓库镜像\"><a class=\"anchor\" href=\"#21-secret拉取私有仓库镜像\">#</a> 2.1 Secret 拉取私有仓库镜像</h5>\n<pre><code># kubectl create secret docker-registry harboradmin \\\n--docker-server=s.hmallleasing.com \\\n--docker-username=admin \\\n--docker-password=Superman*2023 \n</code></pre>\n<h5 id=\"22-创建ssl-secret\"><a class=\"anchor\" href=\"#22-创建ssl-secret\">#</a> 2.2 创建 ssl Secret</h5>\n<pre><code># kubectl create secret tls dev.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev\n</code></pre>\n<h5 id=\"23-基于命令创建generic-secret\"><a class=\"anchor\" href=\"#23-基于命令创建generic-secret\">#</a> 2.3 基于命令创建 generic Secret</h5>\n<pre><code>#1.通过from-env-file创建\n# cat db.conf \nusername=xuyong\npasswd=Superman*2023\n\n# kubectl create secret generic dbconf --from-env-file=./db.conf\n\n#2.通过from-literal创建\nkubectl create secret generic db-user-pass \\\n    --from-literal=username=admin \\\n    --from-literal=password='S!B\\*d$zDsb='\n</code></pre>\n<h5 id=\"24-secret加密-解密\"><a class=\"anchor\" href=\"#24-secret加密-解密\">#</a> 2.4 Secret 加密、解密</h5>\n<pre><code>1.加密\n# echo -n &quot;Superman*2023&quot; | base64\nU3VwZXJtYW4qMjAyMw==\n\n2.解密\n# echo &quot;U3VwZXJtYW4qMjAyMw==&quot; | base64 --decode\n</code></pre>\n<h5 id=\"25-基于文件创建非加密generic-secret\"><a class=\"anchor\" href=\"#25-基于文件创建非加密generic-secret\">#</a> 2.5 基于文件创建非加密 generic Secret</h5>\n<pre><code># kubectl get secret dbconf -oyaml\napiVersion: v1\ndata:\n  passwd: U3VwZXJtYW4qMjAyMw==\n  username: eHV5b25n\nkind: Secret\nmetadata:\n  name: dbconf\n  namespace: default\ntype: Opaque\n</code></pre>\n<h5 id=\"2-6-基于yaml创建加密generic-secret\"><a class=\"anchor\" href=\"#2-6-基于yaml创建加密generic-secret\">#</a> 2. 6 基于 yaml 创建加密 generic Secret</h5>\n<pre><code># cat mysql-secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysql-secret\n  namespace: dev\nstringData:\n  MYSQL_ROOT_PASSWORD: Superman*2023\ntype: Opaque\n</code></pre>\n<h5 id=\"27-deployment挂载secret示例\"><a class=\"anchor\" href=\"#27-deployment挂载secret示例\">#</a> 2.7 Deployment 挂载 Secret 示例</h5>\n<pre><code>[root@k8s-master01 study]# cat cm-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: MYSQL_ROOT_PASSWORD  \n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: MYSQL_ROOT_PASSWORD\n</code></pre>\n<h4 id=\"3-configmapsecret热更新\"><a class=\"anchor\" href=\"#3-configmapsecret热更新\">#</a> 3. ConfigMap&amp;Secret 热更新</h4>\n<pre><code># kubectl create cm nginxconfig --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f -\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/858611107.html",
            "url": "http://ixuyong.cn/posts/858611107.html",
            "title": "K8s服务发布Service",
            "date_published": "2025-04-14T11:25:51.000Z",
            "content_html": "<h3 id=\"k8s服务发布service\"><a class=\"anchor\" href=\"#k8s服务发布service\">#</a> K8s 服务发布 Service</h3>\n<h4 id=\"1-service类型\"><a class=\"anchor\" href=\"#1-service类型\">#</a> 1. Service 类型</h4>\n<p>Kubernetes Service Type（服务类型）主要包括以下几种：</p>\n<ul>\n<li>ClusterIP：在集群内部使用，默认值，只能从集群中访问。</li>\n<li>NodePort：在所有安装了 Kube-Proxy 的节点上打开一个端口，此端口可以代理至后端 Pod，可以通过 NodePort 从集群外部访问集群内的服务，格式为 NodeIP:NodePort。</li>\n<li>LoadBalancer：使用云提供商的负载均衡器公开服务，成本较高。</li>\n<li>ExternalName：通过返回定义的 CNAME 别名，没有设置任何类型的代理，需要 1.7 或更高版本 kube-dns 支持。</li>\n</ul>\n<h5 id=\"11-nodeport类型\"><a class=\"anchor\" href=\"#11-nodeport类型\">#</a> 1.1 NodePort 类型</h5>\n<p>如果将 Service 的 type 字段设置为 NodePort，则 Kubernetes 将从 --service-node-port-range 参数指定的范围（默认为 30000-32767）中自动分配端口，也可以手动指定 NodePort，创建该 Service 后，集群每个节点都将暴露一个端口，通过某个宿主机的 IP + 端口即可访问到后端的应用。</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: NodePort\n</code></pre>\n<h5 id=\"12-clusterip类型\"><a class=\"anchor\" href=\"#12-clusterip类型\">#</a> 1.2 ClusterIP 类型</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: ClusterIP\n</code></pre>\n<h5 id=\"13-使用service代理k8s外部服务\"><a class=\"anchor\" href=\"#13-使用service代理k8s外部服务\">#</a> 1.3 使用 Service 代理 K8s 外部服务</h5>\n<p>使用场景：</p>\n<ul>\n<li>希望在生产环境中使用某个固定的名称而非 IP 地址访问外部的中间件服务；</li>\n<li>希望 Service 指向另一个 Namespace 中或其他集群中的服务；</li>\n<li>正在将工作负载转移到 Kubernetes 集群，但是一部分服务仍运行在 Kubernetes 集群之外的 backend。</li>\n</ul>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: mysql-svc-external\n  name: mysql-svc-external\nspec:\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306 \n    protocol: TCP\n    targetPort: 3306\n  type: ClusterIP\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  labels:\n    app: mysql-svc-external\n  name: mysql-svc-external\nsubsets:\n- addresses:\n  - ip: 192.168.40.150\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n</code></pre>\n<h5 id=\"14-externalname-service\"><a class=\"anchor\" href=\"#14-externalname-service\">#</a> 1.4 ExternalName Service</h5>\n<p>ExternalName Service 是 Service 的特例，它没有 Selector，也没有定义任何端口和 Endpoint，它通过返回该外部服务的别名来提供服务。</p>\n<p>比如可以定义一个 Service，后端设置为一个外部域名，这样通过 Service 的名称即可访问到该域名。使用 nslookup 解析以下文件定义的 Service，集群的 DNS <a href=\"http://xn--my-uu2cmg2cx7mswf9rko5lsx1a5n3h.database.example.com\">服务将返回一个值为 my.database.example.com</a> 的 CNAME 记录：</p>\n<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: my-service\n  namespace: prod\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n</code></pre>\n<h5 id=\"15-多端口-service\"><a class=\"anchor\" href=\"#15-多端口-service\">#</a> 1.5 多端口 Service</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n    - port: 443\n      targetPort: 443\n      protocol: TCP\n      name: https\n  selector:\n    app: nginx\n  type: ClusterIP\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/108692210.html",
            "url": "http://ixuyong.cn/posts/108692210.html",
            "title": "K8s资源调度deployment、statefulset、daemonset",
            "date_published": "2025-04-14T11:25:00.000Z",
            "content_html": "<h3 id=\"k8s资源调度deployment-statefulset-daemonset\"><a class=\"anchor\" href=\"#k8s资源调度deployment-statefulset-daemonset\">#</a> K8s 资源调度 deployment、statefulset、daemonset</h3>\n<h4 id=\"1-无状态应用管理-deployment\"><a class=\"anchor\" href=\"#1-无状态应用管理-deployment\">#</a> 1. 无状态应用管理 Deployment</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:1.21.0\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n</code></pre>\n<p>示例解析：</p>\n<ol>\n<li>\n<p>nginx-deploy：Deployment 的名称；</p>\n</li>\n<li>\n<p>replicas： 创建 Pod 的副本数；</p>\n</li>\n<li>\n<p>selector：定义 Deployment 如何找到要管理的 Pod，与 template 的 label（标签）对应，apiVersion 为 apps/v1 必须指定该字段；</p>\n</li>\n<li>\n<p>template 字段包含以下字段：</p>\n<ul>\n<li>\n<p>app: nginx-deploy 使用 label（标签）标记 Pod；</p>\n</li>\n<li>\n<p>spec：表示 Pod 运行一个名字为 nginx 的容器；</p>\n</li>\n<li>\n<p>image：运行此 Pod 使用的镜像；</p>\n</li>\n<li>\n<p>Port：容器用于发送和接收流量的端口。</p>\n</li>\n</ul>\n</li>\n</ol>\n<h5 id=\"11-更新-deployment\"><a class=\"anchor\" href=\"#11-更新-deployment\">#</a> 1.1 更新 Deployment</h5>\n<p>假如更新 Nginx Pod 的 image 使用 nginx:latest，并使用 --record 记录当前更改的参数，后期回滚时可以查看到对应的信息：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record\n</code></pre>\n<p>更新过程为新旧交替更新，首先新建一个 Pod，当 Pod 状态为 Running 时，删除一个旧的 Pod，同时再创建一个新的 Pod。当触发一个更新后，会有新的 ReplicaSet 产生，旧的 ReplicaSet 会被保存，查看此时 ReplicaSet，可以从 AGE 或 READY 看出来新旧 ReplicaSet：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get rs\nNAME                      DESIRED   CURRENT   READY   AGE\nnginx-deploy-65bfb77869   0         0         0       50s\nnginx-deploy-85b94dddb4   3         3         3       8s\n</code></pre>\n<p>通过 describe 查看 Deployment 的详细信息：</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl describe deploy nginx-deploy\nName:                   nginx-deploy\nNamespace:              default\nCreationTimestamp:      Mon, 14 Apr 2025 11:28:03 +0800\nLabels:                 app=nginx-deploy\nAnnotations:            app: nginx-deploy\n                        deployment.kubernetes.io/revision: 2\n                        kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\nSelector:               app=nginx-deploy\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=nginx-deploy\n  Containers:\n   nginx-deploy:\n    Image:         nginx:latest\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Environment:   &lt;none&gt;\n    Mounts:        &lt;none&gt;\n  Volumes:         &lt;none&gt;\n  Node-Selectors:  &lt;none&gt;\n  Tolerations:     &lt;none&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  nginx-deploy-65bfb77869 (0/0 replicas created)\nNewReplicaSet:   nginx-deploy-85b94dddb4 (3/3 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  71s   deployment-controller  Scaled up replica set nginx-deploy-65bfb77869 from 0 to 3\n  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 0 to 1\n  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 3 to 2\n  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 1 to 2\n  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 2 to 1\n  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 2 to 3\n  Normal  ScalingReplicaSet  26s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 1 to 0\n</code></pre>\n<p>在 describe 中可以看出，第一次创建时，它创建了一个名为 nginx-deploy-65bfb77869 的 ReplicaSet，并直接将其扩展为 3 个副本。更新部署时，它创建了一个新的 ReplicaSet，命名为 nginx-deploy-85b94dddb4，并将其副本数扩展为 1，然后将旧的 ReplicaSet 缩小为 2，这样至少可以有 2 个 Pod 可用，最多创建了 4 个 Pod。以此类推，使用相同的滚动更新策略向上和向下扩展新旧 ReplicaSet，最终新的 ReplicaSet 可以拥有 3 个副本，并将旧的 ReplicaSet 缩小为 0。</p>\n<h5 id=\"12-回滚-deployment\"><a class=\"anchor\" href=\"#12-回滚-deployment\">#</a> 1.2 回滚 Deployment</h5>\n<p>当更新了版本不稳定或配置不合理时，可以对其进行回滚操作，假设我们又进行了几次更新（此处以更新镜像版本触发更新，更改配置效果类似）：</p>\n<pre><code># kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record\n# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record\n</code></pre>\n<p>使用 kubectl rollout history 查看更新历史：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy\ndeployment.apps/nginx-deploy \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\n3         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true\n4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n</code></pre>\n<p>查看 Deployment 某次更新的详细信息，使用 --revision 指定某次更新版本号：</p>\n<pre><code># kubectl rollout history deployment nginx-deploy --revision=4\ndeployment.apps/nginx-deploy with revision #4\nPod Template:\n  Labels:\tapp=nginx-deploy\n\tpod-template-hash=65b576b795\n  Annotations:\tkubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n  Containers:\n   nginx-deploy:\n    Image:\tnginx:1.21.2\n    Port:\t&lt;none&gt;\n    Host Port:\t&lt;none&gt;\n    Environment:\t&lt;none&gt;\n    Mounts:\t&lt;none&gt;\n  Volumes:\t&lt;none&gt;\n  Node-Selectors:\t&lt;none&gt;\n  Tolerations:\t&lt;none&gt;\n</code></pre>\n<p>如果只需要回滚到上一个稳定版本，使用 kubectl rollout undo 即可：</p>\n<pre><code># kubectl rollout undo deployment nginx-deploy\n</code></pre>\n<p>再次查看更新历史，发现 REVISION3 回到了 nginx:1.21.1：</p>\n<pre><code># kubectl rollout history deployment nginx-deploy\ndeployment.apps/nginx-deploy \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\n4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n5         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true\n</code></pre>\n<p>如果要回滚到指定版本，使用 --to-revision 参数：</p>\n<pre><code># kubectl rollout undo deployment nginx-deploy --to-revision=2\n</code></pre>\n<h5 id=\"13-扩容-deployment\"><a class=\"anchor\" href=\"#13-扩容-deployment\">#</a> 1.3 扩容 Deployment</h5>\n<p>当公司访问量变大，或者有预期内的活动时，三个 Pod 可能已无法支撑业务时，可以提前对其进行扩展。</p>\n<p>使用 kubectl scale 动态调整 Pod 的副本数，比如增加 Pod 为 5 个：</p>\n<pre><code># kubectl scale deployment nginx-deploy --replicas=5\n</code></pre>\n<p>查看 Pod，此时 Pod 已经变成了 5 个：</p>\n<pre><code># kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nnginx-deploy-85b94dddb4-2qrh6   1/1     Running   0          2m9s\nnginx-deploy-85b94dddb4-gvkqj   1/1     Running   0          2m10s\nnginx-deploy-85b94dddb4-mdfjs   1/1     Running   0          22s\nnginx-deploy-85b94dddb4-rhgpr   1/1     Running   0          2m8s\nnginx-deploy-85b94dddb4-vwjhl   1/1     Running   0          22s\n</code></pre>\n<h5 id=\"14-暂停和恢复-deployment-更新\"><a class=\"anchor\" href=\"#14-暂停和恢复-deployment-更新\">#</a> 1.4 暂停和恢复 Deployment 更新</h5>\n<p>上述演示的均为更改某一处的配置，更改后立即触发更新，大多数情况下可能需要针对一个资源文件更改多处地方，而并不需要多次触发更新，此时可以使用 Deployment 暂停功能，临时禁用更新操作，对 Deployment 进行多次修改后在进行更新。</p>\n<p>使用 kubectl rollout pause 命令即可暂停 Deployment 更新：</p>\n<pre><code># kubectl rollout pause deployment nginx-deploy\n</code></pre>\n<p>然后对 Deployment 进行相关更新操作，比如先更新镜像，然后对其资源进行限制（如果使用的是 kubectl edit 命令，可以直接进行多次修改，无需暂停更新，kubectlset 命令一般会集成在 CICD 流水线中）：</p>\n<pre><code># kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.3\n# kubectl set resources deployment nginx-deploy -c=nginx-deploy --limits=cpu=200m,memory=512Mi\n</code></pre>\n<p>通过 rollout history 可以看到没有新的更新：</p>\n<pre><code>#  kubectl rollout history deployment nginx-deploy\n</code></pre>\n<p>进行完最后一处配置更改后，使用 kubectl rollout resume 恢复 Deployment 更新：</p>\n<pre><code># kubectl rollout resume deployment nginx-deploy\n</code></pre>\n<p>可以查看到恢复更新的 Deployment 创建了一个新的 RS（ReplicaSet 缩写）：</p>\n<pre><code># kubectl get rs\n</code></pre>\n<p>可以查看 Deployment 的 image（镜像）已经变为 nginx:1.21.3</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n</code></pre>\n<h5 id=\"15-更新-deployment-的注意事项\"><a class=\"anchor\" href=\"#15-更新-deployment-的注意事项\">#</a> 1.5 更新 Deployment 的注意事项</h5>\n<p>在默认情况下，revision 保留 10 个旧的 ReplicaSet，其余的将在后台进行垃圾回收，可以在.spec.revisionHistoryLimit 设置保留 ReplicaSet 的个数。当设置为 0 时，不保留历史记录。</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  namespace: default\n  labels:\n    app: nginx-deploy\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:1.21.3\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  revisionHistoryLimit: 10\n</code></pre>\n<p>更新策略：</p>\n<ul>\n<li>spec.strategy.type==Recreate，表示重建，先删掉旧的 Pod 再创建新的 Pod；</li>\n</ul>\n<pre><code>  strategy:\n    type: Recreate\n</code></pre>\n<ul>\n<li>\n<p>spec.strategy.type==RollingUpdate，表示滚动更新，可以指定 maxUnavailable 和 maxSurge 来控制滚动更新过程；</p>\n<ul>\n<li>\n<p>spec.strategy.rollingUpdate.maxUnavailable，指定在回滚更新时最大不可用的 Pod 数量，可选字段，默认为 25%，可以设置为数字或百分比，如果 maxSurge 为 0，则该值不能为 0；</p>\n</li>\n<li>\n<p>spec.strategy.rollingUpdate.maxSurge 可以超过期望值的最大 Pod 数，可选字段，默认为 25%，可以设置成数字或百分比，如果 maxUnavailable 为 0，则该值不能为 0。</p>\n</li>\n</ul>\n</li>\n</ul>\n<pre><code>  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n</code></pre>\n<h4 id=\"2-有状态应用管理-statefulset\"><a class=\"anchor\" href=\"#2-有状态应用管理-statefulset\">#</a> 2. 有状态应用管理 StatefulSet</h4>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web\n  namespace: default\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: ClusterIP\n  clusterIP: None\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx\n  namespace: default\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          resources:\n            limits:\n              cpu: '1'\n              memory: 1Gi\n            requests:\n              cpu: 100m\n              memory: 128Mi\n      restartPolicy: Always\n  serviceName: web\n</code></pre>\n<ul>\n<li>kind: Service 定义了一个名字为 web 的 Headless Service，创建的 Service 格式为 nginx-0.web.default.svc.cluster.local，其他的类似，因为没有指定 Namespace（命名空间），所以默认部署在 default；</li>\n<li>kind: StatefulSet 定义了一个名字为 nginx 的 StatefulSet，replicas 表示部署 Pod 的副本数，本实例为 3。</li>\n</ul>\n<h5 id=\"21-创建-statefulset\"><a class=\"anchor\" href=\"#21-创建-statefulset\">#</a> 2.1 创建 StatefulSet</h5>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods\nNAME      READY   STATUS    RESTARTS   AGE\nnginx-0   1/1     Running   0          8m51s\nnginx-1   1/1     Running   0          8m50s\nnginx-2   1/1     Running   0          8m48s\n[root@k8s-master01 ~]# kubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   6d1h\nweb          ClusterIP   None         &lt;none&gt;        80/TCP    9m28s\n[root@k8s-master01 ~]# kubectl get sts\nNAME    READY   AGE\nnginx   3/3     8m58s\n</code></pre>\n<h5 id=\"22-statefulset创建pod流程\"><a class=\"anchor\" href=\"#22-statefulset创建pod流程\">#</a> 2.2 StatefulSet 创建 Pod 流程</h5>\n<p>StatefulSet 管理的 Pod 部署和扩展规则如下：</p>\n<ul>\n<li>对于具有 N 个副本的 StatefulSet，将按顺序从 0 到 N-1 开始创建 Pod；</li>\n<li>当删除 Pod 时，将按照 N-1 到 0 的反顺序终止；</li>\n<li>在缩放 Pod 之前，必须保证当前的 Pod 是 Running（运行中）或者 Ready（就绪）；</li>\n<li>在终止 Pod 之前，它所有的继任者必须是完全关闭状态。</li>\n</ul>\n<p>StatefulSet 的 pod.Spec.TerminationGracePeriodSeconds（终止 Pod 的等待时间）不应该指定为 0，设置为 0 对 StatefulSet 的 Pod 是极其不安全的做法，优雅地删除 StatefulSet 的 Pod 是非常有必要的，而且是安全的，因为它可以确保在 Kubelet 从 APIServer 删除之前，让 Pod 正常关闭。</p>\n<p>当创建上面的 Nginx 实例时，Pod 将按 nginx-0、nginx-1、nginx-2 的顺序部署 3 个 Pod。在 nginx-0 处于 Running 或者 Ready 之前，nginx-1 不会被部署，相同的，nginx-2 在 web-1 未处于 Running 和 Ready 之前也不会被部署。如果在 nginx-1 处于 Running 和 Ready 状态时，nginx-0 变成 Failed 失败）状态，那么 nginx-2 将不会被启动，直到 nginx-0 恢复为 Running 和 Ready 状态。</p>\n<p>如果用户将 StatefulSet 的 replicas 设置为 1，那么 nginx-2 将首先被终止，在完全关闭并删除 nginx-2 之前，不会删除 nginx-1。如果 nginx-2 终止并且完全关闭后，nginx-0 突然失败，那么在 nginx-0 未恢复成 Running 或者 Ready 时，nginx-1 不会被删除。</p>\n<h5 id=\"23-tatefulset-扩容和缩容\"><a class=\"anchor\" href=\"#23-tatefulset-扩容和缩容\">#</a> 2.3 tatefulSet 扩容和缩容</h5>\n<p>和 Deployment 类似，可以通过更新 replicas 字段扩容 / 缩容 StatefulSet，也可以使用 kubectlscale、kubectl edit 和 kubectl patch 来扩容 / 缩容一个 StatefulSet。</p>\n<pre><code># kubectl scale sts nginx --replicas=5\n</code></pre>\n<h5 id=\"24-statefulset-更新策略\"><a class=\"anchor\" href=\"#24-statefulset-更新策略\">#</a> 2.4 StatefulSet 更新策略</h5>\n<p><strong>On Delete 策略</strong></p>\n<p>OnDelete 更新策略实现了传统（1.7 版本之前）的行为，它也是默认的更新策略。当我们选择这个更新策略并修改 StatefulSet 的.spec.template 字段时，StatefulSet 控制器不会自动更新 Pod，必须手动删除 Pod 才能使控制器创建新的 Pod。</p>\n<pre><code>  updateStrategy:\n    type: OnDelete\n</code></pre>\n<p><strong>RollingUpdate 策略</strong></p>\n<p>RollingUpdate（滚动更新）更新策略会自动更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序进行滚动更新。</p>\n<pre><code>  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 0\n</code></pre>\n<h5 id=\"25-分段更新\"><a class=\"anchor\" href=\"#25-分段更新\">#</a> 2.5 分段更新</h5>\n<p>将分区改为 2，此时会自动更新 nginx-2、nginx-3、nginx-4（因为之前更改了更新策略），但是不会更新 nginx-0 和 nginx-1：</p>\n<pre><code>  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 2\n</code></pre>\n<p>将 sts 镜像为 nginx:1.21.1</p>\n<pre><code># kubectl set image sts nginx nginx=nginx:1.21.1\n</code></pre>\n<p>按照上述方式，可以实现分阶段更新，类似于灰度 / 金丝雀发布。查看最终的结果如下：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image\n    - image: nginx:latest\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:latest\n      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8\n    - image: nginx:latest\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:latest\n      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n</code></pre>\n<h5 id=\"26-statefulset-挂载动态存储\"><a class=\"anchor\" href=\"#26-statefulset-挂载动态存储\">#</a> 2.6 StatefulSet 挂载动态存储</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx \n  serviceName: &quot;nginx&quot;\n  replicas: 3 1\n  template:\n    metadata:\n      labels:\n        app: nginx \n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.20\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      storageClassName: &quot;rook-ceph-block&quot;\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>\n<h4 id=\"3守护进程集-daemonset\"><a class=\"anchor\" href=\"#3守护进程集-daemonset\">#</a> 3. 守护进程集 DaemonSet</h4>\n<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ds\n  labels:\n    app: nginx-ds\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-ds\n  template:\n    metadata:\n      labels:\n        app: nginx-ds\n    spec:\n      containers:\n        - name: nginx-ds\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<p>此时会在每个节点创建一个 Pod：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-ds-47dxc   1/1     Running   0          56s   172.16.85.213    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-ds-4m89f   1/1     Running   0          56s   172.16.32.143    k8s-master01   &lt;none&gt;           &lt;none&gt;\nnginx-ds-mtpc2   1/1     Running   0          56s   172.16.195.12    k8s-master03   &lt;none&gt;           &lt;none&gt;\nnginx-ds-t5rxc   1/1     Running   0          56s   172.16.122.142   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-ds-x86kc   1/1     Running   0          56s   172.16.58.222    k8s-node02     &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<p>指定节点部署 Pod</p>\n<pre><code>      nodeSelector:\n        ingress: 'true'\n</code></pre>\n<p>更新和回滚 DaemonSet</p>\n<pre><code># kubectl set image ds nginx-ds nginx-ds=1.21.0 --record=true\n# kubectl rollout undo daemonset &lt;daemonset-name&gt; --to-revision=&lt;revision&gt;\n</code></pre>\n<p>DaemonSet 的更新和回滚与 Deployment 类似，此处不再演示。</p>\n<h4 id=\"4-hpa\"><a class=\"anchor\" href=\"#4-hpa\">#</a> 4. HPA</h4>\n<p>创建 deployment、service</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-hpa-svc\n  namespace: default\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx-hpa\n  type: ClusterIP\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-hpa\n  labels:\n    app: nginx-hpa\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-hpa\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-hpa\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-hpa\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<p>创建 HPA</p>\n<pre><code># kubectl autoscale deployment nginx-hpa --cpu-percent=10 --min=1 --max=10\n# kubectl get hpa\nNAME        REFERENCE              TARGETS       MINPODS   MAXPODS   REPLICAS   AGE\nnginx-hpa   Deployment/nginx-hpa   cpu: 0%/10%   1         10        1          16s\n\n</code></pre>\n<p>测试自动扩缩容</p>\n<pre><code>while true; do wget -q -O- http://10.96.18.221 &gt; /dev/null; done\n[root@k8s-master01 ~]# kubectl get pods\nNAME                        READY   STATUS    RESTARTS   AGE\nnginx-hpa-d8bcbdf7d-4mkxp   1/1     Running   0          66s\nnginx-hpa-d8bcbdf7d-974q5   1/1     Running   0          6m36s\nnginx-hpa-d8bcbdf7d-g6p2h   1/1     Running   0          66s\nnginx-hpa-d8bcbdf7d-lvvsq   1/1     Running   0          111s\nnginx-hpa-d8bcbdf7d-tgqmr   1/1     Running   0          111s\nnginx-hpa-d8bcbdf7d-tzfbs   1/1     Running   0          21s\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/1771242682.html",
            "url": "http://ixuyong.cn/posts/1771242682.html",
            "title": "K8s零宕机服务发布-探针",
            "date_published": "2025-04-14T11:23:48.000Z",
            "content_html": "<h3 id=\"k8s零宕机服务发布-探针\"><a class=\"anchor\" href=\"#k8s零宕机服务发布-探针\">#</a> K8s 零宕机服务发布 - 探针</h3>\n<h4 id=\"1-pod状态及-pod-故障排查命令\"><a class=\"anchor\" href=\"#1-pod状态及-pod-故障排查命令\">#</a> 1. Pod 状态及 Pod 故障排查命令</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">状态</th>\n<th style=\"text-align:left\">说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">Pending（挂起）</td>\n<td style=\"text-align:left\">Pod 已被 Kubernetes 系统接收，但仍有一个或多个容器未被创建，可以通过 kubectl describe 查看处于 Pending 状态的原因</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Running（运行中）</td>\n<td style=\"text-align:left\">Pod 已经被绑定到一个节点上，并且所有的容器都已经被创建，而且至少有一个是运行状态，或者是正在启动或者重启，可以通过 kubectl logs 查看 Pod 的日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Succeeded（成功）</td>\n<td style=\"text-align:left\">所有容器执行成功并终止，并且不会再次重启，可以通过 kubectl logs 查看 Pod 日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Failed（失败）</td>\n<td style=\"text-align:left\">所有容器都已终止，并且至少有一个容器以失败的方式终止，也就是说这个容器要么以非零状态退出，要么被系统终止，可以通过 logs 和 describe 查看 Pod 日志和状态</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Unknown（未知）</td>\n<td style=\"text-align:left\">通常是由于通信问题造成的无法获得 Pod 的状态</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ImagePullBackOff ErrImagePull</td>\n<td style=\"text-align:left\">镜像拉取失败，一般是由于镜像不存在、网络不通或者需要登录认证引起的，可以使用 describe 命令查看具体原因</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CrashLoopBackOff</td>\n<td style=\"text-align:left\">容器启动失败，可以通过 logs 命令查看具体原因，一般为启动命令不正确，健康检查不通过等</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">OOMKilled</td>\n<td style=\"text-align:left\">容器内存溢出，一般是容器的内存 Limit 设置的过小，或者程序本身有内存溢出，可以通过 logs 查看程序启动日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Terminating</td>\n<td style=\"text-align:left\">Pod 正在被删除，可以通过 describe 查看状态</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">SysctlForbidden</td>\n<td style=\"text-align:left\">Pod 自定义了内核配置，但 kubelet 没有添加内核配置或配置的内核参数不支持，可以通过 describe 查看具体原因</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Completed</td>\n<td style=\"text-align:left\">容器内部主进程退出，一般计划任务执行结束会显示该状态，此时可以通过 logs 查看容器日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ContainerCreating</td>\n<td style=\"text-align:left\">Pod 正在创建，一般为正在下载镜像，或者有配置不当的地方，可以通过 describe 查看具体原因</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"2-pod镜像拉取策略\"><a class=\"anchor\" href=\"#2-pod镜像拉取策略\">#</a> 2. Pod 镜像拉取策略</h4>\n<p>通过 spec.containers [].imagePullPolicy 参数可以指定镜像的拉取策略，目前支持的策略如下：</p>\n<table>\n<thead>\n<tr>\n<th>操作方式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Always</td>\n<td>总是拉取，当镜像 tag 为 latest 时，且 imagePullPolicy 未配置，默认为 Always</td>\n</tr>\n<tr>\n<td>Never</td>\n<td>不管是否存在都不会拉取</td>\n</tr>\n<tr>\n<td>IfNotPresent</td>\n<td>镜像不存在时拉取镜像，如果 tag 为非 latest，且 imagePullPolicy 未配置，默认为 IfNotPresent</td>\n</tr>\n</tbody>\n</table>\n<p>更改镜像拉取策略为 IfNotPresent：</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n</code></pre>\n<h4 id=\"3-pod-重启策略\"><a class=\"anchor\" href=\"#3-pod-重启策略\">#</a> 3. <strong>Pod</strong> 重启策略</h4>\n<table>\n<thead>\n<tr>\n<th>操作方式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Always</td>\n<td>默认策略。容器失效时，自动重启该容器</td>\n</tr>\n<tr>\n<td>OnFailure</td>\n<td>容器以不为 0 的状态码终止，自动重启该容器</td>\n</tr>\n<tr>\n<td>Never</td>\n<td>无论何种状态，都不会重启</td>\n</tr>\n</tbody>\n</table>\n<p>指定重启策略为 Always ：</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n</code></pre>\n<h4 id=\"4-pod的三种探针\"><a class=\"anchor\" href=\"#4-pod的三种探针\">#</a> 4. Pod 的三种探针</h4>\n<table>\n<thead>\n<tr>\n<th>种类</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>startupProbe</td>\n<td>Kubernetes1.16 新加的探测方式，用于判断容器内的应用程序是否已经启动。如果配置了 startupProbe，就会先禁用其他探测，直到它成功为止。如果探测失败，Kubelet 会杀死容器，之后根据重启策略进行处理，如果探测成功，或没有配置 startupProbe，则状态为成功，之后就不再探测。</td>\n</tr>\n<tr>\n<td>livenessProbe</td>\n<td>用于探测容器是否在运行，如果探测失败，kubelet 会 “杀死” 容器并根据重启策略进行相应的处理。如果未指定该探针，将默认为 Success</td>\n</tr>\n<tr>\n<td>readinessProbe</td>\n<td>一般用于探测容器内的程序是否健康，即判断容器是否为就绪（Ready）状态。如果是，则可以处理请求，反之 Endpoints Controller 将从所有的 Service 的 Endpoints 中删除此容器所在 Pod 的 IP 地址。如果未指定，将默认为 Success</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"5-pod探针的实现方式\"><a class=\"anchor\" href=\"#5-pod探针的实现方式\">#</a> 5. Pod 探针的实现方式</h4>\n<table>\n<thead>\n<tr>\n<th>实现方式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ExecAction</td>\n<td>在容器内执行一个指定的命令，如果命令返回值为 0，则认为容器健康</td>\n</tr>\n<tr>\n<td>TCPSocketAction</td>\n<td>通过 TCP 连接检查容器指定的端口，如果端口开放，则认为容器健康</td>\n</tr>\n<tr>\n<td>HTTPGetAction</td>\n<td>对指定的 URL 进行 Get 请求，如果状态码在 200~400 之间，则认为容器健康</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"6-健康检查配置\"><a class=\"anchor\" href=\"#6-健康检查配置\">#</a> 6. 健康检查配置</h4>\n<p>配置健康检查：</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          startupProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          livenessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          readinessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            httpGet:\n              path: /index.html\n              port: 80\n              scheme: HTTP\n      restartPolicy: Always\n</code></pre>\n<h4 id=\"7-prestop和-poststart配置\"><a class=\"anchor\" href=\"#7-prestop和-poststart配置\">#</a> 7. PreStop 和 PostStart 配置</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          startupProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          livenessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          readinessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            httpGet:\n              path: /index.html\n              port: 80\n              scheme: HTTP\n          lifecycle:\n            postStart:\n              exec:\n                command:\n                  - sh\n                  - '-c'\n                  - mkdir /data\n            preStop:\n              exec:\n                command:\n                  - sh\n                  - '-c'\n                  - sleep 30\n      restartPolicy: Always\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3071070979.html",
            "url": "http://ixuyong.cn/posts/3071070979.html",
            "title": "一键永久激活Window、office教程",
            "date_published": "2025-04-10T13:32:09.000Z",
            "content_html": "<h3 id=\"一键永久激活window-office教程\"><a class=\"anchor\" href=\"#一键永久激活window-office教程\">#</a> 一键永久激活 Window、office 教程</h3>\n<p>1、按下 Win 键 + R，调出运行对话框，输入 powershell 并回车，启动命令提示符窗口。接着输入以下指令执行激活：</p>\n<pre><code>irm https://get.activated.win | iex\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/ilMT403.png\" alt=\"1.png\" /></p>\n<p>该脚本包含四个功能：首个命令用于 Windows 系统永久激活，第二个用于 Office 永久激活，第三个将系统有效期延长至 2038 年，第四个则实现每 180 天自动循环激活。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/taJbKQr.png\" alt=\"2.png\" /></p>\n<p>2. 我们再次使用 Windows 徽标 + R 快捷键打开运行框，输入 slmgr.vbs/xpr 就可以看到系统已经永久激活了。</p>\n<pre><code>slmgr.vbs /xpr\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/JMWlUpc.png\" alt=\"3.png\" /></p>\n<p>以上，既然看到这里了，如果觉得不错，随手点个赞、打赏一下吧，⭐～谢谢你看我的文章，我们下次再见。</p>\n",
            "tags": [
                "Windows"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/985149017.html",
            "url": "http://ixuyong.cn/posts/985149017.html",
            "title": "二进制高可用安装K8S集群",
            "date_published": "2025-04-10T12:58:40.000Z",
            "content_html": "<h2 id=\"二进制高可用安装k8s集群\"><a class=\"anchor\" href=\"#二进制高可用安装k8s集群\">#</a> 二进制高可用安装 K8s 集群</h2>\n<h4 id=\"1-基本配置\"><a class=\"anchor\" href=\"#1-基本配置\">#</a> 1. 基本配置</h4>\n<h5 id=\"11-基本环境配置\"><a class=\"anchor\" href=\"#11-基本环境配置\">#</a> 1.1 基本环境配置</h5>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP 地址</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master 节点 * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived 虚拟 IP（不占用机器）</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker 节点 * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* 配置信息 *</strong></em></th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>系统版本</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod 网段</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service 网段</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>所有节点</mark>更改主机名（其它节点按需修改）：</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>所有节点</mark>配置 hosts，修改 /etc/hosts 如下：</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>所有节点</mark>配置 yum 源：</p>\n<pre><code># 配置基础源\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>所有节点</mark>必备工具安装：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>所有节点</mark>关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>所有节点</mark>关闭 swap 分区：</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>所有节点</mark>安装 ntpdate：</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>所有节点</mark>同步时间并配置上海时区：</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# 加入到crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>所有节点</mark>配置 limit：</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>所有节点</mark>升级系统：</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 节点</mark>免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上</em></p>\n<p><mark>Master01 节点</mark>下载安装所有的源码文件：</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-内核配置\"><a class=\"anchor\" href=\"#12-内核配置\">#</a> 1.2 内核配置</h5>\n<p><mark>所有节点</mark>安装 ipvsadm：</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>所有节点</mark>配置 ipvs 模块：</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>所有节点</mark>创建 ipvs.conf，并配置开机自动加载：</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# 加入以下内容\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>所有节点</mark>然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>所有节点</mark>内核优化配置：</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>所有节点</mark>应用配置：</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>配置完内核后，重启机器，之后查看内核模块是否已自动加载：</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-高可用组件安装\"><a class=\"anchor\" href=\"#2-高可用组件安装\">#</a> 2. 高可用组件安装</h4>\n<p><em>注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装</em></p>\n<p><em>注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。</em></p>\n<p><mark>所有 Master 节点</mark>通过 yum 安装 HAProxy 和 KeepAlived：</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 HAProxy，需要注意黄色部分的 IP：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:8443       #HAProxy监听端口\n  bind 127.0.0.1:8443     #HAProxy监听端口\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IP地址\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IP地址\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IP地址\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 KeepAlived，需要注意黄色部分的配置。</p>\n<p><mark>Master01 节点</mark>的配置：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #网卡名称\n    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #网卡名称\n    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #网卡名称\n    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>所有 master 节点</mark>配置 KeepAlived 健康检查文件：</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>所有 master 节点</mark>配置健康检查文件添加执行权限：</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>所有 master 节点</mark>启动 haproxy 和 keepalived：</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的</p>\n<pre><code>所有节点测试VIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等</p>\n<ul>\n<li>所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld</li>\n<li>所有节点查看 selinux 状态，必须为 disable：getenforce</li>\n<li>master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy</li>\n<li>master 节点查看监听端口：netstat -lntp</li>\n</ul>\n<p>如果以上都没有问题，需要确认：</p>\n<ol>\n<li>\n<p>是否是公有云机器</p>\n</li>\n<li>\n<p>是否是私有云机器（类似 OpenStack）</p>\n</li>\n</ol>\n<p>上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询</p>\n<h4 id=\"3-runtime安装\"><a class=\"anchor\" href=\"#3-runtime安装\">#</a> 3. Runtime 安装</h4>\n<p>如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。</p>\n<h5 id=\"31-安装containerd\"><a class=\"anchor\" href=\"#31-安装containerd\">#</a> 3.1 安装 Containerd</h5>\n<p><mark>所有节点</mark>配置安装源：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>所有节点</mark>安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p>首先配置 Containerd 所需的模块（<mark>所有节点</mark>）：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载模块：</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>所有节点</mark>，配置 Containerd 所需的内核：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载内核：</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>生成 Containerd 的配置文件：</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>更改 Containerd 的 Cgroup 和 Pause 镜像配置：</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>启动 Containerd，并配置开机自启动：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>所有节点</mark>配置 crictl 客户端连接的运行时位置（可选）：</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-k8s及etcd安装\"><a class=\"anchor\" href=\"#4-k8s及etcd安装\">#</a> 4 . K8S 及 etcd 安装</h4>\n<p><mark>Master01</mark> 下载 kubernetes 安装包（1.32.3 需要更改为你看到的最新版本）：</p>\n<pre><code>[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz\n</code></pre>\n<p>最新版获取地址：<a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md\">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/</a></p>\n<p><mark>以下操作都在 master01 执行</mark></p>\n<p>下载 etcd 安装包：<a href=\"https://github.com/etcd-io/etcd/releases/\">https://github.com/etcd-io/etcd/releases/</a></p>\n<pre><code>[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz\n</code></pre>\n<p>解压 kubernetes 安装文件：</p>\n<pre><code>[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125;\n</code></pre>\n<p>解压 etcd 安装文件：</p>\n<pre><code>[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&#123;,ctl&#125;\n</code></pre>\n<p>版本查看：</p>\n<pre><code>[root@k8s-master01 ~]# kubelet --version\nKubernetes v1.32.3\n[root@k8s-master01 ~]# etcdctl version\netcdctl version: 3.5.16\nAPI version: 3.5\n</code></pre>\n<p>将组件发送到其他节点</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\nWorkNodes='k8s-node01 k8s-node02'\nfor NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done\nfor NODE in $WorkNodes; do     scp /usr/local/bin/kube&#123;let,-proxy&#125; $NODE:/usr/local/bin/ ; done\n</code></pre>\n<p><mark>Master01 节点</mark>切换到 1.32.x 分支（其他版本可以切换到其他分支，.x 即可，不需要更改为具体的小版本）：</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x\n</code></pre>\n<h4 id=\"5-生成证书\"><a class=\"anchor\" href=\"#5-生成证书\">#</a> 5 . 生成证书</h4>\n<p><em><mark>二进制安装最关键步骤，一步错误全盘皆输，一定要注意每个步骤都要是正确的</mark></em></p>\n<p><mark>Master01</mark> 下载生成证书工具（下载不成功可以去百度网盘）</p>\n<pre><code>wget &quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl\nwget &quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson\nchmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson\n</code></pre>\n<h5 id=\"51-etcd证书\"><a class=\"anchor\" href=\"#51-etcd证书\">#</a> 5.1 Etcd 证书</h5>\n<p><mark>所有 Master 节点</mark>创建 etcd 证书目录：</p>\n<pre><code>mkdir /etc/etcd/ssl -p\n</code></pre>\n<p><mark>所有节点</mark>创建 kubernetes 相关目录：</p>\n<pre><code>mkdir -p /etc/kubernetes/pki\n</code></pre>\n<p><mark>Master01 节点</mark>生成 etcd 证书</p>\n<p>生成证书的 CSR（证书签名请求文件，配置了一些域名、公司、单位）文件：</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\n# 生成etcd CA证书和CA证书的key\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca\n\n\ncfssl gencert \\\n   -ca=/etc/etcd/ssl/etcd-ca.pem \\\n   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\\n   -config=ca-config.json \\\n   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \\\n   -profile=kubernetes \\\n   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd\n\n执行结果\n[INFO] generate received request\n \t[INFO] received CSR\n     [INFO] generating key: rsa-2048\n     [INFO] encoded CSR\n     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033\n</code></pre>\n<p>将证书复制到其他 master 节点</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\n\nfor NODE in $MasterNodes; do\n     ssh $NODE &quot;mkdir -p /etc/etcd/ssl&quot;\n     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do\n       scp /etc/etcd/ssl/$&#123;FILE&#125; $NODE:/etc/etcd/ssl/$&#123;FILE&#125;\n     done\n done\n</code></pre>\n<h5 id=\"52-k8s组件证书\"><a class=\"anchor\" href=\"#52-k8s组件证书\">#</a> 5.2 K8s 组件证书</h5>\n<p><mark>Master01</mark> 生成 kubernetes CA 证书：</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca\n</code></pre>\n<h6 id=\"521-apiserver证书\"><a class=\"anchor\" href=\"#521-apiserver证书\">#</a> 5.2.1 APIServer 证书</h6>\n<p>注意：10.96.0. 是 k8s service 的网段，如果说需要更改 k8s service 网段，那就需要更改 10.96.0.1</p>\n<pre><code>cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver\n</code></pre>\n<p>生成 apiserver 的聚合证书：：</p>\n<pre><code>cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca \n\ncfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client\n</code></pre>\n<p>返回结果（忽略警告）：</p>\n<pre><code>2020/12/11 18:15:28 [INFO] generate received request\n2020/12/11 18:15:28 [INFO] received CSR\n2020/12/11 18:15:28 [INFO] generating key: rsa-2048\n\n2020/12/11 18:15:28 [INFO] encoded CSR\n2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845\n2020/12/11 18:15:28 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for\nwebsites. For more information see the Baseline Requirements for the Issuance and Management\nof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);\nspecifically, section 10.2.3 (&quot;Information Requirements&quot;).\n</code></pre>\n<h6 id=\"522-controllermanager\"><a class=\"anchor\" href=\"#522-controllermanager\">#</a> 5.2.2 ControllerManager</h6>\n<p>生成 controller-manage 的证书：</p>\n<pre><code class=\"language-\\\">cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager\n\n注意：修改黄色部分的IP地址\n# set-cluster：设置一个集群项，\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# 设置一个环境项，一个上下文\nkubectl config set-context system:kube-controller-manager@kubernetes \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# set-credentials 设置一个用户项\n\nkubectl config set-credentials system:kube-controller-manager \\\n     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\\n     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n\n# 使用某个环境当做默认环境\n\nkubectl config use-context system:kube-controller-manager@kubernetes \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n</code></pre>\n<h6 id=\"523-scheduler证书\"><a class=\"anchor\" href=\"#523-scheduler证书\">#</a> 5.2.3 Scheduler 证书</h6>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler\n\n注意：修改黄色部分的IP地址\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\n\nkubectl config set-credentials system:kube-scheduler \\\n     --client-certificate=/etc/kubernetes/pki/scheduler.pem \\\n     --client-key=/etc/kubernetes/pki/scheduler-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config set-context system:kube-scheduler@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-scheduler \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config use-context system:kube-scheduler@kubernetes \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n</code></pre>\n<h6 id=\"524-生成管理员证书\"><a class=\"anchor\" href=\"#524-生成管理员证书\">#</a> 5.2.4 生成管理员证书</h6>\n<p>Kubectl /etc/Kubernetes/admin.conf ~/.kube/config</p>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin\n\n注意：修改黄色部分的IP\n\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig\nkubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n</code></pre>\n<h6 id=\"525-创建serviceaccount证书\"><a class=\"anchor\" href=\"#525-创建serviceaccount证书\">#</a> 5.2.5 创建 ServiceAccount 证书</h6>\n<p>创建一对公钥，用来签发 ServiceAccount 的 Token：</p>\n<pre><code>openssl genrsa -out /etc/kubernetes/pki/sa.key 2048\n</code></pre>\n<p>返回结果：</p>\n<pre><code>Generating RSA private key, 2048 bit long modulus (2 primes)\n...................................................................................+++++\n...............+++++\ne is 65537 (0x010001)\n</code></pre>\n<pre><code> openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n</code></pre>\n<p>发送证书至其他节点：</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do \n  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do \n    scp /etc/kubernetes/pki/$&#123;FILE&#125; $NODE:/etc/kubernetes/pki/$&#123;FILE&#125;;\n  done; \n  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do \n    scp /etc/kubernetes/$&#123;FILE&#125; $NODE:/etc/kubernetes/$&#123;FILE&#125;;\n  done;\ndone\n</code></pre>\n<p>查看证书文件：</p>\n<pre><code>[root@k8s-master01 pki]# ls /etc/kubernetes/pki/\nadmin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem\nadmin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem\nadmin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr\n[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l\n23\n</code></pre>\n<h4 id=\"6-kubernetes组件配置\"><a class=\"anchor\" href=\"#6-kubernetes组件配置\">#</a> 6. Kubernetes 组件配置</h4>\n<h5 id=\"61-ecd配置\"><a class=\"anchor\" href=\"#61-ecd配置\">#</a> 6.1 Ecd 配置</h5>\n<p>Etcd 配置大致相同，注意修改每个 Master 节点的 etcd 配置的主机名和 IP 地址</p>\n<h6 id=\"611-master01\"><a class=\"anchor\" href=\"#611-master01\">#</a> 6.1.1 Master01</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master01'     # k8s-master01名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.71:2380'            # k8s-master01 IP\nlisten-client-urls: 'https://192.168.1.71:2379,http://127.0.0.1:2379'   # k8s-master01 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.71:2380'  # k8s-master01 IP\nadvertise-client-urls: 'https://192.168.1.71:2379'        # k8s-master01 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'     # k8s-master01、k8s-master02、k8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"612-master02\"><a class=\"anchor\" href=\"#612-master02\">#</a> 6.1.2 Master02</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\t\nname: 'k8s-master02'   # k8s-master02名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.72:2380'      # k8s-master02 IP\nlisten-client-urls: 'https://192.168.1.72:2379,http://127.0.0.1:2379'    # k8s-master02 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.72:2380'    # k8s-master02 IP\nadvertise-client-urls: 'https://192.168.1.72:2379'     # k8s-master02 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'             # k8s-master01、k8s-master02、k8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"613-master03\"><a class=\"anchor\" href=\"#613-master03\">#</a> 6.1.3 Master03</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master03'           # k8s-master03名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.73:2380'           # k8s-master03 IP\nlisten-client-urls: 'https://192.168.1.73:2379,http://127.0.0.1:2379'       # k8s-master03 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.73:2380'      # k8s-master03 IP\nadvertise-client-urls: 'https://192.168.1.73:2379'            # k8s-master03 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'                # k8s-master01、k8s-master02、k8s-master03 IP\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"614-启动etcd\"><a class=\"anchor\" href=\"#614-启动etcd\">#</a> 6.1.4 启动 Etcd</h6>\n<p><mark>所有 Master 节点</mark>创建 etcd service 并启动</p>\n<pre><code># vim /usr/lib/systemd/system/etcd.service\n[Unit]\nDescription=Etcd Service\nDocumentation=https://coreos.com/etcd/docs/latest/\nAfter=network.target\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml\nRestart=on-failure\nRestartSec=10\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nAlias=etcd3.service\n</code></pre>\n<p><mark>所有 Master 节点</mark>创建 etcd 的证书目录：</p>\n<pre><code>mkdir /etc/kubernetes/pki/etcd\nln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/\nsystemctl daemon-reload\nsystemctl enable --now etcd\n</code></pre>\n<p>查看 etcd 状态：</p>\n<pre><code>export ETCDCTL_API=3\netcdctl --endpoints=&quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n</code></pre>\n<h5 id=\"62-apiserver配置\"><a class=\"anchor\" href=\"#62-apiserver配置\">#</a> 6.2 APIServer 配置</h5>\n<h6 id=\"621-master01\"><a class=\"anchor\" href=\"#621-master01\">#</a> 6.2.1 Master01</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.71 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"622-master02\"><a class=\"anchor\" href=\"#622-master02\">#</a> 6.2.2 Master02</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.72 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"623-master03\"><a class=\"anchor\" href=\"#623-master03\">#</a> 6.2.3 Master03</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.73 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"624-启动apiserver\"><a class=\"anchor\" href=\"#624-启动apiserver\">#</a> 6.2.4 启动 apiserver</h6>\n<p><mark>所有 Master 节点</mark>开启 kube-apiserver：</p>\n<pre><code>systemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserver\n</code></pre>\n<p>检测 kube-server 状态：</p>\n<pre><code># systemctl status kube-apiserver\n\n● kube-apiserver.service – Kubernetes API Server\n   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)\n   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s ago \n</code></pre>\n<p>如果系统日志有这些提示可以忽略:</p>\n<pre><code>Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to “pick_first”\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;CONNECTING &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;READY &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = “transport is closing”\n</code></pre>\n<h5 id=\"63-controllermanage\"><a class=\"anchor\" href=\"#63-controllermanage\">#</a> 6.3 ControllerManage</h5>\n<p><mark>所有 Master 节点</mark>配置 kube-controller-manager service（所有 master 节点配置一样）</p>\n<p>注意：本文档使用的 k8s Pod 网段为 172.16.0.0/16，该网段不能和宿主机的网段、k8s Service 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\n      --v=2 \\\n      --root-ca-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\\n      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\\n      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --leader-elect=true \\\n      --use-service-account-credentials=true \\\n      --node-monitor-grace-period=40s \\\n      --node-monitor-period=5s \\\n      --controllers=*,bootstrapsigner,tokencleaner \\\n      --allocate-node-cidrs=true \\\n      --cluster-cidr=172.16.0.0/16 \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\\n      --node-cidr-mask-size=24\n      \nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>所有 Master 节点</mark>启动 kube-controller-manager</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service → /usr/lib/systemd/system/kube-controller-manager.service.\n</code></pre>\n<p>查看启动状态</p>\n<pre><code>[root@k8s-master01 pki]# systemctl  status kube-controller-manager\n● kube-controller-manager.service – Kubernetes Controller Manager\n   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)\n Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago\n     Docs: https://github.com/  ubernetes/  ubernetes\n Main PID: 7518 (kube-controller)\n</code></pre>\n<h5 id=\"64-scheduler\"><a class=\"anchor\" href=\"#64-scheduler\">#</a> 6.4 Scheduler</h5>\n<p>所有 Master 节点配置 kube-scheduler service（所有 master 节点配置一样）</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service \n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\n      --v=2 \\\n      --leader-elect=true \\\n      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>启动 scheduler：</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-scheduler\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service → /usr/lib/systemd/system/kube-scheduler.service.\n[root@k8s-master01 pki]# systemctl status kube-scheduler\n● kube-scheduler.service - Kubernetes Scheduler\n   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)\n   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago\n     Docs: https://github.com/kubernetes/kubernetes\n Main PID: 5815 (kube-scheduler)\n    Tasks: 9\n   Memory: 19.8M\n</code></pre>\n<h4 id=\"7-tls-bootstrapping配置\"><a class=\"anchor\" href=\"#7-tls-bootstrapping配置\">#</a> 7. TLS Bootstrapping 配置</h4>\n<p>只需要在<mark> Master01</mark> 创建 bootstrap</p>\n<p>注意： 修改黄色部分的 IP 地址</p>\n<pre><code>cd /root/k8s-ha-install/bootstrap\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n\n[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config\n</code></pre>\n<p>可以正常查询集群状态，才可以继续往下，否则不行，需要排查 k8s 组件是否有故障（只要有结果即可，如果返回不一样不影响）</p>\n<pre><code># kubectl get cs\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS    MESSAGE   ERROR\ncontroller-manager   Healthy   ok        \nscheduler            Healthy   ok        \netcd-0               Healthy   ok\n</code></pre>\n<p>创建 bootstrap 相关资源：</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml \nsecret/bootstrap-token-c8ad9c created\nclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created\nclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created\nclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created\n</code></pre>\n<h4 id=\"8-node节点配置\"><a class=\"anchor\" href=\"#8-node节点配置\">#</a> 8. Node 节点配置</h4>\n<h5 id=\"81-复制证书\"><a class=\"anchor\" href=\"#81-复制证书\">#</a> 8.1 复制证书</h5>\n<p><mark>Master01 节点</mark>复制证书至其他节点：</p>\n<pre><code>cd /etc/kubernetes/\n\nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do\n     ssh $NODE mkdir -p /etc/kubernetes/pki\n     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do\n       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&#123;FILE&#125;\n done\n done\n</code></pre>\n<p>执行结果：</p>\n<pre><code>ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    \n…\nbootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00\n</code></pre>\n<h5 id=\"82-kubelet配置\"><a class=\"anchor\" href=\"#82-kubelet配置\">#</a> 8.2 Kubelet 配置</h5>\n<p><mark>所有节点</mark>创建 Kubelet 配置目录</p>\n<pre><code>mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/\n</code></pre>\n<p><mark>所有节点</mark>配置 kubelet service</p>\n<pre><code>[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kubelet\n\nRestart=always\nStartLimitInterval=0\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>所有节点</mark>配置 kubelet service 的配置文件（也可以写到 kubelet.service）：</p>\n<pre><code># Runtime为Containerd\n# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf\n\n[Service]\nEnvironment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&quot;\nEnvironment=&quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot;\nEnvironment=&quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&quot;\nEnvironment=&quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' &quot;\nExecStart=\nExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS\n</code></pre>\n<p><mark>所有节点</mark>创建 kubelet 的配置文件</p>\n<p><em>注意：如果更改了 k8s 的 service 网段，需要更改 kubelet-conf.yml 的 clusterDNS: 配置，改成 k8s Service 网段的第十个地址，比如 10.96.0.10</em></p>\n<pre><code>[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml\n\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\ncgroupDriver: systemd\ncgroupsPerQOS: true\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerLogMaxFiles: 5\ncontainerLogMaxSize: 10Mi\ncontentType: application/vnd.kubernetes.protobuf\ncpuCFSQuota: true\ncpuManagerPolicy: none\ncpuManagerReconcilePeriod: 10s\nenableControllerAttachDetach: true\nenableDebuggingHandlers: true\nenforceNodeAllocatable:\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhairpinMode: promiscuous-bridge\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\niptablesDropBit: 15\niptablesMasqueradeBit: 14\nkubeAPIBurst: 10\nkubeAPIQPS: 5\nmakeIPTablesUtilChains: true\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nregistryBurst: 10\nregistryPullQPS: 5\nresolvConf: /etc/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 2m0s\nserializeImagePulls: true\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h0m0s\nsyncFrequency: 1m0s\nvolumeStatsAggPeriod: 1m0s\n</code></pre>\n<p>启动<mark>所有节点</mark> kubelet</p>\n<pre><code>systemctl daemon-reload\nsystemctl enable --now kubelet\n</code></pre>\n<p>此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<p><em>如果有很多报错日志，或者有大量看不懂的报错，说明 kubelet 的配置有误，需要检查 kubelet 配置</em></p>\n<p>Master01 查看集群状态 (Ready 或 NotReady 都正常)</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl get node\n</code></pre>\n<h5 id=\"83-kube-proxy配置\"><a class=\"anchor\" href=\"#83-kube-proxy配置\">#</a> 8.3 kube-proxy 配置</h5>\n<p><em>注意，如果不是高可用集群，192.168.1.70:8443 改为 master01 的地址，8443 改为 apiserver 的端口，默认是 6443</em></p>\n<p>生成 kube-proxy 的证书，以下操作只在<mark> Master01</mark> 执行</p>\n<pre><code>cd /root/k8s-ha-install/pki\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config set-credentials system:kube-proxy \\\n     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \\\n     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\nkubectl config set-context system:kube-proxy@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-proxy \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config use-context system:kube-proxy@kubernetes \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n</code></pre>\n<p>将 kubeconfig 发送至其他节点</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n\nfor NODE in k8s-node01 k8s-node02; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n</code></pre>\n<p><mark>所有节点</mark>添加 kube-proxy 的配置和 service 文件：</p>\n<pre><code>vim /usr/lib/systemd/system/kube-proxy.service\n\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\n  --config=/etc/kubernetes/kube-proxy.yaml \\\n  --v=2\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>如果更改了集群 Pod 的网段，需要更改 kube-proxy.yaml 的 clusterCIDR 为自己的 Pod 网段：</p>\n<pre><code>vim /etc/kubernetes/kube-proxy.yaml\n\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nclientConnection:\n  acceptContentTypes: &quot;&quot;\n  burst: 10\n  contentType: application/vnd.kubernetes.protobuf\n  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n  qps: 5\nclusterCIDR: 172.16.0.0/16 \nconfigSyncPeriod: 15m0s\nconntrack:\n  max: null\n  maxPerCore: 32768\n  min: 131072\n  tcpCloseWaitTimeout: 1h0m0s\n  tcpEstablishedTimeout: 24h0m0s\nenableProfiling: false\nhealthzBindAddress: 0.0.0.0:10256\nhostnameOverride: &quot;&quot;\niptables:\n  masqueradeAll: false\n  masqueradeBit: 14\n  minSyncPeriod: 0s\n  syncPeriod: 30s\nipvs:\n  masqueradeAll: true\n  minSyncPeriod: 5s\n  scheduler: &quot;rr&quot;\n  syncPeriod: 30s\nkind: KubeProxyConfiguration\nmetricsBindAddress: 127.0.0.1:10249\nmode: &quot;ipvs&quot;\nnodePortAddresses: null\noomScoreAdj: -999\nportRange: &quot;&quot;\nudpIdleTimeout: 250ms\n</code></pre>\n<p><mark>所有节点</mark>启动 kube-proxy</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload\n[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /usr/lib/systemd/system/kube-proxy.service.\n</code></pre>\n<p>此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<h4 id=\"9-calico组件的安装\"><a class=\"anchor\" href=\"#9-calico组件的安装\">#</a> 9. Calico 组件的安装</h4>\n<p>以下步骤只在 master01 执行：</p>\n<pre><code>cd /root/k8s-ha-install/calico/\n</code></pre>\n<p>更改 calico 的网段，主要需要将红色部分的网段，改为自己的 Pod 网段</p>\n<pre><code>sed -i &quot;s#POD_CIDR#172.16.0.0/16#g&quot; calico.yaml\n</code></pre>\n<p><em>检查网段是自己的 Pod 网段， grep &quot;IPV4POOL_CIDR&quot; calico.yaml  -A 1</em></p>\n<p>查看容器和节点状态：</p>\n<pre><code>[root@k8s-master01 calico]# kubectl get po -n kube-system\nNAME                                       READY   STATUS    RESTARTS      AGE\ncalico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s\ncalico-node-8fxqp                          1/1     Running   0             85s\ncalico-node-8nkfl                          1/1     Running   0             86s\ncalico-node-pmpf4                          1/1     Running   0             86s\ncalico-node-vnlk7                          1/1     Running   0             86s\ncalico-node-xpchb                          1/1     Running   0             85s\ncalico-typha-67c6dc57d6-259t8              1/1     Running   0             86s\n</code></pre>\n<p><em>如果容器状态异常可以使用 kubectl describe 或者 kubectl logs 查看容器的日志</em></p>\n<ol>\n<li>Kubectl logs -f POD_NAME -n kube-system</li>\n<li>Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system</li>\n</ol>\n<h4 id=\"10-安装coredns\"><a class=\"anchor\" href=\"#10-安装coredns\">#</a> 10. 安装 CoreDNS</h4>\n<pre><code>cd /root/k8s-ha-install/\n</code></pre>\n<p>如果更改了 k8s service 的网段需要将 coredns 的 serviceIP 改成 k8s service 网段的第十个 IP</p>\n<pre><code>COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk '&#123;print $3&#125;'`0\nsed -i &quot;s#KUBEDNS_SERVICE_IP#$&#123;COREDNS_SERVICE_IP&#125;#g&quot; CoreDNS/coredns.yaml\n</code></pre>\n<p>安装 coredns</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml \nserviceaccount/coredns created\nclusterrole.rbac.authorization.k8s.io/system:coredns created\nclusterrolebinding.rbac.authorization.k8s.io/system:coredns created\nconfigmap/coredns created\ndeployment.apps/coredns created\nservice/kube-dns created\n</code></pre>\n<h4 id=\"11-metrics部署\"><a class=\"anchor\" href=\"#11-metrics部署\">#</a> 11. Metrics 部署</h4>\n<p>在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。</p>\n<p>以下操作均在<mark> master01 节点</mark>执行，安装 metrics server:</p>\n<pre><code>cd /root/k8s-ha-install/metrics-server\nkubectl  create -f . \n\nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>等待 metrics server 启动然后查看状态：</p>\n<pre><code># kubectl  top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-master01   231m         5%     1620Mi          42%       \nk8s-master02   274m         6%     1203Mi          31%       \nk8s-master03   202m         5%     1251Mi          32%       \nk8s-node01     69m          1%     667Mi           17%       \nk8s-node02     73m          1%     650Mi           16%\n</code></pre>\n<p>如果有如下报错，可以等待 10 分钟后，再次查看：</p>\n<pre><code>Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)\n</code></pre>\n<h4 id=\"12-dashboard部署\"><a class=\"anchor\" href=\"#12-dashboard部署\">#</a> 12. Dashboard 部署</h4>\n<h5 id=\"121-安装dashboard\"><a class=\"anchor\" href=\"#121-安装dashboard\">#</a> 12.1 安装 Dashboard</h5>\n<p>Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"122-登录dashboard\"><a class=\"anchor\" href=\"#122-登录dashboard\">#</a> 12.2 登录 dashboard</h5>\n<p>在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>更改 dashboard 的 svc 为 NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）</em></p>\n<p>查看端口号：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：</p>\n<p>访问 Dashboard：<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>创建登录 Token：</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"14-containerd配置镜像加速\"><a class=\"anchor\" href=\"#14-containerd配置镜像加速\">#</a> 14. Containerd 配置镜像加速</h4>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>所有节点重新启动 Containerd：</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"15-docker配置镜像加速\"><a class=\"anchor\" href=\"#15-docker配置镜像加速\">#</a> 15. Docker 配置镜像加速</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>所有节点重新启动 Docker：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/2628187572.html",
            "url": "http://ixuyong.cn/posts/2628187572.html",
            "title": "MySQL运维DBA应用与实践",
            "date_published": "2025-04-09T14:02:40.000Z",
            "content_html": "<h3 id=\"mysql运维dba应用与实践\"><a class=\"anchor\" href=\"#mysql运维dba应用与实践\">#</a> MySQL 运维 DBA 应用与实践</h3>\n<h4 id=\"1日志\"><a class=\"anchor\" href=\"#1日志\">#</a> 1. 日志</h4>\n<p>在任何一种数据库中，都会有各种各样的日志，这些日志记录了数据库运行的各个方面。可以帮助数据库管理员追踪数据库曾经发生的一些事情。</p>\n<p>对于 MySQL 数据库，提供了四种不同的日志帮助我们追踪。</p>\n<ul>\n<li>\n<p>错误日志</p>\n</li>\n<li>\n<p>二进制日志</p>\n</li>\n<li>\n<p>查询日志</p>\n</li>\n<li>\n<p>慢查询日志</p>\n</li>\n</ul>\n<h5 id=\"11-错误日志\"><a class=\"anchor\" href=\"#11-错误日志\">#</a> 1.1 错误日志</h5>\n<p>错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld (MySQL 服务) 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，建议首先查看此日志。</p>\n<p>该日志是默认开启的，默认存放目录 /var/log/，默认的日志文件名为 mysqld.log。查看日志位置；</p>\n<pre><code>mysql&gt; show variables like '%log_error%';\n+---------------------+---------------------+\n| Variable_name       | Value               |\n+---------------------+---------------------+\n| binlog_error_action | ABORT_SERVER        |\n| log_error           | /var/log/mysqld.log |\n| log_error_verbosity | 3                   |\n+---------------------+---------------------+\n</code></pre>\n<h5 id=\"12-二进制日志\"><a class=\"anchor\" href=\"#12-二进制日志\">#</a> 1.2 二进制日志</h5>\n<p>二进制日志 (BINLOG) 记录了所有的 DDL (数据定义语言) 语句和 DML (数据操纵语言) 语句，但不包括数据查询（SELECT、 SHOW）语句。</p>\n<p>作用:</p>\n<p>①. 灾难时的数据恢复；</p>\n<p>②. MySQL 的主从复制。</p>\n<p>在 MySQL5.7 版本中，默认二进制日志是关闭着的，涉及到的参数如下:</p>\n<h6 id=\"121-开启-bin-log记录\"><a class=\"anchor\" href=\"#121-开启-bin-log记录\">#</a> 1.2.1 开启 bin-log 记录</h6>\n<pre><code>1.1改修配置文件\n[root@db01 ~]# vim /etc/my.cnf\nserver-id=1\nlog-bin=mysql-bin\nmax_binlog_size=500M\nexpire_logs_days=15\n\n1.2查看是否开启binlog.\nmysql&gt; show variables like 'log_%';\n+----------------------------------------+--------------------------------+\n| Variable_name                          | Value                          |\n+----------------------------------------+--------------------------------+\n| log_bin                                | ON                             |\n| log_bin_basename                       | /var/lib/mysql/mysql-bin       |\n| log_bin_index                          | /var/lib/mysql/mysql-bin.index |\n| log_bin_trust_function_creators        | OFF                            |\n| log_bin_use_v1_row_events              | OFF                            |\n| log_builtin_as_identified_by_password  | OFF                            |\n| log_error                              | /var/log/mysqld.log            |\n| log_error_verbosity                    | 3                              |\n| log_output                             | FILE                           |\n| log_queries_not_using_indexes          | OFF                            |\n| log_slave_updates                      | OFF                            |\n| log_slow_admin_statements              | OFF                            |\n| log_slow_slave_statements              | OFF                            |\n| log_statements_unsafe_for_binlog       | ON                             |\n| log_syslog                             | OFF                            |\n| log_syslog_facility                    | daemon                         |\n| log_syslog_include_pid                 | ON                             |\n| log_syslog_tag                         |                                |\n| log_throttle_queries_not_using_indexes | 0                              |\n| log_timestamps                         | UTC                            |\n| log_warnings                           | 2                              |\n+----------------------------------------+--------------------------------+\n\n1.3查看binlog\nmysql&gt; show binary logs;\n+------------------+-----------+\n| Log_name         | File_size |\n+------------------+-----------+\n| mysql-bin.000001 |     36825 |\n| mysql-bin.000002 |    200464 |\n| mysql-bin.000003 |    419809 |\n+------------------+-----------+\n\n1.4查看binlog日志保存天数 \n# 0表示永久保留，expire_logs_days：保留指定日期范围内的binlog历史日志，上示例设置的15天内\nmysql&gt; show variables like 'expire_logs_days';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| expire_logs_days | 15    |\n+------------------+-------+\n1 row in set (0.00 sec)\n\n1.5查看binlog日志保存大小\n#max_binlog_size：bin log日志每达到设定大小后，会使用新的bin log日志。如mysql-bin.000002达到500M后，创建并使用mysql-bin.000003文件作为日志记录。\nmysql&gt; show variables like 'max_binlog_size';\n+-----------------+-----------+\n| Variable_name   | Value     |\n+-----------------+-----------+\n| max_binlog_size | 524288000 |\n+-----------------+-----------+\n\n1.6手动执行flush logs\n#将会new一个新文件用于记录binlog\nmysql&gt; flush logs;\n\n1.7手动清理binlog\n#将mysql-bin.000010之前的日志清理掉\nmysql&gt; purge binary logs to 'mysql-bin.000010';\nQuery OK, 0 rows affected (0.01 sec)\n\n#删除2022-04-21 18:08:00之前的binlog日志\nmysql&gt; purge binary logs before '2022-04-21 18:08:00';\n\n#清除全部binlog\nmysql&gt; reset master;\n</code></pre>\n<h6 id=\"122-日志格式\"><a class=\"anchor\" href=\"#122-日志格式\">#</a> <strong>1.2.2 日志格式</strong></h6>\n<p>MySQL 服务器中提供了多种格式来记录二进制日志，具体格式及特点如下：</p>\n<table>\n<thead>\n<tr>\n<th><strong>日志格式</strong></th>\n<th><strong>含义</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>STATEMENT</td>\n<td>基于 SQL 语句的日志记录，记录的是 SQL 语句，对数据进行修改的 SQL 都会记录在日志文件中。</td>\n</tr>\n<tr>\n<td>ROW</td>\n<td>基于行的日志记录，记录的是每一行的数据变更。(默认)</td>\n</tr>\n<tr>\n<td>MIXED</td>\n<td>混合了 STATEMENT 和 ROW 两种格式，默认采用 STATEMENT, 在某些特殊情况下会自动切换为 ROW 进行记录。</td>\n</tr>\n</tbody>\n</table>\n<pre><code>mysql&gt; show variables like 'binlog_format';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| binlog_format | ROW   |\n+---------------+-------+\n</code></pre>\n<p>由于日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 <code>mysqlbinlog</code>  来查看，具体语法:</p>\n<pre><code>mysqlbinlog [ 参数选项] logfilename\n参数选项:\n\t-d\t\t\t指定数据库名称，只列出指定的数据库相关操作。\n\t-o\t\t\t忽略掉日志中的前n行命令。\n\t-v\t\t\t将行事件(数据变更)重构为SQL语句\n\t-vv\t\t\t将行事件(数据变更)重构为SQL语句，并输出注释信息\n</code></pre>\n<pre><code>mysql&gt; use zh;\nDatabase changed\nmysql&gt; show tables;\n+----------------+\n| Tables_in_zh   |\n+----------------+\n| account        |\n| course         |\n| dept           |\n| emp            |\n| score          |\n| student        |\n| student_course |\n| tb_user        |\n| tb_user_edu    |\n| user           |\n| user1          |\n+----------------+\n11 rows in set (0.00 sec)\n\nmysql&gt;  update tb_user_edu set university = &quot;北京大学&quot;;\nQuery OK, 4 rows affected (0.00 sec)\nRows matched: 4  Changed: 4  Warnings: 0\n\n#二进制日志查看\n[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000001 \n</code></pre>\n<h6 id=\"123-修改binlog格式\"><a class=\"anchor\" href=\"#123-修改binlog格式\">#</a> 1.2.3 修改 binlog 格式</h6>\n<pre><code>[root@db01 ~]# vim /etc/my.cnf\n...\nbinlog_format=STATEMENT\n...\n[root@db01 ~]# systemctl restart mysqld\n\nmysql&gt;  update tb_user_edu set university = '清华大学';\n[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000002 \n...\nSET TIMESTAMP=1701440373/*!*/;\nupdate tb_user_edu set university = '清华大学'\n...\n</code></pre>\n<h5 id=\"13-查询日志\"><a class=\"anchor\" href=\"#13-查询日志\">#</a> 1.3 查询日志</h5>\n<p>查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的 SQL 语句。默认情况下，<strong>查询日志是未开启的</strong>。如果需要开启查询日志，可以设置以下配置︰</p>\n<pre><code>mysql&gt; show variables like '%general%';\n+------------------+-------------------------+\n| Variable_name    | Value                   |\n+------------------+-------------------------+\n| general_log      | OFF                     |\n| general_log_file | /var/lib/mysql/db01.log |\n+------------------+-------------------------+\n2 rows in set (0.00 sec)\n\n#开启查询日志功能\n[root@db01 ~]# cat /etc/my.cnf\ngeneral_log=1\ngeneral_log_file=/var/lib/mysql/mysql_query.log \n[root@db01 ~]# systemctl restart mysqld\n\n[root@db01 ~]# tail -f /var/lib/mysql/mysql_query.log \n2023-12-01T14:31:28.554384Z\t    2 Field List\tstudent \n2023-12-01T14:31:28.554743Z\t    2 Field List\tstudent_course \n2023-12-01T14:31:35.737041Z\t    2 Query\tshow variables like '%general%'\n2023-12-01T14:31:37.345179Z\t    2 Query\tshow variables like '%general%'\n2023-12-01T14:32:17.593471Z\t    2 Query\tSELECT DATABASE()\n2023-12-01T14:32:17.593651Z\t    2 Init DB\tzh\n2023-12-01T14:32:25.249258Z\t    2 Query\tselect * from emp\n</code></pre>\n<h5 id=\"14-慢查询日志\"><a class=\"anchor\" href=\"#14-慢查询日志\">#</a> 1.4 慢查询日志</h5>\n<p>慢查询<a href=\"https://so.csdn.net/so/search?q=%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95&amp;spm=1001.2101.3001.7020\">日志记录</a>了所有执行时间超过参数 <code>long_ query_time</code>  设置值并且扫描记录数不小于 <code>min_examined_row_limit</code>  的所有的 SQL 语句的日志，默认未开启。<strong> <code>long_query_time</code>  默认为 10 秒，最小为 0，精度可以到微秒。</strong></p>\n<pre><code>[root@db01 ~]# vim /etc/my.cnf\n#慢查询日志\nslow_query_log=on\n##执行时间参数\nlong_query_time=2\n# 若没有指定，默认名字为hostname_slow.log\nslow_query_log_file = /var/lib/mysql/slow-query.log\n[root@db01 ~]# systemctl restart mysqld\n\n#制造慢查询并执行\nmysql&gt; select sleep(3);\n[root@db01 ~]# tail -f /var/lib/mysql/slow-query.log \n/usr/sbin/mysqld, Version: 5.7.43-log (MySQL Community Server (GPL)). started with:\nTcp port: 0  Unix socket: /var/lib/mysql/mysql.sock\nTime                 Id Command    Argument\n# Time: 2023-12-01T14:47:57.763735Z\n# User@Host: root[root] @ localhost []  Id:     2\n# Query_time: 3.001229  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0\nuse zh;\nSET timestamp=1701442077;\nselect sleep(3);\n</code></pre>\n<p>默认情况下，不会记录管理语句，也不会记录不使用索引进行查找的查询。可以使用 <code>log_slow_admin_statements</code>  和更改此行为 <code>log_queries_not_using_indexes</code> , 如下所述。</p>\n<pre><code>#记录执行较慢的管理语句\nlog_slow_admin_statements = 1\n#记录执行较慢的未使用索引的语句\nlog_queries_not_using_indexes = 1\n</code></pre>\n<h4 id=\"2-主从复制\"><a class=\"anchor\" href=\"#2-主从复制\">#</a> 2. 主从复制</h4>\n<h5 id=\"21-主从复制的概述\"><a class=\"anchor\" href=\"#21-主从复制的概述\">#</a> 2.1 主从复制的概述</h5>\n<p>主从复制是指将<strong>主数据库的 DDL 和 DML 操作</strong>通过<strong>二进制日志</strong>传到<strong>从库服务器</strong>中，然后在从库上对这些日志重新执行 (也叫重做) ，从而使得从库和主库的数据保持同步。</p>\n<p><a href=\"https://imgse.com/i/pEgO0Mj\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgO0Mj.png\" alt=\"pEgO0Mj.png\" /></a></p>\n<p>MySQL 支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库， 实现链状复制。</p>\n<p>MySQL 复制的有点主要包含以下三个方面：</p>\n<ol>\n<li>主库出现问题，可以快速切换到从库提供服务；</li>\n<li>实现读写分离，降低主库的访问压力；（如果增删改对主库 查询对从库）</li>\n<li>可以在从库中执行备份，以避免备份期间影响主库服务。</li>\n</ol>\n<h5 id=\"22-主从复制的原理\"><a class=\"anchor\" href=\"#22-主从复制的原理\">#</a> 2.2 主从复制的原理</h5>\n<p><a href=\"https://imgse.com/i/pEgOdzQ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgOdzQ.png\" alt=\"pEgOdzQ.png\" /></a></p>\n<p>从上图来看，复制分成三步：</p>\n<ol>\n<li>Master 主库在事务提交时，会把数据变更记录在二进制日志文件 Binlog 中。</li>\n<li>从库 IO 线程读取主库的二进制日志文件 Binlog，写入到从库的中继日志 Relay Log。</li>\n<li>slave 重做中继日志中的事件，SQL 线程将改变反映它自己的数据。</li>\n</ol>\n<h5 id=\"23-主从复制的搭建\"><a class=\"anchor\" href=\"#23-主从复制的搭建\">#</a> 2.3 主从复制的搭建</h5>\n<p><strong>主从复制的搭建步骤</strong>：</p>\n<ol>\n<li>准备主从复制服务器环境</li>\n<li>完成主库配置</li>\n<li>完成从库配置</li>\n</ol>\n<h6 id=\"231-服务器准备\"><a class=\"anchor\" href=\"#231-服务器准备\">#</a> 2.3.1 服务器准备</h6>\n<p><a href=\"https://imgse.com/i/pEgODLn\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgODLn.png\" alt=\"pEgODLn.png\" /></a></p>\n<h6 id=\"232-主库配置\"><a class=\"anchor\" href=\"#232-主库配置\">#</a> 2.3.2 主库配置</h6>\n<p><strong>#1. 安装 MySQL</strong></p>\n<pre><code>#1、关闭防火墙、selinux、环境配置\n[root@db01 ~]# hostnamectl set-hostname db01\n[root@db01 ~]# systemctl stop firewalld\n[root@db01 ~]# systemctl disable firewalld\n[root@db01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@db01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@db01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@db01 ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@db01 ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@db01 ~]# ntpdate time2.aliyun.com\n[root@db01 ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@db01 ~]# mkdir /soft /data /scripts /backup\n\n#2、安装Mysql5.7\n[root@db01 ~]# yum install -y mysql-community-server\n[root@db01 ~]# systemctl start mysqld &amp;&amp; systemctl enable mysqld\n\n[root@db01 ~]# mysql -uroot -p$(awk '/temporary password/&#123;print $NF&#125;' /var/log/mysqld.log)\nmysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'passwd';\nmysql&gt; grant all on *.* to 'root'@'192.168.1.%' identified by 'passwd';\n\n#3、允许root用户在任何地方进行远程登录，并具有所有库任何操作权限，具体操作如下：\nmysql -u root -p&quot;youpass&quot;\nmysql&gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'passwd' WITH GRANT OPTION;\nFLUSH PRIVILEGES;\n\n#4.配置主库\n[root@db01 ~]# vim /etc/my.cnf\nserver-id=1                #mysql服务ID，保证整个集群环境中唯一， 取值范围: 1 - 2^&#123;32&#125;-1\nlog-bin=mysql-bin          #启动二进制日志\nread-only=0                #是否只读,1代表只读, 0代表读写\n#binlog-ignore-db=mysql    #忽略的数据，指不需要同步的数据库\n#binlog-do-db=db01         #指定同步的数据库\n[root@db01 ~]# systemctl restart mysqld\n\n#5.创建repl用户，并设置密码，该用户可在任意主机连接该MySQL服务\nmysql&gt; grant replication slave on *.* to 'repl'@'%' identified by 'passwd';\n\n#6.查看master位置点\nmysql&gt; show master status;        \n+------------------+----------+--------------+------------------+-------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n+------------------+----------+--------------+------------------+-------------------+\n| mysql-bin.000006 |      889 |              |                  |                   |\n+------------------+----------+--------------+------------------+-------------------+\n1 row in set (0.00 sec)\n</code></pre>\n<h6 id=\"233-从库配置\"><a class=\"anchor\" href=\"#233-从库配置\">#</a> 2.3.3 从库配置</h6>\n<table>\n<thead>\n<tr>\n<th>参数名</th>\n<th>含义</th>\n<th><strong>8.0.23 之前</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SOURCE_HOST</td>\n<td>主库 IP 地址</td>\n<td>MASTER_HOST</td>\n</tr>\n<tr>\n<td>SOURCE_USER</td>\n<td>连接主库的用户名</td>\n<td>MASTER_USER</td>\n</tr>\n<tr>\n<td>SOURCE_PASSWORD</td>\n<td>连接主库的密码</td>\n<td>MASTER_PASSWORD</td>\n</tr>\n<tr>\n<td>SOURCE_LOG FILE</td>\n<td>binlog 日志文件名</td>\n<td>MASTER LOG_FILE</td>\n</tr>\n<tr>\n<td>SOURCE_LOG POS</td>\n<td>binlog 日志文件位置</td>\n<td>MASTER_LOG_POS</td>\n</tr>\n</tbody>\n</table>\n<pre><code>#1.配置从库\n[root@db02 ~]# vim /etc/my.cnf\nserver-id=2           #mysql服务ID\nread-only=1           #是否只读,1代表只读, 0代表读写\n[root@db02 ~]# systemctl restart mysqld\n\n#2..配置从服务器，连接主服务器\nmysql&gt; change master to master_host='192.168.40.150',master_user='repl',master_password='passwd',master_log_file='mysql-bin.000006',master_log_pos=889;\n\n#3.开启从库\nmysql&gt; start slave;\nQuery OK, 0 rows affected (0.00 sec)\n\n#4.检查主从复制状态\nmysql&gt; show slave status\\G\n*************************** 1. row ***************************\n               Slave_IO_State: Waiting for master to send event\n                  Master_Host: 192.168.40.150\n                  Master_User: repl\n                  Master_Port: 3306\n                Connect_Retry: 60\n              Master_Log_File: mysql-bin.000006\n          Read_Master_Log_Pos: 889\n               Relay_Log_File: db02-relay-bin.000002\n                Relay_Log_Pos: 320\n        Relay_Master_Log_File: mysql-bin.000006\n             Slave_IO_Running: Yes\n            Slave_SQL_Running: Yes\n              Replicate_Do_DB: \n          Replicate_Ignore_DB: \n           Replicate_Do_Table: \n       Replicate_Ignore_Table: \n      Replicate_Wild_Do_Table: \n  Replicate_Wild_Ignore_Table: \n                   Last_Errno: 0\n                   Last_Error: \n                 Skip_Counter: 0\n          Exec_Master_Log_Pos: 889\n              Relay_Log_Space: 526\n              Until_Condition: None\n               Until_Log_File: \n                Until_Log_Pos: 0\n           Master_SSL_Allowed: No\n           Master_SSL_CA_File: \n           Master_SSL_CA_Path: \n              Master_SSL_Cert: \n            Master_SSL_Cipher: \n               Master_SSL_Key: \n        Seconds_Behind_Master: 0\nMaster_SSL_Verify_Server_Cert: No\n                Last_IO_Errno: 0\n                Last_IO_Error: \n               Last_SQL_Errno: 0\n               Last_SQL_Error: \n  Replicate_Ignore_Server_Ids: \n             Master_Server_Id: 1\n                  Master_UUID: 9b911bea-43e6-11ee-b239-000c29074f5d\n             Master_Info_File: /var/lib/mysql/master.info\n                    SQL_Delay: 0\n          SQL_Remaining_Delay: NULL\n      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates\n           Master_Retry_Count: 86400\n                  Master_Bind: \n      Last_IO_Error_Timestamp: \n     Last_SQL_Error_Timestamp: \n               Master_SSL_Crl: \n           Master_SSL_Crlpath: \n           Retrieved_Gtid_Set: \n            Executed_Gtid_Set: \n                Auto_Position: 0\n         Replicate_Rewrite_DB: \n                 Channel_Name: \n           Master_TLS_Version: \n1 row in set (0.00 sec)\n</code></pre>\n<h4 id=\"3-分库分表\"><a class=\"anchor\" href=\"#3-分库分表\">#</a> 3. <a href=\"https://so.csdn.net/so/search?q=%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8&amp;spm=1001.2101.3001.7020\">分库分表</a></h4>\n<h5 id=\"31-分库分表介绍\"><a class=\"anchor\" href=\"#31-分库分表介绍\">#</a> 3.1 分库分表介绍</h5>\n<h6 id=\"311-现在的问题\"><a class=\"anchor\" href=\"#311-现在的问题\">#</a> 3.1.1 现在的问题</h6>\n<p><strong>单数据库</strong></p>\n<p>所有数据都是存放在一个<a href=\"https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%BA%93%E6%96%87%E4%BB%B6&amp;spm=1001.2101.3001.7020\">数据库文件</a>里的，经过常年累月，内存不足了怎么办？</p>\n<p><a href=\"https://imgse.com/i/pEgOyd0\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgOyd0.png\" alt=\"pEgOyd0.png\" /></a></p>\n<p>随着互联网及移动互联网的发展，应用系统的数据量也是成指数式增长，若采用单数据库进行数据存储，存在以下性能瓶颈：</p>\n<p>IO 瓶颈：热点数据太多，数据库缓存不足，产生大量磁盘 IO，效率较低。请求数据太多，带宽不够，网络 IO 瓶颈。<br />\nCPU 瓶颈： 排序、分组、连接查询、聚合统计等 SQL 会耗费大量的 CPU 资源，请求数太多，CPU 出现瓶颈。</p>\n<p><a href=\"https://imgse.com/i/pEgO6oV\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgO6oV.png\" alt=\"pEgO6oV.png\" /></a></p>\n<p><strong>分库分表的中心思想：<br />\n将数据分散存储，使得单一数据库 / 表的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的。</strong></p>\n<h6 id=\"312-拆分策略\"><a class=\"anchor\" href=\"#312-拆分策略\">#</a> 3.1.2 拆分策略</h6>\n<p><a href=\"https://imgse.com/i/pE2dAXt\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2dAXt.png\" alt=\"pE2dAXt.png\" /></a></p>\n<h6 id=\"313-垂直拆分策略\"><a class=\"anchor\" href=\"#313-垂直拆分策略\">#</a> 3.1.3 垂直拆分策略</h6>\n<p><a href=\"https://imgse.com/i/pE2dnAS\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2dnAS.png\" alt=\"pE2dnAS.png\" /></a></p>\n<p>特点:</p>\n<ol>\n<li>每个库的表结构都不一样。</li>\n<li>每个库的数据也不一样 。</li>\n<li>所有，库的并集是全量数据。</li>\n</ol>\n<p><a href=\"https://imgse.com/i/pE2dQpj\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2dQpj.png\" alt=\"pE2dQpj.png\" /></a></p>\n<p>特点:</p>\n<ol>\n<li>每个表的结构都不一样。</li>\n<li>每个表的数据也术一样，一般通过一列 (主键 / 外键) 关联。</li>\n<li>所有表的并集是全量数据。</li>\n</ol>\n<h6 id=\"314-水平拆分策略\"><a class=\"anchor\" href=\"#314-水平拆分策略\">#</a> 3.1.4 水平拆分策略</h6>\n<p><a href=\"https://imgse.com/i/pE2d3Xq\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2d3Xq.png\" alt=\"pE2d3Xq.png\" /></a></p>\n<p>水平分库：以 “字段” 为依据，改为以 “行（记录）” 为依据。讲一个库的数据拆分到多个库</p>\n<p>特点：</p>\n<ol>\n<li>每个库的表结构都一样。</li>\n<li>每个库的数据都不一样。</li>\n<li>所有库的并集是全量数据。</li>\n</ol>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/8Vp5L6j.png\" alt=\"1.png\" /></p>\n<p>特点：</p>\n<ol>\n<li>每个表的表结构都一样 。</li>\n<li>每个表的数据都不一样 。</li>\n<li>所有表的并集是全量数据。</li>\n</ol>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/2ctPFwi.png\" alt=\"2.png\" /></p>\n<ul>\n<li>shardingJDBC：基于 AOP 原理，在应用程序中对本地执行的 SQL 进行拦截，解析、改写、路由处理。需要自行编码配置实现，只支持 java 语言，性能较高。</li>\n<li>MyCat：数据库分库分表中间件，不用调整代码即可实现分库分表，支持多种语言，性能不及前者。</li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/lgs1r8g.png\" alt=\"3.png\" /></p>\n<h5 id=\"32-mycat概述\"><a class=\"anchor\" href=\"#32-mycat概述\">#</a> 3.2 Mycat 概述</h5>\n<p>Mycat 是开源的、活跃的、基于 Java 语言编写的<strong> MySQL 数据库中间件</strong>。可以像使用 mysql 一样来使用 mycat，对于开发人员来说根本感觉不到 mycat 的存在。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/KFB4gQ8.png\" alt=\"5.png\" /></p>\n<p>优势：</p>\n<ul>\n<li>性能可靠稳定</li>\n<li>强大的技术团队</li>\n<li>体系完善</li>\n<li>社区活跃</li>\n</ul>\n<p>Mycat 是采用 java 语言开发的开源的数据库中间件，支持 Windows 和 Linux 运行环境，下面介绍 MyCat 的 Linux 中的环境搭建。 我们需要在准备好的服务器中安装如下软件。</p>\n<table>\n<thead>\n<tr>\n<th>服务器</th>\n<th>安装软件</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>192.168.40.213</td>\n<td>JDK、Mycat</td>\n<td>MyCat 中间件服务器</td>\n</tr>\n<tr>\n<td>192.168.40.210</td>\n<td>MySQL</td>\n<td>分片服务器</td>\n</tr>\n<tr>\n<td>192.168.40.211</td>\n<td>MySQL</td>\n<td>分片服务器</td>\n</tr>\n<tr>\n<td>192.168.40.212</td>\n<td>MySQL</td>\n<td>分片服务器</td>\n</tr>\n</tbody>\n</table>\n<p>JDK 安装</p>\n<pre><code>#解压jdk\n[root@mycat ~]# tar xf jdk-8u371-linux-x64.tar.gz -C /usr/local\n[root@mycat ~]# ln -s /usr/local/jdk1.8.0_371/ /usr/local/jdk\n\n# 添加环境变量\n[root@mycat ~]# vim /etc/profile.d/jdk.sh \nexport JAVA_HOME=/usr/local/jdk\nexport PATH=$PATH:$JAVA_HOME/bin\nexport JRE_HOME=$JAVA_HOME/jre \nexport CLASSPATH=$JAVA_HOME/lib/:$JRE_HOME/lib/\n\n[root@mycat ~]# source /etc/profile\n[root@mycat ~]# java -version\n</code></pre>\n<p>Mycat 安装</p>\n<pre><code>[root@mycat ~]# tar xf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz -C /usr/local/\n[root@mycat ~]# ll /usr/local/mycat/\ntotal 12\ndrwxr-xr-x 2 root root  190 Dec  2 22:15 bin\ndrwxrwxrwx 2 root root    6 Mar  1  2016 catlet\ndrwxrwxrwx 4 root root 4096 Dec  2 22:15 conf\ndrwxr-xr-x 2 root root 4096 Dec  2 22:15 lib\ndrwxrwxrwx 2 root root    6 Oct 28  2016 logs\n-rwxrwxrwx 1 root root  217 Oct 28  2016 version.txt\n\n#上传jar包\n[root@mycat ~]# rz /usr/local/mycat/lib/mysql-connector-java-8.0.25.jar\n[root@mycat lib]# chmod 777 mysql-connector-java-8.0.25.jar \n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/n86yXtx.png\" alt=\"4.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/U26clQE.png\" alt=\"6.png\" /></p>\n<h6 id=\"321-mycat入门\"><a class=\"anchor\" href=\"#321-mycat入门\">#</a> 3.2.1 Mycat 入门</h6>\n<p>由于 tb_gorder 表中数据量很大，磁盘 IO 及容量都到达了瓶颈，现在需要对 tb_order 表进行数据分片，分为三个数据节点，每一个节点主机位于不同的服务器上，具体的结构，参考下图：</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/YjmWPQf.png\" alt=\"5.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/PQqJdjJ.png\" alt=\"8.png\" /></p>\n<h6 id=\"322-mycat配置\"><a class=\"anchor\" href=\"#322-mycat配置\">#</a> 3.2.2 Mycat 配置</h6>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/iXUxPhi.png\" alt=\"9.png\" /></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;DB01&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;TB_ORDER&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot; /&gt;\n\t&lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;db01&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;db01&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;db01&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/KkUttwJ.png\" alt=\"10.png\" /></p>\n<pre><code>[root@mycat mycat]# cat /usr/local/mycat/conf/server.xml \n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!-- - - Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); \n\t- you may not use this file except in compliance with the License. - You \n\tmay obtain a copy of the License at - - http://www.apache.org/licenses/LICENSE-2.0 \n\t- - Unless required by applicable law or agreed to in writing, software - \n\tdistributed under the License is distributed on an &quot;AS IS&quot; BASIS, - WITHOUT \n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. - See the \n\tLicense for the specific language governing permissions and - limitations \n\tunder the License. --&gt;\n&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt;\n&lt;mycat:server xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;system&gt;\n\t&lt;property name=&quot;useSqlStat&quot;&gt;0&lt;/property&gt;  &lt;!-- 1为开启实时统计、0为关闭 --&gt;\n\t&lt;property name=&quot;useGlobleTableCheck&quot;&gt;0&lt;/property&gt;  &lt;!-- 1为开启全加班一致性检测、0为关闭 --&gt;\n\n\t\t&lt;property name=&quot;sequnceHandlerType&quot;&gt;2&lt;/property&gt;\n      &lt;!--  &lt;property name=&quot;useCompression&quot;&gt;1&lt;/property&gt;--&gt; &lt;!--1为开启mysql压缩协议--&gt;\n        &lt;!--  &lt;property name=&quot;fakeMySQLVersion&quot;&gt;5.6.20&lt;/property&gt;--&gt; &lt;!--设置模拟的MySQL版本号--&gt;\n\t&lt;!-- &lt;property name=&quot;processorBufferChunk&quot;&gt;40960&lt;/property&gt; --&gt;\n\t&lt;!-- \n\t&lt;property name=&quot;processors&quot;&gt;1&lt;/property&gt; \n\t&lt;property name=&quot;processorExecutor&quot;&gt;32&lt;/property&gt; \n\t --&gt;\n\t\t&lt;!--默认为type 0: DirectByteBufferPool | type 1 ByteBufferArena--&gt;\n\t\t&lt;property name=&quot;processorBufferPoolType&quot;&gt;0&lt;/property&gt;\n\t\t&lt;!--默认是65535 64K 用于sql解析时最大文本长度 --&gt;\n\t\t&lt;!--&lt;property name=&quot;maxStringLiteralLength&quot;&gt;65535&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;sequnceHandlerType&quot;&gt;0&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;backSocketNoDelay&quot;&gt;1&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;frontSocketNoDelay&quot;&gt;1&lt;/property&gt;--&gt;\n\t\t&lt;!--&lt;property name=&quot;processorExecutor&quot;&gt;16&lt;/property&gt;--&gt;\n\t\t&lt;!--\n\t\t\t&lt;property name=&quot;serverPort&quot;&gt;8066&lt;/property&gt; &lt;property name=&quot;managerPort&quot;&gt;9066&lt;/property&gt; \n\t\t\t&lt;property name=&quot;idleTimeout&quot;&gt;300000&lt;/property&gt; &lt;property name=&quot;bindIp&quot;&gt;0.0.0.0&lt;/property&gt; \n\t\t\t&lt;property name=&quot;frontWriteQueueSize&quot;&gt;4096&lt;/property&gt; &lt;property name=&quot;processors&quot;&gt;32&lt;/property&gt; --&gt;\n\t\t&lt;!--分布式事务开关，0为不过滤分布式事务，1为过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤），2为不过滤分布式事务,但是记录分布式事务日志--&gt;\n\t\t&lt;property name=&quot;handleDistributedTransactions&quot;&gt;0&lt;/property&gt;\n\t\t\n\t\t\t&lt;!--\n\t\t\toff heap for merge/order/group/limit      1开启   0关闭\n\t\t--&gt;\n\t\t&lt;property name=&quot;useOffHeapForMerge&quot;&gt;1&lt;/property&gt;\n\n\t\t&lt;!--\n\t\t\t单位为m\n\t\t--&gt;\n\t\t&lt;property name=&quot;memoryPageSize&quot;&gt;1m&lt;/property&gt;\n\n\t\t&lt;!--\n\t\t\t单位为k\n\t\t--&gt;\n\t\t&lt;property name=&quot;spillsFileBufferSize&quot;&gt;1k&lt;/property&gt;\n\n\t\t&lt;property name=&quot;useStreamOutput&quot;&gt;0&lt;/property&gt;\n\n\t\t&lt;!--\n\t\t\t单位为m\n\t\t--&gt;\n\t\t&lt;property name=&quot;systemReserveMemorySize&quot;&gt;384m&lt;/property&gt;\n\n\n\t\t&lt;!--是否采用zookeeper协调切换  --&gt;\n\t\t&lt;property name=&quot;useZKSwitch&quot;&gt;true&lt;/property&gt;\n\n\n\t&lt;/system&gt;\n\t\n\t&lt;!-- 全局SQL防火墙设置 --&gt;\n\t&lt;!-- \n\t&lt;firewall&gt; \n\t   &lt;whitehost&gt;\n\t      &lt;host host=&quot;127.0.0.1&quot; user=&quot;mycat&quot;/&gt;\n\t      &lt;host host=&quot;127.0.0.2&quot; user=&quot;mycat&quot;/&gt;\n\t   &lt;/whitehost&gt;\n       &lt;blacklist check=&quot;false&quot;&gt;\n       &lt;/blacklist&gt;\n\t&lt;/firewall&gt;\n\t--&gt;\n\t\n\t&lt;user name=&quot;root&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;DB01&lt;/property&gt;\n\t\t\n\t\t&lt;!-- 表级 DML 权限设置 --&gt;\n\t\t&lt;!-- \t\t\n\t\t&lt;privileges check=&quot;false&quot;&gt;\n\t\t\t&lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt;\n\t\t\t\t&lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt;\n\t\t\t\t&lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt;\n\t\t\t&lt;/schema&gt;\n\t\t&lt;/privileges&gt;\t\t\n\t\t --&gt;\n\t&lt;/user&gt;\n\n\t&lt;user name=&quot;user&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;DB01&lt;/property&gt;\n\t\t&lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;\n\t&lt;/user&gt;\n\n&lt;/mycat:server&gt;\n</code></pre>\n<h6 id=\"323-mycat启动\"><a class=\"anchor\" href=\"#323-mycat启动\">#</a> 3.2.3 Mycat 启动</h6>\n<pre><code>#1.启动mycat\n[root@mycat mycat]# ./bin/mycat restart\n\n#2.wrapper.log日志中常见错误\nERROR | wrapper | 2021/1/10 13:31:05 | Startup failed: Timed out waiting for signal from JVM.\nERROR | wrapper | 2021/1/10 13:31:05 | JVM did not exit on request, terminated\n\n#3.启动Mycat超时,前往wrapper.conf配置超时策略\n[root@mycat mycat]# vim /usr/local/mycat/conf/wrapper.conf\n...\nwrapper.startup.timeout=300     //添加此行，超时时间300秒\nwrapper.ping.timeout=120\n\n#4.查看mycat是否启动\n[root@mycat mycat]# tail -f logs/wrapper.log\n...\nINFO   | jvm 1    | 2023/12/02 22:53:44 | MyCAT Server startup successfully. see logs in logs/mycat.log\n[root@mycat mycat]# netstat -lntp|grep 8066\ntcp6       0      0 :::8066                 :::*                    LISTEN      18028/java\n</code></pre>\n<h6 id=\"324-分片测试\"><a class=\"anchor\" href=\"#324-分片测试\">#</a> 3.2.4 分片测试</h6>\n<pre><code>[root@db3 ~]#  mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql: [Warning] Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 3\nServer version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)\n\nCopyright (c) 2000, 2023, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| DB01     |\n+----------+\n1 row in set (0.00 sec)\n\nmysql&gt; use DB01;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql&gt; show tables;\n+----------------+\n| Tables in DB01 |\n+----------------+\n| tb_order       |\n+----------------+\n1 row in set (0.00 sec)\nmysql&gt; CREATE TABLE TB_ORDER(\n    -&gt; id BIGINT(20) NOT NULL,\n    -&gt; title VARCHAR(100) NOT NULL,\n    -&gt; PRIMARY KEY (id)\n    -&gt; )ENGINE=INNODB DEFAULT CHARSET=utf8;\nQuery OK, 0 rows affected (0.04 sec)\n OK!\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(1,'guods1');\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(2,'guods2');\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(3,'guods3');\nmysql&gt;INSERT INTO TB_ORDER(id,title) VALUES(4,'guods4');\nmysql&gt; select * from TB_ORDER;\n+------+--------+\n| id   | title  |\n+------+--------+\n|    1 | guods1 |\n|    2 | guods2 |\n|    3 | guods3 |\n|    4 | guods4 |\n+------+--------+\n4 rows in set (0.03 sec)\n</code></pre>\n<p><strong>数据写入到 db1 中，因为 mycat 分片规则为 0-50000000 存入节点 1,5000001-10000000 存入节点 2,10000001-15000000 存入节点 3，15000001 以上无法插入数据，需要增加数据节点。</strong></p>\n<pre><code>[root@mycat mycat]# vim conf/rule.xml\n...\n        &lt;tableRule name=&quot;auto-sharding-long&quot;&gt;\n                &lt;rule&gt;\n                        &lt;columns&gt;id&lt;/columns&gt;\n                        &lt;algorithm&gt;rang-long&lt;/algorithm&gt;\n                &lt;/rule&gt;\n        &lt;/tableRule&gt;\n\n....\n       &lt;function name=&quot;rang-long&quot;\n                class=&quot;io.mycat.route.function.AutoPartitionByLong&quot;&gt;\n                &lt;property name=&quot;mapFile&quot;&gt;autopartition-long.txt&lt;/property&gt;\n        &lt;/function&gt;\n\n...\n\n[root@mycat mycat]# cat conf/autopartition-long.txt\n# range start-end ,data node index\n# K=1000,M=10000.\n0-500M=0\n500M-1000M=1\n\n#5000001-10000000存入节点2 \nmysql&gt; INSERT INTO TB_ORDER(id,title) VALUES(5000001,'guods5000001');\nQuery OK, 1 row affected (0.01 sec)\n OK!\n \n#10000001-15000000存入节点3 \nmysql&gt; INSERT INTO TB_ORDER(id,title) VALUES(10000001,'guods10000001');\nQuery OK, 1 row affected (0.00 sec)\n OK!\n\n#15000001以上无法插入数据，需要增加数据节点\nmysql&gt; INSERT INTO TB_ORDER(id,title) VALUES(15000001,'guods15000001');\nERROR 1064 (HY000): can't find any valid datanode :TB_ORDER -&gt; ID -&gt; 15000001\n</code></pre>\n<h5 id=\"33-mycat配置\"><a class=\"anchor\" href=\"#33-mycat配置\">#</a> 3.3 Mycat 配置</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/I9QLBBR.png\" alt=\"11.png\" /></p>\n<h6 id=\"331-schema标签\"><a class=\"anchor\" href=\"#331-schema标签\">#</a> 3.3.1 Schema 标签</h6>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/TmYK7fP.png\" alt=\"13.png\" /></p>\n<p>schema 标签用于定义 MyCat 实例中的逻辑库，一个 MyCat 实例中，可以有多个逻辑库，可以通过 schema 标签来划分不同的逻辑库。MyCat 中的逻辑库的概念，等同于 MySQL 中的 database 概念，需要操作某个逻辑库下的表时也需要切换逻辑库 (use xxx)。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/SGo0DCv.png\" alt=\"14.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/XtDxXWj.png\" alt=\"15.png\" /></p>\n<h6 id=\"332-datanode标签\"><a class=\"anchor\" href=\"#332-datanode标签\">#</a> 3.3.2 Datanode 标签</h6>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/phHZ48F.png\" alt=\"16.png\" /></p>\n<h6 id=\"333-datahost标签\"><a class=\"anchor\" href=\"#333-datahost标签\">#</a> 3.3.3 Datahost 标签</h6>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/fXBnwcS.png\" alt=\"17.png\" /></p>\n<h6 id=\"334-rulexml\"><a class=\"anchor\" href=\"#334-rulexml\">#</a> 3.3.4 rule.xml</h6>\n<p>rule.xml 中定义所有拆分表的规则，在使用过程中可以灵活的使用分片算法，或者对同一个分片算法使用不同的参数，它让分片过程可配置化。主要包含两类标签： <code>tableRule</code> 、 <code>Function</code> 。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Ecm1Nvr.png\" alt=\"18.png\" /></p>\n<h6 id=\"335-serverxml\"><a class=\"anchor\" href=\"#335-serverxml\">#</a> 3.3.5 server.xml</h6>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/xIDxYpu.png\" alt=\"19.png\" /></p>\n<h5 id=\"34-mycat分片\"><a class=\"anchor\" href=\"#34-mycat分片\">#</a> 3.4 Mycat 分片</h5>\n<h6 id=\"341-分库分表-mycat分片-垂直分库\"><a class=\"anchor\" href=\"#341-分库分表-mycat分片-垂直分库\">#</a> 3.4.1 分库分表 - MyCat 分片 - 垂直分库</h6>\n<p>场景：在业务系统中，涉及以下表结构，但是由于用户与订单每天都会产生大量的数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分，原有的数据库表如下。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/plSdAyY.png\" alt=\"20.png\" /></p>\n<p><strong>ps: 分库不需要指定 rule，涉及分表需要使用 rule；</strong></p>\n<p><strong>环境准备</strong></p>\n<p>①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。<br />\n②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。<br />\n③三台服务器关闭防火墙或者开放对应的端口。<br />\n④分别在三台 MySQL 中创建数据库 shopping。<br />\n<img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/uMZB18q.png\" alt=\"21.png\" /></p>\n<p><strong>schema.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t&lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>server.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/server.xml \n...\n\t&lt;user name=&quot;root&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING&lt;/property&gt;\n\t\t\n\t\t&lt;!-- 表级 DML 权限设置 --&gt;\n\t\t&lt;!-- \t\t\n\t\t&lt;privileges check=&quot;false&quot;&gt;\n\t\t\t&lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt;\n\t\t\t\t&lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt;\n\t\t\t\t&lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt;\n\t\t\t&lt;/schema&gt;\n\t\t&lt;/privileges&gt;\t\t\n\t\t --&gt;\n\t&lt;/user&gt;\n\n\t&lt;user name=&quot;user&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING&lt;/property&gt;\n\t\t&lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;\n\t&lt;/user&gt;\n\n&lt;/mycat:server&gt;\n</code></pre>\n<p><strong>分库分表 - MyCat 分片 - 垂直分库 - 测试</strong></p>\n<p><strong>垂直分库 - 测试</strong></p>\n<pre><code>#1.重启mycat\n[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log \n...\nINFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log\ncreate database shopping default charset utf8mb4;\n\n#2.在3台节点创建shopping数据库\nmysql&gt; create database shopping default charset utf8mb4;\nmysql&gt; create database shopping default charset utf8mb4;\nmysql&gt; create database shopping default charset utf8mb4;\n\n#3.登入mycat\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| SHOPPING |\n+----------+\n1 row in set (0.01 sec)\n\n#4.查看逻辑库\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| SHOPPING |\n+----------+\n1 row in set (0.01 sec)\n\n#5.切换到SHOPPING数据库\nmysql&gt; use SHOPPING;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\n\n#6.查看逻辑表\nmysql&gt; show tables;\n+--------------------+\n| Tables in SHOPPING |\n+--------------------+\n| tb_areas_city      |\n| tb_areas_provinces |\n| tb_areas_region    |\n| tb_goods_base      |\n| tb_goods_brand     |\n| tb_goods_cat       |\n| tb_goods_desc      |\n| tb_goods_item      |\n| tb_order_item      |\n| tb_order_master    |\n| tb_order_pay_log   |\n| tb_user            |\n| tb_user_address    |\n+--------------------+\n13 rows in set (0.00 sec)\n\n#7.上传shopping-table.sql表结构文件与shopping-insert.sql数据文件\n\n#8.执行shopping-table.sql文件\nmysql&gt; source /root/shopping-table.sql\n\n#9.执行shopping-insert.sql文件\nmysql&gt; source /root/shopping-insert.sql\n\n#10.查看三个数据库可以发现（根据schema.xml配置文件的配置进行了实现）\n①192.168.40.210的数据库中存放了 tb_goods_base、tb_goods_brand、tb_goods_cat、tb_goods_desc、tb_goods_item这五张表\n②192.168.40.211的数据库中存放了 tb_order_item、tb_order_master、tb_order_pay_log这三张表；\n③192.168.40.212的数据库中存放了 tb_user、tb_user_address、tb_areas_provinces、tb_areas_city、tb_areas_region这五张表\n</code></pre>\n<p><strong>exam1: 查询用户的收件人及收件人地址信息 (包含省、市、区)。</strong></p>\n<pre><code>mysql&gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;\n+-----------+-----------+-----------+-----------+-----------+--------------------+\n| user_id   | contact   | province  | city      | area      | address            |\n+-----------+-----------+-----------+-----------+-----------+--------------------+\n| deng      | 叶问      | 北京市    | 市辖区    | 西城区    | 咏春武馆总部       |\n| java00001 | 李佳红    | 北京市    | 市辖区    | 崇文区    | 修正大厦           |\n| deng      | 李小龙    | 北京市    | 市辖区    | 崇文区    | 永春武馆           |\n| zhaoliu   | 赵三      | 北京市    | 市辖区    | 宣武区    | 西直门             |\n| java00001 | 李嘉诚    | 北京市    | 市辖区    | 朝阳区    | 金燕龙办公楼       |\n| java00001 | 李佳星    | 北京市    | 市辖区    | 朝阳区    | 中腾大厦           |\n+-----------+-----------+-----------+-----------+-----------+--------------------+\n</code></pre>\n<p><em><strong>ps: 此查询语句只涉及了一个分片所以查询成功</strong></em></p>\n<p><strong>exam2: 查询每一笔订单及订单的收件地址信息 (包含省、市、区)。</strong></p>\n<pre><code>mysql&gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;\nERROR 1064 (HY000): invalid route in sql, multi tables found but datanode has no intersection  sql:SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid\n</code></pre>\n<p><em><strong>ps: 此查询语句涉及多个分片所以查询报错，为了解决这个问题需要进行全局表配置</strong></em></p>\n<p><strong>全局表配置</strong></p>\n<p>对于省、市、区 / 县表 tb_areas_provinces，tb_areas_city，tb_areas_region，是属于数据字典表，在多个业务模块中都可能会遇到，可以将其设置为全局表，利于业务操作。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/EqJJ3Yv.png\" alt=\"22.png\" /></p>\n<p><strong>1. 修改 MyCat—schema.xml 文件配置</strong></p>\n<p><strong>schema.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\n\t\t&lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot;/&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot;/&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t&lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>2. 全局表测试</strong></p>\n<pre><code>#1.删除3个节点上原有表\n\n#2.重启mycat\n[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log \n...\nINFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log\ncreate database shopping default charset utf8mb4;\n\n#3.执行shopping-table.sql文件\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql&gt; source /root/shopping-table.sql\n\n#4.执行shopping-insert.sql文件\nmysql&gt; source /root/shopping-insert.sql\n\n#5 exam1:查询用户的收件人及收件人地址信息(包含省、市、区)。\nmysql&gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;\n\n#6 exam2:查询每一笔订单及订单的收件地址信息(包含省、市、区)\nmysql&gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;\n</code></pre>\n<h6 id=\"342-分库分表-mycat分片-水平分表\"><a class=\"anchor\" href=\"#342-分库分表-mycat分片-水平分表\">#</a> 3.4.2 分库分表 - MyCat 分片 - 水平分表</h6>\n<ul>\n<li><strong>水平分表</strong></li>\n</ul>\n<p><strong>场景</strong>：在业务系统中，有一张表（日志表），业务系统每天都会产生大量的日志数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/0kMP4Ru.png\" alt=\"23.png\" /></p>\n<p><strong>准备环境：</strong></p>\n<p>①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。<br />\n②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。<br />\n③三台服务器关闭防火墙或者开放对应的端口。<br />\n④分别在三台 MySQL 中创建数据库 itcast。<br />\n<img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/zTm8XwU.png\" alt=\"24.png\" /></p>\n<p><strong>1. 三台 MySQL 中创建数据库 itcast</strong></p>\n<pre><code>mysql&gt; create database itcast default charset utf8mb4;\nmysql&gt; create database itcast default charset utf8mb4;\nmysql&gt; create database itcast default charset utf8mb4;\n</code></pre>\n<p><strong>2.MyCat—server.xml 文件配置</strong></p>\n<p><strong>server.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\n                &lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t&lt;/schema&gt;\n\n        &lt;schema name=&quot;ITCAST&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n        \t&lt;table name=&quot;tb_log&quot; dataNode=&quot;dn4,dn5,dn6&quot; primaryKey=&quot;id&quot; rule=&quot;mod-long&quot; /&gt;\n        &lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\n\t&lt;dataNode name=&quot;dn4&quot; dataHost=&quot;dhost1&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn5&quot; dataHost=&quot;dhost2&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn6&quot; dataHost=&quot;dhost3&quot; database=&quot;itcast&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>3.MyCat—server.xml 文件配置</strong></p>\n<p><strong>server.xml 文件配置如下：</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/server.xml \n...\n\t&lt;user name=&quot;root&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING,ITCAST&lt;/property&gt;\n\t\t\n\t\t&lt;!-- 表级 DML 权限设置 --&gt;\n\t\t&lt;!-- \t\t\n\t\t&lt;privileges check=&quot;false&quot;&gt;\n\t\t\t&lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt;\n\t\t\t\t&lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt;\n\t\t\t\t&lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt;\n\t\t\t&lt;/schema&gt;\n\t\t&lt;/privileges&gt;\t\t\n\t\t --&gt;\n\t&lt;/user&gt;\n\n\t&lt;user name=&quot;user&quot;&gt;\n\t\t&lt;property name=&quot;password&quot;&gt;Superman*2023&lt;/property&gt;\n\t\t&lt;property name=&quot;schemas&quot;&gt;SHOPPING,ITCAST&lt;/property&gt;\n\t\t&lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;\n\t&lt;/user&gt;\n\n&lt;/mycat:server&gt;\n</code></pre>\n<p><strong>4.MyCat 启动</strong></p>\n<pre><code>#1.重启mycat\n[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log \n...\nINFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log\ncreate database shopping default charset utf8mb4;\n\n#2.登入mycat\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| ITCAST   |\n| SHOPPING |\n+----------+\n2 rows in set (0.00 sec)\n\nmysql&gt; use ITCAST;\nmysql&gt; show tables;\n+------------------+\n| Tables in ITCAST |\n+------------------+\n| tb_log           |\n+------------------+\n\n#3.创建表结构及数据导入\nmysql&gt; CREATE TABLE tb_log (\n    -&gt;   id bigint(20) NOT NULL COMMENT 'ID',\n    -&gt;   model_name varchar(200) DEFAULT NULL COMMENT '模块名',\n    -&gt;   model_value varchar(200) DEFAULT NULL COMMENT '模块值',\n    -&gt;   return_value varchar(200) DEFAULT NULL COMMENT '返回值',\n    -&gt;   return_class varchar(200) DEFAULT NULL COMMENT '返回值类型',\n    -&gt;   operate_user varchar(20) DEFAULT NULL COMMENT '操作用户',\n    -&gt;   operate_time varchar(20) DEFAULT NULL COMMENT '操作时间',\n    -&gt;   param_and_value varchar(500) DEFAULT NULL COMMENT '请求参数名及参数值',\n    -&gt;   operate_class varchar(200) DEFAULT NULL COMMENT '操作类',\n    -&gt;   operate_method varchar(200) DEFAULT NULL COMMENT '操作方法',\n    -&gt;   cost_time bigint(20) DEFAULT NULL COMMENT '执行方法耗时, 单位 ms',\n    -&gt;   source int(1) DEFAULT NULL COMMENT '来源 : 1 PC , 2 Android , 3 IOS',\n    -&gt;   PRIMARY KEY (id)\n    -&gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\nQuery OK, 0 rows affected (0.09 sec)\n OK!\n查看三个数据库可以发现表和表结构都有了\n\n#4.添加数据\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('1','user','insert','success','java.lang.String','10001','2022-01-06 18:12:28','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','insert','10',1);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('2','user','insert','success','java.lang.String','10001','2022-01-06 18:12:27','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','insert','23',1);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('3','user','update','success','java.lang.String','10001','2022-01-06 18:16:45','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','update','34',1);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('4','user','update','success','java.lang.String','10001','2022-01-06 18:16:45','&#123;\\&quot;age\\&quot;:\\&quot;20\\&quot;,\\&quot;name\\&quot;:\\&quot;Tom\\&quot;,\\&quot;gender\\&quot;:\\&quot;1\\&quot;&#125;','cn.itcast.controller.UserController','update','13',2);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('5','user','insert','success','java.lang.String','10001','2022-01-06 18:30:31','&#123;\\&quot;age\\&quot;:\\&quot;200\\&quot;,\\&quot;name\\&quot;:\\&quot;TomCat\\&quot;,\\&quot;gender\\&quot;:\\&quot;0\\&quot;&#125;','cn.itcast.controller.UserController','insert','29',3);\nINSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES('6','user','find','success','java.lang.String','10001','2022-01-06 18:30:31','&#123;\\&quot;age\\&quot;:\\&quot;200\\&quot;,\\&quot;name\\&quot;:\\&quot;TomCat\\&quot;,\\&quot;gender\\&quot;:\\&quot;0\\&quot;&#125;','cn.itcast.controller.UserController','find','29',2);\n\n查看三个数据库内的tb_log表发现有数据了，数据的分布规则是 id模以3的结果为0的数据分布在第一个节点，id模以3的结果为1的数据分布在第二个节点，id模以3的结果为2的数据分布在第三个节点\n</code></pre>\n<h5 id=\"33-分库分表-分片规则\"><a class=\"anchor\" href=\"#33-分库分表-分片规则\">#</a> 3.3 分库分表 - 分片规则</h5>\n<h6 id=\"331-分库分表-分片规则-范围分片\"><a class=\"anchor\" href=\"#331-分库分表-分片规则-范围分片\">#</a> 3.3.1 分库分表 - 分片规则 - 范围分片</h6>\n<p><strong>范围分片</strong>：根据指定的字段及其配置的范围与数据节点的对应情况，来决定该数据属于哪一个分片。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/sdd8bvs.png\" alt=\"25.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/R3ecZ4k.png\" alt=\"26.png\" /></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/autopartition-long.txt\n# range start-end ,data node index\n# K=1000,M=10000.\n0-500M=0\n500M-1000M=1\n1000M-1500M=2\n</code></pre>\n<h6 id=\"332-分库分表-分片规则-取模分片\"><a class=\"anchor\" href=\"#332-分库分表-分片规则-取模分片\">#</a> 3.3.2 分库分表 - 分片规则 - 取模分片</h6>\n<p><strong>取模分片</strong>：根据指定的字段值与节点数量进行求模运算，根据运算结果，来决定该数据属于哪一个分片。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Xvn6sHi.png\" alt=\"1.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/aaey4H2.png\" alt=\"2.png\" /></p>\n<h6 id=\"333-分库分表-分片规则-一致性hash算法\"><a class=\"anchor\" href=\"#333-分库分表-分片规则-一致性hash算法\">#</a> 3.3.3 分库分表 - 分片规则 - 一致性 hash 算法</h6>\n<p><strong>一致性 hash 算法</strong>：所谓一致性哈希，相同的哈希因子计算值总是被划分到相同的分区表中，不会因为分区节点的增加而改变原来数据的分区位置。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/6ANYtsD.png\" alt=\"3.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/8i8c5Le.png\" alt=\"4.png\" /></p>\n<p><strong>一致性 hash 测试</strong></p>\n<p>schema.xml 配置</p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml \n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n\t&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n\t\t&lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt;\n\t\t\n\t\t&lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\t\t&lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt;\n\n                &lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t\t&lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn1,dn2,dn3&quot; primaryKey=&quot;id&quot; type=&quot;global&quot; /&gt;\n\t&lt;/schema&gt;\n\n        &lt;schema name=&quot;ITCAST&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;\n        \t&lt;table name=&quot;tb_log&quot; dataNode=&quot;dn4,dn5,dn6&quot; primaryKey=&quot;id&quot; rule=&quot;mod-long&quot; /&gt;\n        \t&lt;table name=&quot;tb_order&quot; dataNode=&quot;dn4,dn5,dn6&quot; primaryKey=&quot;id&quot; rule=&quot;sharding-by-murmur&quot; /&gt;\n        &lt;/schema&gt;\n\t\n\t&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;\n\t&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;\n\n\t&lt;dataNode name=&quot;dn4&quot; dataHost=&quot;dhost1&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn5&quot; dataHost=&quot;dhost2&quot; database=&quot;itcast&quot; /&gt;\n\t&lt;dataNode name=&quot;dn6&quot; dataHost=&quot;dhost3&quot; database=&quot;itcast&quot; /&gt;\n\t\n\t&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n\t\n\t&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;\n\t\t\t  writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n\t\t&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n\t\t\n\t\t&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;Superman*2023&quot; /&gt;\n\t&lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><strong>rule.xml 配置</strong></p>\n<pre><code>[root@mycat ~]# cat /usr/local/mycat/conf/rule.xml \n...\n\t&lt;function name=&quot;murmur&quot;\n\t\tclass=&quot;io.mycat.route.function.PartitionByMurmurHash&quot;&gt;\n\t\t&lt;property name=&quot;seed&quot;&gt;0&lt;/property&gt;&lt;!-- 默认是0 --&gt;\n\t\t&lt;property name=&quot;count&quot;&gt;3&lt;/property&gt;&lt;!-- 要分片的数据库节点数量，必须指定，否则没法分片 --&gt;\n\t\t&lt;property name=&quot;virtualBucketTimes&quot;&gt;160&lt;/property&gt;&lt;!-- 一个实际的数据库节点被映射为这么多虚拟节点，默认是160倍，也就是虚拟节点数是物理节点数的160倍 --&gt;\n\t\t&lt;!-- &lt;property name=&quot;weightMapFile&quot;&gt;weightMapFile&lt;/property&gt; 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写，以从0开始到count-1的整数值也就是节点索引为key，以节点权重值为值。所有权重值必须是正整数，否则以1代替 --&gt;\n\t\t&lt;!-- &lt;property name=&quot;bucketMapPath&quot;&gt;/etc/mycat/bucketMapPath&lt;/property&gt; \n\t\t\t用于测试时观察各物理节点与虚拟节点的分布情况，如果指定了这个属性，会把虚拟节点的murmur hash值与物理节点的映射按行输出到这个文件，没有默认值，如果不指定，就不会输出任何东西 --&gt;\n\t&lt;/function&gt;\n...\n</code></pre>\n<p><strong>重启 mycat 并插入数据测试</strong></p>\n<pre><code>[root@mycat ~]# /usr/local/mycat/bin/mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n\n[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log\n...\nINFO   | jvm 1    | 2023/12/03 22:17:47 | MyCAT Server startup successfully. see logs in logs/mycat.log\n\n[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p'Superman*2023'\nServer version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)\n\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| ITCAST   |\n| SHOPPING |\n+----------+\n2 rows in set (0.00 sec)\n\nmysql&gt; use ITCAST;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql&gt; show tables;\n+------------------+\n| Tables in ITCAST |\n+------------------+\n| tb_log           |\n| tb_order         |\n+------------------+\n2 rows in set (0.00 sec)\n\n#创建表结构\ncreate table tb_order(\n    id  varchar(100) not null primary key,\n    money   int null,\n    content varchar(200) null\n);\n\n#插入数据\nINSERT INTO tb_order (id, money, content) VALUES ('b92fdaaf-6fc4-11ec-b831-482ae33c4a2d', 10, 'b92fdaf8-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b93482b6-6fc4-11ec-b831-482ae33c4a2d', 20, 'b93482d5-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b937e246-6fc4-11ec-b831-482ae33c4a2d', 50, 'b937e25d-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b93be2dd-6fc4-11ec-b831-482ae33c4a2d', 100, 'b93be2f9-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b93f2d68-6fc4-11ec-b831-482ae33c4a2d', 130, 'b93f2d7d-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9451b98-6fc4-11ec-b831-482ae33c4a2d', 30, 'b9451bcc-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9488ec1-6fc4-11ec-b831-482ae33c4a2d', 560, 'b9488edb-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b94be6e6-6fc4-11ec-b831-482ae33c4a2d', 10, 'b94be6ff-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b94ee10d-6fc4-11ec-b831-482ae33c4a2d', 123, 'b94ee12c-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b952492a-6fc4-11ec-b831-482ae33c4a2d', 145, 'b9524945-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b95553ac-6fc4-11ec-b831-482ae33c4a2d', 543, 'b95553c8-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9581cdd-6fc4-11ec-b831-482ae33c4a2d', 17, 'b9581cfa-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b95afc0f-6fc4-11ec-b831-482ae33c4a2d', 18, 'b95afc2a-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b95daa99-6fc4-11ec-b831-482ae33c4a2d', 134, 'b95daab2-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b9667e3c-6fc4-11ec-b831-482ae33c4a2d', 156, 'b9667e60-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b96ab489-6fc4-11ec-b831-482ae33c4a2d', 175, 'b96ab4a5-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b96e2942-6fc4-11ec-b831-482ae33c4a2d', 180, 'b96e295b-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b97092ec-6fc4-11ec-b831-482ae33c4a2d', 123, 'b9709306-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b973727a-6fc4-11ec-b831-482ae33c4a2d', 230, 'b9737293-6fc4-11ec-b831-482ae33c4a2d');\nINSERT INTO tb_order (id, money, content) VALUES ('b978840f-6fc4-11ec-b831-482ae33c4a2d', 560, 'b978843c-6fc4-11ec-b831-482ae33c4a2d');\n</code></pre>\n<p>PS：数据按一致性 hash 分布在不同节点</p>\n",
            "tags": [
                "MySQL"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/2771271649.html",
            "url": "http://ixuyong.cn/posts/2771271649.html",
            "title": "云原生K8s安全专家CKS认证考题详解",
            "date_published": "2025-04-09T13:38:39.000Z",
            "content_html": "<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"抱歉, 这个密码看着不太对, 请再试试。\" data-whm=\"抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-xray\">\n      <input class=\"hbe hbe-input-field hbe-input-field-xray\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-xray\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-xray\">您好, 这里需要输入密码。</span>\n      </label>\n      <svg class=\"hbe hbe-graphic hbe-graphic-xray\" width=\"300%\" height=\"100%\" viewBox=\"0 0 1200 60\" preserveAspectRatio=\"none\">\n        <path d=\"M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0\"></path>\n        <path d=\"M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0\"></path>\n      </svg>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/1414180692.html",
            "url": "http://ixuyong.cn/posts/1414180692.html",
            "title": "Redis集群（主从+哨兵）模式",
            "date_published": "2025-04-09T11:50:06.000Z",
            "content_html": "<h3 id=\"redis集群主从哨兵模式\"><a class=\"anchor\" href=\"#redis集群主从哨兵模式\">#</a> Redis 集群（主从 + 哨兵）模式</h3>\n<h3 id=\"一-什么是redis主从复制\"><a class=\"anchor\" href=\"#一-什么是redis主从复制\">#</a> 一、什么是 redis 主从复制？</h3>\n<p>主从复制，是指将一台 Redis 服务器的数据，复制到其他的 Redis 服务器。前者称为主节点 (master)，后者称为从节点 (slave), 数据的复制是单向的，只能由主节点到从节点。master 以写为主，slave 以读为主。</p>\n<p><a href=\"https://imgse.com/i/pEgTlKx\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgTlKx.png\" alt=\"pEgTlKx.png\" /></a></p>\n<h3 id=\"二-主从复制的作用\"><a class=\"anchor\" href=\"#二-主从复制的作用\">#</a> 二、主从复制的作用</h3>\n<p>数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。<br />\n故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。<br />\n负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。<br />\n读写分离：用于实现读写分离，主库写、从库读，读写分离不仅可以提高服务器的负载能力，同时可根据需求的变化，改变从库的数量；<br />\n高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是 Redis 高可用的基础。</p>\n<h3 id=\"三-实现主从复制\"><a class=\"anchor\" href=\"#三-实现主从复制\">#</a> 三、实现主从复制</h3>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP</th>\n<th>角色</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>redis01</td>\n<td>192.168.40.101</td>\n<td>master</td>\n</tr>\n<tr>\n<td>redis02</td>\n<td>192.168.40.102</td>\n<td>slave</td>\n</tr>\n<tr>\n<td>redis03</td>\n<td>192.168.40.103</td>\n<td>slave</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"31-关闭防火墙-selinux\"><a class=\"anchor\" href=\"#31-关闭防火墙-selinux\">#</a> 3.1 关闭防火墙、selinux</h4>\n<pre><code>[root@master01 ~]# hostnamectl set-hostname redis01\n[root@redis01 ~]# systemctl stop firewalld\n[root@redis01 ~]# systemctl disable firewalld\n[root@redis01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@redis01 ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@redis01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@redis01 ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@redis01 ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@redis01 ~]# ntpdate time2.aliyun.com\n[root@redis01 ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/null\n[root@redis01 ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h4 id=\"32-安装redis\"><a class=\"anchor\" href=\"#32-安装redis\">#</a> 3.2 安装 redis</h4>\n<pre><code>[root@redis01 ~]# yum install gcc-c++ -y\n[root@redis01 soft]# wget https://download.redis.io/releases/redis-6.2.11.tar.gz\n[root@redis01 soft]# tar xf redis-6.2.11.tar.gz \n[root@redis01 soft]# ln -s /soft/redis-6.2.11 /soft/redis\n[root@redis01 soft]# cd /soft/redis\n[root@redis01 redis]# make            #执行make编译\n[root@redis01 redis]# make install    #将 src下的许多可执行文件复制到/usr/local/bin 目录下\n[root@redis01 redis]# redis-server /soft/redis/redis.conf &amp;\n[root@redis01 redis]# netstat -lntp|grep redis\ntcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      69686/redis-server  \ntcp6       0      0 ::1:6379                :::*                    LISTEN      69686/redis-server     \n[root@redis01 redis]# redis-cli shutdown      #关闭Redis服务\n</code></pre>\n<h4 id=\"33-redis配置文件说明\"><a class=\"anchor\" href=\"#33-redis配置文件说明\">#</a> 3.3 redis 配置文件说明</h4>\n<pre><code>[root@db01 redis]# vim redis.conf \nbind 127.0.0.1      \t\t# 绑定的ip\nprotected-mode yes  \t\t# 保护模式\nport 6379           \t\t# 端口设置\ndaemonize yes               # 后台启动\nbind 127.0.0.1      \t\t# 绑定的ip\nprotected-mode yes  \t\t# 保护模式\nport 6379           \t\t# 端口设置\nloglevel notice     \t\t# 记录日志级别\nlogfile &quot;redis.log&quot;         # 日志的文件位置名\ndir ./               \t\t# 日志存储目录\ndatabases 16        \t\t# 数据库的数量，默认是 16 个数据库\nalways-show-logo yes \t\t# 是否总是显示LOGO\n\n# 如果900s内，如果至少有一个1 key进行了修改，我们及进行持久化操作\nsave 900 1\n# 如果300s内，如果至少10 key进行了修改，我们及进行持久化操作\nsave 300 10\n# 如果60s内，如果至少10000 key进行了修改，我们及进行持久化操作\nsave 60 10000\n# 我们之后学习持久化，会自己定义这个测试！\nstop-writes-on-bgsave-error yes   # 持久化如果出错，是否还需要继续工作！\nrdbcompression yes                # 是否压缩 rdb 文件，需要消耗一些cpu资源！\nrdbchecksum yes                   # 保存rdb文件的时候，进行错误的检查校验！\ndbfilename dump.rdb               # rdb 文件保存的名称！\ndir ./                            # rdb 文件保存的目录！\n\nslaveof 192.168.1.154 6379        # 配置主从复制\nrequirepass foobared              # 配置redis登录密码\n\nappendonly no    # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！\nappendfilename &quot;appendonly.aof&quot;   # 持久化的文件的名字\n# appendfsync always        # 每次修改都会 sync。消耗性能\nappendfsync everysec        # 每秒执行一次 sync，可能会丢失这1s的数据！\n# appendfsync no            # 不执行 sync，这个时候操作系统自己同步数据，速度最快！\nno-appendfsync-on-rewrite   #重写时是否可以运用appendsync，默认no，可以保证数据的安全性\n</code></pre>\n<h4 id=\"34-redis环境配置\"><a class=\"anchor\" href=\"#34-redis环境配置\">#</a> 3.4 redis 环境配置</h4>\n<p>#修改 maser 配置文件</p>\n<pre><code>vim redis.conf\nbind 192.168.40.101 #绑定本机ip地址\nport 6739          #绑定端口号\ndaemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no\npidfile /var/run/redis_6379.pid\nlogfile &quot;redis.log&quot;   #redis日志文件\nrequirepass Superman*2023  #本地redis密码\nmasterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到\nprotected-mode yes    #保护模式\n</code></pre>\n<p>#修改 slave01 配置文件</p>\n<pre><code>vim redis.conf\nbind 192.168.40.102 #绑定本机ip地址\nport 6739          #绑定端口号\ndaemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no\npidfile /var/run/redis_6379.pid\nlogfile &quot;redis.log&quot;   #redis日志文件\nreplicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置\nrequirepass Superman*2023  #本地redis密码\nmasterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到\nprotected-mode yes    #保护模式\n</code></pre>\n<p>#修改 slave02 配置文件</p>\n<pre><code>vim redis.conf\nbind 192.168.40.103 #绑定本机ip地址\nport 6739          #绑定端口号\ndaemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no\npidfile /var/run/redis_6379.pid\nlogfile &quot;redis.log&quot;   #redis日志文件\nreplicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置\nrequirepass Superman*2023  #本地redis密码\nmasterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到\nprotected-mode yes    #保护模式\n</code></pre>\n<h4 id=\"35-启动3台redis服务\"><a class=\"anchor\" href=\"#35-启动3台redis服务\">#</a> 3.5 启动 3 台 redis 服务</h4>\n<pre><code>#启动redis01\n[root@redis01 redis]# redis-server /soft/redis/redis.conf\n[root@redis0[root@redis01 redis]# redis-server /soft/redis/redis.conf redis]# netstat -lntp|grep redis\ntcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server \n\n#启动redis02\n[root@redis02 redis]# redis-server /soft/redis/redis.conf\n[root@redis02 redis]# netstat -lntp|grep redis\ntcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server\n\n启动redis03\n[root@redis03 redis]# redis-server /soft/redis/redis.conf\n[root@redis03 redis]# netstat -lntp|grep redis\ntcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server \n</code></pre>\n<h4 id=\"36-查看主从状态\"><a class=\"anchor\" href=\"#36-查看主从状态\">#</a> 3.6 查看主从状态</h4>\n<pre><code>#主节点\n[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023\n192.168.40.101:6379&gt; info replication\n# Replication\nrole:master\nconnected_slaves:2\nslave0:ip=192.168.40.102,port=6379,state=online,offset=616,lag=0\nslave1:ip=192.168.40.103,port=6379,state=online,offset=616,lag=0\nmaster_failover_state:no-failover\nmaster_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:616\nsecond_repl_offset:-1\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1\nrepl_backlog_histlen:616\n\n#从节点\n[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n192.168.40.103:6379&gt; info replication\n# Replication\nrole:slave\nmaster_host:192.168.40.101\nmaster_port:6379\nmaster_link_status:up\nmaster_last_io_seconds_ago:1\nmaster_sync_in_progress:0\nslave_read_repl_offset:812\nslave_repl_offset:812\nslave_priority:100\nslave_read_only:1\nreplica_announced:1\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:812\nsecond_repl_offset:-1\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:295\nrepl_backlog_histlen:518\n</code></pre>\n<h4 id=\"37-测试主从\"><a class=\"anchor\" href=\"#37-测试主从\">#</a> 3.7 测试主从</h4>\n<pre><code>[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n192.168.40.101:6379&gt; set k1 v1\nOK\n192.168.40.101:6379&gt; set k2 v2\nOK\n\n[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023\nWarning: Using a password with '-a' or '-u' option on the command line interface may not be safe.\n192.168.40.103:6379&gt; get k1\n&quot;v1&quot;\n192.168.40.103:6379&gt; get k2\n&quot;v2&quot;\n</code></pre>\n<p><strong>注意:</strong><br />\n1、主机可以写，从机不能写，只能读。主机中的所有数据都会保存到从机中去。<br />\n2、主机断开连接，从机依旧连接到主机的，但是没有写操作，这个时候，主机如果回来了，从机依旧可以直接获取到主机写的信息！<br />\n3、如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从主机中获取值！<br />\n4、主从复制原理<br />\n Slave 启动成功连接到 master 后会发送一个 sync 同步命令<br />\n Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，并完成一次完全同步。<br />\n全量复制：slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。<br />\n增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave，完成同步，但是只要是重新连接 master，一次完全同步（全量复制）将被自动执行！ 主机的数据一定可以在从机中看到。</p>\n<h3 id=\"四-哨兵模式搭建\"><a class=\"anchor\" href=\"#四-哨兵模式搭建\">#</a> 四、哨兵模式搭建</h3>\n<p>1、什么是 redis 哨兵？<br />\nRedisSentinel 是 Redis 的高可用性解决方案，由一个或多个 Sentinel（哨兵）实例组成。它可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，它的主要功能如下：<br />\n监控 (Monitoring)：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。<br />\n通知 (Notification)：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。<br />\n故障迁移：当主服务器不能正常工作时，Sentinel 会自动进行故障迁移，也就是主从切换。<br />\n统一的配置：管理连接者询问 sentinel 取得主从的地址。</p>\n<p>2、哨兵原理是什么？<br />\nSentinel 使用的算法核心是 Raft 算法，主要用途就是用于分布式系统，系统容错，以及 Leader 选举，每个 Sentinel 都需要定期的执行以下任务：<br />\n每个 Sentinel 会自动发现其他 Sentinel 和从服务器，它以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。<br />\n如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。<br />\n如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有 Sentinel 要以每秒一次的频率确认主服务器的确进入了主观下线状态。<br />\n如果一个主服务器被标记为主观下线， 并且有足够数量的 Sentinel（至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断，那么这个主服务器被标记为客观下线。<br />\n在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO 命令。当一个主服务器 Sentinel 标记为客观下线时，Sentinel 向下线主服务器的所有从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。<br />\n当没有足够数量的 Sentinel 同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向 Sentinel 的 PING 命令返回有效回复时， 主服务器的主管下线状态就会被移除。</p>\n<p><a href=\"https://imgse.com/i/pEgT1r6\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgT1r6.png\" alt=\"pEgT1r6.png\" /></a></p>\n<h4 id=\"41-搭建哨兵\"><a class=\"anchor\" href=\"#41-搭建哨兵\">#</a> 4.1 搭建哨兵</h4>\n<p><em>在每台服务器上部署一个哨兵，配置方式如下:</em></p>\n<pre><code>[root@redis01 redis]# vim sentinel.conf\n#端口默认为26379。\nport 26379\n#关闭保护模式，可以外部访问。\nprotected-mode no\n#设置为后台启动。\ndaemonize yes\n#日志文件。\nlogfile &quot;/soft/redis/sentinel.log&quot;\n#指定服务器IP地址和端口，并且指定当有2台哨兵认为主机挂了，则对主机进行容灾切换。注意:三台哨兵这里的ip配置均为主节点ip 和端口\nsentinel monitor mymaster 192.168.40.101 6379 2\n#当在Redis实例中开启了requirepass，这里就需要提供密码。\nsentinel auth-pass mymaster psw66\n#这里设置了主机多少秒无响应，则认为挂了。\nsentinel down-after-milliseconds mymaster 3000\n#主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的\nsnetinel parallel-syncs mymaster 1\n#故障转移的超时时间，这里设置为三分钟。\nsentinel failover-timeout mymaster 180000\n</code></pre>\n<h4 id=\"42-启动三台服务器上的哨兵\"><a class=\"anchor\" href=\"#42-启动三台服务器上的哨兵\">#</a> 4.2 启动三台服务器上的哨兵</h4>\n<pre><code>#启动redis01的sentine\n[root@redis01 redis]# redis-sentinel /soft/redis/sentinel.conf\n[root@redis01 redis]#  netstat -lntp|grep redis\ntcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      33536/redis-sentine \ntcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server \ntcp6       0      0 :::26379                :::*                    LISTEN      33536/redis-sentine\n\n#启动redis02的sentine\n[root@redis02 redis]# redis-sentinel /soft/redis/sentinel.conf\n[root@redis02 redis]#  netstat -lntp|grep redis\ntcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      18757/redis-sentine \ntcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server  \ntcp6       0      0 :::26379                :::*                    LISTEN      18757/redis-sentine\n\n#启动redis03的sentine\n[root@redis03 redis]# redis-sentinel /soft/redis/sentinel.conf                     \n[root@redis03 redis]# netstat -lntp|grep redis\ntcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      19745/redis-sentine \ntcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server  \ntcp6       0      0 :::26379                :::*                    LISTEN      19745/redis-sentine\n</code></pre>\n<h4 id=\"43-连接客户端\"><a class=\"anchor\" href=\"#43-连接客户端\">#</a> 4.3 连接客户端</h4>\n<pre><code>[root@redis01 redis]# redis-cli -p 26379\n127.0.0.1:26379&gt;  info sentinel\n# Sentinel\nsentinel_masters:1\nsentinel_tilt:0\nsentinel_running_scripts:0\nsentinel_scripts_queue_length:0\nsentinel_simulate_failure_flags:0\nmaster0:name=mymaster,status=ok,address=192.168.40.101:6379,slaves=2,sentinels=3\n</code></pre>\n<h4 id=\"44-redis容灾切换\"><a class=\"anchor\" href=\"#44-redis容灾切换\">#</a> 4.4 redis 容灾切换</h4>\n<pre><code>#连接redis客户端\n[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 \n#验证密码\n192.168.40.101:6379&gt; auth Superman*2023\nOK\n#关闭redis服务\n192.168.40.101:6379&gt; shutdown\nnot connected&gt;\n#退出客户端\nnot connected&gt; exit\n</code></pre>\n<p>关闭主节点之后，我们去查看哨兵日志:</p>\n<pre><code>[root@redis01 ~]# tail -f /soft/redis/sentinel.log \n91936:X 14 Apr 2023 23:26:23.838 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n91936:X 14 Apr 2023 23:26:23.838 # Redis version=6.2.11, bits=64, commit=00000000, modified=0, pid=91936, just started\n91936:X 14 Apr 2023 23:26:23.838 # Configuration loaded\n91936:X 14 Apr 2023 23:26:23.838 * monotonic clock: POSIX clock_gettime\n91936:X 14 Apr 2023 23:26:23.839 * Running mode=sentinel, port=26379.\n91936:X 14 Apr 2023 23:26:23.839 # Sentinel ID is 835b4c8544fb250af5fd479f834ee369cc4f388e\n91936:X 14 Apr 2023 23:26:23.839 # +monitor master mymaster 192.168.40.101 6379 quorum 2\n\n\n\n91936:X 14 Apr 2023 23:31:25.329 # +sdown master mymaster 192.168.40.101 6379   #这里应该是发现主节点宕机\n91936:X 14 Apr 2023 23:31:25.359 # +new-epoch 5\n91936:X 14 Apr 2023 23:31:25.360 # +vote-for-leader ab43979285cb47b1b459aeb0ab91b63fa9d1a989 5\n91936:X 14 Apr 2023 23:31:25.401 # +odown master mymaster 192.168.40.101 6379 #quorum 3/2 两个哨兵都觉得主节点宕机了\n91936:X 14 Apr 2023 23:31:25.401 # Next failover delay: I will not start a failover before Fri Apr 14 23:37:25 2023\n91936:X 14 Apr 2023 23:31:26.468 # +config-update-from sentinel ab43979285cb47b1b459aeb0ab91b63fa9d1a989 192.168.40.102 26379 @ mymaster 192.168.40.101 6379\n91936:X 14 Apr 2023 23:31:26.468 # +switch-master mymaster 192.168.40.101 6379 192.168.40.103 6379 #通过投票选举40.103为新的主节点\n91936:X 14 Apr 2023 23:31:26.468 * +slave slave 192.168.40.102:6379 192.168.40.102 6379 @ mymaster 192.168.40.103 6379\n91936:X 14 Apr 2023 23:31:26.469 * +slave slave 192.168.40.101:6379 192.168.40.101 6379 @ mymaster 192.168.40.103 6379\n</code></pre>\n<p>下面我们去 40.103 下查看哨兵主从切换是否成功</p>\n<pre><code>[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103\n192.168.40.103:6379&gt; auth Superman*2023\nOK\n192.168.40.103:6379&gt; info replication\n# Replication\nrole:master   # 40.103变成主节点了\nconnected_slaves:1   # 下面的从机个数为1\nslave0:ip=192.168.40.102,port=6379,state=online,offset=108708,lag=1\nmaster_failover_state:no-failover\nmaster_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3\nmaster_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935\nmaster_repl_offset:108851\nsecond_repl_offset:59887\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1\nrepl_backlog_histlen:108851\n</code></pre>\n<p>重新连接挂掉的主节点</p>\n<pre><code>[root@redis01 redis]# redis-server redis.conf \n[root@redis01 redis]#  redis-cli -p 6379 -h 192.168.40.101\n192.168.40.101:6379&gt; auth Superman*2023\nOK\n192.168.40.101:6379&gt; info replication\n# Replication\nrole:slave          #主节点连接回来之后自动变成了从节点，并且成功连上了主机\nmaster_host:192.168.40.103\nmaster_port:6379\nmaster_link_status:up\nmaster_last_io_seconds_ago:1\nmaster_sync_in_progress:0\nslave_read_repl_offset:130607\nslave_repl_offset:130607\nslave_priority:100\nslave_read_only:1\nreplica_announced:1\nconnected_slaves:0\nmaster_failover_state:no-failover\nmaster_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:130607\nsecond_repl_offset:-1\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:126982\nrepl_backlog_histlen:3626\n</code></pre>\n<p>再去主节点确认一下</p>\n<pre><code>192.168.40.103:6379&gt; info replication\n# Replication\nrole:master\nconnected_slaves:2   #两个从节点\nslave0:ip=192.168.40.102,port=6379,state=online,offset=147879,lag=1\nslave1:ip=192.168.40.101,port=6379,state=online,offset=147879,lag=1\nmaster_failover_state:no-failover\nmaster_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3\nmaster_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935\nmaster_repl_offset:148165\nsecond_repl_offset:59887\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1\nrepl_backlog_histlen:148165\n</code></pre>\n<p>五、哨兵模式的优缺点<br />\n 1. 优点</p>\n<p>哨兵集群，基于主从复制模式，所有的主从配置优点，它全有</p>\n<p>主从可以切换，故障可以转移，系统的可用性就会更好</p>\n<p>哨兵模式就是主从模式的升级，手动到自动，更加健壮！</p>\n<p>2. 缺点</p>\n<p>Redis 不好在线扩容，集群容量一旦到达上限，在线扩容就十分麻烦</p>\n<p>哨兵模式的配置繁琐</p>\n<p>3. 哨兵模式的配置文件详解</p>\n<pre><code># Example sentinel.conf\n# 哨兵sentinel实例运行的端口 默认26379\nport 26379\n \n# 哨兵sentinel的工作目录\ndir /tmp\n \n# 哨兵sentinel监控的redis主节点的 ip port\n# master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。\n# quorum 配置多少个sentinel哨兵统一认为master主节点失联 那么这时客观上认为主节点失联了\n# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;\nsentinel monitor mymaster 127.0.0.1 6379 2\n  \n# 当在Redis实例中开启了requirepass foobared 授权密码这样所有连接Redis实例的客户端都要提供 密码\n# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码\n# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;\nsentinel auth-pass mymaster MySUPER--secret-0123passw0rd\n \n# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时哨兵主观上认为主节点下线 默认30秒\n# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;\nsentinel down-after-milliseconds mymaster 30000\n \n# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长， 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。\n# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;\nsentinel parallel-syncs mymaster 1\n \n# 故障转移的超时时间 failover-timeout 可以用在以下这些方面：\n#1. 同一个sentinel对同一个master两次failover之间的间隔时间。\n#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那 里同步数据时。\n#3.当想要取消一个正在进行的failover所需要的时间。\n#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时， slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了 # 默认三分钟\n# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt; bilibili：\nsentinel failover-timeout mymaster 180000\n \n# SCRIPTS EXECUTION\n#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知 相关人员。\n#对于脚本的运行结果有以下规则：\n#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10\n#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。\n#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。\n#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。\n#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等）， 将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信 息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配 置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无 法正常启动成功。\n#通知脚本\n# shell编程\n# sentinel notification-script &lt;master-name&gt; &lt;script-path&gt; sentinel\nnotification-script mymaster /var/redis/notify.sh\n \n# 客户端重新配置主节点参数脚本\n# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已 经发生改变的信息。\n# 以下参数将会在调用脚本时传给脚本:\n# &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt; # 目前&lt;state&gt;总是“failover”,\n# &lt;role&gt;是“leader”或者“observer”中的一个。\n# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通 信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。\n# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt; sentinel client-reconfig-\nscript mymaster /var/redis/reconfig.sh\n</code></pre>\n<p><em>再去看一下 redis 的配置文件和哨兵的配置文件，你会惊讶的发现，里边的配置文件已经被改过来了。</em></p>\n<pre><code>cat redis.con\n...\nreplicaof 192.168.40.103 6379\n</code></pre>\n",
            "tags": [
                "Redis"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3166738000.html",
            "url": "http://ixuyong.cn/posts/3166738000.html",
            "title": "Kubeadm高可用安装K8s集群",
            "date_published": "2025-04-09T10:28:34.000Z",
            "content_html": "<h2 id=\"kubeadm高可用安装k8s集群\"><a class=\"anchor\" href=\"#kubeadm高可用安装k8s集群\">#</a> Kubeadm 高可用安装 K8s 集群</h2>\n<h4 id=\"1-基本配置\"><a class=\"anchor\" href=\"#1-基本配置\">#</a> 1. 基本配置</h4>\n<h5 id=\"11-基本环境配置\"><a class=\"anchor\" href=\"#11-基本环境配置\">#</a> 1.1 基本环境配置</h5>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP 地址</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master 节点 * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived 虚拟 IP（不占用机器）</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker 节点 * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* 配置信息 *</strong></em></th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>系统版本</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod 网段</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service 网段</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>所有节点</mark>更改主机名（其它节点按需修改）：</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>所有节点</mark>配置 hosts，修改 /etc/hosts 如下：</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>所有节点</mark>配置 yum 源：</p>\n<pre><code># 配置基础源\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>所有节点</mark>必备工具安装：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>所有节点</mark>关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>所有节点</mark>关闭 swap 分区：</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>所有节点</mark>安装 ntpdate：</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>所有节点</mark>同步时间并配置上海时区：</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# 加入到crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>所有节点</mark>配置 limit：</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>所有节点</mark>升级系统：</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 节点</mark>免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上</em></p>\n<p><mark>Master01 节点</mark>下载安装所有的源码文件：</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-内核配置\"><a class=\"anchor\" href=\"#12-内核配置\">#</a> 1.2 内核配置</h5>\n<p><mark>所有节点</mark>安装 ipvsadm：</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>所有节点</mark>配置 ipvs 模块：</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>所有节点</mark>创建 ipvs.conf，并配置开机自动加载：</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# 加入以下内容\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>所有节点</mark>然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>所有节点</mark>内核优化配置：</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>所有节点</mark>应用配置：</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>配置完内核后，重启机器，之后查看内核模块是否已自动加载：</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-高可用组件安装\"><a class=\"anchor\" href=\"#2-高可用组件安装\">#</a> 2. 高可用组件安装</h4>\n<p><em>注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装</em></p>\n<p><em>注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。</em></p>\n<p><mark>所有 Master 节点</mark>通过 yum 安装 HAProxy 和 KeepAlived：</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 HAProxy，需要注意黄色部分的 IP：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:16443       #HAProxy监听端口\n  bind 127.0.0.1:16443     #HAProxy监听端口\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IP地址\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IP地址\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IP地址\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 KeepAlived，需要注意黄色部分的配置。</p>\n<p><mark>Master01 节点</mark>的配置：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #网卡名称\n    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #网卡名称\n    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #网卡名称\n    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>所有 master 节点</mark>配置 KeepAlived 健康检查文件：</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>所有 master 节点</mark>配置健康检查文件添加执行权限：</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>所有 master 节点</mark>启动 haproxy 和 keepalived：</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的</p>\n<pre><code>所有节点测试VIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等</p>\n<ul>\n<li>所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld</li>\n<li>所有节点查看 selinux 状态，必须为 disable：getenforce</li>\n<li>master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy</li>\n<li>master 节点查看监听端口：netstat -lntp</li>\n</ul>\n<p>如果以上都没有问题，需要确认：</p>\n<ol>\n<li>\n<p>是否是公有云机器</p>\n</li>\n<li>\n<p>是否是私有云机器（类似 OpenStack）</p>\n</li>\n</ol>\n<p>上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询</p>\n<h4 id=\"3-runtime安装\"><a class=\"anchor\" href=\"#3-runtime安装\">#</a> 3. Runtime 安装</h4>\n<p>如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。</p>\n<h5 id=\"31-安装containerd\"><a class=\"anchor\" href=\"#31-安装containerd\">#</a> 3.1 安装 Containerd</h5>\n<p><mark>所有节点</mark>配置安装源：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>所有节点</mark>安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p>首先配置 Containerd 所需的模块（<mark>所有节点</mark>）：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载模块：</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>所有节点</mark>，配置 Containerd 所需的内核：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载内核：</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>生成 Containerd 的配置文件：</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>更改 Containerd 的 Cgroup 和 Pause 镜像配置：</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>启动 Containerd，并配置开机自启动：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>所有节点</mark>配置 crictl 客户端连接的运行时位置（可选）：</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-安装kubernetes组件\"><a class=\"anchor\" href=\"#4-安装kubernetes组件\">#</a> 4 . 安装 Kubernetes 组件</h4>\n<p><mark>所有节点</mark>配置源（注意更改版本号）：</p>\n<pre><code>cat &lt;&lt;EOF | tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key\nEOF\n</code></pre>\n<p>首先在<mark> Master01 节点</mark>查看最新的 Kubernetes 版本是多少：</p>\n<pre><code># yum list kubeadm.x86_64 --showduplicates | sort -r\n</code></pre>\n<p><mark>所有节点</mark>安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：</p>\n<pre><code># yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y\n</code></pre>\n<p><mark>所有节点</mark>设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now kubelet\n</code></pre>\n<p><em>此时 kubelet 是起不来的，日志会有报错不影响！</em></p>\n<h4 id=\"5-集群初始化\"><a class=\"anchor\" href=\"#5-集群初始化\">#</a> 5 . 集群初始化</h4>\n<p>以下操作在<mark> master01</mark>（注意黄色部分）：</p>\n<pre><code>vim kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 7t2weq.bjbawausm0jaxury\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.1.71\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  name: k8s-master01\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n---\napiServer:\n  certSANs:\n  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443\ncontrollerManager: &#123;&#125;\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突\n  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突\nscheduler: &#123;&#125;\n</code></pre>\n<p><mark>master01 节点</mark>更新 kubeadm 文件：</p>\n<pre><code>kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml\n</code></pre>\n<p>将 new.yaml 文件复制到<mark>其他 master 节点</mark>:</p>\n<pre><code>for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done\n</code></pre>\n<p>之后<mark>所有 Master 节点</mark>提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：</p>\n<pre><code>kubeadm config images pull --config /root/new.yaml \n</code></pre>\n<p>正确的反馈信息如下（<em><strong>* 版本可能不一样 *</strong></em>）：</p>\n<pre><code>[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml \n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0\n</code></pre>\n<p><mark>Master01 节点</mark>初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：</p>\n<pre><code>kubeadm init --config /root/new.yaml  --upload-certs\n</code></pre>\n<p>初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：</p>\n<pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n# 不要复制文档当中的，要去使用节点生成的\n  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94\n</code></pre>\n<p><mark>Master01 节点</mark>配置环境变量，用于访问 Kubernetes 集群：</p>\n<pre><code>cat &lt;&lt;EOF &gt;&gt; /root/.bashrc\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nEOF\nsource /root/.bashrc\n</code></pre>\n<p><mark>Master01 节点</mark>查看节点状态：（显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE   VERSION\nk8s-master01   NotReady   control-plane   24s   v1.32.3\n</code></pre>\n<p>采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：</p>\n<pre><code class=\"language-\\\"># kubectl get pods -n kube-system\n</code></pre>\n<h5 id=\"51-初始化失败排查\"><a class=\"anchor\" href=\"#51-初始化失败排查\">#</a> 5.1 初始化失败排查</h5>\n<p>如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：</p>\n<pre><code>kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube\n</code></pre>\n<p>如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：</p>\n<pre><code>tail -f /var/log/messages | grep -v &quot;not found&quot;\n</code></pre>\n<p>经常出错的原因：</p>\n<ol>\n<li>Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对</li>\n<li>new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443</li>\n<li>new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突</li>\n<li>VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错</li>\n</ol>\n<h5 id=\"52-高可用master\"><a class=\"anchor\" href=\"#52-高可用master\">#</a> 5.2 高可用 Master</h5>\n<p><strong>其他 master</strong> 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n</code></pre>\n<p>查看当前状态：（如果显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\n</code></pre>\n<h5 id=\"53-token过期处理\"><a class=\"anchor\" href=\"#53-token过期处理\">#</a> 5.3 Token 过期处理</h5>\n<p>注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。</p>\n<p>Token 过期后生成新的 token：</p>\n<pre><code>kubeadm token create --print-join-command\n</code></pre>\n<p>Master 需要生成 --certificate-key：</p>\n<pre><code>kubeadm init phase upload-certs  --upload-certs\n</code></pre>\n<h4 id=\"6-node节点的配置\"><a class=\"anchor\" href=\"#6-node节点的配置\">#</a> 6. Node 节点的配置</h4>\n<p>Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5\n</code></pre>\n<p>所有节点初始化完成后，查看集群状态（NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\nk8s-node01     NotReady   &lt;none&gt;          13s     v1.32.3\nk8s-node02     NotReady   &lt;none&gt;          10s     v1.32.3\n</code></pre>\n<h4 id=\"7-calico组件的安装\"><a class=\"anchor\" href=\"#7-calico组件的安装\">#</a> 7. Calico 组件的安装</h4>\n<p><mark>所有节点</mark>禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：</p>\n<pre><code>cat &gt;&gt;/etc/NetworkManager/conf.d/calico.conf&lt;&lt;EOF\n[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali\nEOF\nsystemctl daemon-reload\nsystemctl restart NetworkManager\n</code></pre>\n<p>以下步骤只在<mark> master01</mark> 执行（.x 不需要更改）：</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x &amp;&amp; cd calico/\n</code></pre>\n<p>修改 Pod 网段：</p>\n<pre><code>POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '&#123;print $NF&#125;'`\n\nsed -i &quot;s#POD_CIDR#$&#123;POD_SUBNET&#125;#g&quot; calico.yaml\nkubectl apply -f calico.yaml\n</code></pre>\n<p>查看容器和节点状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -n kube-system\nNAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h\ncalico-node-7mzmb                          1/1     Running   0          24h\ncalico-node-ljqnl                          1/1     Running   0          24h\ncalico-node-njqlb                          1/1     Running   0          24h\ncalico-node-ph4m4                          1/1     Running   0          24h\ncalico-node-rx8rl                          1/1     Running   0          24h\ncoredns-76fccbbb6b-76559                   1/1     Running   0          24h\ncoredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h\netcd-k8s-master01                          1/1     Running   0          24h\netcd-k8s-master02                          1/1     Running   0          24h\netcd-k8s-master03                          1/1     Running   0          24h\nkube-apiserver-k8s-master01                1/1     Running   0          24h\nkube-apiserver-k8s-master02                1/1     Running   0          24h\nkube-apiserver-k8s-master03                1/1     Running   0          24h\nkube-controller-manager-k8s-master01       1/1     Running   0          24h\nkube-controller-manager-k8s-master02       1/1     Running   0          24h\nkube-controller-manager-k8s-master03       1/1     Running   0          24h\nkube-proxy-9dtz4                           1/1     Running   0          24h\nkube-proxy-jh7rl                           1/1     Running   0          24h\nkube-proxy-jvvwt                           1/1     Running   0          24h\nkube-proxy-sh89l                           1/1     Running   0          24h\nkube-proxy-t2j49                           1/1     Running   0          24h\nkube-scheduler-k8s-master01                1/1     Running   0          24h\nkube-scheduler-k8s-master02                1/1     Running   0          24h\nkube-scheduler-k8s-master03                1/1     Running   0          24h\nmetrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h\n</code></pre>\n<p>此时节点全部变为 Ready 状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get nodes\nNAME           STATUS   ROLES           AGE   VERSION\nk8s-master01   Ready    control-plane   24h   v1.32.3\nk8s-master02   Ready    control-plane   24h   v1.32.3\nk8s-master03   Ready    control-plane   24h   v1.32.3\nk8s-node01     Ready    &lt;none&gt;          24h   v1.32.3\nk8s-node02     Ready    &lt;none&gt;          24h   v1.32.3\n</code></pre>\n<h4 id=\"8-metrics部署\"><a class=\"anchor\" href=\"#8-metrics部署\">#</a> 8. Metrics 部署</h4>\n<p>在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。</p>\n<p>将<mark> Master01 节点</mark>的 front-proxy-ca.crt 复制到所有 Node 节点</p>\n<pre><code>scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt\n\nscp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt\n</code></pre>\n<p>以下操作均在<mark> master01 节点</mark>执行:</p>\n<p>安装 metrics server</p>\n<pre><code>cd /root/k8s-ha-install/kubeadm-metrics-server\n\n# kubectl  create -f comp.yaml \nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>查看状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server\nNAME                              READY   STATUS    RESTARTS   AGE\nmetrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h\n</code></pre>\n<p>等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl top node\nNAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \nk8s-master01   132m         3%       932Mi           5%          \nk8s-master02   131m         3%       845Mi           5%          \nk8s-master03   148m         3%       912Mi           5%          \nk8s-node01     54m          1%       600Mi           3%          \nk8s-node02     49m          1%       602Mi           3%          \n[root@k8s-master01 ~]#  kubectl top po -A\nNAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   \ningress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            \ningress-nginx          ingress-nginx-controller-r978m               1m           104Mi           \nkrm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            \nkrm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             \nkrm                    nginx-574cf48959-vcfjs                       0m           2Mi             \nkube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            \nkube-system            calico-node-7mzmb                            16m          176Mi           \nkube-system            calico-node-ljqnl                            15m          182Mi           \nkube-system            calico-node-njqlb                            19m          180Mi           \nkube-system            calico-node-ph4m4                            15m          178Mi           \nkube-system            calico-node-rx8rl                            17m          180Mi           \nkube-system            coredns-76fccbbb6b-76559                     2m           16Mi            \nkube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            \nkube-system            etcd-k8s-master01                            22m          86Mi            \nkube-system            etcd-k8s-master02                            27m          84Mi            \nkube-system            etcd-k8s-master03                            22m          84Mi            \nkube-system            kube-apiserver-k8s-master01                  22m          267Mi           \nkube-system            kube-apiserver-k8s-master02                  20m          242Mi           \nkube-system            kube-apiserver-k8s-master03                  18m          241Mi           \nkube-system            kube-controller-manager-k8s-master01         6m           69Mi            \nkube-system            kube-controller-manager-k8s-master02         2m           21Mi            \nkube-system            kube-controller-manager-k8s-master03         1m           19Mi            \nkube-system            kube-proxy-9dtz4                             11m          30Mi            \nkube-system            kube-proxy-jh7rl                             1m           27Mi            \nkube-system            kube-proxy-jvvwt                             17m          29Mi            \nkube-system            kube-proxy-sh89l                             1m           29Mi            \nkube-system            kube-proxy-t2j49                             16m          29Mi            \nkube-system            kube-scheduler-k8s-master01                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master02                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master03                  6m           25Mi            \nkube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            \nkubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            \nkubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  \n</code></pre>\n<h4 id=\"9-dashboard部署\"><a class=\"anchor\" href=\"#9-dashboard部署\">#</a> 9. Dashboard 部署</h4>\n<h5 id=\"91-安装dashboard\"><a class=\"anchor\" href=\"#91-安装dashboard\">#</a> 9.1 安装 Dashboard</h5>\n<p>Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"92-登录dashboard\"><a class=\"anchor\" href=\"#92-登录dashboard\">#</a> 9.2 登录 dashboard</h5>\n<p>在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>更改 dashboard 的 svc 为 NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）</em></p>\n<p>查看端口号：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：</p>\n<p>访问 Dashboard：<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>创建登录 Token：</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"10必看一些必须的配置更改\"><a class=\"anchor\" href=\"#10必看一些必须的配置更改\">#</a> 10.【必看】一些必须的配置更改</h4>\n<p>将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：</p>\n<p>在 master01 节点执行：</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\nmode: ipvs\n</code></pre>\n<p>更新 Kube-Proxy 的 Pod：</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>验证 Kube-Proxy 模式:</p>\n<pre><code>[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode\nipvs\n</code></pre>\n<h4 id=\"11必看注意事项\"><a class=\"anchor\" href=\"#11必看注意事项\">#</a> 11.【必看】注意事项</h4>\n<p>注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。</p>\n<p>启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。</p>\n<p>其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。</p>\n<p>kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\n</code></pre>\n<p>进行更改，更改完成后，可以通过 patch 重启 kube-proxy</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-\n</code></pre>\n<h4 id=\"12-containerd配置镜像加速\"><a class=\"anchor\" href=\"#12-containerd配置镜像加速\">#</a> 12. Containerd 配置镜像加速</h4>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>所有节点重新启动 Containerd：</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"13-docker配置镜像加速\"><a class=\"anchor\" href=\"#13-docker配置镜像加速\">#</a> 13. Docker 配置镜像加速</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>所有节点重新启动 Docker：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/1922841233.html",
            "url": "http://ixuyong.cn/posts/1922841233.html",
            "title": "Rsync服务实践",
            "date_published": "2025-03-30T12:45:48.000Z",
            "content_html": "<h3 id=\"ursync服务实践u\"><a class=\"anchor\" href=\"#ursync服务实践u\">#</a> <u>Rsync 服务实践</u></h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"1rsync服务端\"><a class=\"anchor\" href=\"#1rsync服务端\">#</a> 1.rsync 服务端</h4>\n<h5 id=\"11-关闭防火墙-selinux\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync\"><a class=\"anchor\" href=\"#12-安装rsync\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件\"><a class=\"anchor\" href=\"#13-修改配置文件\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-rsync客户端\"><a class=\"anchor\" href=\"#2-rsync客户端\">#</a> 2. rsync 客户端</h4>\n<h5 id=\"21-安装rsync\"><a class=\"anchor\" href=\"#21-安装rsync\">#</a> 2.1 安装 rsync</h5>\n<pre><code>[root@db01 ~]# yum install nfs-utils -y\n</code></pre>\n<h5 id=\"22-配置传输密码\"><a class=\"anchor\" href=\"#22-配置传输密码\">#</a> 2.2 配置传输密码</h5>\n<p>方法 1：将密码写入文件</p>\n<pre><code>[root@db01 ~]#  echo 'your passwd' &gt; /etc/rsync.pass\n[root@db01 ~]# cat /etc/rsync.pass \nyour passwd\n[root@db01 ~]# chmod 600 /etc/rsync.pass\n--测试收发数据：\n[root@db01 ~]# rsync -avz --password-file=/etc/rsync.pass /root/test rsync_backup@192.168.40.101::backup_file\nsending incremental file list\n\nsent 47 bytes  received 20 bytes  134.00 bytes/sec\ntotal size is 0  speedup is 0.00\n</code></pre>\n<p>方法 2：使用密码环境变量 RSYNC_PASSWORD</p>\n<pre><code>[root@db01 ~]# export RSYNC_PASSWORD='your passwd'\n--测试收发数据：\n[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file\nsending incremental file list\n\nsent 47 bytes  received 20 bytes  134.00 bytes/sec\ntotal size is 0  speedup is 0.00\n</code></pre>\n<h3 id=\"ursync企业级备份案例u\"><a class=\"anchor\" href=\"#ursync企业级备份案例u\">#</a> <u>Rsync 企业级备份案例</u></h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<p><strong>客户端需求</strong></p>\n<ul>\n<li>客户端每天凌晨 3 点备份 MySQL 至 /backup 下以 &quot;主机名_IP 地址_当前时间命名&quot; 的目录中</li>\n<li>客户端推送 /backup 目录下数据备份目录至 Rsync 备份服务器</li>\n<li>客户端只保留最近七天的备份数据，避免浪费磁盘空间</li>\n</ul>\n<p><strong>服务端需求</strong></p>\n<ul>\n<li>服务端部署 rsync 服务，用于接收用户的备份数据</li>\n<li>服务端每天校验客户端推送过来的数据是否完整，并将结果以邮件的方式发送给管理员</li>\n<li>服务端仅保留 6 个月的备份数据</li>\n</ul>\n<p><strong>注意</strong>：所有服务器的备份目录均为 /backup，所有脚本存放目录均为 /scripts。</p>\n<h4 id=\"1-服务端部署rsync服务\"><a class=\"anchor\" href=\"#1-服务端部署rsync服务\">#</a> <strong>1. 服务端部署 rsync 服务</strong></h4>\n<h5 id=\"11-关闭防火墙-selinux-2\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux-2\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync-2\"><a class=\"anchor\" href=\"#12-安装rsync-2\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件-2\"><a class=\"anchor\" href=\"#13-修改配置文件-2\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限-2\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限-2\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启-2\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启-2\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-rsync客户端-2\"><a class=\"anchor\" href=\"#2-rsync客户端-2\">#</a> 2. rsync 客户端</h4>\n<h5 id=\"21-安装rsync-2\"><a class=\"anchor\" href=\"#21-安装rsync-2\">#</a> 2.1 安装 rsync</h5>\n<pre><code>[root@db01 ~]# yum install nfs-utils -y\n</code></pre>\n<h5 id=\"22-测试客户端备份数据并推送至rsync服务器\"><a class=\"anchor\" href=\"#22-测试客户端备份数据并推送至rsync服务器\">#</a> 2.2 测试客户端备份数据并推送至 rsync 服务器</h5>\n<pre><code>[root@db01 ~]# export RSYNC_PASSWORD='your passwd'\n[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file\n</code></pre>\n<h5 id=\"23-客户端备份数据并推送至rsync服务器\"><a class=\"anchor\" href=\"#23-客户端备份数据并推送至rsync服务器\">#</a> <strong>2.3 客户端备份数据并推送至 rsync 服务器</strong></h5>\n<pre><code>[root@db01 ~]# mkdir /scripts\n[root@db01 ~]# cat /scripts/mysql_backup.sh \n#!/bin/bash\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\n\n#1、定义变量\nHost=$(hostname)\nIp=$(ifconfig ens192 | awk 'NR==2&#123;print $2&#125;')\nDate=$(date +%F)\nBackupDir=/backup/mysql\nDest=$&#123;BackupDir&#125;/$&#123;Host&#125;_$&#123;Ip&#125;_$&#123;Date&#125;\nFILE_NAME=mysql_backup_`date '+%Y%m%d%H%M%S'`;\nOLDBINLOG=/var/lib/mysql/oldbinlog\n\n#2、创建备份目录\nif [ ! -d $Dest ];then\n  mkdir -p $Dest\nfi\n\n#3、备份目录\n/usr/bin/mysqldump -u'root' -p'your passwd' nf_flms &gt; $Dest/nf-flms_$&#123;FILE_NAME&#125;.sql\ntar -czvf $Dest/$&#123;FILE_NAME&#125;.tar.gz $Dest/nf-flms_$&#123;FILE_NAME&#125;.sql\nrm -rf $Dest/*$&#123;FILE_NAME&#125;.sql\necho &quot;Your database backup successfully&quot;\n\n#4、校验\nmd5sum $Dest/* &gt;$Dest/backup_check_$Date\n\n#5、将备份目录推动到rsync服务端\nRsync_Ip=192.168.1.145\nRsync_user=rsync_backup\nRsync_Module=backup_mysql\nexport RSYNC_PASSWORD=your passwd\nrsync -avz $Dest $Rsync_user@$Rsync_Ip::$Rsync_Module\n\n#6、删除15天备份目录\nfind $Dest -type d -mtime +15 | xargs rm -rf\necho &quot;remove file  successfully&quot;\n\n[root@db01 ~]# chmod +x /scripts/etc_backup.sh\n[root@db01 ~]# crontab -e\n00 03 * * * /bin/bash /scripts/mysql_backup.sh &amp;&gt; /dev/null\n</code></pre>\n<h5 id=\"24-服务端校验数据并将结果以邮件发送给管理员\"><a class=\"anchor\" href=\"#24-服务端校验数据并将结果以邮件发送给管理员\">#</a> <strong>2.4 服务端校验数据并将结果以邮件发送给管理员</strong></h5>\n<h6 id=\"241-配置邮件服务\"><a class=\"anchor\" href=\"#241-配置邮件服务\">#</a> 2.4.1 配置邮件服务</h6>\n<pre><code>[root@backup ~]# yum -y install mailx\n[root@backup ~]# cat /etc/mail.rc      #最后一行插入\nset from=373370405@qq.com\nset smtp=smtps://smtp.qq.com:465\nset smtp-auth-user=373370405@qq.com\nset smtp-auth-password=**********   # 发件邮箱的授权码\nset smtp-auth=login\nset ssl-verify=ignore\nset nss-config-dir=/etc/pki/nssdb\n</code></pre>\n<h6 id=\"242-发送邮件测试\"><a class=\"anchor\" href=\"#242-发送邮件测试\">#</a> 2.4.2 发送邮件测试</h6>\n<pre><code>[root@backup ~]#  echo Hello World | mail -s test 373370405@qq.com &amp;&gt; /dev/null\n</code></pre>\n<h6 id=\"243-配置脚本校验数据并将结果发送给管理员\"><a class=\"anchor\" href=\"#243-配置脚本校验数据并将结果发送给管理员\">#</a> 2.4.3 配置脚本校验数据并将结果发送给管理员</h6>\n<pre><code>[root@backup mysql]# cat /scripts/check_backup.sh \n#!/bin/bash\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\n\n#1、定义变量\nPath=/backup/mysql\nDate=$(date +%F)\n\n#2、查看flag文件，并对对文件进行校验,然后将校验的结果保存至result_时间\nfind $Path -type f -name &quot;backup_check_$&#123;Date&#125;*&quot;|xargs md5sum -c &gt;$Path/result_$&#123;Date&#125;\n\n#3、将校验结果发送邮件给管理员\nmail -s &quot;Mysql Backup&quot; 373370405@qq.com &lt;$Path/result_$&#123;Date&#125; &amp;&gt; /dev/null\n\n#4、删除超过7天的校验结果文件，删除超过180天的备份数据文件\nfind $Path -type f -name &quot;result*&quot; -mtime +7 | xargs rm -rf\nfind $Path -type f -mtime +180 | xargs rm -rf\n</code></pre>\n<h6 id=\"244-写计划任务\"><a class=\"anchor\" href=\"#244-写计划任务\">#</a> <strong>2.4.4 写计划任务</strong></h6>\n<pre><code>[root@backup ~]# chmod +x /scripts/check_backup.sh \n[root@db01 ~]# crontab -e\n00 06 * * * /bin/bash /scripts/mysql_backup.sh &amp;&gt; /dev/null\n</code></pre>\n<h3 id=\"rsyncsersync实现数据实时同步\"><a class=\"anchor\" href=\"#rsyncsersync实现数据实时同步\">#</a> Rsync+sersync 实现数据实时同步</h3>\n<p><strong>环境准备</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">主机名</th>\n<th style=\"text-align:center\"><strong>IP</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">server</td>\n<td style=\"text-align:center\">192.168.40.101</td>\n<td>rsync 服务端</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">client</td>\n<td style=\"text-align:center\">192.168.40.102</td>\n<td>rsync 客户</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"1rsync服务端-2\"><a class=\"anchor\" href=\"#1rsync服务端-2\">#</a> 1.rsync 服务端</h4>\n<h5 id=\"11-关闭防火墙-selinux-3\"><a class=\"anchor\" href=\"#11-关闭防火墙-selinux-3\">#</a> 1.1 关闭防火墙、selinux</h5>\n<pre><code>[root@localhost ~]# hostnamectl set-hostname backup\n[root@localhost ~]# bash\n[root@backup ~]# hostnamectl set-hostname aizj_lb01\n[root@backup ~]# systemctl stop firewalld\n[root@backup ~]# systemctl disable firewalld\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@backup ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y\n[root@backup ~]# yum update -y --exclude=kernel* &amp;&amp; reboot\n[root@backup ~]# echo 'Asia/Shanghai' &gt;/etc/timezone\n[root@backup ~]# ntpdate time2.aliyun.com\n[root@backup ~]# crontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;&gt; /dev/nul\n[root@backup ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h5 id=\"12-安装rsync-3\"><a class=\"anchor\" href=\"#12-安装rsync-3\">#</a> 1.2 安装 rsync</h5>\n<pre><code>[root@backup ~]# yum install -y rsync\n[root@server ~]# systemctl start rsyncd\n[root@server ~]# systemctl enable rsyncd\n[root@backup ~]# useradd -M -s /sbin/nologin rsync\n[root@backup ~]# mkdir -p /backup/mysql  /backup/file\n[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file \n</code></pre>\n<h5 id=\"13-修改配置文件-3\"><a class=\"anchor\" href=\"#13-修改配置文件-3\">#</a> 1.3 修改配置文件</h5>\n<p><em><mark>#生产环境中取消注释，导致备份数据报错</mark></em></p>\n<pre><code>#带注释配置文件\n[root@backup ~]# vim /etc/rsyncd.conf\nuid = rsync             #运行服务的用户\ngid = rsync             #运行服务的组\nport = 873              #服务监听端口\nfake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性\nuse chroot = no         #禁锢目录,不允许获取root权限\nmax connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接\ntimeout = 600           #超时时间\nignore errors          #忽略错误\nread only = false      #客户是否只读\nlist = false           #不允许查看模块信息\nauth users = rsync_backup         #定义虚拟用户，用户数据传输\nsecrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件\nlog file = /var/log/rsyncd.log    #日志文件存放的位置\n[backup_mysql]         #模块名\ncomment = welcome to rsync_backup\npath = /backup/mysql   #数据存放目录\n[backup_file]          #模块名\ncomment = welcome to rsync_backup\npath = /backup/file    #数据存放目录 \n\n#不带注释配置文件\n[root@backup ~]# cat /etc/rsyncd.conf\nuid = rsync        \ngid = rsync         \nport = 873     \nfake super = yes     \nuse chroot = no        \nmax connections = 200  \ntimeout = 600         \nignore errors       \nread only = false    \nlist = false          \nauth users = rsync_backup        \nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log    \n[backup_mysql]       \ncomment = welcome to rsync_backup\npath = /backup/mysql  \n[backup_file]         \ncomment = welcome to rsync_backup\npath = /backup/file \n</code></pre>\n<h5 id=\"4-创建虚拟用户密码文件并设置权限-3\"><a class=\"anchor\" href=\"#4-创建虚拟用户密码文件并设置权限-3\">#</a> 4. 创建虚拟用户密码文件并设置权限</h5>\n<pre><code>[root@backup ~]# cat /etc/rsync.passwd\nrsync_backup:your passwd\n[root@backup ~]# chmod 600 /etc/rsync.passwd\n[root@backup ~]# systemctl restart rsyncd &amp;&amp; systemctl status rsyncd\n</code></pre>\n<h5 id=\"5-检查服务端口是否开启-3\"><a class=\"anchor\" href=\"#5-检查服务端口是否开启-3\">#</a> 5. 检查服务端口是否开启</h5>\n<pre><code>[root@backup ~]# netstat -lntp | grep &quot;rsync&quot;\ntcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         \ntcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync \n</code></pre>\n<h4 id=\"2-客户端安装sersync\"><a class=\"anchor\" href=\"#2-客户端安装sersync\">#</a> 2. 客户端安装 sersync</h4>\n<p><strong>2.1 安装 sercync 依赖</strong></p>\n<pre><code>[root@nfs ~]# yum install -y inotify-tools rsync\n</code></pre>\n<p><strong>2.2 安装 sercync</strong></p>\n<pre><code>[root@nfs ~]# mkdir -p /soft\n[root@nfs ~]# cd /soft/\n[root@nfs ~]# wget https://down.whsir.com/downloads/sersync2.5.4_64bit_binary_stable_final.tar.gz\n[root@nfs soft]# tar -xf sersync2.5.4_64bit_binary_stable_final.tar.gz\n[root@nfs soft]# mv GNU-Linux-x86 /usr/local/sersync\n</code></pre>\n<h5 id=\"23-修改配置文件\"><a class=\"anchor\" href=\"#23-修改配置文件\">#</a> 2.3 <strong>修改配置文件</strong></h5>\n<pre><code>[root@nfs soft]# cd /usr/local/sersync/\n[root@nfs sersync]# cp confxml.xml confxml.xml.bak\n[root@nfs sersync]# vim confxml.xml\n...\n5    &lt;fileSystem xfs=&quot;true&quot;/&gt;    #第5行 false改为true\n13          &lt;delete start=&quot;true&quot;/&gt; #第13-20行 false改为true,#说明：监控以上变化推送\n14        &lt;createFolder start=&quot;true&quot;/&gt;\n15        &lt;createFile start=&quot;false&quot;/&gt;\n16        &lt;closeWrite start=&quot;true&quot;/&gt;\n17        &lt;moveFrom start=&quot;true&quot;/&gt;\n18        &lt;moveTo start=&quot;true&quot;/&gt;\n19        &lt;attrib start=&quot;true&quot;/&gt;\n20        &lt;modify start=&quot;true&quot;/&gt;\n24        &lt;localpath watch=&quot;/data&quot;&gt;      #监控的本地目录\n25             &lt;remote ip=&quot;192.168.1.145&quot; name=&quot;backup_file&quot;/&gt;  #rsync服务端IP和模块名backup_file\n30      &lt;commonParams params=&quot;-avz&quot;/&gt;  #rsync命令选项\n31      &lt;auth start=&quot;true&quot; users=&quot;rsync_backup&quot; passwordfile=&quot;/etc/rsync.passwd&quot;/&gt; #rsync认证信息\n...\n</code></pre>\n<h5 id=\"24-生成密码文件\"><a class=\"anchor\" href=\"#24-生成密码文件\">#</a> 2.4 生成密码文件</h5>\n<pre><code>[root@nfs sersync]# echo 'your passwd' &gt; /etc/rsync.passwd\n[root@nfs sersync]# chmod 600 /etc/rsync.passwd\n</code></pre>\n<h5 id=\"25-启动sersync\"><a class=\"anchor\" href=\"#25-启动sersync\">#</a> 2.5 启动 sersync</h5>\n<pre><code>[root@nfs sersync]# ln -s /usr/local/sersync/sersync2 /usr/bin/\n[root@nfs sersync]# sersync2 -dro /usr/local/sersync/confxml.xml     #针对配置文件confxml.xml启动sersync\n</code></pre>\n<p><strong>2.5 设置 sersync 开机自启</strong></p>\n<pre><code>[root@qzj_nfs sersync]# vim /etc/rc.d/rc.local   \n/usr/local/sersync/sersync2 -d -r -o  /usr/local/sersync/confxml.xml  #在最后添加一行\n[root@qzj_nfs sersync]# chmod +x /etc/rc.d/rc.local\n</code></pre>\n<p><strong>2.6 测试</strong></p>\n<p><em>在客户端 /data 目录增删改目录文件，rsync 服务端数据存放目录变化</em></p>\n<pre><code>[root@backup backup]# watch ls\n</code></pre>\n<p><strong>2.7 添加脚本监控 sersync 是否正常运行</strong></p>\n<pre><code>[root@nfs sersync]# cat /scripts/check_sersync.sh \n#!/bin/sh\nsersync=&quot;/usr/local/sersync/sersync2&quot;\nconfxml=&quot;/usr/local/sersync/confxml.xml&quot;\nstatus=$(ps aux |grep 'sersync2'|grep -v 'grep'|wc -l)\nif [ $status -eq 0 ];\nthen\n$sersync -d -r -o $confxml &amp;\nelse\nexit 0;\nfi\n\n[root@nfs sersync]# chmod +x /scripts/check_sersync.sh\n[root@nfs sersync]# crontab -l\n*/5 * * * * /usr/bin/sh /scripts/check_sersync.sh &amp;&gt; /dev/null\n</code></pre>\n<p><em><strong>补充： 多实例情况</strong></em><br />\n 1、配置多个 confxml.xml 文件（比如：www、bbs、blog.... 等等）<br />\n2、修改端口、同步路径、模块名称<br />\n 3、根据不同的需求同步对应的实例文件<br />\n /usr/local/sersync/sersync2 -dro /usr/local/sersync/www_confxml.xml<br />\n/usr/local/sersync/sersync2 -dro /usr/local/sersync/bbs_confxml.xml</p>\n",
            "tags": [
                "rsync"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3071070978.html",
            "url": "http://ixuyong.cn/posts/3071070978.html",
            "title": "企业级私有仓库Harbor搭建",
            "date_published": "2025-03-30T08:17:00.000Z",
            "content_html": "<h3 id=\"企业级私有仓库harbor\"><a class=\"anchor\" href=\"#企业级私有仓库harbor\">#</a> 企业级私有仓库 Harbor</h3>\n<p>企业部署 Kuberetes 集群环境之后，我们就可以将原来在传统虚拟机上运行的业务，迁移到 kubernetes 上，让 Kubernetes 通过容器的方式来管理。而一旦我们需要将传统业务使用容器的方式运行起来，就需要构建很多镜像，那么这些镜像就需要有一个专门的位置存储起来，为我们提供镜像上传和镜像下载等功能。但我们不能使用阿里云或者 Dockerhub 等仓库，首先拉取速度比较慢，其次镜像的安全性无法保证，所以就需要部署一个私有的镜像仓库来管理这些容器镜像。同时该仓库还需要提供高可用功能，确保随时都能上传和下载可用的容器镜像。</p>\n<h4 id=\"1-关闭防火墙-selinux-环境配置\"><a class=\"anchor\" href=\"#1-关闭防火墙-selinux-环境配置\">#</a> 1、关闭防火墙、Selinux、环境配置</h4>\n<pre><code>[root@harbor ~]# sudo mkdir -p /etc/docker\n[root@harbor ~]# hostnamectl set-hostname harbor\n[root@harbor ~]# systemctl stop firewalld\n[root@harbor ~]# systemctl disable firewalld\n[root@harbor ~]# sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux\n[root@harbor ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate -y\n[root@harbor ~]# yum update -y\n[root@harbor ~]# mkdir /soft /data /scripts /backup\n</code></pre>\n<h4 id=\"2-docker安装\"><a class=\"anchor\" href=\"#2-docker安装\">#</a> 2、Docker 安装</h4>\n<pre><code>[root@harbor ~]# yum install -y yum-utils device-mapper-persistent-data lvm2\n[root@harbor ~]# curl -o /etc/yum.repos.d/docker-ce.repo  https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n[root@harbor ~]# yum list docker-ce --showduplicates |sort -r \n[root@harbor ~]# yum install docker-ce docker-compose -y\n</code></pre>\n<h4 id=\"3-配置docker加速\"><a class=\"anchor\" href=\"#3-配置docker加速\">#</a> 3、配置 Docker 加速</h4>\n<pre><code>[root@harbor ~]# sudo mkdir -p /etc/docker\n[root@harbor ~]# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n[root@harbor ~]# systemctl enable docker --now\n</code></pre>\n<h4 id=\"4-安装harbor\"><a class=\"anchor\" href=\"#4-安装harbor\">#</a> 4、安装 Harbor</h4>\n<pre><code>[root@harbor ~]# cd /soft/\n[root@harbor ~]# wget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz\n[root@harbor soft]# tar xf harbor-offline-installer-v2.6.1.tgz\n[root@harbor soft]# cd harbor\n[root@harbor harbor]# vim harbor.yml\nhostname: 192.168.1.134\n...\n#https:\n#  # https port for harbor, default is 443\n#  port: 443\n#  # The path of cert and key files for nginx\n#  certificate: /your/certificate/path\n#  private_key: /your/private/key/path\n...\nharbor_admin_password: Harbor12345\n[root@harbor harbor]#  ./install.sh\n</code></pre>\n<h4 id=\"5-配置nginx负载均衡调度\"><a class=\"anchor\" href=\"#5-配置nginx负载均衡调度\">#</a> 5、配置 Nginx 负载均衡调度</h4>\n<pre><code>[root@lb ~]# vim s.hmallleasing.com.conf\nserver &#123;\n    listen 443 ssl;\n    server_name harbor.hmallleasing.com;\n    client_max_body_size 1G; \n    ssl_prefer_server_ciphers on;\n    ssl_certificate  /etc/nginx/sslkey/_.hmallleasing.com_chain.crt;\n    ssl_certificate_key  /etc/nginx/sslkey/_.hmallleasing.com_key.key;\n    location / &#123;\n        proxy_pass http://192.168.1.134;\n#      include proxy_params;\n#        proxy_set_header Host $http_host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        proxy_connect_timeout 30;\n        proxy_send_timeout 60;\n        proxy_read_timeout 60;\n        \n        proxy_buffering on;\n        proxy_buffer_size 32k;\n        proxy_buffers 4 128k;\n        proxy_temp_file_write_size 10240k;\t\t\n        proxy_max_temp_file_size 10240k;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n</code></pre>\n<h4 id=\"6-推送镜像至harbor\"><a class=\"anchor\" href=\"#6-推送镜像至harbor\">#</a> 6、推送镜像至 Harbor</h4>\n<pre><code>[root@harbor harbor]# docker tag beae173ccac6 harbor.hmallleasing.com/ops/busybox.v1\n[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1\n[root@harbor harbor]# docker login harbor.hmallleasing.com\n[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1\n</code></pre>\n<h4 id=\"7-harbor停止与启动\"><a class=\"anchor\" href=\"#7-harbor停止与启动\">#</a> 7、Harbor 停止与启动</h4>\n<pre><code>#停用Harbor\n[root@harbor harbor]# pwd\n/soft/harbor\n[root@harbor harbor]# docker-compose stop\n #启动Harbor\n[root@harbor harbor]# docker-compose up -d\n[root@harbor harbor]# docker-compose start\n</code></pre>\n",
            "tags": [
                "Harbor"
            ]
        }
    ]
}