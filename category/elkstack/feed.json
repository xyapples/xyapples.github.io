{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生 • All posts by \"elkstack\" category",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://ixuyong.cn",
    "items": [
        {
            "id": "http://ixuyong.cn/posts/170066797.html",
            "url": "http://ixuyong.cn/posts/170066797.html",
            "title": "消费租赁项目Kubernetes基于ELK日志分析与实践",
            "date_published": "2025-05-25T06:35:21.000Z",
            "content_html": "<h3 id=\"消费租赁项目kubernetes基于elk日志分析与实践\"><a class=\"anchor\" href=\"#消费租赁项目kubernetes基于elk日志分析与实践\">#</a> 消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</h3>\n<h6 id=\"snipaste_2025-05-25_13-43-46jpg\"><a class=\"anchor\" href=\"#snipaste_2025-05-25_13-43-46jpg\">#</a> <img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg\" alt=\"Snipaste_2025-05-25_13-43-46.jpg\" /></h6>\n<h4 id=\"1-elk创建namespace和secrets\"><a class=\"anchor\" href=\"#1-elk创建namespace和secrets\">#</a> 1. ELK 创建 Namespace 和 Secrets</h4>\n<pre><code># kubectl create ns logging\n# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd\n</code></pre>\n<h4 id=\"2-交付zookeeper集群至k8s\"><a class=\"anchor\" href=\"#2-交付zookeeper集群至k8s\">#</a> 2. 交付 Zookeeper 集群至 K8S</h4>\n<h5 id=\"21-制作zk集群镜像\"><a class=\"anchor\" href=\"#21-制作zk集群镜像\">#</a> 2.1 制作 ZK 集群镜像</h5>\n<h6 id=\"211-dockerfile\"><a class=\"anchor\" href=\"#211-dockerfile\">#</a> 2.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、拷贝Zookeeper压缩包和配置文件\nENV VERSION=3.8.4\nADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf\n\n# 2、对Zookeeper文件夹名称重新命名\nRUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper\n\n# 3、拷贝eentrpoint的启动脚本文件\nADD ./entrypoint.sh /entrypoint.sh\n\n# 4、暴露Zookeeper端口\nEXPOSE 2181 2888 3888\n\n# 5、执行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"212-zoocfg\"><a class=\"anchor\" href=\"#212-zoocfg\">#</a> 2.1.2 zoo.cfg</h6>\n<pre><code># cat zoo.cfg \n# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。\ntickTime=&#123;ZOOK_TICKTIME&#125;\n\n# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime\ninitLimit=&#123;ZOOK_INIT_LIMIT&#125;\n\n# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime\nsyncLimit=&#123;ZOOK_SYNC_LIMIT&#125;\n \n# 数据保存目录\ndataDir=&#123;ZOOK_DATA_DIR&#125;\n\n# 日志保存目录\ndataLogDir=&#123;ZOOK_LOG_DIR&#125;\n\n# 客户端连接端口\nclientPort=&#123;ZOOK_CLIENT_PORT&#125;\n\n# 客户端最大连接数。# 根据自己实际情况设置，默认为60个\nmaxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;\n\n# 客户端获取 zookeeper 服务的当前状态及相关信息\n4lw.commands.whitelist=*\n\n# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口\n</code></pre>\n<h6 id=\"213-entrypoint\"><a class=\"anchor\" href=\"#213-entrypoint\">#</a> 2.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n#设定变量\nZOOK_BIN_DIR=/zookeeper/bin\nZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg\n\n# 2、对配置文件中的字符串进行变量替换\nsed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;\n\n# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来\nfor server in $&#123;ZOOK_SERVERS&#125;\ndo\n\techo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;\ndone\n\n# 4、在datadir目录中创建myid的文件，并填入对应的编号\nZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))\necho $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid\n\n#5、前台运行Zookeeper\ncd $&#123;ZOOK_BIN_DIR&#125;\n./zkServer.sh start-foreground\n</code></pre>\n<h6 id=\"214-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#214-构建镜像并推送仓库\">#</a> 2.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .\n# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4\n</code></pre>\n<h5 id=\"22-迁移zookeeper至k8s\"><a class=\"anchor\" href=\"#22-迁移zookeeper至k8s\">#</a> 2.2  迁移 zookeeper 至 K8S</h5>\n<h6 id=\"221-zookeeper-headless\"><a class=\"anchor\" href=\"#221-zookeeper-headless\">#</a> 2.2.1 zookeeper-headless</h6>\n<pre><code># cat 01-zookeeper-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: zookeeper\n  ports:\n  - name: client\n    port: 2181\n    targetPort: 2181\n  - name: leader-follwer\n    port: 2888\n    targetPort: 2888\n  - name: selection\n    port: 3888\n    targetPort: 3888\n</code></pre>\n<h6 id=\"222-zookeeper-sts\"><a class=\"anchor\" href=\"#222-zookeeper-sts\">#</a> 2.2.2 zookeeper-sts</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper               \n  namespace: logging\nspec:\n  serviceName: &quot;zookeeper-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;zookeeper&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: zookeeper\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 2181\n        - name: leader-follwer\n          containerPort: 2888\n        - name: selection\n          containerPort: 3888\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: data\n        - name: data\n          mountPath: /logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"223-更新资源清单\"><a class=\"anchor\" href=\"#223-更新资源清单\">#</a> 2.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml \n[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml\n[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging\nNAME          READY   STATUS    RESTARTS   AGE\nzookeeper-0   1/1     Running   0          17m\nzookeeper-1   1/1     Running   0          14m\nzookeeper-2   1/1     Running   0          11m\n</code></pre>\n<h6 id=\"224-检查zookeeper集群状态\"><a class=\"anchor\" href=\"#224-检查zookeeper集群状态\">#</a> 2.2.4 检查 zookeeper 集群状态</h6>\n<pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: leader\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\n</code></pre>\n<h6 id=\"225-连接zookeeper集群\"><a class=\"anchor\" href=\"#225-连接zookeeper集群\">#</a> 2.2.5 连接 Zookeeper 集群</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh\n# /zookeeper/bin/zkCli.sh -server zookeeper-svc\n[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu\nCreated /hello\n[zk: zookeeper-svc(CONNECTED) 1] get /hello\noldxu\n</code></pre>\n<h4 id=\"3-交付kafka集群至k8s\"><a class=\"anchor\" href=\"#3-交付kafka集群至k8s\">#</a> 3. 交付 Kafka 集群至 K8S</h4>\n<h5 id=\"31-制作kafka集群镜像\"><a class=\"anchor\" href=\"#31-制作kafka集群镜像\">#</a> 3.1 制作 Kafka 集群镜像</h5>\n<h6 id=\"311-dockerfile\"><a class=\"anchor\" href=\"#311-dockerfile\">#</a> 3.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=2.12-2.2.0\nADD ./kafka_$&#123;VERSION&#125;.tgz /\nADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties\n\n# 3、修改kafka的名称\nRUN mv /kafka_$&#123;VERSION&#125; /kafka\n\n# 4、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 5、暴露kafka端口 9999是jmx的端口\nEXPOSE 9092 9999\n\n# 6、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"312-serverproperties\"><a class=\"anchor\" href=\"#312-serverproperties\">#</a> 3.1.2 server.properties</h6>\n<pre><code class=\"language-'\"># cat server.properties \n############################# Server Basics ############################# \n# broker的id，值为整数，且必须唯一，在一个集群中不能重复\nbroker.id=&#123;BROKER_ID&#125;\n\n############################# Socket Server Settings ############################# \n# kafka监听端口，默认9092\nlisteners=PLAINTEXT://&#123;LISTENERS&#125;:9092\n\n# 处理网络请求的线程数量，默认为3个\nnum.network.threads=3\n\n# 执行磁盘IO操作的线程数量，默认为8个 \nnum.io.threads=8\n\n# socket服务发送数据的缓冲区大小，默认100KB\nsocket.send.buffer.bytes=102400\n\n# socket服务接受数据的缓冲区大小，默认100KB\nsocket.receive.buffer.bytes=102400\n\n# socket服务所能接受的一个请求的最大大小，默认为100M\nsocket.request.max.bytes=104857600\n\n############################# Log Basics ############################# \n# kafka存储消息数据的目录\nlog.dirs=&#123;KAFKA_DATA_DIR&#125;\n\n# 每个topic默认的partition\nnum.partitions=1\n\n# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。\ndefault.replication.factor=3\n\n# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量\nnum.recovery.threads.per.data.dir=1\n\n############################# Log Flush Policy ############################# \n# 消息刷新到磁盘中的消息条数阈值\nlog.flush.interval.messages=10000\n\n# 消息刷新到磁盘中的最大时间间隔,1s\nlog.flush.interval.ms=1000\n\n############################# Log Retention Policy ############################# \n# 日志保留小时数，超时会自动删除，默认为7天\nlog.retention.hours=168\n\n# 日志保留大小，超出大小会自动删除，默认为1G\n#log.retention.bytes=1073741824\n\n# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件\nlog.segment.bytes=1073741824\n\n# 每隔多长时间检测数据是否达到删除条件,300s\nlog.retention.check.interval.ms=300000\n\n############################# Zookeeper ############################# \n# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开\nzookeeper.connect=&#123;ZOOK_SERVERS&#125;\n\n# 连接zookeeper的超时时间,6s\nzookeeper.connection.timeout.ms=6000\n</code></pre>\n<h6 id=\"313-entrypoint\"><a class=\"anchor\" href=\"#313-entrypoint\">#</a> 3.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 变量\nKAFKA_DIR=/kafka\nKAFKA_CONF=/kafka/config/server.properties\n\n# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一\nBROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))\nLISTENERS=$(hostname -i)\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;\n\n# 3、启动Kafka\ncd $&#123;KAFKA_DIR&#125;/bin\nsed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh\n./kafka-server-start.sh ../config/server.properties\n</code></pre>\n<h6 id=\"314-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#314-构建镜像并推送仓库\">#</a> 3.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2\n</code></pre>\n<h5 id=\"32-迁移kafka集群至k8s\"><a class=\"anchor\" href=\"#32-迁移kafka集群至k8s\">#</a> 3.2 迁移 Kafka 集群至 K8S</h5>\n<h6 id=\"321-kafka-headless\"><a class=\"anchor\" href=\"#321-kafka-headless\">#</a> 3.2.1 kafka-headless</h6>\n<pre><code># cat 01-kafka-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: kafka\n  ports:\n  - name: client\n    port: 9092\n    targetPort: 9092\n  - name: jmx\n    port: 9999\n    targetPort: 9999\n</code></pre>\n<h6 id=\"322-kafka-sts\"><a class=\"anchor\" href=\"#322-kafka-sts\">#</a> 3.2.2 kafka-sts</h6>\n<pre><code># cat 02-kafka-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  namespace: logging\nspec:\n  serviceName: &quot;kafka-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kafka\n  template:\n    metadata:\n      labels:\n        app: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;kafka&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kafka\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 9092\n        - name: jmxport\n          containerPort: 9999\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"323-更新资源清单\"><a class=\"anchor\" href=\"#323-更新资源清单\">#</a> 3.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml \n[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml\n[root@k8s-master01 02-kafka]# kubectl get pods -n logging \nNAME          READY   STATUS    RESTARTS       AGE\nkafka-0       1/1     Running   0              5m49s\nkafka-1       1/1     Running   0              4m43s\nkafka-2       1/1     Running   0              3m40s\n</code></pre>\n<h6 id=\"324-检查kafka集群\"><a class=\"anchor\" href=\"#324-检查kafka集群\">#</a> 3.2.4 检查 Kafka 集群</h6>\n<pre><code>1.创建一个topic\nroot@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu\n\n2.模拟消息发布\nroot@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu\n&gt;hello kubernetes\n&gt;hello world\n\n3.模拟消息订阅\nroot@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning\nhello kubernetes\nhello world\n</code></pre>\n",
            "tags": [
                "ELKStack"
            ]
        }
    ]
}