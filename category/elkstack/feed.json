{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生 • All posts by \"elkstack\" category",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://ixuyong.cn",
    "items": [
        {
            "id": "http://ixuyong.cn/posts/570469260.html",
            "url": "http://ixuyong.cn/posts/570469260.html",
            "title": "ELK收集Kubernetes组件日志分析与实践",
            "date_published": "2025-06-05T11:06:21.000Z",
            "content_html": "<h3 id=\"elk收集kubernetes组件日志分析与实践\"><a class=\"anchor\" href=\"#elk收集kubernetes组件日志分析与实践\">#</a> ELK 收集 Kubernetes 组件日志分析与实践</h3>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg\" alt=\"Snipaste_2025-05-25_13-43-46.jpg\" /></p>\n<h4 id=\"一-elk创建namespace和secrets\"><a class=\"anchor\" href=\"#一-elk创建namespace和secrets\">#</a> 一、ELK 创建 Namespace 和 Secrets</h4>\n<pre><code># kubectl create ns logging\n# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd\n</code></pre>\n<h4 id=\"二-交付zookeeper集群至k8s\"><a class=\"anchor\" href=\"#二-交付zookeeper集群至k8s\">#</a> 二、交付 Zookeeper 集群至 K8S</h4>\n<h5 id=\"21-制作zk集群镜像\"><a class=\"anchor\" href=\"#21-制作zk集群镜像\">#</a> 2.1 制作 ZK 集群镜像</h5>\n<h6 id=\"211-dockerfile\"><a class=\"anchor\" href=\"#211-dockerfile\">#</a> 2.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、拷贝Zookeeper压缩包和配置文件\nENV VERSION=3.8.4\nADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf\n\n# 2、对Zookeeper文件夹名称重新命名\nRUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper\n\n# 3、拷贝eentrpoint的启动脚本文件\nADD ./entrypoint.sh /entrypoint.sh\n\n# 4、暴露Zookeeper端口\nEXPOSE 2181 2888 3888\n\n# 5、执行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"212-zoocfg\"><a class=\"anchor\" href=\"#212-zoocfg\">#</a> 2.1.2 zoo.cfg</h6>\n<pre><code># cat zoo.cfg \n# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。\ntickTime=&#123;ZOOK_TICKTIME&#125;\n\n# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime\ninitLimit=&#123;ZOOK_INIT_LIMIT&#125;\n\n# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime\nsyncLimit=&#123;ZOOK_SYNC_LIMIT&#125;\n \n# 数据保存目录\ndataDir=&#123;ZOOK_DATA_DIR&#125;\n\n# 日志保存目录\ndataLogDir=&#123;ZOOK_LOG_DIR&#125;\n\n# 客户端连接端口\nclientPort=&#123;ZOOK_CLIENT_PORT&#125;\n\n# 客户端最大连接数。# 根据自己实际情况设置，默认为60个\nmaxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;\n\n# 客户端获取 zookeeper 服务的当前状态及相关信息\n4lw.commands.whitelist=*\n\n# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口\n</code></pre>\n<h6 id=\"213-entrypoint\"><a class=\"anchor\" href=\"#213-entrypoint\">#</a> 2.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n#设定变量\nZOOK_BIN_DIR=/zookeeper/bin\nZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg\n\n# 2、对配置文件中的字符串进行变量替换\nsed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;\n\n# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来\nfor server in $&#123;ZOOK_SERVERS&#125;\ndo\n\techo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;\ndone\n\n# 4、在datadir目录中创建myid的文件，并填入对应的编号\nZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))\necho $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid\n\n#5、前台运行Zookeeper\ncd $&#123;ZOOK_BIN_DIR&#125;\n./zkServer.sh start-foreground\n</code></pre>\n<h6 id=\"214-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#214-构建镜像并推送仓库\">#</a> 2.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .\n# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4\n</code></pre>\n<h5 id=\"22-迁移zookeeper至k8s\"><a class=\"anchor\" href=\"#22-迁移zookeeper至k8s\">#</a> 2.2  迁移 zookeeper 至 K8S</h5>\n<h6 id=\"221-zookeeper-headless\"><a class=\"anchor\" href=\"#221-zookeeper-headless\">#</a> 2.2.1 zookeeper-headless</h6>\n<pre><code># cat 01-zookeeper-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: zookeeper\n  ports:\n  - name: client\n    port: 2181\n    targetPort: 2181\n  - name: leader-follwer\n    port: 2888\n    targetPort: 2888\n  - name: selection\n    port: 3888\n    targetPort: 3888\n</code></pre>\n<h6 id=\"222-zookeeper-sts\"><a class=\"anchor\" href=\"#222-zookeeper-sts\">#</a> 2.2.2 zookeeper-sts</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper               \n  namespace: logging\nspec:\n  serviceName: &quot;zookeeper-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;zookeeper&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: zookeeper\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 2181\n        - name: leader-follwer\n          containerPort: 2888\n        - name: selection\n          containerPort: 3888\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: data\n        - name: data\n          mountPath: /logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"223-更新资源清单\"><a class=\"anchor\" href=\"#223-更新资源清单\">#</a> 2.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml \n[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml\n[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging\nNAME          READY   STATUS    RESTARTS   AGE\nzookeeper-0   1/1     Running   0          17m\nzookeeper-1   1/1     Running   0          14m\nzookeeper-2   1/1     Running   0          11m\n</code></pre>\n<h6 id=\"224-检查zookeeper集群状态\"><a class=\"anchor\" href=\"#224-检查zookeeper集群状态\">#</a> 2.2.4 检查 zookeeper 集群状态</h6>\n<pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: leader\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\n</code></pre>\n<h6 id=\"225-连接zookeeper集群\"><a class=\"anchor\" href=\"#225-连接zookeeper集群\">#</a> 2.2.5 连接 Zookeeper 集群</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh\n# /zookeeper/bin/zkCli.sh -server zookeeper-svc\n[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu\nCreated /hello\n[zk: zookeeper-svc(CONNECTED) 1] get /hello\noldxu\n</code></pre>\n<h4 id=\"三-交付kafka集群至k8s\"><a class=\"anchor\" href=\"#三-交付kafka集群至k8s\">#</a> 三、 交付 Kafka 集群至 K8S</h4>\n<h5 id=\"31-制作kafka集群镜像\"><a class=\"anchor\" href=\"#31-制作kafka集群镜像\">#</a> 3.1 制作 Kafka 集群镜像</h5>\n<h6 id=\"311-dockerfile\"><a class=\"anchor\" href=\"#311-dockerfile\">#</a> 3.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=2.12-2.2.0\nADD ./kafka_$&#123;VERSION&#125;.tgz /\nADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties\n\n# 3、修改kafka的名称\nRUN mv /kafka_$&#123;VERSION&#125; /kafka\n\n# 4、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 5、暴露kafka端口 9999是jmx的端口\nEXPOSE 9092 9999\n\n# 6、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"312-serverproperties\"><a class=\"anchor\" href=\"#312-serverproperties\">#</a> 3.1.2 server.properties</h6>\n<pre><code class=\"language-'\"># cat server.properties \n############################# Server Basics ############################# \n# broker的id，值为整数，且必须唯一，在一个集群中不能重复\nbroker.id=&#123;BROKER_ID&#125;\n\n############################# Socket Server Settings ############################# \n# kafka监听端口，默认9092\nlisteners=PLAINTEXT://&#123;LISTENERS&#125;:9092\n\n# 处理网络请求的线程数量，默认为3个\nnum.network.threads=3\n\n# 执行磁盘IO操作的线程数量，默认为8个 \nnum.io.threads=8\n\n# socket服务发送数据的缓冲区大小，默认100KB\nsocket.send.buffer.bytes=102400\n\n# socket服务接受数据的缓冲区大小，默认100KB\nsocket.receive.buffer.bytes=102400\n\n# socket服务所能接受的一个请求的最大大小，默认为100M\nsocket.request.max.bytes=104857600\n\n############################# Log Basics ############################# \n# kafka存储消息数据的目录\nlog.dirs=&#123;KAFKA_DATA_DIR&#125;\n\n# 每个topic默认的partition\nnum.partitions=1\n\n# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。\ndefault.replication.factor=3\n\n# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量\nnum.recovery.threads.per.data.dir=1\n\n############################# Log Flush Policy ############################# \n# 消息刷新到磁盘中的消息条数阈值\nlog.flush.interval.messages=10000\n\n# 消息刷新到磁盘中的最大时间间隔,1s\nlog.flush.interval.ms=1000\n\n############################# Log Retention Policy ############################# \n# 日志保留小时数，超时会自动删除，默认为7天\nlog.retention.hours=168\n\n# 日志保留大小，超出大小会自动删除，默认为1G\n#log.retention.bytes=1073741824\n\n# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件\nlog.segment.bytes=1073741824\n\n# 每隔多长时间检测数据是否达到删除条件,300s\nlog.retention.check.interval.ms=300000\n\n############################# Zookeeper ############################# \n# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开\nzookeeper.connect=&#123;ZOOK_SERVERS&#125;\n\n# 连接zookeeper的超时时间,6s\nzookeeper.connection.timeout.ms=6000\n</code></pre>\n<h6 id=\"313-entrypoint\"><a class=\"anchor\" href=\"#313-entrypoint\">#</a> 3.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 变量\nKAFKA_DIR=/kafka\nKAFKA_CONF=/kafka/config/server.properties\n\n# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一\nBROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))\nLISTENERS=$(hostname -i)\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;\n\n# 3、启动Kafka\ncd $&#123;KAFKA_DIR&#125;/bin\nsed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh\n./kafka-server-start.sh ../config/server.properties\n</code></pre>\n<h6 id=\"314-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#314-构建镜像并推送仓库\">#</a> 3.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2\n</code></pre>\n<h5 id=\"32-迁移kafka集群至k8s\"><a class=\"anchor\" href=\"#32-迁移kafka集群至k8s\">#</a> 3.2 迁移 Kafka 集群至 K8S</h5>\n<h6 id=\"321-kafka-headless\"><a class=\"anchor\" href=\"#321-kafka-headless\">#</a> 3.2.1 kafka-headless</h6>\n<pre><code># cat 01-kafka-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: kafka\n  ports:\n  - name: client\n    port: 9092\n    targetPort: 9092\n  - name: jmx\n    port: 9999\n    targetPort: 9999\n</code></pre>\n<h6 id=\"322-kafka-sts\"><a class=\"anchor\" href=\"#322-kafka-sts\">#</a> 3.2.2 kafka-sts</h6>\n<pre><code># cat 02-kafka-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  namespace: logging\nspec:\n  serviceName: &quot;kafka-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kafka\n  template:\n    metadata:\n      labels:\n        app: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;kafka&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kafka\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 9092\n        - name: jmxport\n          containerPort: 9999\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"323-更新资源清单\"><a class=\"anchor\" href=\"#323-更新资源清单\">#</a> 3.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml \n[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml\n[root@k8s-master01 02-kafka]# kubectl get pods -n logging \nNAME          READY   STATUS    RESTARTS       AGE\nkafka-0       1/1     Running   0              5m49s\nkafka-1       1/1     Running   0              4m43s\nkafka-2       1/1     Running   0              3m40s\n\n#查看kafka是否注册到zookeeper\n[root@k8s-master01 02-kafka]# kubectl exec -it zookeeper-0 -n logging -- /bin/bash\nroot@zookeeper-0:/# /zookeeper/bin/zkCli.sh \n[zk: localhost:2181(CONNECTED) 2] get /brokers/ids/1\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.85.201:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.85.201&quot;,&quot;timestamp&quot;:&quot;1748162470218&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n[zk: localhost:2181(CONNECTED) 3] get /brokers/ids/2\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.58.205:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.58.205&quot;,&quot;timestamp&quot;:&quot;1748162532658&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n[zk: localhost:2181(CONNECTED) 4] get /brokers/ids/3\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.195.1:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.195.1&quot;,&quot;timestamp&quot;:&quot;1748162649250&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n</code></pre>\n<h6 id=\"324-检查kafka集群\"><a class=\"anchor\" href=\"#324-检查kafka集群\">#</a> 3.2.4 检查 Kafka 集群</h6>\n<pre><code>1.创建一个topic\nroot@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu\n\n2.模拟消息发布\nroot@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu\n&gt;hello kubernetes\n&gt;hello world\n\n3.模拟消息订阅\nroot@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning\nhello kubernetes\nhello world\n</code></pre>\n<h4 id=\"四-交付efak至k8s\"><a class=\"anchor\" href=\"#四-交付efak至k8s\">#</a> 四、交付 efak 至 K8S</h4>\n<h5 id=\"41-制作efak镜像\"><a class=\"anchor\" href=\"#41-制作efak镜像\">#</a> 4.1 制作 efak 镜像</h5>\n<h6 id=\"411-dockerfile\"><a class=\"anchor\" href=\"#411-dockerfile\">#</a> 4.1.1 Dockerfile</h6>\n<pre><code>[root@manager 03-efak]# cat Dockerfile \nFROM openjdk:8\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=3.0.1\nADD ./efak-web-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./system-config.properties /efak-web-$&#123;VERSION&#125;/conf/system-config.properties\n\n# 3、修改efak的名称\nRUN mv /efak-web-$&#123;VERSION&#125; /efak\n\n# 4、环境变量\nENV KE_HOME=/efak\nENV PATH=$PATH:$KE_HOME/bin\n\n# 5、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 6、暴露kafka端口 9999是jmx的端口\nEXPOSE 8048\n\n# 7、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"412-system-config\"><a class=\"anchor\" href=\"#412-system-config\">#</a> 4.1.2 system-config</h6>\n<pre><code># cat system-config.properties \n######################################\n# 填写 zookeeper集群列表\n######################################\nefak.zk.cluster.alias=cluster1\ncluster1.zk.list=&#123;ZOOK_SERVERS&#125;\n\n######################################\n# broker 最大规模数量\n######################################\ncluster1.efak.broker.size=20\n\n######################################\n# zk 客户端线程数\n######################################\nkafka.zk.limit.size=32\n\n######################################\n# EFAK webui 端口\n######################################\nefak.webui.port=8048\n\n######################################\n# kafka offset storage\n######################################\ncluster1.efak.offset.storage=kafka\n\n######################################\n# kafka jmx uri\n######################################\ncluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi\n\n######################################\n# kafka metrics 指标，默认存储15天\n######################################\nefak.metrics.charts=true\nefak.metrics.retain=15\n\n######################################\n# kafka sql topic records max\n######################################\nefak.sql.topic.records.max=5000\nefak.sql.topic.preview.records.max=10\n\n######################################\n# delete kafka topic token\n######################################\nefak.topic.token=keadmin\n\n######################################\n# kafka sqlite 数据库地址（需要修改存储路径）\n######################################\nefak.driver=org.sqlite.JDBC\nefak.url=jdbc:sqlite:&#123;EFAK_DATA_DIR&#125;/db/ke.db\nefak.username=root\nefak.password=www.kafka-eagle.org\n</code></pre>\n<h6 id=\"413-entrypoint\"><a class=\"anchor\" href=\"#413-entrypoint\">#</a> 4.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 1、变量\nEFAK_DIR=/efak\nEFAK_CONF=/efak/conf/system-config.properties\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;EFAK_DATA_DIR&#125;@$&#123;EFAK_DIR&#125;@g  $&#123;EFAK_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;EFAK_CONF&#125;\n\n# 3、启动efka\n$&#123;EFAK_DIR&#125;/bin/ke.sh start\ntail -f $&#123;EFAK_DIR&#125;/logs/ke_console.out\n</code></pre>\n<h6 id=\"414-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#414-构建镜像并推送仓库\">#</a> 4.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0\n</code></pre>\n<h5 id=\"42-迁移efak至k8s\"><a class=\"anchor\" href=\"#42-迁移efak至k8s\">#</a> 4.2 迁移 efak 至 K8S</h5>\n<h6 id=\"421-efak-deploy\"><a class=\"anchor\" href=\"#421-efak-deploy\">#</a> 4.2.1 efak-deploy</h6>\n<pre><code># cat 01-efak-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efak\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: efak\n  template:\n    metadata:\n      labels:\n        app: efak\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: efak\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 \n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8048\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n</code></pre>\n<h6 id=\"422-efak-service\"><a class=\"anchor\" href=\"#422-efak-service\">#</a> 4.2.2 efak-service</h6>\n<pre><code># cat 02-efak-service.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: efak-svc\n  namespace: logging\nspec:\n  selector:\n    app: efak\n  ports:\n  - port: 8048\n    targetPort: 8048\n</code></pre>\n<h6 id=\"423-efak-ingress\"><a class=\"anchor\" href=\"#423-efak-ingress\">#</a> 4.2.3 efak-ingress</h6>\n<pre><code># cat 03-efak-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: efak-ingress\n  namespace: logging\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;efak.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: efak-svc\n            port: \n              number: 8048\n</code></pre>\n<h6 id=\"424-更新资源清单\"><a class=\"anchor\" href=\"#424-更新资源清单\">#</a> 4.2.4 更新资源清单</h6>\n<pre><code>[root@k8s-master01 03-efak]# kubectl apply -f 01-efak-deploy.yaml \n[root@k8s-master01 03-efak]# kubectl apply -f 02-efak-service.yaml \n[root@k8s-master01 03-efak]# kubectl apply -f 03-efak-ingress.yaml \n</code></pre>\n<h6 id=\"425-访问efka\"><a class=\"anchor\" href=\"#425-访问efka\">#</a> 4.2.5 访问 efka</h6>\n<p>1、初始用户名密码 admin   123456</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Nq16u4z.png\" alt=\"1.png\" /></p>\n<p>2、查看 Topics</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/9Bin9cr.png\" alt=\"2.png\" /></p>\n<p>3、查看 kafka 集群状态</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/U76YIck.png\" alt=\"3.png\" /></p>\n<p>4、查看 Zookeeper 集群状态</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/cY5LeWx.png\" alt=\"4.png\" /></p>\n<h4 id=\"五-交付elastic集群\"><a class=\"anchor\" href=\"#五-交付elastic集群\">#</a> 五、交付 Elastic 集群</h4>\n<ul>\n<li>ES 集群是由多个节点组成的，通过 <a href=\"http://cluster.name\">cluster.name</a> 设置 ES 集群名称，同时用于区分其它的 ES 集群。</li>\n<li>每个节点通过 <a href=\"http://node.name\">node.name</a> 参数来设定所在集群的节点名称。</li>\n<li>节点使用 discovery.send_hosts 参数来设定集群节点的列表。</li>\n<li>集群在第一次启动时，需要初始化，同时需要指定参与选举的 master 节点 IP，或节点名称。</li>\n<li>每个节点可以通过 node.master:true 设定为 master 角色，通过 node.data:true 设定为 data 角色。</li>\n</ul>\n<pre><code>[root@k8s-master01 ~]# grep &quot;^[a-Z]&quot; /etc/elasticsearch/elasticsearch.yml\n# 集群名称cluster.name: my-oldxu\n# 节点名称node.name: node1\n# 数据存储路径path.data: /var/lib/elasticsearch\n# 日志存储路径path.logs: /var/log/elasticsearch\n# 监听在本地哪个地址上network.host: 10.0.0.100\n# 监听端口http.port: 9200\n# 集群主机列表discovery.seed_hosts: [&quot;ip1&quot;, &quot;ip2&quot;, &quot;ip3&quot;]\n# 仅第一次启动集群时进行选举（可以填写node.name的名称）cluster.initial_master_nodes: [&quot;node01&quot;, &quot;node02&quot;, &quot;node03&quot;]\n</code></pre>\n<h5 id=\"51-下载elastic镜像\"><a class=\"anchor\" href=\"#51-下载elastic镜像\">#</a> 5.1 下载 elastic 镜像</h5>\n<pre><code># docker pull elasticsearch:7.17.6\n# docker tag elasticsearch:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6\n</code></pre>\n<h5 id=\"52-交付es-service\"><a class=\"anchor\" href=\"#52-交付es-service\">#</a> 5.2 交付 ES-Service</h5>\n<p>创建 es-headlessService，为每个 ES Pod 设定固定的 DNS 名称，无论它是 Master 或是 Data，易或是 Coordinating</p>\n<pre><code># cat 01-es-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: es-svc\n  namespace: logging\nspec:\n  selector:\n    app: es\n  clusterIP: None\n  ports:\n  - name: cluster\n    port: 9200\n    targetPort: 9200\n  - name: transport\n    port: 9300\n    targetPort: 9300\n</code></pre>\n<h5 id=\"53-交付es-master节点\"><a class=\"anchor\" href=\"#53-交付es-master节点\">#</a> 5.3 交付 ES-Master 节点</h5>\n<ol>\n<li>\n<p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data ；</p>\n</li>\n<li>\n<p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p>\n</li>\n<li>\n<p>ES 启动是通过 ENV 环境变量传参来完成的；</p>\n<ul>\n<li>\n<p>集群名称、节点名称、角色类型；</p>\n</li>\n<li>\n<p>discovery.seed_hosts 集群地址列表；</p>\n</li>\n<li>\n<p>cluster.initial_master_nodes 初始集群参与选举的 master 节点名称；</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre><code># cat 02-es-master.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-master\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 3           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: master\n  template:\n    metadata:\n      labels:\n        app: es\n        role: master\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;master&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;true&quot;\n        - name: node.data\n          value: &quot;false&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n        - name: cluster.initial_master_nodes\n          value: &quot;es-master-0,es-master-1,es-master-2&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<pre><code>[root@k8s-master01 04-elasticsearch]# cat 03-es-data.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-data\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 2           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: data\n  template:\n    metadata:\n      labels:\n        app: es\n        role: data\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;data&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;false&quot;\n        - name: node.data\n          value: &quot;true&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"54-交付es-data节点\"><a class=\"anchor\" href=\"#54-交付es-data节点\">#</a> 5.4 交付 ES-Data 节点</h5>\n<ol>\n<li>\n<p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data</p>\n</li>\n<li>\n<p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p>\n</li>\n<li>\n<p>ES 启动是通过 ENV 环境变量传参来完成的</p>\n<ul>\n<li>\n<p>集群名称、节点名称、角色类型</p>\n</li>\n<li>\n<p>discovery.seed_hosts 集群地址列表</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre><code># cat 03-es-data.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-data\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 2           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: data\n  template:\n    metadata:\n      labels:\n        app: es\n        role: data\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;data&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;false&quot;\n        - name: node.data\n          value: &quot;true&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"55-更新资源清单\"><a class=\"anchor\" href=\"#55-更新资源清单\">#</a> 5.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 04-elasticsearch]# kubectl apply -f 01-es-svc.yaml \n[root@k8s-master01 04-elasticsearch]# kubectl apply -f 02-es-master.yaml \n[root@k8s-master01 04-elasticsearch]# kubectl apply -f 03-es-data.yaml \n</code></pre>\n<h5 id=\"56-验证es集群\"><a class=\"anchor\" href=\"#56-验证es集群\">#</a> 5.6 验证 ES 集群</h5>\n<pre><code>#1.解析headlessService获取对应ES集群任一节点的IP地址\n# dig @10.96.0.10 es-svc.logging.svc.cluster.local  +short\n172.16.58.229\n172.16.122.191\n172.16.195.21\n172.16.122.129\n172.16.32.164\n\n#2.通过curl访问ES，检查ES集群是否正常（如果仅交付Master，没有data节点，集群状态可能会Red，因为没有数据节点进行数据存储；）\n# curl -XGET &quot;http://172.16.122.129:9200/_cluster/health?pretty&quot;\n&#123;\n  &quot;cluster_name&quot; : &quot;es-cluster&quot;,\n  &quot;status&quot; : &quot;green&quot;,\n  &quot;timed_out&quot; : false,\n  &quot;number_of_nodes&quot; : 5,\n  &quot;number_of_data_nodes&quot; : 2,\n  &quot;active_primary_shards&quot; : 3,\n  &quot;active_shards&quot; : 6,\n  &quot;relocating_shards&quot; : 0,\n  &quot;initializing_shards&quot; : 0,\n  &quot;unassigned_shards&quot; : 0,\n  &quot;delayed_unassigned_shards&quot; : 0,\n  &quot;number_of_pending_tasks&quot; : 0,\n  &quot;number_of_in_flight_fetch&quot; : 0,\n  &quot;task_max_waiting_in_queue_millis&quot; : 0,\n  &quot;active_shards_percent_as_number&quot; : 100.0\n&#125;\n\n#3.查看ES各个节点详情\n# curl -XGET &quot;http://172.16.122.129:9200/_cat/nodes&quot;\n172.16.122.129 16 33 20 0.38 0.56 0.38 ilmr       - es-master-2\n172.16.58.229  66 33 22 0.64 0.66 0.44 ilmr       * es-master-1\n172.16.122.191 52 34 15 0.38 0.56 0.38 cdfhilrstw - es-data-0\n172.16.195.21  38 35 19 0.38 0.53 0.36 cdfhilrstw - es-data-1\n172.16.32.164  31 33 12 0.28 0.50 0.59 ilmr       - es-master-0\n</code></pre>\n<h4 id=\"六-交付kibana可视化\"><a class=\"anchor\" href=\"#六-交付kibana可视化\">#</a> 六、交付 Kibana 可视化</h4>\n<h5 id=\"61-下载kibana镜像\"><a class=\"anchor\" href=\"#61-下载kibana镜像\">#</a> 6.1 下载 kibana 镜像</h5>\n<pre><code># docker pull kibana:7.17.6\n# docker tag kibana:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6\n</code></pre>\n<h5 id=\"62-kibana-deploy\"><a class=\"anchor\" href=\"#62-kibana-deploy\">#</a> 6.2 kibana-deploy</h5>\n<ol>\n<li>Kibana 需要连接 ES 集群，通过 ELASTICSEARCH_HOSTS 变量来传递 ES 集群地址</li>\n<li>kibana 通过 I18N_LOCALE 来传递语言环境</li>\n<li>Kibana 通过 SERVER_PUBLICBASEURL 来传递服务访问的公开地址</li>\n</ol>\n<pre><code># cat 01-kibana-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kibana\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kibana\n  template:\n    metadata:\n      labels:\n        app: kibana\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kibana\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n        ports:\n        - containerPort: 5601\n        env:\n        - name: ELASTICSEARCH_HOSTS\n          value: '[&quot;http://es-data-0.es-svc:9200&quot;,&quot;http://es-data-1.es-svc:9200&quot;]'\n        - name: I18N_LOCALE\n          value: &quot;zh-CN&quot;\n        - name: SERVER_PUBLICBASEURL\n          value: &quot;http://kibana.hmallleasing.com&quot;   #kibana访问UI\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n</code></pre>\n<h5 id=\"63-kibana-svc\"><a class=\"anchor\" href=\"#63-kibana-svc\">#</a> 6.3 kibana-svc</h5>\n<pre><code># cat 02-kibana-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kibana-svc\n  namespace: logging\nspec:\n  selector:\n    app: kibana\n  ports:\n  - name: web\n    port: 5601\n    targetPort: 5601\n</code></pre>\n<h5 id=\"64-kibana-ingress\"><a class=\"anchor\" href=\"#64-kibana-ingress\">#</a> 6.4 kibana-ingress</h5>\n<pre><code># cat 03-kibana-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kibana-ingress\n  namespace: logging\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;kibana.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kibana-svc\n            port:\n              number: 5601\n</code></pre>\n<h5 id=\"65-更新资源清单\"><a class=\"anchor\" href=\"#65-更新资源清单\">#</a> 6.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 05-kibana]# kubectl apply -f 01-kibana-deploy.yaml \n[root@k8s-master01 05-kibana]# kubectl apply -f 02-kibana-svc.yaml \n[root@k8s-master01 05-kibana]# kubectl apply -f 03-kibana-ingress.yaml\n\n[root@k8s-master01 05-kibana]# kubectl get pods -n logging\nNAME                      READY   STATUS    RESTARTS   AGE\nefak-5cdc74bf59-nrhb4     1/1     Running   0          5h33m\nes-data-0                 1/1     Running   0          16m\nes-data-1                 1/1     Running   0          15m\nes-master-0               1/1     Running   0          17m\nes-master-1               1/1     Running   0          15m\nes-master-2               1/1     Running   0          12m\nkafka-0                   1/1     Running   0          5h39m\nkafka-1                   1/1     Running   0          5h39m\nkafka-2                   1/1     Running   0          5h38m\nkibana-5ccc46864b-ndzx9   1/1     Running   0          118s\nzookeeper-0               1/1     Running   0          5h42m\nzookeeper-1               1/1     Running   0          5h42m\nzookeeper-2               1/1     Running   0          5h41m\n</code></pre>\n<h5 id=\"66-访问kibana\"><a class=\"anchor\" href=\"#66-访问kibana\">#</a> 6.6 访问 kibana</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/sUXTx1J.png\" alt=\"1.png\" /></p>\n<h4 id=\"七-daemonset运行日志agent实践\"><a class=\"anchor\" href=\"#七-daemonset运行日志agent实践\">#</a> 七、DaemonSet 运行日志 Agent 实践</h4>\n<h5 id=\"71-部署架构说明\"><a class=\"anchor\" href=\"#71-部署架构说明\">#</a> 7.1 部署架构说明</h5>\n<p>对于那些将日志输出到，stdout 与 stderr 的 Pod，可以直接使用 DaemonSet 控制器在每个 Node 节点上运行一个 filebeat、logstash、fluentd 容器进行统一的收集，而后写入到日志存储系统</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/UOlaNE1.png\" alt=\"1.png\" /></p>\n<h5 id=\"72-创建serviceaccount\"><a class=\"anchor\" href=\"#72-创建serviceaccount\">#</a> 7.2 创建 ServiceAccount</h5>\n<pre><code> kubectl create serviceaccount filebeat -n logging\n</code></pre>\n<h5 id=\"73-创建clusterrole\"><a class=\"anchor\" href=\"#73-创建clusterrole\">#</a> 7.3 创建 ClusterRole</h5>\n<pre><code>kubectl create clusterrole filebeat --verb=get,list,watch --resource=namespace,pods,nodes\n</code></pre>\n<h5 id=\"74-创建clusterrolebinding\"><a class=\"anchor\" href=\"#74-创建clusterrolebinding\">#</a> 7.4 创建 ClusterRolebinding</h5>\n<pre><code>kubectl create clusterrolebinding filebeat --serviceaccount=logging:filebeat --clusterrole=filebeat\n</code></pre>\n<h5 id=\"75-交付filebeat\"><a class=\"anchor\" href=\"#75-交付filebeat\">#</a> 7.5 交付 Filebeat</h5>\n<h6 id=\"751-下载filebeat镜像\"><a class=\"anchor\" href=\"#751-下载filebeat镜像\">#</a> <strong>7.5.1 下载 filebeat 镜像</strong></h6>\n<pre><code># docker pull docker.elastic.co/beats/filebeat:7.17.6\n# docker tag docker.elastic.co/beats/filebeat:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat:7.17.6\n</code></pre>\n<h6 id=\"752-交付filebeat\"><a class=\"anchor\" href=\"#752-交付filebeat\">#</a> <strong>7.5.2 交付 filebeat</strong></h6>\n<ol>\n<li>从 ConfigMap 中挂载 filebeat.yaml 配置文件；</li>\n<li>挂载 /var/log、/var/lib/docker/containers 日志相关目录；</li>\n<li>使用 hostPath 方式挂载 /usr/share/filebeat/data 数据目录，该目录下有一个 registry 文件，里面记录了 filebeat 采集日志位置的相关内容，比如文件 offset、source、timestamp 等，如果 Pod 发生异常后 K8S 自动将 Pod 进行重启，不挂载的情况下 registry 会被重置，将导致日志文件又从 offset=0 开始采集，会造成重复收集日志。</li>\n</ol>\n<pre><code>[root@k8s-master01 07-filebeat-daemoset]# cat 02-filebeat-ds.yaml \napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: filebeat\n  namespace: logging\nspec:\n  selector:\n    matchLabels:\n      app: filebeat\n  template:\n    metadata:\n      labels:\n        app: filebeat\n    spec:\n      serviceAccountName: &quot;filebeat&quot;\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        operator: &quot;Exists&quot;\n        effect: &quot;NoSchedule&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: filebeat\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat:7.17.6\n        args: [\n          &quot;-c&quot;,&quot;/etc/filebeat.yml&quot;,\n          &quot;-e&quot;\n        ]\n        securityContext:\n          runAsUser: 0\n        resources:\n          limits:\n            memory: 300Mi\n        volumeMounts:\n        - name: config                          # 从ConfigMap中读取\n          mountPath: /etc/filebeat.yml\n          subPath: filebeat.yml\n        - name: varlog\n          mountPath: /var/log\n          readOnly: true\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: data\n          mountPath: /usr/share/filebeat/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: config\n        configMap:\n          name: filebeat-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: data\n        hostPath:\n          path: /var/lib/filebeat-data\n          type: DirectoryOrCreate\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot; \n</code></pre>\n<h6 id=\"753-filebeat配置文件\"><a class=\"anchor\" href=\"#753-filebeat配置文件\">#</a> 7.5.3 Filebeat 配置文件</h6>\n<pre><code># cat 01-filebeat-configmap.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: filebeat-config\n  namespace: logging\n\ndata:\n  filebeat.yml: |-\n    # ============================== Filebeat inputs ==============================\n    logging.level: warning\n    filebeat.inputs:\n    - type: log\n      enabled: true\n      encoding: utf-8\n      paths: /var/log/messages\n      include_lines: ['kubelet']            # 获取与kubelet相关的日志\n      fields:                               # 添加filebeat字段\n        namespace: kubelet\n      fields_under_root: true\n\n    # ============================== Filebeat autodiscover ============================\n    filebeat.autodiscover:\n      providers:\n        - type: kubernetes\n          templates:\n          - condition:\t\t\t\t\t\t# 匹配kube-system名称空间下所有日志\n              equals:\n                kubernetes.namespace: kube-system\n            config:\n              - type: container\n                stream: all\t\t\t\t\t# 收集stdout、stderr类型日志，all是所有\n                encoding: utf-8\n                paths: /var/log/containers/*-$&#123;data.kubernetes.container.id&#125;.log\n                exclude_lines: ['info']     # 排除info相关的日志\n\n          - condition:                      # 收集ingress-nginx命名空间下stdout日志\n              equals:\n                kubernetes.namespace: ingress-nginx\n            config:\n              - type: container\n                stream: stdout\n                encoding: utf-8\n                paths: /var/log/containers/*-$&#123;data.kubernetes.container.id&#125;.log\n                json.keys_under_root: true  # 默认将json解析存储至messages，true则不存储至message\n                json.overwrite_keys: true   # 覆盖默认message字段，使用自定义json格式的key\n                #exclude_lines: ['kibana']   # 与kibana相关的则排除\n\n          - condition:                              # 收集ingress-nginx命名空间下stderr日志\n              equals:\n                kubernetes.namespace: ingress-nginx\n            config:\n              - type: container\n                stream: stderr\n                encoding: utf-8\n                paths:\n                  - /var/log/containers/*-$&#123;data.kubernetes.container.id&#125;.log\n\n    # ============================== Filebeat Processors ===========================\n    processors:\n      - rename:                              # 重写kubernetes源数据信息\n          fields:\n          - from: &quot;kubernetes.namespace&quot;\n            to: &quot;namespace&quot;\n          - from: &quot;kubernetes.pod.name&quot;\n            to: &quot;podname&quot;\n          - from: &quot;kubernetes.pod.ip&quot;\n            to: &quot;podip&quot;\n      - drop_fields:                        # 删除无用的字段\n          fields: [&quot;host&quot;,&quot;agent&quot;,&quot;ecs&quot;,&quot;input&quot;,&quot;container&quot;,&quot;kubernetes&quot;]\n\n    # ================================== Kafka Output ===================================\n    output.kafka:\n      hosts: [&quot;kafka-0.kafka-svc:9092&quot;,&quot;kafka-1.kafka-svc:9092&quot;,&quot;kafka-2.kafka-svc:9092&quot;]\n      topic: &quot;app-%&#123;[namespace]&#125;&quot;\t# %&#123;[namespace]&#125; 会自动将其转换为namespace对应的值\n      required_acks: 1              # 保证消息可靠，0不保证，1等待写入主分区（默认）-1等待写入副本分区\n      compression: gzip             # 压缩\n      max_message_bytes: 1000000    # 每条消息最大的长度，多余的被删除\n</code></pre>\n<h6 id=\"754-收集ingress-nginx名称空间\"><a class=\"anchor\" href=\"#754-收集ingress-nginx名称空间\">#</a> 7.5.4 收集 ingress-nginx 名称空间</h6>\n<p>修改 Ingress 日志输出格式</p>\n<pre><code># kubectl edit configmap -n ingress-nginx ingress-nginx-controller \napiVersion: v1\ndata:\n  ...\n  log-format-upstream: '&#123;&quot;timestamp&quot;:&quot;$time_iso8601&quot;,&quot;domain&quot;:&quot;$server_name&quot;,&quot;hostname&quot;:&quot;$hostname&quot;,&quot;remote_user&quot;:&quot;$remote_user&quot;,&quot;clientip&quot;:&quot;$remote_addr&quot;,&quot;proxy_protocol_addr&quot;:&quot;$proxy_protocol_addr&quot;,&quot;@source&quot;:&quot;$server_addr&quot;,&quot;host&quot;:&quot;$http_host&quot;,&quot;request&quot;:&quot;$request&quot;,&quot;args&quot;:&quot;$args&quot;,&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&quot;status&quot;:&quot;$status&quot;,&quot;upstream_status&quot;:&quot;$upstream_status&quot;,&quot;bytes&quot;:&quot;$body_bytes_sent&quot;,&quot;responsetime&quot;:&quot;$request_time&quot;,&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&quot;proxy_upstream_name&quot;:&quot;$proxy_upstream_name&quot;,&quot;x_forwarded&quot;:&quot;$http_x_forwarded_for&quot;,&quot;upstream_response_length&quot;:&quot;$upstream_response_length&quot;,&quot;referer&quot;:&quot;$http_referer&quot;,&quot;user_agent&quot;:&quot;$http_user_agent&quot;,&quot;request_length&quot;:&quot;$request_length&quot;,&quot;request_method&quot;:&quot;$request_method&quot;,&quot;scheme&quot;:&quot;$scheme&quot;,&quot;k8s_ingress_name&quot;:&quot;$ingress_name&quot;,&quot;k8s_service_name&quot;:&quot;$service_name&quot;,&quot;k8s_service_port&quot;:&quot;$service_port&quot;&#125;'\n</code></pre>\n<h6 id=\"755-更新资源清单\"><a class=\"anchor\" href=\"#755-更新资源清单\">#</a> 7.5.5 更新资源清单</h6>\n<pre><code>kubectl apply -f 01-filebeat-configmap.yaml\nkubectl apply -f 02-filebeat-ds.yaml\n</code></pre>\n<h6 id=\"756-检查kafka对应topic\"><a class=\"anchor\" href=\"#756-检查kafka对应topic\">#</a> 7.5.6 检查 kafka 对应 Topic</h6>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/shg5F6T.png\" alt=\"PixPin_2025-06-05_16-02-45.png\" /></p>\n<h4 id=\"八-交付logstash\"><a class=\"anchor\" href=\"#八-交付logstash\">#</a> 八、 交付 Logstash</h4>\n<h5 id=\"81-下载logstash镜像\"><a class=\"anchor\" href=\"#81-下载logstash镜像\">#</a> 8.1 下载 Logstash 镜像</h5>\n<pre><code># docker pull docker.elastic.co/logstash/logstash-oss:7.17.6\n# docker tag docker.elastic.co/logstash/logstash-oss:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n</code></pre>\n<h5 id=\"82-如何交付logstash\"><a class=\"anchor\" href=\"#82-如何交付logstash\">#</a> 8.2 如何交付 Logstash</h5>\n<ol>\n<li>Logstash 需要设定环境变量来调整主配置文件参数，比如：worker 运行数量，以及批量处理的最大条目是多少；</li>\n<li>Logstash 需要调整 JVM 堆内存使用的范围，没办法传参调整，但可以通过 postStart 来修改其文件对应的 jvm 参数；</li>\n<li>Logstash 需要配置文件，读取 Kafka 数据，而后通过 filter 处理，最后输出至 ES</li>\n</ol>\n<pre><code># /usr/share/logstash/config/logstash.yml\n# 可通过变量传参修改\npipeline.workers: 2\npipeline.batch.size: 1000\n# /usr/share/logstash/config/jvm.options-Xms512m-Xmx512m\n# /usr/share/logstash/config/logstash.conf\ninput &#123;\n\tkafka\n&#125;\nfilter &#123;\n\n&#125;\n</code></pre>\n<h5 id=\"83-准备logstash配置\"><a class=\"anchor\" href=\"#83-准备logstash配置\">#</a> 8.3 准备 logstash 配置</h5>\n<p><strong>input 段含义</strong></p>\n<ol>\n<li>所有数据都从 kafka 集群中获取；</li>\n<li>获取 kafka 集群中 topic，主要有 app-kube-system、app-ingress-nginx、app-kubelet</li>\n</ol>\n<p><strong>filter 段含义</strong></p>\n<ol>\n<li>判断 namespace 等于 kubelet，则为其添加一个索引字段名称；</li>\n<li>判断 namespace 等于 kube-system，则为其添加一个索引字段名称；</li>\n<li>判断 namespace 等于 ingress-nginx，并且 stream 等于 stderr，则为其添加一个索引字段名称；</li>\n<li>判断 namespace 等于 ingress-nginx，并且 stream 等于 stdout，则使用 geoip 获取地址来源，使用 useragent 模块分析来访客户端设备，使用 date 处理时间，使用 mutate 转换对应字段格式，最后添加一个索引字段名称；</li>\n</ol>\n<pre><code># cat logstash-node.conf \ninput &#123;\n\tkafka &#123;\n\tbootstrap_servers =&gt; &quot;kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092&quot;\n        group_id =&gt; &quot;logstash-node&quot;     # 消费者组名称\n        consumer_threads =&gt; &quot;3&quot;         # 理想情况下，您应该拥有与分区数一样多的线程,以实现完美的平衡\n        topics =&gt; [&quot;app-kube-system&quot;,&quot;app-ingress-nginx&quot;,&quot;app-kubelet&quot;]\n\t    codec =&gt; json\n    &#125;\n&#125;\n\nfilter &#123;\n##########################################################################\n\tif &quot;kubelet&quot; in [namespace] &#123;\n\t\tmutate &#123; \n \t\t\tadd_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;\n\t\t&#125;\n\t&#125;\n\n##########################################################################\n\tif &quot;kube-system&quot; in [namespace] &#123;\n\t\tmutate &#123; \n \t\t\tadd_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;\n\t\t&#125;\n\t&#125;\n\t\n##########################################################################\n\tif [namespace] == &quot;ingress-nginx&quot; and [stream] == &quot;stdout&quot; &#123;\n\t\tgeoip &#123;\n        \t\tsource =&gt; &quot;clientip&quot;\n    \t\t&#125;\n\t\tuseragent &#123;\n        \t\tsource =&gt; &quot;user_agent&quot;\n\t\t\ttarget =&gt; &quot;user_agent&quot;\n        \t&#125;\n\n    \t\tdate &#123;\n\t\t\t# 2022-10-08T13:13:20.000Z\n        \t\tmatch =&gt; [&quot;timestamp&quot;,&quot;ISO8601&quot;]\n        \t\ttarget =&gt; &quot;@timestamp&quot;\n        \t\ttimezone =&gt; &quot;Asia/Shanghai&quot;\t\n    \t\t&#125;\n\t\t\n\t\tmutate &#123; \n\t\t\tconvert =&gt; &#123; \n\t\t\t\t&quot;bytes&quot; =&gt; &quot;integer&quot;\n\t\t\t\t&quot;responsetime&quot; =&gt; &quot;float&quot;\n\t\t\t\t&quot;upstreamtime&quot; =&gt; &quot;float&quot;\n\t\t\t&#125;\n\t\t\tadd_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;[stream]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;\n\t\t&#125;\n\t&#125;\n\n##########################################################################\n\tif [namespace] == &quot;ingress-nginx&quot; and [stream] == &quot;stderr&quot; &#123;\n\t\tmutate &#123; \n \t\t  add_field =&gt; &#123; &quot;target_index&quot; =&gt; &quot;app-%&#123;[namespace]&#125;-%&#123;[stream]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;\n\t\t&#125;\n\t&#125;\n&#125;\n\noutput &#123;\n    stdout &#123;\n        codec =&gt; rubydebug\n    &#125;\n    elasticsearch &#123;\n        hosts =&gt; [&quot;es-data-0.es-svc:9200&quot;,&quot;es-data-1.es-svc:9200&quot;]\n        index =&gt; &quot;%&#123;[target_index]&#125;&quot;\n        template_overwrite =&gt; true\n    &#125;\n&#125;\n</code></pre>\n<h5 id=\"84-创建configmap\"><a class=\"anchor\" href=\"#84-创建configmap\">#</a> 8.4 创建 ConfigMap</h5>\n<pre><code>kubectl create configmap logstash-node-conf --from-file=logstash.conf=conf/logstash-node.conf -n logging\n</code></pre>\n<h5 id=\"85-创建service\"><a class=\"anchor\" href=\"#85-创建service\">#</a> 8.5 创建 Service</h5>\n<pre><code># cat 01-logstash-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: logstash-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: logstash\n  ports:\n  - port: 9600\n    targetPort: 9600\n</code></pre>\n<h5 id=\"86-交付logstash\"><a class=\"anchor\" href=\"#86-交付logstash\">#</a> 8.6 交付 Logstash</h5>\n<pre><code>[root@k8s-master01 08-logstash]# cat 02-logstash-node-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: logstash-node\n  namespace: logging\nspec:\n  serviceName: &quot;logstash-svc&quot;\n  replicas: 1\n  selector:\n    matchLabels:\n      app: logstash\n      env: node\n  template:\n    metadata:\n      labels:\n        app: logstash\n        env: node\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: logstash\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6 \n        args: [&quot;-f&quot;,&quot;config/logstash.conf&quot;]                     # 启动时指定加载的配置文件\n        resources:\n          limits:\n            memory: 1024Mi\n        env:\n        - name: PIPELINE_WORKERS\n          value: &quot;2&quot;\n        - name: PIPELINE_BATCH_SIZE\n          value: &quot;10000&quot;\n        lifecycle:\n          postStart:                                            # 设定JVM\n            exec:\n              command:\n              - &quot;/bin/bash&quot;\n              - &quot;-c&quot;\n              - &quot;sed -i -e '/^-Xms/c-Xms512m' -e '/^-Xmx/c-Xmx512m' /usr/share/logstash/config/jvm.options&quot;\n        volumeMounts:\n        - name: data                                            # 持久化数据目录\n          mountPath: /usr/share/logstash/data\n        - name: conf\n          mountPath: /usr/share/logstash/config/logstash.conf\n          subPath: logstash.conf\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: conf\n        configMap:\n          name: logstash-node-conf\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"87-更新资源清单\"><a class=\"anchor\" href=\"#87-更新资源清单\">#</a> 8.7 更新资源清单</h5>\n<pre><code>kubectl apply -f 01-logstash-svc.yaml \nkubectl apply -f 02-logstash-node-sts.yaml\n</code></pre>\n<h5 id=\"88-检查kibana索引\"><a class=\"anchor\" href=\"#88-检查kibana索引\">#</a> 8.8 检查 kibana 索引</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/DB4w8Ln.png\" alt=\"2.png\" /></p>\n<h4 id=\"九-kibana可视化\"><a class=\"anchor\" href=\"#九-kibana可视化\">#</a> 九、Kibana 可视化</h4>\n<h5 id=\"91-创建索引\"><a class=\"anchor\" href=\"#91-创建索引\">#</a> 9.1 创建索引</h5>\n<p>kube-system 索引</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/agOzRmb.png\" alt=\"3.png\" /></p>\n<p>ingress-stdout 索引</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Iv0xBBe.png\" alt=\"4.png\" /></p>\n<p>ingress-stderr 索引</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/RpbsKpc.png\" alt=\"1.png\" /></p>\n<p>kubelet 索引</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/2JBN6fi.png\" alt=\"5.png\" /></p>\n<h5 id=\"92-日志展示\"><a class=\"anchor\" href=\"#92-日志展示\">#</a> 9.2 日志展示</h5>\n<p>app-ingress-nginx-stdout 索引日志</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Yyf0gyS.png\" alt=\"1.png\" /></p>\n<p>app-ingress-nginx-stderr 索引日志</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/dUWbjv3.png\" alt=\"2.png\" /></p>\n<p>app-kube-system 索引日志</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/HQO7ECM.png\" alt=\"3.png\" /></p>\n<p>app-kubelet 索引日志</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/HN2BpY7.png\" alt=\"4.png\" /></p>\n<h5 id=\"93-图形展示\"><a class=\"anchor\" href=\"#93-图形展示\">#</a> 9.3 图形展示</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/fcx5E8m.png\" alt=\"5.png\" /></p>\n",
            "tags": [
                "ELKStack"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/170066797.html",
            "url": "http://ixuyong.cn/posts/170066797.html",
            "title": "消费租赁项目Kubernetes基于ELK日志分析与实践",
            "date_published": "2025-05-25T06:35:21.000Z",
            "content_html": "<h3 id=\"消费租赁项目kubernetes基于elk日志分析与实践\"><a class=\"anchor\" href=\"#消费租赁项目kubernetes基于elk日志分析与实践\">#</a> 消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</h3>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg\" alt=\"Snipaste_2025-05-25_13-43-46.jpg\" /></p>\n<h4 id=\"一-elk创建namespace和secrets\"><a class=\"anchor\" href=\"#一-elk创建namespace和secrets\">#</a> 一、ELK 创建 Namespace 和 Secrets</h4>\n<pre><code># kubectl create ns logging\n# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd\n</code></pre>\n<h4 id=\"二-交付zookeeper集群至k8s\"><a class=\"anchor\" href=\"#二-交付zookeeper集群至k8s\">#</a> 二、交付 Zookeeper 集群至 K8S</h4>\n<h5 id=\"21-制作zk集群镜像\"><a class=\"anchor\" href=\"#21-制作zk集群镜像\">#</a> 2.1 制作 ZK 集群镜像</h5>\n<h6 id=\"211-dockerfile\"><a class=\"anchor\" href=\"#211-dockerfile\">#</a> 2.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、拷贝Zookeeper压缩包和配置文件\nENV VERSION=3.8.4\nADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf\n\n# 2、对Zookeeper文件夹名称重新命名\nRUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper\n\n# 3、拷贝eentrpoint的启动脚本文件\nADD ./entrypoint.sh /entrypoint.sh\n\n# 4、暴露Zookeeper端口\nEXPOSE 2181 2888 3888\n\n# 5、执行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"212-zoocfg\"><a class=\"anchor\" href=\"#212-zoocfg\">#</a> 2.1.2 zoo.cfg</h6>\n<pre><code># cat zoo.cfg \n# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。\ntickTime=&#123;ZOOK_TICKTIME&#125;\n\n# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime\ninitLimit=&#123;ZOOK_INIT_LIMIT&#125;\n\n# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime\nsyncLimit=&#123;ZOOK_SYNC_LIMIT&#125;\n \n# 数据保存目录\ndataDir=&#123;ZOOK_DATA_DIR&#125;\n\n# 日志保存目录\ndataLogDir=&#123;ZOOK_LOG_DIR&#125;\n\n# 客户端连接端口\nclientPort=&#123;ZOOK_CLIENT_PORT&#125;\n\n# 客户端最大连接数。# 根据自己实际情况设置，默认为60个\nmaxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;\n\n# 客户端获取 zookeeper 服务的当前状态及相关信息\n4lw.commands.whitelist=*\n\n# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口\n</code></pre>\n<h6 id=\"213-entrypoint\"><a class=\"anchor\" href=\"#213-entrypoint\">#</a> 2.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n#设定变量\nZOOK_BIN_DIR=/zookeeper/bin\nZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg\n\n# 2、对配置文件中的字符串进行变量替换\nsed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;\n\n# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来\nfor server in $&#123;ZOOK_SERVERS&#125;\ndo\n\techo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;\ndone\n\n# 4、在datadir目录中创建myid的文件，并填入对应的编号\nZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))\necho $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid\n\n#5、前台运行Zookeeper\ncd $&#123;ZOOK_BIN_DIR&#125;\n./zkServer.sh start-foreground\n</code></pre>\n<h6 id=\"214-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#214-构建镜像并推送仓库\">#</a> 2.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .\n# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4\n</code></pre>\n<h5 id=\"22-迁移zookeeper至k8s\"><a class=\"anchor\" href=\"#22-迁移zookeeper至k8s\">#</a> 2.2  迁移 zookeeper 至 K8S</h5>\n<h6 id=\"221-zookeeper-headless\"><a class=\"anchor\" href=\"#221-zookeeper-headless\">#</a> 2.2.1 zookeeper-headless</h6>\n<pre><code># cat 01-zookeeper-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: zookeeper\n  ports:\n  - name: client\n    port: 2181\n    targetPort: 2181\n  - name: leader-follwer\n    port: 2888\n    targetPort: 2888\n  - name: selection\n    port: 3888\n    targetPort: 3888\n</code></pre>\n<h6 id=\"222-zookeeper-sts\"><a class=\"anchor\" href=\"#222-zookeeper-sts\">#</a> 2.2.2 zookeeper-sts</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper               \n  namespace: logging\nspec:\n  serviceName: &quot;zookeeper-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;zookeeper&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: zookeeper\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 2181\n        - name: leader-follwer\n          containerPort: 2888\n        - name: selection\n          containerPort: 3888\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: data\n        - name: data\n          mountPath: /logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"223-更新资源清单\"><a class=\"anchor\" href=\"#223-更新资源清单\">#</a> 2.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml \n[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml\n[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging\nNAME          READY   STATUS    RESTARTS   AGE\nzookeeper-0   1/1     Running   0          17m\nzookeeper-1   1/1     Running   0          14m\nzookeeper-2   1/1     Running   0          11m\n</code></pre>\n<h6 id=\"224-检查zookeeper集群状态\"><a class=\"anchor\" href=\"#224-检查zookeeper集群状态\">#</a> 2.2.4 检查 zookeeper 集群状态</h6>\n<pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: leader\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\n</code></pre>\n<h6 id=\"225-连接zookeeper集群\"><a class=\"anchor\" href=\"#225-连接zookeeper集群\">#</a> 2.2.5 连接 Zookeeper 集群</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh\n# /zookeeper/bin/zkCli.sh -server zookeeper-svc\n[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu\nCreated /hello\n[zk: zookeeper-svc(CONNECTED) 1] get /hello\noldxu\n</code></pre>\n<h4 id=\"三-交付kafka集群至k8s\"><a class=\"anchor\" href=\"#三-交付kafka集群至k8s\">#</a> 三、 交付 Kafka 集群至 K8S</h4>\n<h5 id=\"31-制作kafka集群镜像\"><a class=\"anchor\" href=\"#31-制作kafka集群镜像\">#</a> 3.1 制作 Kafka 集群镜像</h5>\n<h6 id=\"311-dockerfile\"><a class=\"anchor\" href=\"#311-dockerfile\">#</a> 3.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=2.12-2.2.0\nADD ./kafka_$&#123;VERSION&#125;.tgz /\nADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties\n\n# 3、修改kafka的名称\nRUN mv /kafka_$&#123;VERSION&#125; /kafka\n\n# 4、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 5、暴露kafka端口 9999是jmx的端口\nEXPOSE 9092 9999\n\n# 6、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"312-serverproperties\"><a class=\"anchor\" href=\"#312-serverproperties\">#</a> 3.1.2 server.properties</h6>\n<pre><code class=\"language-'\"># cat server.properties \n############################# Server Basics ############################# \n# broker的id，值为整数，且必须唯一，在一个集群中不能重复\nbroker.id=&#123;BROKER_ID&#125;\n\n############################# Socket Server Settings ############################# \n# kafka监听端口，默认9092\nlisteners=PLAINTEXT://&#123;LISTENERS&#125;:9092\n\n# 处理网络请求的线程数量，默认为3个\nnum.network.threads=3\n\n# 执行磁盘IO操作的线程数量，默认为8个 \nnum.io.threads=8\n\n# socket服务发送数据的缓冲区大小，默认100KB\nsocket.send.buffer.bytes=102400\n\n# socket服务接受数据的缓冲区大小，默认100KB\nsocket.receive.buffer.bytes=102400\n\n# socket服务所能接受的一个请求的最大大小，默认为100M\nsocket.request.max.bytes=104857600\n\n############################# Log Basics ############################# \n# kafka存储消息数据的目录\nlog.dirs=&#123;KAFKA_DATA_DIR&#125;\n\n# 每个topic默认的partition\nnum.partitions=1\n\n# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。\ndefault.replication.factor=3\n\n# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量\nnum.recovery.threads.per.data.dir=1\n\n############################# Log Flush Policy ############################# \n# 消息刷新到磁盘中的消息条数阈值\nlog.flush.interval.messages=10000\n\n# 消息刷新到磁盘中的最大时间间隔,1s\nlog.flush.interval.ms=1000\n\n############################# Log Retention Policy ############################# \n# 日志保留小时数，超时会自动删除，默认为7天\nlog.retention.hours=168\n\n# 日志保留大小，超出大小会自动删除，默认为1G\n#log.retention.bytes=1073741824\n\n# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件\nlog.segment.bytes=1073741824\n\n# 每隔多长时间检测数据是否达到删除条件,300s\nlog.retention.check.interval.ms=300000\n\n############################# Zookeeper ############################# \n# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开\nzookeeper.connect=&#123;ZOOK_SERVERS&#125;\n\n# 连接zookeeper的超时时间,6s\nzookeeper.connection.timeout.ms=6000\n</code></pre>\n<h6 id=\"313-entrypoint\"><a class=\"anchor\" href=\"#313-entrypoint\">#</a> 3.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 变量\nKAFKA_DIR=/kafka\nKAFKA_CONF=/kafka/config/server.properties\n\n# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一\nBROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))\nLISTENERS=$(hostname -i)\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;\n\n# 3、启动Kafka\ncd $&#123;KAFKA_DIR&#125;/bin\nsed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh\n./kafka-server-start.sh ../config/server.properties\n</code></pre>\n<h6 id=\"314-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#314-构建镜像并推送仓库\">#</a> 3.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2\n</code></pre>\n<h5 id=\"32-迁移kafka集群至k8s\"><a class=\"anchor\" href=\"#32-迁移kafka集群至k8s\">#</a> 3.2 迁移 Kafka 集群至 K8S</h5>\n<h6 id=\"321-kafka-headless\"><a class=\"anchor\" href=\"#321-kafka-headless\">#</a> 3.2.1 kafka-headless</h6>\n<pre><code># cat 01-kafka-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: kafka\n  ports:\n  - name: client\n    port: 9092\n    targetPort: 9092\n  - name: jmx\n    port: 9999\n    targetPort: 9999\n</code></pre>\n<h6 id=\"322-kafka-sts\"><a class=\"anchor\" href=\"#322-kafka-sts\">#</a> 3.2.2 kafka-sts</h6>\n<pre><code># cat 02-kafka-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  namespace: logging\nspec:\n  serviceName: &quot;kafka-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kafka\n  template:\n    metadata:\n      labels:\n        app: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;kafka&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kafka\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 9092\n        - name: jmxport\n          containerPort: 9999\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"323-更新资源清单\"><a class=\"anchor\" href=\"#323-更新资源清单\">#</a> 3.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml \n[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml\n[root@k8s-master01 02-kafka]# kubectl get pods -n logging \nNAME          READY   STATUS    RESTARTS       AGE\nkafka-0       1/1     Running   0              5m49s\nkafka-1       1/1     Running   0              4m43s\nkafka-2       1/1     Running   0              3m40s\n\n#查看kafka是否注册到zookeeper\n[root@k8s-master01 02-kafka]# kubectl exec -it zookeeper-0 -n logging -- /bin/bash\nroot@zookeeper-0:/# /zookeeper/bin/zkCli.sh \n[zk: localhost:2181(CONNECTED) 2] get /brokers/ids/1\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.85.201:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.85.201&quot;,&quot;timestamp&quot;:&quot;1748162470218&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n[zk: localhost:2181(CONNECTED) 3] get /brokers/ids/2\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.58.205:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.58.205&quot;,&quot;timestamp&quot;:&quot;1748162532658&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n[zk: localhost:2181(CONNECTED) 4] get /brokers/ids/3\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.195.1:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.195.1&quot;,&quot;timestamp&quot;:&quot;1748162649250&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n</code></pre>\n<h6 id=\"324-检查kafka集群\"><a class=\"anchor\" href=\"#324-检查kafka集群\">#</a> 3.2.4 检查 Kafka 集群</h6>\n<pre><code>1.创建一个topic\nroot@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu\n\n2.模拟消息发布\nroot@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu\n&gt;hello kubernetes\n&gt;hello world\n\n3.模拟消息订阅\nroot@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning\nhello kubernetes\nhello world\n</code></pre>\n<h4 id=\"四-交付efak至k8s\"><a class=\"anchor\" href=\"#四-交付efak至k8s\">#</a> 四、交付 efak 至 K8S</h4>\n<h5 id=\"41-制作efak镜像\"><a class=\"anchor\" href=\"#41-制作efak镜像\">#</a> 4.1 制作 efak 镜像</h5>\n<h6 id=\"411-dockerfile\"><a class=\"anchor\" href=\"#411-dockerfile\">#</a> 4.1.1 Dockerfile</h6>\n<pre><code>[root@manager 03-efak]# cat Dockerfile \nFROM openjdk:8\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=3.0.1\nADD ./efak-web-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./system-config.properties /efak-web-$&#123;VERSION&#125;/conf/system-config.properties\n\n# 3、修改efak的名称\nRUN mv /efak-web-$&#123;VERSION&#125; /efak\n\n# 4、环境变量\nENV KE_HOME=/efak\nENV PATH=$PATH:$KE_HOME/bin\n\n# 5、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 6、暴露kafka端口 9999是jmx的端口\nEXPOSE 8048\n\n# 7、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"412-system-config\"><a class=\"anchor\" href=\"#412-system-config\">#</a> 4.1.2 system-config</h6>\n<pre><code># cat system-config.properties \n######################################\n# 填写 zookeeper集群列表\n######################################\nefak.zk.cluster.alias=cluster1\ncluster1.zk.list=&#123;ZOOK_SERVERS&#125;\n\n######################################\n# broker 最大规模数量\n######################################\ncluster1.efak.broker.size=20\n\n######################################\n# zk 客户端线程数\n######################################\nkafka.zk.limit.size=32\n\n######################################\n# EFAK webui 端口\n######################################\nefak.webui.port=8048\n\n######################################\n# kafka offset storage\n######################################\ncluster1.efak.offset.storage=kafka\n\n######################################\n# kafka jmx uri\n######################################\ncluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi\n\n######################################\n# kafka metrics 指标，默认存储15天\n######################################\nefak.metrics.charts=true\nefak.metrics.retain=15\n\n######################################\n# kafka sql topic records max\n######################################\nefak.sql.topic.records.max=5000\nefak.sql.topic.preview.records.max=10\n\n######################################\n# delete kafka topic token\n######################################\nefak.topic.token=keadmin\n\n######################################\n# kafka sqlite 数据库地址（需要修改存储路径）\n######################################\nefak.driver=org.sqlite.JDBC\nefak.url=jdbc:sqlite:&#123;EFAK_DATA_DIR&#125;/db/ke.db\nefak.username=root\nefak.password=www.kafka-eagle.org\n</code></pre>\n<h6 id=\"413-entrypoint\"><a class=\"anchor\" href=\"#413-entrypoint\">#</a> 4.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 1、变量\nEFAK_DIR=/efak\nEFAK_CONF=/efak/conf/system-config.properties\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;EFAK_DATA_DIR&#125;@$&#123;EFAK_DIR&#125;@g  $&#123;EFAK_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;EFAK_CONF&#125;\n\n# 3、启动efka\n$&#123;EFAK_DIR&#125;/bin/ke.sh start\ntail -f $&#123;EFAK_DIR&#125;/logs/ke_console.out\n</code></pre>\n<h6 id=\"414-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#414-构建镜像并推送仓库\">#</a> 4.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0\n</code></pre>\n<h5 id=\"42-迁移efak至k8s\"><a class=\"anchor\" href=\"#42-迁移efak至k8s\">#</a> 4.2 迁移 efak 至 K8S</h5>\n<h6 id=\"421-efak-deploy\"><a class=\"anchor\" href=\"#421-efak-deploy\">#</a> 4.2.1 efak-deploy</h6>\n<pre><code># cat 01-efak-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efak\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: efak\n  template:\n    metadata:\n      labels:\n        app: efak\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: efak\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 \n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8048\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n</code></pre>\n<h6 id=\"422-efak-service\"><a class=\"anchor\" href=\"#422-efak-service\">#</a> 4.2.2 efak-service</h6>\n<pre><code># cat 02-efak-service.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: efak-svc\n  namespace: logging\nspec:\n  selector:\n    app: efak\n  ports:\n  - port: 8048\n    targetPort: 8048\n</code></pre>\n<h6 id=\"423-efak-ingress\"><a class=\"anchor\" href=\"#423-efak-ingress\">#</a> 4.2.3 efak-ingress</h6>\n<pre><code># cat 03-efak-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: efak-ingress\n  namespace: logging\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;efak.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: efak-svc\n            port: \n              number: 8048\n</code></pre>\n<h6 id=\"424-更新资源清单\"><a class=\"anchor\" href=\"#424-更新资源清单\">#</a> 4.2.4 更新资源清单</h6>\n<pre><code>[root@k8s-master01 03-efak]# kubectl apply -f 01-efak-deploy.yaml \n[root@k8s-master01 03-efak]# kubectl apply -f 02-efak-service.yaml \n[root@k8s-master01 03-efak]# kubectl apply -f 03-efak-ingress.yaml \n</code></pre>\n<h6 id=\"425-访问efka\"><a class=\"anchor\" href=\"#425-访问efka\">#</a> 4.2.5 访问 efka</h6>\n<p>1、初始用户名密码 admin   123456</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Nq16u4z.png\" alt=\"1.png\" /></p>\n<p>2、查看 Topics</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/9Bin9cr.png\" alt=\"2.png\" /></p>\n<p>3、查看 kafka 集群状态</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/U76YIck.png\" alt=\"3.png\" /></p>\n<p>4、查看 Zookeeper 集群状态</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/cY5LeWx.png\" alt=\"4.png\" /></p>\n<h4 id=\"五-交付elastic集群\"><a class=\"anchor\" href=\"#五-交付elastic集群\">#</a> 五、交付 Elastic 集群</h4>\n<ul>\n<li>ES 集群是由多个节点组成的，通过 <a href=\"http://cluster.name\">cluster.name</a> 设置 ES 集群名称，同时用于区分其它的 ES 集群。</li>\n<li>每个节点通过 <a href=\"http://node.name\">node.name</a> 参数来设定所在集群的节点名称。</li>\n<li>节点使用 discovery.send_hosts 参数来设定集群节点的列表。</li>\n<li>集群在第一次启动时，需要初始化，同时需要指定参与选举的 master 节点 IP，或节点名称。</li>\n<li>每个节点可以通过 node.master:true 设定为 master 角色，通过 node.data:true 设定为 data 角色。</li>\n</ul>\n<pre><code>[root@k8s-master01 ~]# grep &quot;^[a-Z]&quot; /etc/elasticsearch/elasticsearch.yml\n# 集群名称cluster.name: my-oldxu\n# 节点名称node.name: node1\n# 数据存储路径path.data: /var/lib/elasticsearch\n# 日志存储路径path.logs: /var/log/elasticsearch\n# 监听在本地哪个地址上network.host: 10.0.0.100\n# 监听端口http.port: 9200\n# 集群主机列表discovery.seed_hosts: [&quot;ip1&quot;, &quot;ip2&quot;, &quot;ip3&quot;]\n# 仅第一次启动集群时进行选举（可以填写node.name的名称）cluster.initial_master_nodes: [&quot;node01&quot;, &quot;node02&quot;, &quot;node03&quot;]\n</code></pre>\n<h5 id=\"51-下载elastic镜像\"><a class=\"anchor\" href=\"#51-下载elastic镜像\">#</a> 5.1 下载 elastic 镜像</h5>\n<pre><code># docker pull elasticsearch:7.17.6\n# docker tag elasticsearch:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6\n</code></pre>\n<h5 id=\"52-交付es-service\"><a class=\"anchor\" href=\"#52-交付es-service\">#</a> 5.2 交付 ES-Service</h5>\n<p>创建 es-headlessService，为每个 ES Pod 设定固定的 DNS 名称，无论它是 Master 或是 Data，易或是 Coordinating</p>\n<pre><code># cat 01-es-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: es-svc\n  namespace: logging\nspec:\n  selector:\n    app: es\n  clusterIP: None\n  ports:\n  - name: cluster\n    port: 9200\n    targetPort: 9200\n  - name: transport\n    port: 9300\n    targetPort: 9300\n</code></pre>\n<h5 id=\"53-交付es-master节点\"><a class=\"anchor\" href=\"#53-交付es-master节点\">#</a> 5.3 交付 ES-Master 节点</h5>\n<ol>\n<li>\n<p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data ；</p>\n</li>\n<li>\n<p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p>\n</li>\n<li>\n<p>ES 启动是通过 ENV 环境变量传参来完成的；</p>\n<ul>\n<li>\n<p>集群名称、节点名称、角色类型；</p>\n</li>\n<li>\n<p>discovery.seed_hosts 集群地址列表；</p>\n</li>\n<li>\n<p>cluster.initial_master_nodes 初始集群参与选举的 master 节点名称；</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre><code># cat 02-es-master.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-master\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 3           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: master\n  template:\n    metadata:\n      labels:\n        app: es\n        role: master\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;master&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;true&quot;\n        - name: node.data\n          value: &quot;false&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n        - name: cluster.initial_master_nodes\n          value: &quot;es-master-0,es-master-1,es-master-2&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<pre><code>[root@k8s-master01 04-elasticsearch]# cat 03-es-data.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-data\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 2           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: data\n  template:\n    metadata:\n      labels:\n        app: es\n        role: data\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;data&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;false&quot;\n        - name: node.data\n          value: &quot;true&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"54-交付es-data节点\"><a class=\"anchor\" href=\"#54-交付es-data节点\">#</a> 5.4 交付 ES-Data 节点</h5>\n<ol>\n<li>\n<p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data</p>\n</li>\n<li>\n<p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p>\n</li>\n<li>\n<p>ES 启动是通过 ENV 环境变量传参来完成的</p>\n<ul>\n<li>\n<p>集群名称、节点名称、角色类型</p>\n</li>\n<li>\n<p>discovery.seed_hosts 集群地址列表</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre><code># cat 03-es-data.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-data\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 2           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: data\n  template:\n    metadata:\n      labels:\n        app: es\n        role: data\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;data&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;false&quot;\n        - name: node.data\n          value: &quot;true&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"55-更新资源清单\"><a class=\"anchor\" href=\"#55-更新资源清单\">#</a> 5.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 04-elasticsearch]# kubectl apply -f 01-es-svc.yaml \n[root@k8s-master01 04-elasticsearch]# kubectl apply -f 02-es-master.yaml \n[root@k8s-master01 04-elasticsearch]# kubectl apply -f 03-es-data.yaml \n</code></pre>\n<h5 id=\"56-验证es集群\"><a class=\"anchor\" href=\"#56-验证es集群\">#</a> 5.6 验证 ES 集群</h5>\n<pre><code>#1.解析headlessService获取对应ES集群任一节点的IP地址\n# dig @10.96.0.10 es-svc.logging.svc.cluster.local  +short\n172.16.58.229\n172.16.122.191\n172.16.195.21\n172.16.122.129\n172.16.32.164\n\n#2.通过curl访问ES，检查ES集群是否正常（如果仅交付Master，没有data节点，集群状态可能会Red，因为没有数据节点进行数据存储；）\n# curl -XGET &quot;http://172.16.122.129:9200/_cluster/health?pretty&quot;\n&#123;\n  &quot;cluster_name&quot; : &quot;es-cluster&quot;,\n  &quot;status&quot; : &quot;green&quot;,\n  &quot;timed_out&quot; : false,\n  &quot;number_of_nodes&quot; : 5,\n  &quot;number_of_data_nodes&quot; : 2,\n  &quot;active_primary_shards&quot; : 3,\n  &quot;active_shards&quot; : 6,\n  &quot;relocating_shards&quot; : 0,\n  &quot;initializing_shards&quot; : 0,\n  &quot;unassigned_shards&quot; : 0,\n  &quot;delayed_unassigned_shards&quot; : 0,\n  &quot;number_of_pending_tasks&quot; : 0,\n  &quot;number_of_in_flight_fetch&quot; : 0,\n  &quot;task_max_waiting_in_queue_millis&quot; : 0,\n  &quot;active_shards_percent_as_number&quot; : 100.0\n&#125;\n\n#3.查看ES各个节点详情\n# curl -XGET &quot;http://172.16.122.129:9200/_cat/nodes&quot;\n172.16.122.129 16 33 20 0.38 0.56 0.38 ilmr       - es-master-2\n172.16.58.229  66 33 22 0.64 0.66 0.44 ilmr       * es-master-1\n172.16.122.191 52 34 15 0.38 0.56 0.38 cdfhilrstw - es-data-0\n172.16.195.21  38 35 19 0.38 0.53 0.36 cdfhilrstw - es-data-1\n172.16.32.164  31 33 12 0.28 0.50 0.59 ilmr       - es-master-0\n</code></pre>\n<h4 id=\"六-交付kibana可视化\"><a class=\"anchor\" href=\"#六-交付kibana可视化\">#</a> 六、交付 Kibana 可视化</h4>\n<h5 id=\"61-下载kibana镜像\"><a class=\"anchor\" href=\"#61-下载kibana镜像\">#</a> 6.1 下载 kibana 镜像</h5>\n<pre><code># docker pull kibana:7.17.6\n# docker tag kibana:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6\n</code></pre>\n<h5 id=\"62-kibana-deploy\"><a class=\"anchor\" href=\"#62-kibana-deploy\">#</a> 6.2 kibana-deploy</h5>\n<ol>\n<li>Kibana 需要连接 ES 集群，通过 ELASTICSEARCH_HOSTS 变量来传递 ES 集群地址</li>\n<li>kibana 通过 I18N_LOCALE 来传递语言环境</li>\n<li>Kibana 通过 SERVER_PUBLICBASEURL 来传递服务访问的公开地址</li>\n</ol>\n<pre><code># cat 01-kibana-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kibana\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kibana\n  template:\n    metadata:\n      labels:\n        app: kibana\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kibana\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n        ports:\n        - containerPort: 5601\n        env:\n        - name: ELASTICSEARCH_HOSTS\n          value: '[&quot;http://es-data-0.es-svc:9200&quot;,&quot;http://es-data-1.es-svc:9200&quot;]'\n        - name: I18N_LOCALE\n          value: &quot;zh-CN&quot;\n        - name: SERVER_PUBLICBASEURL\n          value: &quot;http://kibana.hmallleasing.com&quot;   #kibana访问UI\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n</code></pre>\n<h5 id=\"63-kibana-svc\"><a class=\"anchor\" href=\"#63-kibana-svc\">#</a> 6.3 kibana-svc</h5>\n<pre><code># cat 02-kibana-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kibana-svc\n  namespace: logging\nspec:\n  selector:\n    app: kibana\n  ports:\n  - name: web\n    port: 5601\n    targetPort: 5601\n</code></pre>\n<h5 id=\"64-kibana-ingress\"><a class=\"anchor\" href=\"#64-kibana-ingress\">#</a> 6.4 kibana-ingress</h5>\n<pre><code># cat 03-kibana-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kibana-ingress\n  namespace: logging\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;kibana.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kibana-svc\n            port:\n              number: 5601\n</code></pre>\n<h5 id=\"65-更新资源清单\"><a class=\"anchor\" href=\"#65-更新资源清单\">#</a> 6.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 05-kibana]# kubectl apply -f 01-kibana-deploy.yaml \n[root@k8s-master01 05-kibana]# kubectl apply -f 02-kibana-svc.yaml \n[root@k8s-master01 05-kibana]# kubectl apply -f 03-kibana-ingress.yaml\n\n[root@k8s-master01 05-kibana]# kubectl get pods -n logging\nNAME                      READY   STATUS    RESTARTS   AGE\nefak-5cdc74bf59-nrhb4     1/1     Running   0          5h33m\nes-data-0                 1/1     Running   0          16m\nes-data-1                 1/1     Running   0          15m\nes-master-0               1/1     Running   0          17m\nes-master-1               1/1     Running   0          15m\nes-master-2               1/1     Running   0          12m\nkafka-0                   1/1     Running   0          5h39m\nkafka-1                   1/1     Running   0          5h39m\nkafka-2                   1/1     Running   0          5h38m\nkibana-5ccc46864b-ndzx9   1/1     Running   0          118s\nzookeeper-0               1/1     Running   0          5h42m\nzookeeper-1               1/1     Running   0          5h42m\nzookeeper-2               1/1     Running   0          5h41m\n</code></pre>\n<h5 id=\"66-访问kibana\"><a class=\"anchor\" href=\"#66-访问kibana\">#</a> 6.6 访问 kibana</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/sUXTx1J.png\" alt=\"1.png\" /></p>\n<h4 id=\"七-filebeat-sidecar收集业务应用日志\"><a class=\"anchor\" href=\"#七-filebeat-sidecar收集业务应用日志\">#</a> 七、filebeat-sidecar 收集业务应用日志</h4>\n<h5 id=\"71-部署架构说明\"><a class=\"anchor\" href=\"#71-部署架构说明\">#</a> 7.1 部署架构说明</h5>\n<p>对于那些能够将日志输出到本地文件的 Pod，我们可以使用 Sidecar 模式方式运行一个日志采集 Agent，对其进行单独收集日志。</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/u20K1ll.png\" alt=\"1.png\" /></p>\n<ol>\n<li>首先需要将 Pod 中的业务容器日志输出至本地文件，而后运行一个 Filebeat 边车容器，采集本地路径下的日志；</li>\n<li>Filebeat 容器需要传递如下变量；\n<ul>\n<li>ENV：了解 Pod 属于隶属于哪个环境；</li>\n<li>PROJECT_NAME：为了后期能在单个索引中区分出不同的项目；</li>\n<li>PodIP：为了让用户清楚该 Pod 属于哪个 IP；</li>\n<li>Node：用于获取该 Pod 所处的节点；</li>\n</ul>\n</li>\n<li>Logstash 根据不同的环境，拉取不同的 topic 数据，然后将数据存储至 ES 对应的索引中；</li>\n<li>Kibana 添加不同环境的 index pattern，而后选择对应环境不同的项目进行日志探索与展示；</li>\n</ol>\n<h5 id=\"72-sidecar部署思路\"><a class=\"anchor\" href=\"#72-sidecar部署思路\">#</a> 7.2 Sidecar 部署思路</h5>\n<ol>\n<li>制作一个业务镜像，要求镜像输出日志至本地；</li>\n<li>制作 Filebeat 镜像，配置 Input、output 等信息；</li>\n<li>采用边车模式运行不同环境的 Pod，确保日志信息能输出至 Kafka 集群；</li>\n<li>准备不同环境下 Logstash 配置文件，而后读取数据写入 ES 集群；</li>\n<li>使用 kibana 添加索引，进行日志探索与展示；</li>\n</ol>\n<h5 id=\"73-制作filebeat镜像\"><a class=\"anchor\" href=\"#73-制作filebeat镜像\">#</a> 7.3 制作 Filebeat 镜像</h5>\n<p><strong>7.3.1 下载 filebeat</strong></p>\n<pre><code>curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.17.6-x86_64.rpm\n</code></pre>\n<p><strong>7.3.2 编写 Dockerfile</strong></p>\n<pre><code># cat Dockerfile \n# 1、基础镜像\nFROM centos:7\n\n# 2、拷贝filebeat\nENV VERSION=7.17.6\nADD ./filebeat-$&#123;VERSION&#125;-x86_64.rpm /\nRUN rpm -ivh /filebeat-$&#123;VERSION&#125;-x86_64.rpm &amp;&amp; \\\n    rm -f /filebeat-$&#123;VERSION&#125;-x86_64.rpm\n\n# 3、拷贝filebeat配置文件（核心）\nADD ./filebeat.yml /etc/filebeat/filebeat.yml\n\n# 4、拷贝启动脚本\nADD ./entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\n# 5、执行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<p><strong>7.3.3 编写 entrypoint</strong></p>\n<pre><code># cat entrypoint.sh \n#启动脚本\n#1、替换filbeat配置文件中的内容\nBeat_Conf=/etc/filebeat/filebeat.yml\n\nsed -i s@&#123;ENV&#125;@$&#123;ENV:-test&#125;@g $&#123;Beat_Conf&#125;\nsed -i s@&#123;PodIP&#125;@$&#123;PodIP:-&quot;no-ip&quot;&#125;@g $&#123;Beat_Conf&#125;\nsed -i s@&#123;Node&#125;@$&#123;Node:-&quot;none&quot;&#125;@g $&#123;Beat_Conf&#125;\nsed -i s@&#123;PROJECT_NAME&#125;@$&#123;PROJECT_NAME:-&quot;no-define&quot;&#125;@g $&#123;Beat_Conf&#125;\nsed -i s@&#123;MULTILINE&#125;@$&#123;MULTILINE:-&quot;^\\\\\\d&#123;2&#125;&quot;&#125;@g $&#123;Beat_Conf&#125;\t\t# \\\\用来转义\nsed -i s@&#123;KAFKA_HOSTS&#125;@$&#123;KAFKA_HOSTS&#125;@g $&#123;Beat_Conf&#125;\n\n# 2、运行filebeat\nfilebeat -e -c /etc/filebeat/filebeat.yml\n</code></pre>\n<p><strong>7.3.4 编写 filebeat 配置</strong></p>\n<p>{ENV}：用于定义环境的变量；<br />\n{PROJECT_NAME}：用于定义项目名称的变量；<br />\n{MULTILINE}：用于定义多行合并的正则变量；<br />\n{KAFKA_HOSTS}：用于定义 KAFKA 集群地址的变量；<br />\n{PodIP}：用于获取该 Pod 地址的变量；<br />\n{Node}：用于获取该 Pod 所处的节点；</p>\n<pre><code>[root@k8s-master01 filebeat_sidecar_dockerfile]# cat filebeat.yml \nfilebeat.inputs:\n- type: log\n  enabled: true\n  paths:\n    - /logu/*.log\n    - /logu/*/*.log\n  tags: [&quot;logu&quot;]\n  fields:\n    topic: &#123;PROJECT_NAME&#125;\n    podip: &#123;PodIP&#125;\n    node: &#123;Node&#125;\n  fields_under_root: true               # 增加的所有字段都为顶级字段\n\n- type: log\n  enabled: true\n  paths:\n    - /logm/*.log\n    - /logm/*/*.log\n  tags: [&quot;logm&quot;]\n  fields:\n    topic: &#123;PROJECT_NAME&#125;\n    podip: &#123;PodIP&#125;\n    node: &#123;Node&#125;\n  fields_under_root: true               # 增加的所有字段都为顶级字段\n  multiline.pattern: '&#123;MULTILINE&#125;'      \n  multiline.negate: true\n  multiline.match: after\n  multiline.max_lines: 10000    #默认最大合并行为500，可根据实际情况调整。\n\noutput.kafka:\n  hosts: [&#123;KAFKA_HOSTS&#125;]\n  topic: app-&#123;ENV&#125;-%&#123;[topic]&#125;\n  required_acks: 1              # 保证消息可靠，0不保证，1等待写入主分区（默认），-1等待写入副本分区\n  compression: gzip             # 压缩\n  max_message_bytes: 1000000    # 每条消息最大的长度，多余的被删除\n</code></pre>\n<p><strong>7.3.5 构建并推送镜像</strong></p>\n<pre><code># docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6\n</code></pre>\n<h5 id=\"74-nf-flms-gateway日志收集\"><a class=\"anchor\" href=\"#74-nf-flms-gateway日志收集\">#</a> 7.4 nf-flms-gateway 日志收集</h5>\n<p><strong>7.4.1 创建 Namespace 和 Secrets</strong></p>\n<pre><code># sed -i &quot;s#dev#prod#g&quot; *.yaml\n# kubectl create ns prod\n# kubectl create secret tls prod-api.hmallleasig.com --key hmallleasing.com.key --cert hmallleasing.com.pem -n prod\n# kubectl create secret docker-registry harbor-admin --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd -n prod\n</code></pre>\n<p><strong>7.4.2 创建 nf-flms-gateway</strong></p>\n<pre><code># cat 01-nf-flms-gateway.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nf-flms-gateway\n  namespace: prod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nf-flms-gateway\n  template:\n    metadata:\n      labels:\n        app: nf-flms-gateway\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: nf-flms-gateway\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-gateway:v2.2 \n        command:\n        - &quot;/bin/sh&quot;\n        - &quot;-c&quot;\n        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-gateway.jar&quot;\n        resources:\n          limits:\n            cpu: '1000m'\n            memory: 1Gi\n          requests:\n            cpu: &quot;200m&quot;\n            memory: &quot;500Mi&quot;\n        ports:\n        - containerPort: 8080\n        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 2\n        livenessProbe:          # 存活探针，不存活会重启\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 2\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: log\n          mountPath: /logs    # 业务容器日志目录\n      - name: filebeat\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6 \n        imagePullPolicy: Always\n        volumeMounts:\n        - name: log\n          mountPath: /logm    # 匹配多行日志\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PodIP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: Node\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: PROJECT_NAME\n          value: &quot;nf-flms-gateway&quot;\n        - name: KAFKA_HOSTS\n          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      - name: log\n        emptyDir: &#123;&#125;\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: gateway-svc\n  namespace: prod\nspec:\n  selector:\n    app: nf-flms-gateway\n  ports:\n  - port: 8080\n    targetPort: 8080\n\n---\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: gateway-ingress\n  namespace: prod\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #禁用https强制跳转\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;prod-api.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: gateway-svc\n            port:\n              number: 8080\n  tls:                  #https\n  - hosts:\n    - prod-api.hmallleasing.com\n    secretName: &quot;prod-api.hmallleasig.com&quot;   #配置默认证书可不添加secretNam\n</code></pre>\n<p><strong>7.4.3 检查 KafkaTopic</strong></p>\n<p>1、检查是否有对应的 topic</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/SkS0yA0.png\" alt=\"2.png\" /></p>\n<p>2、点击对应的 Preview，查看 topic 中的最新数据</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/0SOP2qi.png\" alt=\"3.png\" /></p>\n<h5 id=\"75-nf-flms-order日志收集\"><a class=\"anchor\" href=\"#75-nf-flms-order日志收集\">#</a> 7.5 nf-flms-order 日志收集</h5>\n<p><strong>7.5.1 创建 PVC</strong></p>\n<p>创建 PVC 存储订单合同、身份证复印件等附件</p>\n<pre><code># cat 02-data-image.yaml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-image\n  namespace: prod\nspec:\n  storageClassName: &quot;nfs-storage&quot;     # 明确指定使用哪个sc的供应商来创建pv\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi                      # 根据业务实际大小进行资源申请\n</code></pre>\n<p><strong>7.5.2 创建 nf-flms-order</strong></p>\n<pre><code># cat 02-nf-flms-order.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nf-flms-order\n  namespace: prod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nf-flms-order\n  template:\n    metadata:\n      labels:\n        app: nf-flms-order\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: nf-flms-order\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-order:v2.0 \n        command:\n        - &quot;/bin/sh&quot;\n        - &quot;-c&quot;\n        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-order.jar&quot;\n        resources:\n          limits:\n            cpu: '1000m'\n            memory: 1Gi\n          requests:\n            cpu: &quot;200m&quot;\n            memory: &quot;500Mi&quot;\n        ports:\n        - containerPort: 8080\n        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 2\n        livenessProbe:          # 存活探针，不存活会重启\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 2\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: data-image\n          mountPath: /data\n        - name: log\n          mountPath: /logs    # 业务容器日志目录\n      - name: filebeat\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: log\n          mountPath: /logm    # 匹配多行日志\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PodIP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: Node\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: PROJECT_NAME\n          value: &quot;nf-flms-order&quot;\n        - name: KAFKA_HOSTS\n          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      - name: data-image\n        persistentVolumeClaim:      \n          claimName: data-image\n      - name: log\n        emptyDir: &#123;&#125;\n</code></pre>\n<p><strong>7.5.3 检查 KafkaTopic</strong></p>\n<p>1、检查是否有对应的 topic</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/AxF7dhj.png\" alt=\"4.png\" /></p>\n<p>2、点击对应的 Preview，查看 topic 中的最新数据</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/bYpKAsJ.png\" alt=\"5.png\" /></p>\n<h5 id=\"76-nf-flms-statistics日志收集\"><a class=\"anchor\" href=\"#76-nf-flms-statistics日志收集\">#</a> 7.6 nf-flms-statistics 日志收集</h5>\n<p><strong>7.6.1 创建 nf-flms-statistics</strong></p>\n<pre><code># cat 03-nf-flms-statistics.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nf-flms-statistics\n  namespace: prod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nf-flms-statistics\n  template:\n    metadata:\n      labels:\n        app: nf-flms-statistics\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: nf-flms-statistics\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-statistics:v2.0 \n        command: \n        - &quot;/bin/sh&quot;\n        - &quot;-c&quot;\n        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-statistics.jar&quot;\n        resources:\n          limits:\n            cpu: '1000m'\n            memory: 1Gi\n          requests:\n            cpu: &quot;200m&quot;\n            memory: &quot;500Mi&quot;\n        ports:\n        - containerPort: 8080\n        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 2\n        livenessProbe:          # 存活探针，不存活会重启\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n          timeoutSeconds: 3\n          successThreshold: 1\n          failureThreshold: 2\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: log\n          mountPath: /logs    # 业务容器日志目录\n      - name: filebeat\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: log\n          mountPath: /logm    # 匹配多行日志\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PodIP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: Node\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: PROJECT_NAME\n          value: &quot;nf-flms-statistics&quot;\n        - name: KAFKA_HOSTS\n          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      - name: log\n        emptyDir: &#123;&#125;\n</code></pre>\n<p><strong>7.6.2 检查 KafkaTopic</strong></p>\n<p>1、检查是否有对应的 topic</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/kD0B7C3.png\" alt=\"6.png\" /></p>\n<p>2、点击对应的 Preview，查看 topic 中的最新数据</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/JO9HMip.png\" alt=\"7.png\" /></p>\n<h5 id=\"77-nf-flms-system日志收集\"><a class=\"anchor\" href=\"#77-nf-flms-system日志收集\">#</a> 7.7 nf-flms-system 日志收集</h5>\n<p><strong>7.7.1 创建 nf-flms-system</strong></p>\n<pre><code># cat 04-nf-flms-system.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nf-flms-system\n  namespace: prod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nf-flms-system\n  template:\n    metadata:\n      labels:\n        app: nf-flms-system\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: nf-flms-system\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-system:v2.0 \n        command:\n        - &quot;/bin/sh&quot;\n        - &quot;-c&quot;\n        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-system.jar&quot;\n        resources:\n          limits:\n            cpu: '1000m'\n            memory: 1Gi\n          requests:\n            cpu: &quot;200m&quot;\n            memory: &quot;500Mi&quot;\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: log\n          mountPath: /logs    # 业务容器日志目录\n      - name: filebeat\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: log\n          mountPath: /logm    # 匹配多行日志\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PodIP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: Node\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: PROJECT_NAME\n          value: &quot;nf-flms-system&quot;\n        - name: KAFKA_HOSTS\n          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      - name: log\n        emptyDir: &#123;&#125;\n</code></pre>\n<p><strong>7.7.2 检查 KafkaTopic</strong></p>\n<p>1、检查是否有对应的 topic</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Rj4wUMF.png\" alt=\"1.png\" /></p>\n<p>2、点击对应的 Preview，查看 topic 中的最新数据</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/JYCPcgP.png\" alt=\"2.png\" /></p>\n<h5 id=\"78-nf-flms-openapi日志收集\"><a class=\"anchor\" href=\"#78-nf-flms-openapi日志收集\">#</a> 7.8 nf-flms-openapi 日志收集</h5>\n<p><strong>7.8.1 创建 nf-flms-openapi</strong></p>\n<pre><code># cat 06-nf-flms-openapi.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nf-flms-openapi\n  namespace: prod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nf-flms-openapi\n  template:\n    metadata:\n      labels:\n        app: nf-flms-openapi\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: nf-flms-openapi\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-openapi:v2.2 \n        command: \n        - &quot;/bin/sh&quot;\n        - &quot;-c&quot;\n        - &quot;java -Xms256m -Xmx1024m -Dspring.profiles.active=prd -Djava.security.egd=file:/dev/./urandom -jar -Duser.timezone=GMT+08 nf-flms-openapi.jar&quot;\n        resources:\n          limits:\n            cpu: '1000m'\n            memory: 1Gi\n          requests:\n            cpu: &quot;200m&quot;\n            memory: &quot;500Mi&quot;\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: log\n          mountPath: /logs    # 业务容器日志目录\n      - name: filebeat\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: log\n          mountPath: /logm    # 匹配多行日志\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PodIP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: Node\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: PROJECT_NAME\n          value: &quot;nf-flms-openapi&quot;\n        - name: KAFKA_HOSTS\n          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      - name: log\n        emptyDir: &#123;&#125;\n</code></pre>\n<p><strong>7.8.2 检查 KafkaTopic</strong></p>\n<p>1、检查是否有对应的 topic</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/esmEJF7.png\" alt=\"3.png\" /></p>\n<p>2、点击对应的 Preview，查看 topic 中的最新数据</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/2HvC3KR.png\" alt=\"4.png\" /></p>\n<h5 id=\"79-nf-flms-ui日志收集\"><a class=\"anchor\" href=\"#79-nf-flms-ui日志收集\">#</a> 7.9 nf-flms-ui 日志收集</h5>\n<p><strong>7.9.1 准备 Nginx 配置文件</strong></p>\n<pre><code># cat prod.hmallleasing.com.conf \nserver &#123;\n        listen 80;\n        server_name prod.hmallleasing.com;\n        root /code/prod;\n\n        location / &#123;\n            index  index.html index.htm;\n        &#125;\n&#125;\n\nserver &#123;\n        listen 80;\n        server_name prod-api.hmallleasing.com;\n\n        location / &#123;\n                proxy_set_header Host $http_host;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header REMOTE-HOST $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_pass http://gateway-svc.prod.svc.cluster.local:8080;\n        &#125;\n&#125;\n</code></pre>\n<p><strong>7.9.2 创建 ConfigMap</strong></p>\n<pre><code>kubectl create configmap nf-flms-ui-conf --from-file=./prod.hmallleasing.com.conf -n prod\n</code></pre>\n<p><strong>7.9.3 创建 nf-flms-ui</strong></p>\n<pre><code>[root@k8s-master01 06-service-all]# cat 07-ui-deploy-ingress.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nf-flms-ui\n  namespace: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nf-flms-ui\n  template:\n    metadata:\n      labels:\n        app: nf-flms-ui\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: nf-flms-ui\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/nf-flms-ui:v1.0\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: '1000m'\n            memory: 1Gi\n          requests:\n            cpu: &quot;200m&quot;\n            memory: &quot;500Mi&quot;\n        readinessProbe:         # 就绪探针，不就绪则从负载均衡移除\n          tcpSocket:\n            port: 80\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 10\n        livenessProbe:          # 存活探针，不存活会重启\n          tcpSocket:\n            port: 80\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: ngxconfs\n          mountPath: /etc/nginx/conf.d/\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: log\n          mountPath: /var/log/nginx/    # 业务容器日志目录\n      - name: filebeat\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/filebeat_sidecar:7.17.6\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: log\n          mountPath: /logu    # 匹配多行日志\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ENV\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: PodIP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: Node\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: PROJECT_NAME\n          value: &quot;nf-flms-ui&quot;\n        - name: KAFKA_HOSTS\n          value: '&quot;kafka-0.kafka-svc.logging:9092&quot;,&quot;kafka-1.kafka-svc.logging:9092&quot;,&quot;kafka-2.kafka-svc.logging:9092&quot;'\n      volumes:\n      - name: ngxconfs\n        configMap:\n          name: nf-flms-ui-conf\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      - name: log\n        emptyDir: &#123;&#125;\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nf-flms-ui-svc\n  namespace: prod\nspec:\n  selector:\n    app: nf-flms-ui\n  ports:\n  - port: 80\n    targetPort: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nf-flms-ui-ingress\n  namespace: prod\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #禁用https强制跳转\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;prod.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nf-flms-ui-svc\n            port:\n              number: 80\n  tls:                  #https\n  - hosts:\n    - prod.hmallleasing.com\n    secretName: &quot;prod-api.hmallleasig.com&quot;   #配置默认证书可不添加secretName\n</code></pre>\n<p><strong>7.9.2 检查 KafkaTopic</strong></p>\n<p>1、检查是否有对应的 topic</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/9Nh2ywz.png\" alt=\"1.png\" /></p>\n<p>2、点击对应的 Preview，查看 topic 中的最新数据</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/xB0bjWA.png\" alt=\"2.png\" /></p>\n<h4 id=\"八-交付生产环境logstash\"><a class=\"anchor\" href=\"#八-交付生产环境logstash\">#</a> 八、交付生产环境 Logstash</h4>\n<h5 id=\"81-拉取镜像\"><a class=\"anchor\" href=\"#81-拉取镜像\">#</a> 8.1 拉取镜像</h5>\n<pre><code># docker pull docker.elastic.co/logstash/logstash-oss:7.17.6\n# docker tag docker.elastic.co/logstash/logstash-oss:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n</code></pre>\n<h5 id=\"82-编写logstash配置\"><a class=\"anchor\" href=\"#82-编写logstash配置\">#</a> 8.2 编写 logstash 配置</h5>\n<pre><code>[root@k8s-master01 conf]# cat logstash-prod.conf \ninput &#123;\n    kafka &#123;\n        bootstrap_servers =&gt; &quot;kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092&quot;\n        group_id =&gt; &quot;logstash-prod&quot;      # 消费者组名称\n        consumer_threads =&gt; &quot;3&quot;          # 理想情况下，配置与分区数一样多的线程，实现均衡\n        topics_pattern =&gt; &quot;app-prod-.*&quot;  # 通过正则表达式匹配要订阅的主题\n    &#125;\n&#125;\n\nfilter &#123;\n\tjson &#123;\n\t\tsource =&gt; &quot;message&quot;\n\t&#125;\n&#125;\n\noutput &#123;\n    stdout &#123;\n        codec =&gt; rubydebug\n    &#125;\n    elasticsearch &#123;\n        hosts =&gt; [&quot;es-data-0.es-svc:9200&quot;,&quot;es-data-1.es-svc:9200&quot;]\n        index =&gt; &quot;app-prod-%&#123;+YYYY.MM.dd&#125;&quot;\n        template_overwrite =&gt; true\n    &#125;\n&#125;\n</code></pre>\n<h5 id=\"83-创建生产环境configmap\"><a class=\"anchor\" href=\"#83-创建生产环境configmap\">#</a> 8.3 创建生产环境 configmap</h5>\n<pre><code>kubectl create configmap logstash-prod-conf --from-file=logstash.conf=conf/logstash-prod.conf -n logging\n</code></pre>\n<h5 id=\"84-创建生产环境logstash\"><a class=\"anchor\" href=\"#84-创建生产环境logstash\">#</a> 8.4 创建生产环境 Logstash</h5>\n<p>1、创建 logstash-svc</p>\n<pre><code># cat 01-logstash-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: logstash-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: logstash\n  ports:\n  - port: 9600\n    targetPort: 9600\n</code></pre>\n<p>2、创建 logstash-StatefulSet</p>\n<pre><code># cat 05-logstash-prod-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: logstash-prod\n  namespace: logging\nspec:\n  serviceName: &quot;logstash-svc&quot;           # 使用此前创建的svc，则无需重复创建\n  replicas: 1\n  selector:\n    matchLabels:\n      app: logstash\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: logstash\n        env: prod\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: logstash\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n        args: [&quot;-f&quot;,&quot;config/logstash.conf&quot;]                     # 启动时指定加载的配置文件\n        resources:\n          limits:\n            memory: 1024Mi\n        env:\n        - name: PIPELINE_WORKERS\n          value: &quot;2&quot;\n        - name: PIPELINE_BATCH_SIZE\n          value: &quot;10000&quot;\n        lifecycle:\n          postStart:                                            # 设定JVM\n            exec:\n              command:\n              - &quot;/bin/bash&quot;\n              - &quot;-c&quot;\n              - &quot;sed -i -e '/^-Xms/c-Xms1024m' -e '/^-Xmx/c-Xmx1024m' /usr/share/logstash/config/jvm.options&quot;\n        volumeMounts:\n        - name: data                                            # 持久化数据目录\n          mountPath: /usr/share/logstash/data\n        - name: conf\n          mountPath: /usr/share/logstash/config/logstash.conf\n          subPath: logstash.conf\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: conf\n        configMap:\n          name: logstash-prod-conf\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;         \n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"85-更新资源清单\"><a class=\"anchor\" href=\"#85-更新资源清单\">#</a> 8.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 08-logstash]# kubectl apply -f 01-logstash-svc.yaml \n[root@k8s-master01 08-logstash]# kubectl apply -f 05-logstash-prod-sts.yaml\n</code></pre>\n<h5 id=\"86-检查es生产环境索引\"><a class=\"anchor\" href=\"#86-检查es生产环境索引\">#</a> 8.6 检查 ES 生产环境索引</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/0OV3zd4.png\" alt=\"1.png\" /></p>\n<h4 id=\"九-交付测试环境logstash\"><a class=\"anchor\" href=\"#九-交付测试环境logstash\">#</a> 九、交付测试环境 Logstash</h4>\n<h5 id=\"91-拉取镜像\"><a class=\"anchor\" href=\"#91-拉取镜像\">#</a> 9.1 拉取镜像</h5>\n<pre><code># docker pull docker.elastic.co/logstash/logstash-oss:7.17.6\n# docker tag docker.elastic.co/logstash/logstash-oss:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n</code></pre>\n<h5 id=\"92-编写logstash配置\"><a class=\"anchor\" href=\"#92-编写logstash配置\">#</a> 9.2 编写 logstash 配置</h5>\n<pre><code>[root@k8s-master01 08-logstash]# cat conf/logstash-test.conf \ninput &#123;\n    kafka &#123;\n        bootstrap_servers =&gt; &quot;kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092&quot;\n        group_id =&gt; &quot;logstash-test&quot;      # 消费者组名称\n        consumer_threads =&gt; &quot;3&quot;          # 理想情况下，配置与分区数一样多的线程，实现均衡\n        topics_pattern =&gt; &quot;app-test-.*&quot;  # 通过正则表达式匹配要订阅的主题\n    &#125;\n&#125;\n\nfilter &#123;\n\tjson &#123;\n\t\tsource =&gt; &quot;message&quot;\n\t&#125;\n&#125;\n\noutput &#123;\n    stdout &#123;\n        codec =&gt; rubydebug\n    &#125;\n    elasticsearch &#123;\n        hosts =&gt; [&quot;es-data-0.es-svc:9200&quot;,&quot;es-data-1.es-svc:9200&quot;]\n        index =&gt; &quot;app-test-%&#123;+YYYY.MM.dd&#125;&quot;\n        template_overwrite =&gt; true\n    &#125;\n&#125;\n</code></pre>\n<h5 id=\"93-创建测试环境configmap\"><a class=\"anchor\" href=\"#93-创建测试环境configmap\">#</a> 9.3 创建测试环境 configmap</h5>\n<pre><code>kubectl create configmap logstash-test-conf --from-file=logstash.conf=conf/logstash-test.conf -n logging\n</code></pre>\n<h5 id=\"94-创建测试环境logstash\"><a class=\"anchor\" href=\"#94-创建测试环境logstash\">#</a> 9.4 创建测试环境 Logstash</h5>\n<p>1、创建 logstash-svc</p>\n<pre><code># cat 01-logstash-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: logstash-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: logstash\n  ports:\n  - port: 9600\n    targetPort: 9600\n</code></pre>\n<p>2、创建 logstash-StatefulSet</p>\n<pre><code>[root@k8s-master01 08-logstash]# cat 03-logstash-test-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: logstash-test\n  namespace: logging\nspec:\n  serviceName: &quot;logstash-svc&quot;           # 使用此前创建的svc，则无需重复创建\n  replicas: 1\n  selector:\n    matchLabels:\n      app: logstash\n      env: test\n  template:\n    metadata:\n      labels:\n        app: logstash\n        env: test\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: logstash\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/logstash-oss:7.17.6\n        args: [&quot;-f&quot;,&quot;config/logstash.conf&quot;]                     # 启动时指定加载的配置文件\n        resources:\n          limits:\n            memory: 1024Mi\n        env:\n        - name: PIPELINE_WORKERS\n          value: &quot;2&quot;\n        - name: PIPELINE_BATCH_SIZE\n          value: &quot;10000&quot;\n        lifecycle:\n          postStart:                                            # 设定JVM\n            exec:\n              command:\n              - &quot;/bin/bash&quot;\n              - &quot;-c&quot;\n              - &quot;sed -i -e '/^-Xms/c-Xms1024m' -e '/^-Xmx/c-Xmx1024m' /usr/share/logstash/config/jvm.options&quot;\n        volumeMounts:\n        - name: data                                            # 持久化数据目录\n          mountPath: /usr/share/logstash/data\n        - name: conf\n          mountPath: /usr/share/logstash/config/logstash.conf\n          subPath: logstash.conf\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone          \n      volumes:\n      - name: conf\n        configMap:\n          name: logstash-test-conf\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;          \n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"95-更新资源清单\"><a class=\"anchor\" href=\"#95-更新资源清单\">#</a> 9.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 08-logstash]# kubectl apply -f 01-logstash-svc.yaml \n[root@k8s-master01 08-logstash]# kubectl apply -f 03-logstash-test-sts.yaml\n</code></pre>\n<h5 id=\"96-检查es测试环境索引\"><a class=\"anchor\" href=\"#96-检查es测试环境索引\">#</a> 9.6 检查 ES 测试环境索引</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/0OV3zd4.png\" alt=\"1.png\" /></p>\n<h4 id=\"十-kibana数据展示\"><a class=\"anchor\" href=\"#十-kibana数据展示\">#</a> 十、Kibana 数据展示</h4>\n<h5 id=\"101-添加生产环境索引\"><a class=\"anchor\" href=\"#101-添加生产环境索引\">#</a> 10.1 添加生产环境索引</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/B5uofDF.png\" alt=\"2.png\" /></p>\n<h5 id=\"102-查看生产环境数据\"><a class=\"anchor\" href=\"#102-查看生产环境数据\">#</a> 10.2 查看生产环境数据</h5>\n<p>kibana-&gt;Discover</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/yY4Y4Sz.png\" alt=\"3.png\" /></p>\n<p>①点击开发环境索引 -&gt;②选择需要查看的字段 -&gt;③进行 filter 筛选项目 -&gt;④选择对应时间段</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/9i3HsC6.png\" alt=\"4.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/PPAQczE.png\" alt=\"5.png\" /></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/NkOwqO3.png\" alt=\"6.png\" /></p>\n<h6 id=\"\"><a class=\"anchor\" href=\"#\">#</a> </h6>\n",
            "tags": [
                "ELKStack"
            ]
        }
    ]
}