{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生 • All posts by \"elkstack\" category",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://ixuyong.cn",
    "items": [
        {
            "id": "http://ixuyong.cn/posts/170066797.html",
            "url": "http://ixuyong.cn/posts/170066797.html",
            "title": "消费租赁项目Kubernetes基于ELK日志分析与实践",
            "date_published": "2025-05-25T06:35:21.000Z",
            "content_html": "<h3 id=\"消费租赁项目kubernetes基于elk日志分析与实践\"><a class=\"anchor\" href=\"#消费租赁项目kubernetes基于elk日志分析与实践\">#</a> 消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</h3>\n<h6 id=\"snipaste_2025-05-25_13-43-46jpg\"><a class=\"anchor\" href=\"#snipaste_2025-05-25_13-43-46jpg\">#</a> <img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg\" alt=\"Snipaste_2025-05-25_13-43-46.jpg\" /></h6>\n<h4 id=\"1-elk创建namespace和secrets\"><a class=\"anchor\" href=\"#1-elk创建namespace和secrets\">#</a> 1. ELK 创建 Namespace 和 Secrets</h4>\n<pre><code># kubectl create ns logging\n# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=passwd\n</code></pre>\n<h4 id=\"2-交付zookeeper集群至k8s\"><a class=\"anchor\" href=\"#2-交付zookeeper集群至k8s\">#</a> 2. 交付 Zookeeper 集群至 K8S</h4>\n<h5 id=\"21-制作zk集群镜像\"><a class=\"anchor\" href=\"#21-制作zk集群镜像\">#</a> 2.1 制作 ZK 集群镜像</h5>\n<h6 id=\"211-dockerfile\"><a class=\"anchor\" href=\"#211-dockerfile\">#</a> 2.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、拷贝Zookeeper压缩包和配置文件\nENV VERSION=3.8.4\nADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf\n\n# 2、对Zookeeper文件夹名称重新命名\nRUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper\n\n# 3、拷贝eentrpoint的启动脚本文件\nADD ./entrypoint.sh /entrypoint.sh\n\n# 4、暴露Zookeeper端口\nEXPOSE 2181 2888 3888\n\n# 5、执行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"212-zoocfg\"><a class=\"anchor\" href=\"#212-zoocfg\">#</a> 2.1.2 zoo.cfg</h6>\n<pre><code># cat zoo.cfg \n# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。\ntickTime=&#123;ZOOK_TICKTIME&#125;\n\n# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime\ninitLimit=&#123;ZOOK_INIT_LIMIT&#125;\n\n# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime\nsyncLimit=&#123;ZOOK_SYNC_LIMIT&#125;\n \n# 数据保存目录\ndataDir=&#123;ZOOK_DATA_DIR&#125;\n\n# 日志保存目录\ndataLogDir=&#123;ZOOK_LOG_DIR&#125;\n\n# 客户端连接端口\nclientPort=&#123;ZOOK_CLIENT_PORT&#125;\n\n# 客户端最大连接数。# 根据自己实际情况设置，默认为60个\nmaxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;\n\n# 客户端获取 zookeeper 服务的当前状态及相关信息\n4lw.commands.whitelist=*\n\n# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口\n</code></pre>\n<h6 id=\"213-entrypoint\"><a class=\"anchor\" href=\"#213-entrypoint\">#</a> 2.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n#设定变量\nZOOK_BIN_DIR=/zookeeper/bin\nZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg\n\n# 2、对配置文件中的字符串进行变量替换\nsed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;\n\n# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来\nfor server in $&#123;ZOOK_SERVERS&#125;\ndo\n\techo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;\ndone\n\n# 4、在datadir目录中创建myid的文件，并填入对应的编号\nZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))\necho $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid\n\n#5、前台运行Zookeeper\ncd $&#123;ZOOK_BIN_DIR&#125;\n./zkServer.sh start-foreground\n</code></pre>\n<h6 id=\"214-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#214-构建镜像并推送仓库\">#</a> 2.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .\n# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4\n</code></pre>\n<h5 id=\"22-迁移zookeeper至k8s\"><a class=\"anchor\" href=\"#22-迁移zookeeper至k8s\">#</a> 2.2  迁移 zookeeper 至 K8S</h5>\n<h6 id=\"221-zookeeper-headless\"><a class=\"anchor\" href=\"#221-zookeeper-headless\">#</a> 2.2.1 zookeeper-headless</h6>\n<pre><code># cat 01-zookeeper-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: zookeeper\n  ports:\n  - name: client\n    port: 2181\n    targetPort: 2181\n  - name: leader-follwer\n    port: 2888\n    targetPort: 2888\n  - name: selection\n    port: 3888\n    targetPort: 3888\n</code></pre>\n<h6 id=\"222-zookeeper-sts\"><a class=\"anchor\" href=\"#222-zookeeper-sts\">#</a> 2.2.2 zookeeper-sts</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper               \n  namespace: logging\nspec:\n  serviceName: &quot;zookeeper-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;zookeeper&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: zookeeper\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 2181\n        - name: leader-follwer\n          containerPort: 2888\n        - name: selection\n          containerPort: 3888\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: data\n        - name: data\n          mountPath: /logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"223-更新资源清单\"><a class=\"anchor\" href=\"#223-更新资源清单\">#</a> 2.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml \n[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml\n[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging\nNAME          READY   STATUS    RESTARTS   AGE\nzookeeper-0   1/1     Running   0          17m\nzookeeper-1   1/1     Running   0          14m\nzookeeper-2   1/1     Running   0          11m\n</code></pre>\n<h6 id=\"224-检查zookeeper集群状态\"><a class=\"anchor\" href=\"#224-检查zookeeper集群状态\">#</a> 2.2.4 检查 zookeeper 集群状态</h6>\n<pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: leader\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\n</code></pre>\n<h6 id=\"225-连接zookeeper集群\"><a class=\"anchor\" href=\"#225-连接zookeeper集群\">#</a> 2.2.5 连接 Zookeeper 集群</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh\n# /zookeeper/bin/zkCli.sh -server zookeeper-svc\n[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu\nCreated /hello\n[zk: zookeeper-svc(CONNECTED) 1] get /hello\noldxu\n</code></pre>\n",
            "tags": [
                "ELKStack"
            ]
        }
    ]
}