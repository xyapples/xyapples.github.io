{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生 • All posts by \"kubernetes\" category",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://imxuyong.cn",
    "items": [
        {
            "id": "http://imxuyong.cn/posts/3833778957.html",
            "url": "http://imxuyong.cn/posts/3833778957.html",
            "title": "K8s计划任务Job、Cronjob",
            "date_published": "2025-04-19T13:00:21.000Z",
            "content_html": "<h3 id=\"k8s计划任务job-cronjob\"><a class=\"anchor\" href=\"#k8s计划任务job-cronjob\">#</a> K8s 计划任务 Job、Cronjob</h3>\n<h4 id=\"1-job配置参数详解\"><a class=\"anchor\" href=\"#1-job配置参数详解\">#</a> 1. Job 配置参数详解</h4>\n<pre><code># cat job.yaml \napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    job-name: echo\n  name: echo\n  namespace: default\nspec:\n  #suspend: true # 1.21+\n  #ttlSecondsAfterFinished: 100\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    spec:\n      containers:\n      - name: echo\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - echo &quot;Hello Job&quot;\n      restartPolicy: Never\n      \n[root@k8s-master01 ~]# kubectl get jobs\nNAME   STATUS     COMPLETIONS   DURATION   AGE\necho   Complete   1/1           70s        2m5s\n\n[root@k8s-master01 ~]# kubectl get pods\nNAME          READY   STATUS      RESTARTS      AGE\necho-564c8    0/1     Completed   0             2m10s\n\n[root@k8s-master01 ~]# kubectl logs echo-564c8\nHello Job\n</code></pre>\n<ul>\n<li>backoffLimit:：如果任务执行失败，失败多少次后不再执行</li>\n<li>completions：有多少个 Pod 执行成功，认为任务是成功的，默认为空和 parallelism 数值一样</li>\n<li>parallelism：并行执行任务的数量，如果 parallelism 数值大于 completions 数值，只会创建 completions 的数量；如果 completions 是 4，并发是 3，第一次会创建 3 个 Pod 执行任务，第二次只会创建一个 Pod 执行任务</li>\n<li>ttlSecondsAfterFinished：Job 在执行结束之后（状态为 completed 或 Failed）自动清理。设置为 0 表示执行结束立即删除，不设置则不会清除，需要开启 TTLAfterFinished 特性</li>\n</ul>\n<h4 id=\"2-cronjob配置参数详解\"><a class=\"anchor\" href=\"#2-cronjob配置参数详解\">#</a> 2. CronJob 配置参数详解</h4>\n<pre><code># cat cronjob.yaml \napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  concurrencyPolicy: Allow   #允许同时运行多个任务\n  failedJobsHistoryLimit: 10  #保留多少失败的任务\n  successfulJobsHistoryLimit: 10  #保留多少已完成的任务\n  #suspend: true             #如果true则取消周期性执行任务\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            command:\n            - sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure \n          \n[root@k8s-master01 ~]# kubectl get  cj\nNAME    SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nhello   */1 * * * *   &lt;none&gt;     False     0        6s              81s\n\n[root@k8s-master01 ~]# kubectl get  jobs\nNAME             STATUS     COMPLETIONS   DURATION   AGE\nhello-29084454   Complete   1/1           4s         72s\nhello-29084455   Complete   1/1           5s         12s\n\n[root@k8s-master01 ~]# kubectl get  pods\nNAME                   READY   STATUS      RESTARTS   AGE\nhello-29084454-hwv7p   0/1     Completed   0          78s\nhello-29084455-vf99w   0/1     Completed   0          18s\n\n[root@k8s-master01 ~]# kubectl logs -f hello-29084455-vf99w\nSat Apr 19 12:55:02 UTC 2025\nHello from the Kubernetes cluster\n</code></pre>\n<ul>\n<li>apiVersion: batch/v1beta1   #1.21+ batch/v1</li>\n<li>schedule：调度周期，和 Linux 一致，分别是分时日月周。</li>\n<li>restartPolicy：重启策略，和 Pod 一致。</li>\n<li>concurrencyPolicy：并发调度策略。可选参数如下：\n<ul>\n<li>Allow：允许同时运行多个任务。</li>\n<li>Forbid：不允许并发运行，如果之前的任务尚未完成，新的任务不会被创建。</li>\n<li>Replace：如果之前的任务尚未完成，新的任务会替换的之前的任务。</li>\n</ul>\n</li>\n<li>suspend：如果设置为 true，则暂停后续的任务，默认为 false。</li>\n<li>successfulJobsHistoryLimit：保留多少已完成的任务，按需配置。</li>\n<li>failedJobsHistoryLimit：保留多少失败的任务。</li>\n</ul>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/169153047.html",
            "url": "http://imxuyong.cn/posts/169153047.html",
            "title": "K8s持久化存储",
            "date_published": "2025-04-18T14:25:17.000Z",
            "content_html": "<h3 id=\"k8s持久化存储\"><a class=\"anchor\" href=\"#k8s持久化存储\">#</a> K8s 持久化存储</h3>\n<h4 id=\"1-volume\"><a class=\"anchor\" href=\"#1-volume\">#</a> 1. Volume</h4>\n<p>Container（容器）中的磁盘文件是短暂的，当容器崩溃时，kubelet 会重新启动容器，Container 会以最干净的状态启动，最初的文件将丢失。另外，当一个 Pod 运行多个 Container 时，各个容器可能需要共享一些文件。Kubernetes Volume 可以解决这两个问题。</p>\n<ul>\n<li>一些需要持久化数据的程序才会用到 Volumes，或者一些需要共享数据的容器需要 volumes。</li>\n<li>日志收集的需求需要在应用程序的容器里面加一个 sidecar，这个容器是一个收集日志的容器，比如 filebeat，它通过 volumes 共享应用程序的日志文件目录。</li>\n</ul>\n<h5 id=\"11-emptydir实现数据共享\"><a class=\"anchor\" href=\"#11-emptydir实现数据共享\">#</a> 1.1 EmptyDir 实现数据共享</h5>\n<p>和上述 volume 不同的是，如果删除 Pod，emptyDir 卷中的数据也将被删除，一般 emptyDir 卷用于 Pod 中的不同 Container 共享数据。</p>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n        - name: share-volume\n          emptyDir: &#123;&#125;\n      containers:\n        - name: nginx\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n            - name: share-volume\n              mountPath: /opt\n        - name: nginx2\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          command:\n            - sh\n            - '-c'\n            - sleep 3600\n          volumeMounts:\n            - name: share-volume\n              mountPath: /mnt\n</code></pre>\n<h5 id=\"12-volumes-hostpath挂载宿主机路径\"><a class=\"anchor\" href=\"#12-volumes-hostpath挂载宿主机路径\">#</a> 1.2 Volumes HostPath 挂载宿主机路径</h5>\n<p>hostPath 卷可将节点上的文件或目录挂载到 Pod 上，用于 Pod 自定义日志输出或访问 Docker 内部的容器等。</p>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n      - name: share-volume\n        emptyDir: &#123;&#125;\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      containers:\n        - name: nginx\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: share-volume\n            mountPath: /opt\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n        - name: nginx2\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          command:\n            - sh\n            - '-c'\n            - sleep 3600\n          volumeMounts:\n          - name: share-volume\n            mountPath: /mnt\n</code></pre>\n<p>hostPath 卷常用的 type（类型）如下：</p>\n<ul>\n<li>type 为空字符串：默认选项，意味着挂载 hostPath 卷之前不会执行任何检查。</li>\n<li>DirectoryOrCreate：如果给定的 path 不存在任何东西，那么将根据需要创建一个权限为 0755 的空目录，和 Kubelet 具有相同的组和权限。</li>\n<li>Directory：目录必须存在于给定的路径下。</li>\n<li>FileOrCreate：如果给定的路径不存储任何内容，则会根据需要创建一个空文件，权限设置为 0644，和 Kubelet 具有相同的组和所有权。</li>\n<li>File：文件，必须存在于给定路径中。</li>\n<li>Socket：UNIX 套接字，必须存在于给定路径中。</li>\n<li>CharDevice：字符设备，必须存在于给定路径中。</li>\n<li>BlockDevice：块设备，必须存在于给定路径中。</li>\n</ul>\n<h5 id=\"13-挂载nfs至容器\"><a class=\"anchor\" href=\"#13-挂载nfs至容器\">#</a> 1.3 挂载 NFS 至容器</h5>\n<pre><code>#1.安装nfs\n# yum install nfs-utils -y       \n# mkdir /data/nfs -p\n# vim /etc/exports \n/data 192.168.1.0/24(rw,no_root_squash)\n# exportfs -arv   \n# systemctl start nfs-server &amp;&amp; systemctl enable nfs-server &amp;&amp; systemctl status nfs-server \n\n#2.测试客户端挂载\n# showmount -e 192.168.1.75\n# mount -t nfs 192.168.1.75:/data/nfs /mnt\n\n#3.Deploy挂载NFS\n[root@k8s-master01 ~]# cat nginx-deploy-nfs.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n      annotations:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n      - name: nfs-volume\n        nfs:\n          server: 192.168.1.75\n          path: /data/nfs\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: nfs-volume\n            mountPath: /usr/share/nginx/html\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n</code></pre>\n<h4 id=\"2-pv-pvc\"><a class=\"anchor\" href=\"#2-pv-pvc\">#</a> 2. PV、PVC</h4>\n<p>PersistentVolume：简称 PV，是由 Kubernetes 管理员设置的存储，可以配置 Ceph、NFS、GlusterFS 等常用存储配置，相对于 Volume 配置，提供了更多的功能，比如生命周期的管理、大小的限制。PV 分为静态和动态。</p>\n<p>PersistentVolumeClaim：简称 PVC，是对存储 PV 的请求，表示需要什么类型的 PV，需要存储的技术人员只需要配置 PVC 即可使用存储，或者 Volume 配置 PVC 的名称即可。</p>\n<h5 id=\"21-pv回收策略\"><a class=\"anchor\" href=\"#21-pv回收策略\">#</a> 2.1 PV 回收策略</h5>\n<ul>\n<li>Retain：保留，该策略允许手动回收资源，当删除 PVC 时，PV 仍然存在，PV 被视为已释放，管理员可以手动回收卷。</li>\n<li>Recycle：回收，如果 Volume 插件支持，Recycle 策略会对卷执行 rm -rf 清理该 PV，并使其可用于下一个新的 PVC，但是本策略将来会被弃用，目前只有 NFS 和 HostPath 支持该策略。</li>\n<li>Delete：删除，如果 Volume 插件支持，删除 PVC 时会同时删除 PV，动态卷默认为 Delete，目前支持 Delete 的存储后端包括 AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder 等。</li>\n<li>可以通过 persistentVolumeReclaimPolicy: Recycle 字段配置</li>\n</ul>\n<h5 id=\"22-pv访问策略\"><a class=\"anchor\" href=\"#22-pv访问策略\">#</a> 2.2 PV 访问策略</h5>\n<ul>\n<li>ReadWriteOnce：可以被单节点以读写模式挂载，命令行中可以被缩写为 RWO。</li>\n<li>ReadOnlyMany：可以被多个节点以只读模式挂载，命令行中可以被缩写为 ROX。</li>\n<li>ReadWriteMany：可以被多个节点以读写模式挂载，命令行中可以被缩写为 RWX。</li>\n<li>ReadWriteOncePod ：只允许被单个 Pod 访问，需要 K8s 1.22 + 以上版本，并且是 CSI 创建的 PV 才可使用，缩写为 RWOP</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Volume Plugin</th>\n<th style=\"text-align:center\">ReadWriteOnce</th>\n<th style=\"text-align:center\">ReadOnlyMany</th>\n<th style=\"text-align:center\">ReadWriteMany</th>\n<th style=\"text-align:center\">ReadWriteOncePod</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">AzureFile</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CephFS</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CSI</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FC</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FlexVolume</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">HostPath</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">iSCSI</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">NFS</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">RBD</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">VsphereVolume</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">- (works when Pods are collocated)</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">PortworxVolume</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">✓</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"23-存储分类\"><a class=\"anchor\" href=\"#23-存储分类\">#</a> 2.3 存储分类</h5>\n<ul>\n<li>文件存储：一些数据可能需要被多个节点使用，比如用户的头像、用户上传的文件等，实现方式：NFS、NAS、FTP、CephFS 等。</li>\n<li>块存储：一些数据只能被一个节点使用，或者是需要将一块裸盘整个挂载使用，比如数据库、Redis 等，实现方式：Ceph、GlusterFS、公有云。</li>\n<li>对象存储：由程序代码直接实现的一种存储方式，云原生应用无状态化常用的实现方式，实现方式：一般是符合 S3 协议的云存储，比如 AWS 的 S3 存储、Minio、七牛云等。</li>\n</ul>\n<h5 id=\"24-pv配置示例nfs\"><a class=\"anchor\" href=\"#24-pv配置示例nfs\">#</a> 2.4 PV 配置示例 NFS</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv1\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: nfs-slow\n  nfs:\n    path: /data/pv1\n    server: 192.168.1.75\n</code></pre>\n<p>capacity：容量配置</p>\n<p>volumeMode：卷的模式，目前支持 Filesystem（文件系统） 和 Block（块），其中 Block 类型需要后端存储支持，默认为文件系统</p>\n<p>accessModes：该 PV 的访问模式</p>\n<p>storageClassName：PV 的类，一个特定类型的 PV 只能绑定到特定类别的 PVC；</p>\n<p>persistentVolumeReclaimPolicy：回收策略</p>\n<p>mountOptions：非必须，新版本中已弃用</p>\n<p>nfs：NFS 服务配置，包括以下两个选项</p>\n<ul>\n<li>path：NFS 上的共享目录</li>\n<li>server：NFS 的 IP 地址</li>\n</ul>\n<h5 id=\"25-pv配置示例hostpath\"><a class=\"anchor\" href=\"#25-pv配置示例hostpath\">#</a> 2.5 PV 配置示例 HostPath</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: hostpath\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: hostpath\n  hostPath:\n    path: &quot;/mnt/data&quot;\n</code></pre>\n<p>hostPath：hostPath 服务配置</p>\n<ul>\n<li>path：宿主机路径</li>\n</ul>\n<h5 id=\"26-pv的状态\"><a class=\"anchor\" href=\"#26-pv的状态\">#</a> 2.6 PV 的状态</h5>\n<ul>\n<li>Available：可用，没有被 PVC 绑定的空闲资源。</li>\n<li>Bound：已绑定，已经被 PVC 绑定。</li>\n<li>Released：已释放，PVC 被删除，但是资源还未被重新使用。</li>\n<li>Failed：失败，自动回收失败。</li>\n</ul>\n<h5 id=\"27-pvc绑定pv\"><a class=\"anchor\" href=\"#27-pvc绑定pv\">#</a> 2.7 PVC 绑定 PV</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  storageClassName: nfs-slow\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi      \n</code></pre>\n<ul>\n<li>PVC 的空间申请大小≤PV 的大小</li>\n<li>PVC 的 StorageClassName 和 PV 的一致</li>\n<li>PVC 的 accessModes 和 PV 的一致</li>\n</ul>\n<h5 id=\"28-depoyment挂载pvc\"><a class=\"anchor\" href=\"#28-depoyment挂载pvc\">#</a> 2.8 Depoyment 挂载 PVC</h5>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      volumes:\n      - name: nfs-pvc-storage  #volume名称\n        persistentVolumeClaim:\n          claimName: nfs-pvc   #PVC名称\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n         - name: nfs-pvc-storage\n          mountPath: /usr/share/nginx/html\n</code></pre>\n<p>挂载 PVC 的 Pod 一直处于 Pending：</p>\n<ul>\n<li>PVC 没有创建成功或 PVC 不存在</li>\n<li>PVC 和 Pod 不在同一个 Namespace</li>\n</ul>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/3992668367.html",
            "url": "http://imxuyong.cn/posts/3992668367.html",
            "title": "K8s配置管理Configmap",
            "date_published": "2025-04-14T13:47:47.000Z",
            "content_html": "<h3 id=\"k8s配置管理configmap\"><a class=\"anchor\" href=\"#k8s配置管理configmap\">#</a> K8s 配置管理 Configmap</h3>\n<h4 id=\"1-configmap\"><a class=\"anchor\" href=\"#1-configmap\">#</a> 1. Configmap</h4>\n<h5 id=\"1-1-基于from-env-file创建configmap\"><a class=\"anchor\" href=\"#1-1-基于from-env-file创建configmap\">#</a> 1. 1 基于 from-env-file 创建 Configmap</h5>\n<pre><code># cat cm_env.conf \npodname=nf-flms-system\npodip=192.168.1.100\nenv=prod\nnacosaddr=nacos.svc.cluster.local\n\n#kubectl create cm cmenv --from-env-file=./cm_env.conf \n</code></pre>\n<h5 id=\"12-基于from-literal创建configmap\"><a class=\"anchor\" href=\"#12-基于from-literal创建configmap\">#</a> 1.2 基于 from-literal 创建 Configmap</h5>\n<pre><code># kubectl create cm cmliteral --from-literal=level=INFO --from-literal=passwd=Superman*2023\n</code></pre>\n<h5 id=\"13-基于from-file创建configmap\"><a class=\"anchor\" href=\"#13-基于from-file创建configmap\">#</a> 1.3 基于 from-file 创建 Configmap</h5>\n<pre><code># cat s.hmallleasing.com.conf \nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    client_max_body_size 1G; \n    location / &#123;\n        proxy_pass http://192.168.1.134;\n        proxy_set_header Host $http_host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        proxy_connect_timeout 30;\n        proxy_send_timeout 60;\n        proxy_read_timeout 60;\n        \n        proxy_buffering on;\n        proxy_buffer_size 32k;\n        proxy_buffers 4 128k;\n        proxy_temp_file_write_size 10240k;\t\t\n        proxy_max_temp_file_size 10240k;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n\n# kubectl create cm nginxconfig --from-file=./s.hmallleasing.com.conf\n</code></pre>\n<h5 id=\"14-deployment挂载configmap示例\"><a class=\"anchor\" href=\"#14-deployment挂载configmap示例\">#</a> 1.4 Deployment 挂载 configmap 示例</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字\n</code></pre>\n<h5 id=\"15-重命名挂载的configmaq-key的名称\"><a class=\"anchor\" href=\"#15-重命名挂载的configmaq-key的名称\">#</a> 1.5 重命名挂载的 configmaq key 的名称</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字\n          items:                # 重命名挂载的configmaq key的名称为nginx.conf\n          - key: s.hmallleasing.com.conf  \n            path: nginx.conf\n \n#查看挂载的configmaq key的名称重命名为nginx.conf\n[root@k8s-master01 cm]# kubectl get pods\nNAME                           READY   STATUS    RESTARTS   AGE\nnginx-deploy-bc476bc56-flln4   1/1     Running   0          10h\nnginx-deploy-bc476bc56-jhsh6   1/1     Running   0          10h\nnginx-deploy-bc476bc56-splv9   1/1     Running   0          10h\n[root@k8s-master01 cm]# kubectl exec -it nginx-deploy-bc476bc56-flln4 -- bash\nroot@nginx-deploy-bc476bc56-flln4:/# ls /etc/nginx/conf.d/\nnginx.conf\n</code></pre>\n<h5 id=\"16-修改挂载的configmaq-权限\"><a class=\"anchor\" href=\"#16-修改挂载的configmaq-权限\">#</a> 1.6 修改挂载的 configmaq 权限</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字\n          items:                # 重命名挂载的configmaq key的名称为nginx.conf\n          - key: s.hmallleasing.com.conf  \n            path: nginx.conf\n            mode: 0644        # 配置挂载权限，针对单个key生效\n          defaultMode: 0666   # 配置挂载权限，针对整个key生效\n    \n#查看挂载权限\nroot@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/nginx.conf \nlrwxrwxrwx 1 root root 17 Apr 16 13:37 /etc/nginx/conf.d/nginx.conf -&gt; ..data/nginx.conf\nroot@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/..data/nginx.conf \n-rw-rw-rw- 1 root root 722 Apr 16 13:37 /etc/nginx/conf.d/..data/nginx.conf\n</code></pre>\n<h5 id=\"17-subpath解决挂载覆盖问题\"><a class=\"anchor\" href=\"#17-subpath解决挂载覆盖问题\">#</a> 1.7 subpath 解决挂载覆盖问题</h5>\n<pre><code>#1.创建configmap\n[root@k8s-master01 cm]# cat nginx.conf \n\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents &#123;\n    worker_connections  512;\n&#125;\n\n\nhttp &#123;\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '\n                      '$status $body_bytes_sent &quot;$http_referer&quot; '\n                      '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n&#125;\n\n[root@k8s-master01 cm]# kubectl create cm nginx-config --from-file=./nginx.conf\n\n#subpath解决挂载覆盖问题\n[root@k8s-master01 study]# cat cm-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # ①批量挂载ConfigMap生成环境变量\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # ②自定义环境变量\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # ③挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # 这个值来自ConfigMap\n              key: level            # 来自ConfigMap的key\n        volumeMounts:\n        - name: config\n          mountPath: &quot;/etc/nginx/nginx.conf&quot;   #只挂在nginx.conf一个文件,不覆盖目录\n          subPath: nginx.conf      \n      volumes:\n      - name: config\n        configMap:\n          name: nginx-config      # 提供你想要挂载的ConfigMap的名字\n</code></pre>\n<h4 id=\"2-secret\"><a class=\"anchor\" href=\"#2-secret\">#</a> 2. Secret</h4>\n<h5 id=\"21-secret拉取私有仓库镜像\"><a class=\"anchor\" href=\"#21-secret拉取私有仓库镜像\">#</a> 2.1 Secret 拉取私有仓库镜像</h5>\n<pre><code># kubectl create secret docker-registry harboradmin \\\n--docker-server=s.hmallleasing.com \\\n--docker-username=admin \\\n--docker-password=Superman*2023 \n</code></pre>\n<h5 id=\"22-创建ssl-secret\"><a class=\"anchor\" href=\"#22-创建ssl-secret\">#</a> 2.2 创建 ssl Secret</h5>\n<pre><code># kubectl create secret tls dev.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev\n</code></pre>\n<h5 id=\"23-基于命令创建generic-secret\"><a class=\"anchor\" href=\"#23-基于命令创建generic-secret\">#</a> 2.3 基于命令创建 generic Secret</h5>\n<pre><code>#1.通过from-env-file创建\n# cat db.conf \nusername=xuyong\npasswd=Superman*2023\n\n# kubectl create secret generic dbconf --from-env-file=./db.conf\n\n#2.通过from-literal创建\nkubectl create secret generic db-user-pass \\\n    --from-literal=username=admin \\\n    --from-literal=password='S!B\\*d$zDsb='\n</code></pre>\n<h5 id=\"24-secret加密-解密\"><a class=\"anchor\" href=\"#24-secret加密-解密\">#</a> 2.4 Secret 加密、解密</h5>\n<pre><code>1.加密\n# echo -n &quot;Superman*2023&quot; | base64\nU3VwZXJtYW4qMjAyMw==\n\n2.解密\n# echo &quot;U3VwZXJtYW4qMjAyMw==&quot; | base64 --decode\n</code></pre>\n<h5 id=\"25-基于文件创建非加密generic-secret\"><a class=\"anchor\" href=\"#25-基于文件创建非加密generic-secret\">#</a> 2.5 基于文件创建非加密 generic Secret</h5>\n<pre><code># kubectl get secret dbconf -oyaml\napiVersion: v1\ndata:\n  passwd: U3VwZXJtYW4qMjAyMw==\n  username: eHV5b25n\nkind: Secret\nmetadata:\n  name: dbconf\n  namespace: default\ntype: Opaque\n</code></pre>\n<h5 id=\"2-6-基于yaml创建加密generic-secret\"><a class=\"anchor\" href=\"#2-6-基于yaml创建加密generic-secret\">#</a> 2. 6 基于 yaml 创建加密 generic Secret</h5>\n<pre><code># cat mysql-secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysql-secret\n  namespace: dev\nstringData:\n  MYSQL_ROOT_PASSWORD: Superman*2023\ntype: Opaque\n</code></pre>\n<h5 id=\"27-deployment挂载secret示例\"><a class=\"anchor\" href=\"#27-deployment挂载secret示例\">#</a> 2.7 Deployment 挂载 Secret 示例</h5>\n<pre><code>[root@k8s-master01 study]# cat cm-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: MYSQL_ROOT_PASSWORD  \n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: MYSQL_ROOT_PASSWORD\n</code></pre>\n<h4 id=\"3-configmapsecret热更新\"><a class=\"anchor\" href=\"#3-configmapsecret热更新\">#</a> 3. ConfigMap&amp;Secret 热更新</h4>\n<pre><code># kubectl create cm nginxconfig --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f -\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/858611107.html",
            "url": "http://imxuyong.cn/posts/858611107.html",
            "title": "K8s服务发布Service",
            "date_published": "2025-04-14T11:25:51.000Z",
            "content_html": "<h3 id=\"k8s服务发布service\"><a class=\"anchor\" href=\"#k8s服务发布service\">#</a> K8s 服务发布 Service</h3>\n<h4 id=\"1-service类型\"><a class=\"anchor\" href=\"#1-service类型\">#</a> 1. Service 类型</h4>\n<p>Kubernetes Service Type（服务类型）主要包括以下几种：</p>\n<ul>\n<li>ClusterIP：在集群内部使用，默认值，只能从集群中访问。</li>\n<li>NodePort：在所有安装了 Kube-Proxy 的节点上打开一个端口，此端口可以代理至后端 Pod，可以通过 NodePort 从集群外部访问集群内的服务，格式为 NodeIP:NodePort。</li>\n<li>LoadBalancer：使用云提供商的负载均衡器公开服务，成本较高。</li>\n<li>ExternalName：通过返回定义的 CNAME 别名，没有设置任何类型的代理，需要 1.7 或更高版本 kube-dns 支持。</li>\n</ul>\n<h5 id=\"11-nodeport类型\"><a class=\"anchor\" href=\"#11-nodeport类型\">#</a> 1.1 NodePort 类型</h5>\n<p>如果将 Service 的 type 字段设置为 NodePort，则 Kubernetes 将从 --service-node-port-range 参数指定的范围（默认为 30000-32767）中自动分配端口，也可以手动指定 NodePort，创建该 Service 后，集群每个节点都将暴露一个端口，通过某个宿主机的 IP + 端口即可访问到后端的应用。</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: NodePort\n</code></pre>\n<h5 id=\"12-clusterip类型\"><a class=\"anchor\" href=\"#12-clusterip类型\">#</a> 1.2 ClusterIP 类型</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: ClusterIP\n</code></pre>\n<h5 id=\"13-使用service代理k8s外部服务\"><a class=\"anchor\" href=\"#13-使用service代理k8s外部服务\">#</a> 1.3 使用 Service 代理 K8s 外部服务</h5>\n<p>使用场景：</p>\n<ul>\n<li>希望在生产环境中使用某个固定的名称而非 IP 地址访问外部的中间件服务；</li>\n<li>希望 Service 指向另一个 Namespace 中或其他集群中的服务；</li>\n<li>正在将工作负载转移到 Kubernetes 集群，但是一部分服务仍运行在 Kubernetes 集群之外的 backend。</li>\n</ul>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: mysql-svc-external\n  name: mysql-svc-external\nspec:\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306 \n    protocol: TCP\n    targetPort: 3306\n  type: ClusterIP\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  labels:\n    app: mysql-svc-external\n  name: mysql-svc-external\nsubsets:\n- addresses:\n  - ip: 192.168.40.150\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n</code></pre>\n<h5 id=\"14-externalname-service\"><a class=\"anchor\" href=\"#14-externalname-service\">#</a> 1.4 ExternalName Service</h5>\n<p>ExternalName Service 是 Service 的特例，它没有 Selector，也没有定义任何端口和 Endpoint，它通过返回该外部服务的别名来提供服务。</p>\n<p>比如可以定义一个 Service，后端设置为一个外部域名，这样通过 Service 的名称即可访问到该域名。使用 nslookup 解析以下文件定义的 Service，集群的 DNS <a href=\"http://xn--my-uu2cmg2cx7mswf9rko5lsx1a5n3h.database.example.com\">服务将返回一个值为 my.database.example.com</a> 的 CNAME 记录：</p>\n<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: my-service\n  namespace: prod\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n</code></pre>\n<h5 id=\"15-多端口-service\"><a class=\"anchor\" href=\"#15-多端口-service\">#</a> 1.5 多端口 Service</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n    - port: 443\n      targetPort: 443\n      protocol: TCP\n      name: https\n  selector:\n    app: nginx\n  type: ClusterIP\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/108692210.html",
            "url": "http://imxuyong.cn/posts/108692210.html",
            "title": "K8s资源调度deployment、statefulset、daemonset",
            "date_published": "2025-04-14T11:25:00.000Z",
            "content_html": "<h3 id=\"k8s资源调度deployment-statefulset-daemonset\"><a class=\"anchor\" href=\"#k8s资源调度deployment-statefulset-daemonset\">#</a> K8s 资源调度 deployment、statefulset、daemonset</h3>\n<h4 id=\"1-无状态应用管理-deployment\"><a class=\"anchor\" href=\"#1-无状态应用管理-deployment\">#</a> 1. 无状态应用管理 Deployment</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:1.21.0\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n</code></pre>\n<p>示例解析：</p>\n<ol>\n<li>\n<p>nginx-deploy：Deployment 的名称；</p>\n</li>\n<li>\n<p>replicas： 创建 Pod 的副本数；</p>\n</li>\n<li>\n<p>selector：定义 Deployment 如何找到要管理的 Pod，与 template 的 label（标签）对应，apiVersion 为 apps/v1 必须指定该字段；</p>\n</li>\n<li>\n<p>template 字段包含以下字段：</p>\n<ul>\n<li>\n<p>app: nginx-deploy 使用 label（标签）标记 Pod；</p>\n</li>\n<li>\n<p>spec：表示 Pod 运行一个名字为 nginx 的容器；</p>\n</li>\n<li>\n<p>image：运行此 Pod 使用的镜像；</p>\n</li>\n<li>\n<p>Port：容器用于发送和接收流量的端口。</p>\n</li>\n</ul>\n</li>\n</ol>\n<h5 id=\"11-更新-deployment\"><a class=\"anchor\" href=\"#11-更新-deployment\">#</a> 1.1 更新 Deployment</h5>\n<p>假如更新 Nginx Pod 的 image 使用 nginx:latest，并使用 --record 记录当前更改的参数，后期回滚时可以查看到对应的信息：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record\n</code></pre>\n<p>更新过程为新旧交替更新，首先新建一个 Pod，当 Pod 状态为 Running 时，删除一个旧的 Pod，同时再创建一个新的 Pod。当触发一个更新后，会有新的 ReplicaSet 产生，旧的 ReplicaSet 会被保存，查看此时 ReplicaSet，可以从 AGE 或 READY 看出来新旧 ReplicaSet：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get rs\nNAME                      DESIRED   CURRENT   READY   AGE\nnginx-deploy-65bfb77869   0         0         0       50s\nnginx-deploy-85b94dddb4   3         3         3       8s\n</code></pre>\n<p>通过 describe 查看 Deployment 的详细信息：</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl describe deploy nginx-deploy\nName:                   nginx-deploy\nNamespace:              default\nCreationTimestamp:      Mon, 14 Apr 2025 11:28:03 +0800\nLabels:                 app=nginx-deploy\nAnnotations:            app: nginx-deploy\n                        deployment.kubernetes.io/revision: 2\n                        kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\nSelector:               app=nginx-deploy\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=nginx-deploy\n  Containers:\n   nginx-deploy:\n    Image:         nginx:latest\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Environment:   &lt;none&gt;\n    Mounts:        &lt;none&gt;\n  Volumes:         &lt;none&gt;\n  Node-Selectors:  &lt;none&gt;\n  Tolerations:     &lt;none&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  nginx-deploy-65bfb77869 (0/0 replicas created)\nNewReplicaSet:   nginx-deploy-85b94dddb4 (3/3 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  71s   deployment-controller  Scaled up replica set nginx-deploy-65bfb77869 from 0 to 3\n  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 0 to 1\n  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 3 to 2\n  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 1 to 2\n  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 2 to 1\n  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 2 to 3\n  Normal  ScalingReplicaSet  26s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 1 to 0\n</code></pre>\n<p>在 describe 中可以看出，第一次创建时，它创建了一个名为 nginx-deploy-65bfb77869 的 ReplicaSet，并直接将其扩展为 3 个副本。更新部署时，它创建了一个新的 ReplicaSet，命名为 nginx-deploy-85b94dddb4，并将其副本数扩展为 1，然后将旧的 ReplicaSet 缩小为 2，这样至少可以有 2 个 Pod 可用，最多创建了 4 个 Pod。以此类推，使用相同的滚动更新策略向上和向下扩展新旧 ReplicaSet，最终新的 ReplicaSet 可以拥有 3 个副本，并将旧的 ReplicaSet 缩小为 0。</p>\n<h5 id=\"12-回滚-deployment\"><a class=\"anchor\" href=\"#12-回滚-deployment\">#</a> 1.2 回滚 Deployment</h5>\n<p>当更新了版本不稳定或配置不合理时，可以对其进行回滚操作，假设我们又进行了几次更新（此处以更新镜像版本触发更新，更改配置效果类似）：</p>\n<pre><code># kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record\n# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record\n</code></pre>\n<p>使用 kubectl rollout history 查看更新历史：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy\ndeployment.apps/nginx-deploy \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\n3         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true\n4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n</code></pre>\n<p>查看 Deployment 某次更新的详细信息，使用 --revision 指定某次更新版本号：</p>\n<pre><code># kubectl rollout history deployment nginx-deploy --revision=4\ndeployment.apps/nginx-deploy with revision #4\nPod Template:\n  Labels:\tapp=nginx-deploy\n\tpod-template-hash=65b576b795\n  Annotations:\tkubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n  Containers:\n   nginx-deploy:\n    Image:\tnginx:1.21.2\n    Port:\t&lt;none&gt;\n    Host Port:\t&lt;none&gt;\n    Environment:\t&lt;none&gt;\n    Mounts:\t&lt;none&gt;\n  Volumes:\t&lt;none&gt;\n  Node-Selectors:\t&lt;none&gt;\n  Tolerations:\t&lt;none&gt;\n</code></pre>\n<p>如果只需要回滚到上一个稳定版本，使用 kubectl rollout undo 即可：</p>\n<pre><code># kubectl rollout undo deployment nginx-deploy\n</code></pre>\n<p>再次查看更新历史，发现 REVISION3 回到了 nginx:1.21.1：</p>\n<pre><code># kubectl rollout history deployment nginx-deploy\ndeployment.apps/nginx-deploy \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\n4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n5         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true\n</code></pre>\n<p>如果要回滚到指定版本，使用 --to-revision 参数：</p>\n<pre><code># kubectl rollout undo deployment nginx-deploy --to-revision=2\n</code></pre>\n<h5 id=\"13-扩容-deployment\"><a class=\"anchor\" href=\"#13-扩容-deployment\">#</a> 1.3 扩容 Deployment</h5>\n<p>当公司访问量变大，或者有预期内的活动时，三个 Pod 可能已无法支撑业务时，可以提前对其进行扩展。</p>\n<p>使用 kubectl scale 动态调整 Pod 的副本数，比如增加 Pod 为 5 个：</p>\n<pre><code># kubectl scale deployment nginx-deploy --replicas=5\n</code></pre>\n<p>查看 Pod，此时 Pod 已经变成了 5 个：</p>\n<pre><code># kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nnginx-deploy-85b94dddb4-2qrh6   1/1     Running   0          2m9s\nnginx-deploy-85b94dddb4-gvkqj   1/1     Running   0          2m10s\nnginx-deploy-85b94dddb4-mdfjs   1/1     Running   0          22s\nnginx-deploy-85b94dddb4-rhgpr   1/1     Running   0          2m8s\nnginx-deploy-85b94dddb4-vwjhl   1/1     Running   0          22s\n</code></pre>\n<h5 id=\"14-暂停和恢复-deployment-更新\"><a class=\"anchor\" href=\"#14-暂停和恢复-deployment-更新\">#</a> 1.4 暂停和恢复 Deployment 更新</h5>\n<p>上述演示的均为更改某一处的配置，更改后立即触发更新，大多数情况下可能需要针对一个资源文件更改多处地方，而并不需要多次触发更新，此时可以使用 Deployment 暂停功能，临时禁用更新操作，对 Deployment 进行多次修改后在进行更新。</p>\n<p>使用 kubectl rollout pause 命令即可暂停 Deployment 更新：</p>\n<pre><code># kubectl rollout pause deployment nginx-deploy\n</code></pre>\n<p>然后对 Deployment 进行相关更新操作，比如先更新镜像，然后对其资源进行限制（如果使用的是 kubectl edit 命令，可以直接进行多次修改，无需暂停更新，kubectlset 命令一般会集成在 CICD 流水线中）：</p>\n<pre><code># kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.3\n# kubectl set resources deployment nginx-deploy -c=nginx-deploy --limits=cpu=200m,memory=512Mi\n</code></pre>\n<p>通过 rollout history 可以看到没有新的更新：</p>\n<pre><code>#  kubectl rollout history deployment nginx-deploy\n</code></pre>\n<p>进行完最后一处配置更改后，使用 kubectl rollout resume 恢复 Deployment 更新：</p>\n<pre><code># kubectl rollout resume deployment nginx-deploy\n</code></pre>\n<p>可以查看到恢复更新的 Deployment 创建了一个新的 RS（ReplicaSet 缩写）：</p>\n<pre><code># kubectl get rs\n</code></pre>\n<p>可以查看 Deployment 的 image（镜像）已经变为 nginx:1.21.3</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n</code></pre>\n<h5 id=\"15-更新-deployment-的注意事项\"><a class=\"anchor\" href=\"#15-更新-deployment-的注意事项\">#</a> 1.5 更新 Deployment 的注意事项</h5>\n<p>在默认情况下，revision 保留 10 个旧的 ReplicaSet，其余的将在后台进行垃圾回收，可以在.spec.revisionHistoryLimit 设置保留 ReplicaSet 的个数。当设置为 0 时，不保留历史记录。</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  namespace: default\n  labels:\n    app: nginx-deploy\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:1.21.3\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  revisionHistoryLimit: 10\n</code></pre>\n<p>更新策略：</p>\n<ul>\n<li>spec.strategy.type==Recreate，表示重建，先删掉旧的 Pod 再创建新的 Pod；</li>\n</ul>\n<pre><code>  strategy:\n    type: Recreate\n</code></pre>\n<ul>\n<li>\n<p>spec.strategy.type==RollingUpdate，表示滚动更新，可以指定 maxUnavailable 和 maxSurge 来控制滚动更新过程；</p>\n<ul>\n<li>\n<p>spec.strategy.rollingUpdate.maxUnavailable，指定在回滚更新时最大不可用的 Pod 数量，可选字段，默认为 25%，可以设置为数字或百分比，如果 maxSurge 为 0，则该值不能为 0；</p>\n</li>\n<li>\n<p>spec.strategy.rollingUpdate.maxSurge 可以超过期望值的最大 Pod 数，可选字段，默认为 25%，可以设置成数字或百分比，如果 maxUnavailable 为 0，则该值不能为 0。</p>\n</li>\n</ul>\n</li>\n</ul>\n<pre><code>  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n</code></pre>\n<h4 id=\"2-有状态应用管理-statefulset\"><a class=\"anchor\" href=\"#2-有状态应用管理-statefulset\">#</a> 2. 有状态应用管理 StatefulSet</h4>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web\n  namespace: default\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: ClusterIP\n  clusterIP: None\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx\n  namespace: default\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          resources:\n            limits:\n              cpu: '1'\n              memory: 1Gi\n            requests:\n              cpu: 100m\n              memory: 128Mi\n      restartPolicy: Always\n  serviceName: web\n</code></pre>\n<ul>\n<li>kind: Service 定义了一个名字为 web 的 Headless Service，创建的 Service 格式为 nginx-0.web.default.svc.cluster.local，其他的类似，因为没有指定 Namespace（命名空间），所以默认部署在 default；</li>\n<li>kind: StatefulSet 定义了一个名字为 nginx 的 StatefulSet，replicas 表示部署 Pod 的副本数，本实例为 3。</li>\n</ul>\n<h5 id=\"21-创建-statefulset\"><a class=\"anchor\" href=\"#21-创建-statefulset\">#</a> 2.1 创建 StatefulSet</h5>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods\nNAME      READY   STATUS    RESTARTS   AGE\nnginx-0   1/1     Running   0          8m51s\nnginx-1   1/1     Running   0          8m50s\nnginx-2   1/1     Running   0          8m48s\n[root@k8s-master01 ~]# kubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   6d1h\nweb          ClusterIP   None         &lt;none&gt;        80/TCP    9m28s\n[root@k8s-master01 ~]# kubectl get sts\nNAME    READY   AGE\nnginx   3/3     8m58s\n</code></pre>\n<h5 id=\"22-statefulset创建pod流程\"><a class=\"anchor\" href=\"#22-statefulset创建pod流程\">#</a> 2.2 StatefulSet 创建 Pod 流程</h5>\n<p>StatefulSet 管理的 Pod 部署和扩展规则如下：</p>\n<ul>\n<li>对于具有 N 个副本的 StatefulSet，将按顺序从 0 到 N-1 开始创建 Pod；</li>\n<li>当删除 Pod 时，将按照 N-1 到 0 的反顺序终止；</li>\n<li>在缩放 Pod 之前，必须保证当前的 Pod 是 Running（运行中）或者 Ready（就绪）；</li>\n<li>在终止 Pod 之前，它所有的继任者必须是完全关闭状态。</li>\n</ul>\n<p>StatefulSet 的 pod.Spec.TerminationGracePeriodSeconds（终止 Pod 的等待时间）不应该指定为 0，设置为 0 对 StatefulSet 的 Pod 是极其不安全的做法，优雅地删除 StatefulSet 的 Pod 是非常有必要的，而且是安全的，因为它可以确保在 Kubelet 从 APIServer 删除之前，让 Pod 正常关闭。</p>\n<p>当创建上面的 Nginx 实例时，Pod 将按 nginx-0、nginx-1、nginx-2 的顺序部署 3 个 Pod。在 nginx-0 处于 Running 或者 Ready 之前，nginx-1 不会被部署，相同的，nginx-2 在 web-1 未处于 Running 和 Ready 之前也不会被部署。如果在 nginx-1 处于 Running 和 Ready 状态时，nginx-0 变成 Failed 失败）状态，那么 nginx-2 将不会被启动，直到 nginx-0 恢复为 Running 和 Ready 状态。</p>\n<p>如果用户将 StatefulSet 的 replicas 设置为 1，那么 nginx-2 将首先被终止，在完全关闭并删除 nginx-2 之前，不会删除 nginx-1。如果 nginx-2 终止并且完全关闭后，nginx-0 突然失败，那么在 nginx-0 未恢复成 Running 或者 Ready 时，nginx-1 不会被删除。</p>\n<h5 id=\"23-tatefulset-扩容和缩容\"><a class=\"anchor\" href=\"#23-tatefulset-扩容和缩容\">#</a> 2.3 tatefulSet 扩容和缩容</h5>\n<p>和 Deployment 类似，可以通过更新 replicas 字段扩容 / 缩容 StatefulSet，也可以使用 kubectlscale、kubectl edit 和 kubectl patch 来扩容 / 缩容一个 StatefulSet。</p>\n<pre><code># kubectl scale sts nginx --replicas=5\n</code></pre>\n<h5 id=\"24-statefulset-更新策略\"><a class=\"anchor\" href=\"#24-statefulset-更新策略\">#</a> 2.4 StatefulSet 更新策略</h5>\n<p><strong>On Delete 策略</strong></p>\n<p>OnDelete 更新策略实现了传统（1.7 版本之前）的行为，它也是默认的更新策略。当我们选择这个更新策略并修改 StatefulSet 的.spec.template 字段时，StatefulSet 控制器不会自动更新 Pod，必须手动删除 Pod 才能使控制器创建新的 Pod。</p>\n<pre><code>  updateStrategy:\n    type: OnDelete\n</code></pre>\n<p><strong>RollingUpdate 策略</strong></p>\n<p>RollingUpdate（滚动更新）更新策略会自动更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序进行滚动更新。</p>\n<pre><code>  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 0\n</code></pre>\n<h5 id=\"25-分段更新\"><a class=\"anchor\" href=\"#25-分段更新\">#</a> 2.5 分段更新</h5>\n<p>将分区改为 2，此时会自动更新 nginx-2、nginx-3、nginx-4（因为之前更改了更新策略），但是不会更新 nginx-0 和 nginx-1：</p>\n<pre><code>  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 2\n</code></pre>\n<p>将 sts 镜像为 nginx:1.21.1</p>\n<pre><code># kubectl set image sts nginx nginx=nginx:1.21.1\n</code></pre>\n<p>按照上述方式，可以实现分阶段更新，类似于灰度 / 金丝雀发布。查看最终的结果如下：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image\n    - image: nginx:latest\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:latest\n      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8\n    - image: nginx:latest\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:latest\n      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n</code></pre>\n<h4 id=\"3守护进程集-daemonset\"><a class=\"anchor\" href=\"#3守护进程集-daemonset\">#</a> 3. 守护进程集 DaemonSet</h4>\n<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ds\n  labels:\n    app: nginx-ds\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-ds\n  template:\n    metadata:\n      labels:\n        app: nginx-ds\n    spec:\n      containers:\n        - name: nginx-ds\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<p>此时会在每个节点创建一个 Pod：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-ds-47dxc   1/1     Running   0          56s   172.16.85.213    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-ds-4m89f   1/1     Running   0          56s   172.16.32.143    k8s-master01   &lt;none&gt;           &lt;none&gt;\nnginx-ds-mtpc2   1/1     Running   0          56s   172.16.195.12    k8s-master03   &lt;none&gt;           &lt;none&gt;\nnginx-ds-t5rxc   1/1     Running   0          56s   172.16.122.142   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-ds-x86kc   1/1     Running   0          56s   172.16.58.222    k8s-node02     &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<p>指定节点部署 Pod</p>\n<pre><code>      nodeSelector:\n        ingress: 'true'\n</code></pre>\n<p>更新和回滚 DaemonSet</p>\n<pre><code># kubectl set image ds nginx-ds nginx-ds=1.21.0 --record=true\n# kubectl rollout undo daemonset &lt;daemonset-name&gt; --to-revision=&lt;revision&gt;\n</code></pre>\n<p>DaemonSet 的更新和回滚与 Deployment 类似，此处不再演示。</p>\n<h4 id=\"4-hpa\"><a class=\"anchor\" href=\"#4-hpa\">#</a> 4. HPA</h4>\n<p>创建 deployment、service</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-hpa-svc\n  namespace: default\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx-hpa\n  type: ClusterIP\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-hpa\n  labels:\n    app: nginx-hpa\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-hpa\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-hpa\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-hpa\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<p>创建 HPA</p>\n<pre><code># kubectl autoscale deployment nginx-hpa --cpu-percent=10 --min=1 --max=10\n# kubectl get hpa\nNAME        REFERENCE              TARGETS       MINPODS   MAXPODS   REPLICAS   AGE\nnginx-hpa   Deployment/nginx-hpa   cpu: 0%/10%   1         10        1          16s\n\n</code></pre>\n<p>测试自动扩缩容</p>\n<pre><code>while true; do wget -q -O- http://10.96.18.221 &gt; /dev/null; done\n[root@k8s-master01 ~]# kubectl get pods\nNAME                        READY   STATUS    RESTARTS   AGE\nnginx-hpa-d8bcbdf7d-4mkxp   1/1     Running   0          66s\nnginx-hpa-d8bcbdf7d-974q5   1/1     Running   0          6m36s\nnginx-hpa-d8bcbdf7d-g6p2h   1/1     Running   0          66s\nnginx-hpa-d8bcbdf7d-lvvsq   1/1     Running   0          111s\nnginx-hpa-d8bcbdf7d-tgqmr   1/1     Running   0          111s\nnginx-hpa-d8bcbdf7d-tzfbs   1/1     Running   0          21s\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/1771242682.html",
            "url": "http://imxuyong.cn/posts/1771242682.html",
            "title": "K8s零宕机服务发布-探针",
            "date_published": "2025-04-14T11:23:48.000Z",
            "content_html": "<h3 id=\"k8s零宕机服务发布-探针\"><a class=\"anchor\" href=\"#k8s零宕机服务发布-探针\">#</a> K8s 零宕机服务发布 - 探针</h3>\n<h4 id=\"1-pod状态及-pod-故障排查命令\"><a class=\"anchor\" href=\"#1-pod状态及-pod-故障排查命令\">#</a> 1. Pod 状态及 Pod 故障排查命令</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">状态</th>\n<th style=\"text-align:left\">说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">Pending（挂起）</td>\n<td style=\"text-align:left\">Pod 已被 Kubernetes 系统接收，但仍有一个或多个容器未被创建，可以通过 kubectl describe 查看处于 Pending 状态的原因</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Running（运行中）</td>\n<td style=\"text-align:left\">Pod 已经被绑定到一个节点上，并且所有的容器都已经被创建，而且至少有一个是运行状态，或者是正在启动或者重启，可以通过 kubectl logs 查看 Pod 的日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Succeeded（成功）</td>\n<td style=\"text-align:left\">所有容器执行成功并终止，并且不会再次重启，可以通过 kubectl logs 查看 Pod 日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Failed（失败）</td>\n<td style=\"text-align:left\">所有容器都已终止，并且至少有一个容器以失败的方式终止，也就是说这个容器要么以非零状态退出，要么被系统终止，可以通过 logs 和 describe 查看 Pod 日志和状态</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Unknown（未知）</td>\n<td style=\"text-align:left\">通常是由于通信问题造成的无法获得 Pod 的状态</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ImagePullBackOff ErrImagePull</td>\n<td style=\"text-align:left\">镜像拉取失败，一般是由于镜像不存在、网络不通或者需要登录认证引起的，可以使用 describe 命令查看具体原因</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CrashLoopBackOff</td>\n<td style=\"text-align:left\">容器启动失败，可以通过 logs 命令查看具体原因，一般为启动命令不正确，健康检查不通过等</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">OOMKilled</td>\n<td style=\"text-align:left\">容器内存溢出，一般是容器的内存 Limit 设置的过小，或者程序本身有内存溢出，可以通过 logs 查看程序启动日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Terminating</td>\n<td style=\"text-align:left\">Pod 正在被删除，可以通过 describe 查看状态</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">SysctlForbidden</td>\n<td style=\"text-align:left\">Pod 自定义了内核配置，但 kubelet 没有添加内核配置或配置的内核参数不支持，可以通过 describe 查看具体原因</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Completed</td>\n<td style=\"text-align:left\">容器内部主进程退出，一般计划任务执行结束会显示该状态，此时可以通过 logs 查看容器日志</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ContainerCreating</td>\n<td style=\"text-align:left\">Pod 正在创建，一般为正在下载镜像，或者有配置不当的地方，可以通过 describe 查看具体原因</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"2-pod镜像拉取策略\"><a class=\"anchor\" href=\"#2-pod镜像拉取策略\">#</a> 2. Pod 镜像拉取策略</h4>\n<p>通过 spec.containers [].imagePullPolicy 参数可以指定镜像的拉取策略，目前支持的策略如下：</p>\n<table>\n<thead>\n<tr>\n<th>操作方式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Always</td>\n<td>总是拉取，当镜像 tag 为 latest 时，且 imagePullPolicy 未配置，默认为 Always</td>\n</tr>\n<tr>\n<td>Never</td>\n<td>不管是否存在都不会拉取</td>\n</tr>\n<tr>\n<td>IfNotPresent</td>\n<td>镜像不存在时拉取镜像，如果 tag 为非 latest，且 imagePullPolicy 未配置，默认为 IfNotPresent</td>\n</tr>\n</tbody>\n</table>\n<p>更改镜像拉取策略为 IfNotPresent：</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n</code></pre>\n<h4 id=\"3-pod-重启策略\"><a class=\"anchor\" href=\"#3-pod-重启策略\">#</a> 3. <strong>Pod</strong> 重启策略</h4>\n<table>\n<thead>\n<tr>\n<th>操作方式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Always</td>\n<td>默认策略。容器失效时，自动重启该容器</td>\n</tr>\n<tr>\n<td>OnFailure</td>\n<td>容器以不为 0 的状态码终止，自动重启该容器</td>\n</tr>\n<tr>\n<td>Never</td>\n<td>无论何种状态，都不会重启</td>\n</tr>\n</tbody>\n</table>\n<p>指定重启策略为 Always ：</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n</code></pre>\n<h4 id=\"4-pod的三种探针\"><a class=\"anchor\" href=\"#4-pod的三种探针\">#</a> 4. Pod 的三种探针</h4>\n<table>\n<thead>\n<tr>\n<th>种类</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>startupProbe</td>\n<td>Kubernetes1.16 新加的探测方式，用于判断容器内的应用程序是否已经启动。如果配置了 startupProbe，就会先禁用其他探测，直到它成功为止。如果探测失败，Kubelet 会杀死容器，之后根据重启策略进行处理，如果探测成功，或没有配置 startupProbe，则状态为成功，之后就不再探测。</td>\n</tr>\n<tr>\n<td>livenessProbe</td>\n<td>用于探测容器是否在运行，如果探测失败，kubelet 会 “杀死” 容器并根据重启策略进行相应的处理。如果未指定该探针，将默认为 Success</td>\n</tr>\n<tr>\n<td>readinessProbe</td>\n<td>一般用于探测容器内的程序是否健康，即判断容器是否为就绪（Ready）状态。如果是，则可以处理请求，反之 Endpoints Controller 将从所有的 Service 的 Endpoints 中删除此容器所在 Pod 的 IP 地址。如果未指定，将默认为 Success</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"5-pod探针的实现方式\"><a class=\"anchor\" href=\"#5-pod探针的实现方式\">#</a> 5. Pod 探针的实现方式</h4>\n<table>\n<thead>\n<tr>\n<th>实现方式</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ExecAction</td>\n<td>在容器内执行一个指定的命令，如果命令返回值为 0，则认为容器健康</td>\n</tr>\n<tr>\n<td>TCPSocketAction</td>\n<td>通过 TCP 连接检查容器指定的端口，如果端口开放，则认为容器健康</td>\n</tr>\n<tr>\n<td>HTTPGetAction</td>\n<td>对指定的 URL 进行 Get 请求，如果状态码在 200~400 之间，则认为容器健康</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"6-健康检查配置\"><a class=\"anchor\" href=\"#6-健康检查配置\">#</a> 6. 健康检查配置</h4>\n<p>配置健康检查：</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          startupProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          livenessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          readinessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            httpGet:\n              path: /index.html\n              port: 80\n              scheme: HTTP\n      restartPolicy: Always\n</code></pre>\n<h4 id=\"7-prestop和-poststart配置\"><a class=\"anchor\" href=\"#7-prestop和-poststart配置\">#</a> 7. PreStop 和 PostStart 配置</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          startupProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          livenessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          readinessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            httpGet:\n              path: /index.html\n              port: 80\n              scheme: HTTP\n          lifecycle:\n            postStart:\n              exec:\n                command:\n                  - sh\n                  - '-c'\n                  - mkdir /data\n            preStop:\n              exec:\n                command:\n                  - sh\n                  - '-c'\n                  - sleep 30\n      restartPolicy: Always\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/985149017.html",
            "url": "http://imxuyong.cn/posts/985149017.html",
            "title": "二进制高可用安装K8S集群",
            "date_published": "2025-04-10T12:58:40.000Z",
            "content_html": "<h2 id=\"二进制高可用安装k8s集群\"><a class=\"anchor\" href=\"#二进制高可用安装k8s集群\">#</a> 二进制高可用安装 K8s 集群</h2>\n<h4 id=\"1-基本配置\"><a class=\"anchor\" href=\"#1-基本配置\">#</a> 1. 基本配置</h4>\n<h5 id=\"11-基本环境配置\"><a class=\"anchor\" href=\"#11-基本环境配置\">#</a> 1.1 基本环境配置</h5>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP 地址</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master 节点 * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived 虚拟 IP（不占用机器）</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker 节点 * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* 配置信息 *</strong></em></th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>系统版本</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod 网段</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service 网段</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>所有节点</mark>更改主机名（其它节点按需修改）：</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>所有节点</mark>配置 hosts，修改 /etc/hosts 如下：</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>所有节点</mark>配置 yum 源：</p>\n<pre><code># 配置基础源\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>所有节点</mark>必备工具安装：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>所有节点</mark>关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>所有节点</mark>关闭 swap 分区：</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>所有节点</mark>安装 ntpdate：</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>所有节点</mark>同步时间并配置上海时区：</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# 加入到crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>所有节点</mark>配置 limit：</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>所有节点</mark>升级系统：</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 节点</mark>免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上</em></p>\n<p><mark>Master01 节点</mark>下载安装所有的源码文件：</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-内核配置\"><a class=\"anchor\" href=\"#12-内核配置\">#</a> 1.2 内核配置</h5>\n<p><mark>所有节点</mark>安装 ipvsadm：</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>所有节点</mark>配置 ipvs 模块：</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>所有节点</mark>创建 ipvs.conf，并配置开机自动加载：</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# 加入以下内容\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>所有节点</mark>然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>所有节点</mark>内核优化配置：</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>所有节点</mark>应用配置：</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>配置完内核后，重启机器，之后查看内核模块是否已自动加载：</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-高可用组件安装\"><a class=\"anchor\" href=\"#2-高可用组件安装\">#</a> 2. 高可用组件安装</h4>\n<p><em>注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装</em></p>\n<p><em>注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。</em></p>\n<p><mark>所有 Master 节点</mark>通过 yum 安装 HAProxy 和 KeepAlived：</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 HAProxy，需要注意黄色部分的 IP：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:8443       #HAProxy监听端口\n  bind 127.0.0.1:8443     #HAProxy监听端口\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IP地址\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IP地址\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IP地址\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 KeepAlived，需要注意黄色部分的配置。</p>\n<p><mark>Master01 节点</mark>的配置：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #网卡名称\n    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #网卡名称\n    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #网卡名称\n    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>所有 master 节点</mark>配置 KeepAlived 健康检查文件：</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>所有 master 节点</mark>配置健康检查文件添加执行权限：</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>所有 master 节点</mark>启动 haproxy 和 keepalived：</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的</p>\n<pre><code>所有节点测试VIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等</p>\n<ul>\n<li>所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld</li>\n<li>所有节点查看 selinux 状态，必须为 disable：getenforce</li>\n<li>master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy</li>\n<li>master 节点查看监听端口：netstat -lntp</li>\n</ul>\n<p>如果以上都没有问题，需要确认：</p>\n<ol>\n<li>\n<p>是否是公有云机器</p>\n</li>\n<li>\n<p>是否是私有云机器（类似 OpenStack）</p>\n</li>\n</ol>\n<p>上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询</p>\n<h4 id=\"3-runtime安装\"><a class=\"anchor\" href=\"#3-runtime安装\">#</a> 3. Runtime 安装</h4>\n<p>如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。</p>\n<h5 id=\"31-安装containerd\"><a class=\"anchor\" href=\"#31-安装containerd\">#</a> 3.1 安装 Containerd</h5>\n<p><mark>所有节点</mark>配置安装源：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>所有节点</mark>安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p>首先配置 Containerd 所需的模块（<mark>所有节点</mark>）：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载模块：</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>所有节点</mark>，配置 Containerd 所需的内核：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载内核：</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>生成 Containerd 的配置文件：</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>更改 Containerd 的 Cgroup 和 Pause 镜像配置：</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>启动 Containerd，并配置开机自启动：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>所有节点</mark>配置 crictl 客户端连接的运行时位置（可选）：</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-k8s及etcd安装\"><a class=\"anchor\" href=\"#4-k8s及etcd安装\">#</a> 4 . K8S 及 etcd 安装</h4>\n<p><mark>Master01</mark> 下载 kubernetes 安装包（1.32.3 需要更改为你看到的最新版本）：</p>\n<pre><code>[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz\n</code></pre>\n<p>最新版获取地址：<a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md\">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/</a></p>\n<p><mark>以下操作都在 master01 执行</mark></p>\n<p>下载 etcd 安装包：<a href=\"https://github.com/etcd-io/etcd/releases/\">https://github.com/etcd-io/etcd/releases/</a></p>\n<pre><code>[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz\n</code></pre>\n<p>解压 kubernetes 安装文件：</p>\n<pre><code>[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125;\n</code></pre>\n<p>解压 etcd 安装文件：</p>\n<pre><code>[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&#123;,ctl&#125;\n</code></pre>\n<p>版本查看：</p>\n<pre><code>[root@k8s-master01 ~]# kubelet --version\nKubernetes v1.32.3\n[root@k8s-master01 ~]# etcdctl version\netcdctl version: 3.5.16\nAPI version: 3.5\n</code></pre>\n<p>将组件发送到其他节点</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\nWorkNodes='k8s-node01 k8s-node02'\nfor NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done\nfor NODE in $WorkNodes; do     scp /usr/local/bin/kube&#123;let,-proxy&#125; $NODE:/usr/local/bin/ ; done\n</code></pre>\n<p><mark>Master01 节点</mark>切换到 1.32.x 分支（其他版本可以切换到其他分支，.x 即可，不需要更改为具体的小版本）：</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x\n</code></pre>\n<h4 id=\"5-生成证书\"><a class=\"anchor\" href=\"#5-生成证书\">#</a> 5 . 生成证书</h4>\n<p><em><mark>二进制安装最关键步骤，一步错误全盘皆输，一定要注意每个步骤都要是正确的</mark></em></p>\n<p><mark>Master01</mark> 下载生成证书工具（下载不成功可以去百度网盘）</p>\n<pre><code>wget &quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl\nwget &quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson\nchmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson\n</code></pre>\n<h5 id=\"51-etcd证书\"><a class=\"anchor\" href=\"#51-etcd证书\">#</a> 5.1 Etcd 证书</h5>\n<p><mark>所有 Master 节点</mark>创建 etcd 证书目录：</p>\n<pre><code>mkdir /etc/etcd/ssl -p\n</code></pre>\n<p><mark>所有节点</mark>创建 kubernetes 相关目录：</p>\n<pre><code>mkdir -p /etc/kubernetes/pki\n</code></pre>\n<p><mark>Master01 节点</mark>生成 etcd 证书</p>\n<p>生成证书的 CSR（证书签名请求文件，配置了一些域名、公司、单位）文件：</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\n# 生成etcd CA证书和CA证书的key\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca\n\n\ncfssl gencert \\\n   -ca=/etc/etcd/ssl/etcd-ca.pem \\\n   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\\n   -config=ca-config.json \\\n   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \\\n   -profile=kubernetes \\\n   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd\n\n执行结果\n[INFO] generate received request\n \t[INFO] received CSR\n     [INFO] generating key: rsa-2048\n     [INFO] encoded CSR\n     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033\n</code></pre>\n<p>将证书复制到其他 master 节点</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\n\nfor NODE in $MasterNodes; do\n     ssh $NODE &quot;mkdir -p /etc/etcd/ssl&quot;\n     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do\n       scp /etc/etcd/ssl/$&#123;FILE&#125; $NODE:/etc/etcd/ssl/$&#123;FILE&#125;\n     done\n done\n</code></pre>\n<h5 id=\"52-k8s组件证书\"><a class=\"anchor\" href=\"#52-k8s组件证书\">#</a> 5.2 K8s 组件证书</h5>\n<p><mark>Master01</mark> 生成 kubernetes CA 证书：</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca\n</code></pre>\n<h6 id=\"521-apiserver证书\"><a class=\"anchor\" href=\"#521-apiserver证书\">#</a> 5.2.1 APIServer 证书</h6>\n<p>注意：10.96.0. 是 k8s service 的网段，如果说需要更改 k8s service 网段，那就需要更改 10.96.0.1</p>\n<pre><code>cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver\n</code></pre>\n<p>生成 apiserver 的聚合证书：：</p>\n<pre><code>cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca \n\ncfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client\n</code></pre>\n<p>返回结果（忽略警告）：</p>\n<pre><code>2020/12/11 18:15:28 [INFO] generate received request\n2020/12/11 18:15:28 [INFO] received CSR\n2020/12/11 18:15:28 [INFO] generating key: rsa-2048\n\n2020/12/11 18:15:28 [INFO] encoded CSR\n2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845\n2020/12/11 18:15:28 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for\nwebsites. For more information see the Baseline Requirements for the Issuance and Management\nof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);\nspecifically, section 10.2.3 (&quot;Information Requirements&quot;).\n</code></pre>\n<h6 id=\"522-controllermanager\"><a class=\"anchor\" href=\"#522-controllermanager\">#</a> 5.2.2 ControllerManager</h6>\n<p>生成 controller-manage 的证书：</p>\n<pre><code class=\"language-\\\">cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager\n\n注意：修改黄色部分的IP地址\n# set-cluster：设置一个集群项，\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# 设置一个环境项，一个上下文\nkubectl config set-context system:kube-controller-manager@kubernetes \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# set-credentials 设置一个用户项\n\nkubectl config set-credentials system:kube-controller-manager \\\n     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\\n     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n\n# 使用某个环境当做默认环境\n\nkubectl config use-context system:kube-controller-manager@kubernetes \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n</code></pre>\n<h6 id=\"523-scheduler证书\"><a class=\"anchor\" href=\"#523-scheduler证书\">#</a> 5.2.3 Scheduler 证书</h6>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler\n\n注意：修改黄色部分的IP地址\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\n\nkubectl config set-credentials system:kube-scheduler \\\n     --client-certificate=/etc/kubernetes/pki/scheduler.pem \\\n     --client-key=/etc/kubernetes/pki/scheduler-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config set-context system:kube-scheduler@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-scheduler \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config use-context system:kube-scheduler@kubernetes \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n</code></pre>\n<h6 id=\"524-生成管理员证书\"><a class=\"anchor\" href=\"#524-生成管理员证书\">#</a> 5.2.4 生成管理员证书</h6>\n<p>Kubectl /etc/Kubernetes/admin.conf ~/.kube/config</p>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin\n\n注意：修改黄色部分的IP\n\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig\nkubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n</code></pre>\n<h6 id=\"525-创建serviceaccount证书\"><a class=\"anchor\" href=\"#525-创建serviceaccount证书\">#</a> 5.2.5 创建 ServiceAccount 证书</h6>\n<p>创建一对公钥，用来签发 ServiceAccount 的 Token：</p>\n<pre><code>openssl genrsa -out /etc/kubernetes/pki/sa.key 2048\n</code></pre>\n<p>返回结果：</p>\n<pre><code>Generating RSA private key, 2048 bit long modulus (2 primes)\n...................................................................................+++++\n...............+++++\ne is 65537 (0x010001)\n</code></pre>\n<pre><code> openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n</code></pre>\n<p>发送证书至其他节点：</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do \n  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do \n    scp /etc/kubernetes/pki/$&#123;FILE&#125; $NODE:/etc/kubernetes/pki/$&#123;FILE&#125;;\n  done; \n  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do \n    scp /etc/kubernetes/$&#123;FILE&#125; $NODE:/etc/kubernetes/$&#123;FILE&#125;;\n  done;\ndone\n</code></pre>\n<p>查看证书文件：</p>\n<pre><code>[root@k8s-master01 pki]# ls /etc/kubernetes/pki/\nadmin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem\nadmin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem\nadmin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr\n[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l\n23\n</code></pre>\n<h4 id=\"6-kubernetes组件配置\"><a class=\"anchor\" href=\"#6-kubernetes组件配置\">#</a> 6. Kubernetes 组件配置</h4>\n<h5 id=\"61-ecd配置\"><a class=\"anchor\" href=\"#61-ecd配置\">#</a> 6.1 Ecd 配置</h5>\n<p>Etcd 配置大致相同，注意修改每个 Master 节点的 etcd 配置的主机名和 IP 地址</p>\n<h6 id=\"611-master01\"><a class=\"anchor\" href=\"#611-master01\">#</a> 6.1.1 Master01</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master01'     # k8s-master01名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.71:2380'            # k8s-master01 IP\nlisten-client-urls: 'https://192.168.1.71:2379,http://127.0.0.1:2379'   # k8s-master01 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.71:2380'  # k8s-master01 IP\nadvertise-client-urls: 'https://192.168.1.71:2379'        # k8s-master01 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'     # k8s-master01、k8s-master02、k8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"612-master02\"><a class=\"anchor\" href=\"#612-master02\">#</a> 6.1.2 Master02</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\t\nname: 'k8s-master02'   # k8s-master02名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.72:2380'      # k8s-master02 IP\nlisten-client-urls: 'https://192.168.1.72:2379,http://127.0.0.1:2379'    # k8s-master02 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.72:2380'    # k8s-master02 IP\nadvertise-client-urls: 'https://192.168.1.72:2379'     # k8s-master02 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'             # k8s-master01、k8s-master02、k8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"613-master03\"><a class=\"anchor\" href=\"#613-master03\">#</a> 6.1.3 Master03</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master03'           # k8s-master03名称\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.73:2380'           # k8s-master03 IP\nlisten-client-urls: 'https://192.168.1.73:2379,http://127.0.0.1:2379'       # k8s-master03 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.73:2380'      # k8s-master03 IP\nadvertise-client-urls: 'https://192.168.1.73:2379'            # k8s-master03 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'                # k8s-master01、k8s-master02、k8s-master03 IP\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"614-启动etcd\"><a class=\"anchor\" href=\"#614-启动etcd\">#</a> 6.1.4 启动 Etcd</h6>\n<p><mark>所有 Master 节点</mark>创建 etcd service 并启动</p>\n<pre><code># vim /usr/lib/systemd/system/etcd.service\n[Unit]\nDescription=Etcd Service\nDocumentation=https://coreos.com/etcd/docs/latest/\nAfter=network.target\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml\nRestart=on-failure\nRestartSec=10\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nAlias=etcd3.service\n</code></pre>\n<p><mark>所有 Master 节点</mark>创建 etcd 的证书目录：</p>\n<pre><code>mkdir /etc/kubernetes/pki/etcd\nln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/\nsystemctl daemon-reload\nsystemctl enable --now etcd\n</code></pre>\n<p>查看 etcd 状态：</p>\n<pre><code>export ETCDCTL_API=3\netcdctl --endpoints=&quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n</code></pre>\n<h5 id=\"62-apiserver配置\"><a class=\"anchor\" href=\"#62-apiserver配置\">#</a> 6.2 APIServer 配置</h5>\n<h6 id=\"621-master01\"><a class=\"anchor\" href=\"#621-master01\">#</a> 6.2.1 Master01</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.71 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"622-master02\"><a class=\"anchor\" href=\"#622-master02\">#</a> 6.2.2 Master02</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.72 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"623-master03\"><a class=\"anchor\" href=\"#623-master03\">#</a> 6.2.3 Master03</h6>\n<p>注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.73 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"624-启动apiserver\"><a class=\"anchor\" href=\"#624-启动apiserver\">#</a> 6.2.4 启动 apiserver</h6>\n<p><mark>所有 Master 节点</mark>开启 kube-apiserver：</p>\n<pre><code>systemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserver\n</code></pre>\n<p>检测 kube-server 状态：</p>\n<pre><code># systemctl status kube-apiserver\n\n● kube-apiserver.service – Kubernetes API Server\n   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)\n   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s ago \n</code></pre>\n<p>如果系统日志有这些提示可以忽略:</p>\n<pre><code>Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to “pick_first”\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;CONNECTING &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;READY &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = “transport is closing”\n</code></pre>\n<h5 id=\"63-controllermanage\"><a class=\"anchor\" href=\"#63-controllermanage\">#</a> 6.3 ControllerManage</h5>\n<p><mark>所有 Master 节点</mark>配置 kube-controller-manager service（所有 master 节点配置一样）</p>\n<p>注意：本文档使用的 k8s Pod 网段为 172.16.0.0/16，该网段不能和宿主机的网段、k8s Service 网段的重复，请按需修改：</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\n      --v=2 \\\n      --root-ca-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\\n      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\\n      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --leader-elect=true \\\n      --use-service-account-credentials=true \\\n      --node-monitor-grace-period=40s \\\n      --node-monitor-period=5s \\\n      --controllers=*,bootstrapsigner,tokencleaner \\\n      --allocate-node-cidrs=true \\\n      --cluster-cidr=172.16.0.0/16 \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\\n      --node-cidr-mask-size=24\n      \nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>所有 Master 节点</mark>启动 kube-controller-manager</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service → /usr/lib/systemd/system/kube-controller-manager.service.\n</code></pre>\n<p>查看启动状态</p>\n<pre><code>[root@k8s-master01 pki]# systemctl  status kube-controller-manager\n● kube-controller-manager.service – Kubernetes Controller Manager\n   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)\n Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago\n     Docs: https://github.com/  ubernetes/  ubernetes\n Main PID: 7518 (kube-controller)\n</code></pre>\n<h5 id=\"64-scheduler\"><a class=\"anchor\" href=\"#64-scheduler\">#</a> 6.4 Scheduler</h5>\n<p>所有 Master 节点配置 kube-scheduler service（所有 master 节点配置一样）</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service \n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\n      --v=2 \\\n      --leader-elect=true \\\n      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>启动 scheduler：</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-scheduler\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service → /usr/lib/systemd/system/kube-scheduler.service.\n[root@k8s-master01 pki]# systemctl status kube-scheduler\n● kube-scheduler.service - Kubernetes Scheduler\n   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)\n   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago\n     Docs: https://github.com/kubernetes/kubernetes\n Main PID: 5815 (kube-scheduler)\n    Tasks: 9\n   Memory: 19.8M\n</code></pre>\n<h4 id=\"7-tls-bootstrapping配置\"><a class=\"anchor\" href=\"#7-tls-bootstrapping配置\">#</a> 7. TLS Bootstrapping 配置</h4>\n<p>只需要在<mark> Master01</mark> 创建 bootstrap</p>\n<p>注意： 修改黄色部分的 IP 地址</p>\n<pre><code>cd /root/k8s-ha-install/bootstrap\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n\n[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config\n</code></pre>\n<p>可以正常查询集群状态，才可以继续往下，否则不行，需要排查 k8s 组件是否有故障（只要有结果即可，如果返回不一样不影响）</p>\n<pre><code># kubectl get cs\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS    MESSAGE   ERROR\ncontroller-manager   Healthy   ok        \nscheduler            Healthy   ok        \netcd-0               Healthy   ok\n</code></pre>\n<p>创建 bootstrap 相关资源：</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml \nsecret/bootstrap-token-c8ad9c created\nclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created\nclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created\nclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created\n</code></pre>\n<h4 id=\"8-node节点配置\"><a class=\"anchor\" href=\"#8-node节点配置\">#</a> 8. Node 节点配置</h4>\n<h5 id=\"81-复制证书\"><a class=\"anchor\" href=\"#81-复制证书\">#</a> 8.1 复制证书</h5>\n<p><mark>Master01 节点</mark>复制证书至其他节点：</p>\n<pre><code>cd /etc/kubernetes/\n\nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do\n     ssh $NODE mkdir -p /etc/kubernetes/pki\n     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do\n       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&#123;FILE&#125;\n done\n done\n</code></pre>\n<p>执行结果：</p>\n<pre><code>ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    \n…\nbootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00\n</code></pre>\n<h5 id=\"82-kubelet配置\"><a class=\"anchor\" href=\"#82-kubelet配置\">#</a> 8.2 Kubelet 配置</h5>\n<p><mark>所有节点</mark>创建 Kubelet 配置目录</p>\n<pre><code>mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/\n</code></pre>\n<p><mark>所有节点</mark>配置 kubelet service</p>\n<pre><code>[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kubelet\n\nRestart=always\nStartLimitInterval=0\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>所有节点</mark>配置 kubelet service 的配置文件（也可以写到 kubelet.service）：</p>\n<pre><code># Runtime为Containerd\n# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf\n\n[Service]\nEnvironment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&quot;\nEnvironment=&quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot;\nEnvironment=&quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&quot;\nEnvironment=&quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' &quot;\nExecStart=\nExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS\n</code></pre>\n<p><mark>所有节点</mark>创建 kubelet 的配置文件</p>\n<p><em>注意：如果更改了 k8s 的 service 网段，需要更改 kubelet-conf.yml 的 clusterDNS: 配置，改成 k8s Service 网段的第十个地址，比如 10.96.0.10</em></p>\n<pre><code>[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml\n\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\ncgroupDriver: systemd\ncgroupsPerQOS: true\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerLogMaxFiles: 5\ncontainerLogMaxSize: 10Mi\ncontentType: application/vnd.kubernetes.protobuf\ncpuCFSQuota: true\ncpuManagerPolicy: none\ncpuManagerReconcilePeriod: 10s\nenableControllerAttachDetach: true\nenableDebuggingHandlers: true\nenforceNodeAllocatable:\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhairpinMode: promiscuous-bridge\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\niptablesDropBit: 15\niptablesMasqueradeBit: 14\nkubeAPIBurst: 10\nkubeAPIQPS: 5\nmakeIPTablesUtilChains: true\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nregistryBurst: 10\nregistryPullQPS: 5\nresolvConf: /etc/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 2m0s\nserializeImagePulls: true\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h0m0s\nsyncFrequency: 1m0s\nvolumeStatsAggPeriod: 1m0s\n</code></pre>\n<p>启动<mark>所有节点</mark> kubelet</p>\n<pre><code>systemctl daemon-reload\nsystemctl enable --now kubelet\n</code></pre>\n<p>此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<p><em>如果有很多报错日志，或者有大量看不懂的报错，说明 kubelet 的配置有误，需要检查 kubelet 配置</em></p>\n<p>Master01 查看集群状态 (Ready 或 NotReady 都正常)</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl get node\n</code></pre>\n<h5 id=\"83-kube-proxy配置\"><a class=\"anchor\" href=\"#83-kube-proxy配置\">#</a> 8.3 kube-proxy 配置</h5>\n<p><em>注意，如果不是高可用集群，192.168.1.70:8443 改为 master01 的地址，8443 改为 apiserver 的端口，默认是 6443</em></p>\n<p>生成 kube-proxy 的证书，以下操作只在<mark> Master01</mark> 执行</p>\n<pre><code>cd /root/k8s-ha-install/pki\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config set-credentials system:kube-proxy \\\n     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \\\n     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\nkubectl config set-context system:kube-proxy@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-proxy \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config use-context system:kube-proxy@kubernetes \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n</code></pre>\n<p>将 kubeconfig 发送至其他节点</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n\nfor NODE in k8s-node01 k8s-node02; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n</code></pre>\n<p><mark>所有节点</mark>添加 kube-proxy 的配置和 service 文件：</p>\n<pre><code>vim /usr/lib/systemd/system/kube-proxy.service\n\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\n  --config=/etc/kubernetes/kube-proxy.yaml \\\n  --v=2\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>如果更改了集群 Pod 的网段，需要更改 kube-proxy.yaml 的 clusterCIDR 为自己的 Pod 网段：</p>\n<pre><code>vim /etc/kubernetes/kube-proxy.yaml\n\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nclientConnection:\n  acceptContentTypes: &quot;&quot;\n  burst: 10\n  contentType: application/vnd.kubernetes.protobuf\n  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n  qps: 5\nclusterCIDR: 172.16.0.0/16 \nconfigSyncPeriod: 15m0s\nconntrack:\n  max: null\n  maxPerCore: 32768\n  min: 131072\n  tcpCloseWaitTimeout: 1h0m0s\n  tcpEstablishedTimeout: 24h0m0s\nenableProfiling: false\nhealthzBindAddress: 0.0.0.0:10256\nhostnameOverride: &quot;&quot;\niptables:\n  masqueradeAll: false\n  masqueradeBit: 14\n  minSyncPeriod: 0s\n  syncPeriod: 30s\nipvs:\n  masqueradeAll: true\n  minSyncPeriod: 5s\n  scheduler: &quot;rr&quot;\n  syncPeriod: 30s\nkind: KubeProxyConfiguration\nmetricsBindAddress: 127.0.0.1:10249\nmode: &quot;ipvs&quot;\nnodePortAddresses: null\noomScoreAdj: -999\nportRange: &quot;&quot;\nudpIdleTimeout: 250ms\n</code></pre>\n<p><mark>所有节点</mark>启动 kube-proxy</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload\n[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /usr/lib/systemd/system/kube-proxy.service.\n</code></pre>\n<p>此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<h4 id=\"9-calico组件的安装\"><a class=\"anchor\" href=\"#9-calico组件的安装\">#</a> 9. Calico 组件的安装</h4>\n<p>以下步骤只在 master01 执行：</p>\n<pre><code>cd /root/k8s-ha-install/calico/\n</code></pre>\n<p>更改 calico 的网段，主要需要将红色部分的网段，改为自己的 Pod 网段</p>\n<pre><code>sed -i &quot;s#POD_CIDR#172.16.0.0/16#g&quot; calico.yaml\n</code></pre>\n<p><em>检查网段是自己的 Pod 网段， grep &quot;IPV4POOL_CIDR&quot; calico.yaml  -A 1</em></p>\n<p>查看容器和节点状态：</p>\n<pre><code>[root@k8s-master01 calico]# kubectl get po -n kube-system\nNAME                                       READY   STATUS    RESTARTS      AGE\ncalico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s\ncalico-node-8fxqp                          1/1     Running   0             85s\ncalico-node-8nkfl                          1/1     Running   0             86s\ncalico-node-pmpf4                          1/1     Running   0             86s\ncalico-node-vnlk7                          1/1     Running   0             86s\ncalico-node-xpchb                          1/1     Running   0             85s\ncalico-typha-67c6dc57d6-259t8              1/1     Running   0             86s\n</code></pre>\n<p><em>如果容器状态异常可以使用 kubectl describe 或者 kubectl logs 查看容器的日志</em></p>\n<ol>\n<li>Kubectl logs -f POD_NAME -n kube-system</li>\n<li>Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system</li>\n</ol>\n<h4 id=\"10-安装coredns\"><a class=\"anchor\" href=\"#10-安装coredns\">#</a> 10. 安装 CoreDNS</h4>\n<pre><code>cd /root/k8s-ha-install/\n</code></pre>\n<p>如果更改了 k8s service 的网段需要将 coredns 的 serviceIP 改成 k8s service 网段的第十个 IP</p>\n<pre><code>COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk '&#123;print $3&#125;'`0\nsed -i &quot;s#KUBEDNS_SERVICE_IP#$&#123;COREDNS_SERVICE_IP&#125;#g&quot; CoreDNS/coredns.yaml\n</code></pre>\n<p>安装 coredns</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml \nserviceaccount/coredns created\nclusterrole.rbac.authorization.k8s.io/system:coredns created\nclusterrolebinding.rbac.authorization.k8s.io/system:coredns created\nconfigmap/coredns created\ndeployment.apps/coredns created\nservice/kube-dns created\n</code></pre>\n<h4 id=\"11-metrics部署\"><a class=\"anchor\" href=\"#11-metrics部署\">#</a> 11. Metrics 部署</h4>\n<p>在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。</p>\n<p>以下操作均在<mark> master01 节点</mark>执行，安装 metrics server:</p>\n<pre><code>cd /root/k8s-ha-install/metrics-server\nkubectl  create -f . \n\nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>等待 metrics server 启动然后查看状态：</p>\n<pre><code># kubectl  top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-master01   231m         5%     1620Mi          42%       \nk8s-master02   274m         6%     1203Mi          31%       \nk8s-master03   202m         5%     1251Mi          32%       \nk8s-node01     69m          1%     667Mi           17%       \nk8s-node02     73m          1%     650Mi           16%\n</code></pre>\n<p>如果有如下报错，可以等待 10 分钟后，再次查看：</p>\n<pre><code>Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)\n</code></pre>\n<h4 id=\"12-dashboard部署\"><a class=\"anchor\" href=\"#12-dashboard部署\">#</a> 12. Dashboard 部署</h4>\n<h5 id=\"121-安装dashboard\"><a class=\"anchor\" href=\"#121-安装dashboard\">#</a> 12.1 安装 Dashboard</h5>\n<p>Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"122-登录dashboard\"><a class=\"anchor\" href=\"#122-登录dashboard\">#</a> 12.2 登录 dashboard</h5>\n<p>在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>更改 dashboard 的 svc 为 NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）</em></p>\n<p>查看端口号：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：</p>\n<p>访问 Dashboard：<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>创建登录 Token：</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"14-containerd配置镜像加速\"><a class=\"anchor\" href=\"#14-containerd配置镜像加速\">#</a> 14. Containerd 配置镜像加速</h4>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>所有节点重新启动 Containerd：</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"15-docker配置镜像加速\"><a class=\"anchor\" href=\"#15-docker配置镜像加速\">#</a> 15. Docker 配置镜像加速</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>所有节点重新启动 Docker：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/2771271649.html",
            "url": "http://imxuyong.cn/posts/2771271649.html",
            "title": "云原生K8s安全专家CKS认证考题详解",
            "date_published": "2025-04-09T13:38:39.000Z",
            "content_html": "<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"抱歉, 这个密码看着不太对, 请再试试。\" data-whm=\"抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-xray\">\n      <input class=\"hbe hbe-input-field hbe-input-field-xray\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-xray\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-xray\">您好, 这里需要输入密码。</span>\n      </label>\n      <svg class=\"hbe hbe-graphic hbe-graphic-xray\" width=\"300%\" height=\"100%\" viewBox=\"0 0 1200 60\" preserveAspectRatio=\"none\">\n        <path d=\"M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0\"></path>\n        <path d=\"M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0\"></path>\n      </svg>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://imxuyong.cn/posts/3166738000.html",
            "url": "http://imxuyong.cn/posts/3166738000.html",
            "title": "Kubeadm高可用安装K8s集群",
            "date_published": "2025-04-09T10:28:34.000Z",
            "content_html": "<h2 id=\"kubeadm高可用安装k8s集群\"><a class=\"anchor\" href=\"#kubeadm高可用安装k8s集群\">#</a> Kubeadm 高可用安装 K8s 集群</h2>\n<h4 id=\"1-基本配置\"><a class=\"anchor\" href=\"#1-基本配置\">#</a> 1. 基本配置</h4>\n<h5 id=\"11-基本环境配置\"><a class=\"anchor\" href=\"#11-基本环境配置\">#</a> 1.1 基本环境配置</h5>\n<table>\n<thead>\n<tr>\n<th>主机名</th>\n<th>IP 地址</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master 节点 * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived 虚拟 IP（不占用机器）</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker 节点 * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* 配置信息 *</strong></em></th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>系统版本</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod 网段</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service 网段</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>所有节点</mark>更改主机名（其它节点按需修改）：</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>所有节点</mark>配置 hosts，修改 /etc/hosts 如下：</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>所有节点</mark>配置 yum 源：</p>\n<pre><code># 配置基础源\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>所有节点</mark>必备工具安装：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>所有节点</mark>关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>所有节点</mark>关闭 swap 分区：</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>所有节点</mark>安装 ntpdate：</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>所有节点</mark>同步时间并配置上海时区：</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# 加入到crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>所有节点</mark>配置 limit：</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# 末尾添加如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>所有节点</mark>升级系统：</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 节点</mark>免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上</em></p>\n<p><mark>Master01 节点</mark>下载安装所有的源码文件：</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-内核配置\"><a class=\"anchor\" href=\"#12-内核配置\">#</a> 1.2 内核配置</h5>\n<p><mark>所有节点</mark>安装 ipvsadm：</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>所有节点</mark>配置 ipvs 模块：</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>所有节点</mark>创建 ipvs.conf，并配置开机自动加载：</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# 加入以下内容\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>所有节点</mark>然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>所有节点</mark>内核优化配置：</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>所有节点</mark>应用配置：</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>配置完内核后，重启机器，之后查看内核模块是否已自动加载：</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-高可用组件安装\"><a class=\"anchor\" href=\"#2-高可用组件安装\">#</a> 2. 高可用组件安装</h4>\n<p><em>注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装</em></p>\n<p><em>注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。</em></p>\n<p><mark>所有 Master 节点</mark>通过 yum 安装 HAProxy 和 KeepAlived：</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 HAProxy，需要注意黄色部分的 IP：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:16443       #HAProxy监听端口\n  bind 127.0.0.1:16443     #HAProxy监听端口\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IP地址\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IP地址\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IP地址\n</code></pre>\n<p><mark>所有 Master 节点</mark>配置 KeepAlived，需要注意黄色部分的配置。</p>\n<p><mark>Master01 节点</mark>的配置：</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #网卡名称\n    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #网卡名称\n    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 节点</mark>的配置：</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #网卡名称\n    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIP地址\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>所有 master 节点</mark>配置 KeepAlived 健康检查文件：</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>所有 master 节点</mark>配置健康检查文件添加执行权限：</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>所有 master 节点</mark>启动 haproxy 和 keepalived：</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的</p>\n<pre><code>所有节点测试VIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等</p>\n<ul>\n<li>所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld</li>\n<li>所有节点查看 selinux 状态，必须为 disable：getenforce</li>\n<li>master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy</li>\n<li>master 节点查看监听端口：netstat -lntp</li>\n</ul>\n<p>如果以上都没有问题，需要确认：</p>\n<ol>\n<li>\n<p>是否是公有云机器</p>\n</li>\n<li>\n<p>是否是私有云机器（类似 OpenStack）</p>\n</li>\n</ol>\n<p>上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询</p>\n<h4 id=\"3-runtime安装\"><a class=\"anchor\" href=\"#3-runtime安装\">#</a> 3. Runtime 安装</h4>\n<p>如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。</p>\n<h5 id=\"31-安装containerd\"><a class=\"anchor\" href=\"#31-安装containerd\">#</a> 3.1 安装 Containerd</h5>\n<p><mark>所有节点</mark>配置安装源：</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>所有节点</mark>安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>可以无需启动 Docker，只需要配置和启动 Containerd 即可。</em></p>\n<p>首先配置 Containerd 所需的模块（<mark>所有节点</mark>）：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载模块：</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>所有节点</mark>，配置 Containerd 所需的内核：</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>所有节点</mark>加载内核：</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>所有节点</mark>生成 Containerd 的配置文件：</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>更改 Containerd 的 Cgroup 和 Pause 镜像配置：</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>所有节点</mark>启动 Containerd，并配置开机自启动：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>所有节点</mark>配置 crictl 客户端连接的运行时位置（可选）：</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-安装kubernetes组件\"><a class=\"anchor\" href=\"#4-安装kubernetes组件\">#</a> 4 . 安装 Kubernetes 组件</h4>\n<p><mark>所有节点</mark>配置源（注意更改版本号）：</p>\n<pre><code>cat &lt;&lt;EOF | tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key\nEOF\n</code></pre>\n<p>首先在<mark> Master01 节点</mark>查看最新的 Kubernetes 版本是多少：</p>\n<pre><code># yum list kubeadm.x86_64 --showduplicates | sort -r\n</code></pre>\n<p><mark>所有节点</mark>安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：</p>\n<pre><code># yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y\n</code></pre>\n<p><mark>所有节点</mark>设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now kubelet\n</code></pre>\n<p><em>此时 kubelet 是起不来的，日志会有报错不影响！</em></p>\n<h4 id=\"5-集群初始化\"><a class=\"anchor\" href=\"#5-集群初始化\">#</a> 5 . 集群初始化</h4>\n<p>以下操作在<mark> master01</mark>（注意黄色部分）：</p>\n<pre><code>vim kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 7t2weq.bjbawausm0jaxury\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.1.71\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  name: k8s-master01\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n---\napiServer:\n  certSANs:\n  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443\ncontrollerManager: &#123;&#125;\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突\n  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突\nscheduler: &#123;&#125;\n</code></pre>\n<p><mark>master01 节点</mark>更新 kubeadm 文件：</p>\n<pre><code>kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml\n</code></pre>\n<p>将 new.yaml 文件复制到<mark>其他 master 节点</mark>:</p>\n<pre><code>for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done\n</code></pre>\n<p>之后<mark>所有 Master 节点</mark>提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：</p>\n<pre><code>kubeadm config images pull --config /root/new.yaml \n</code></pre>\n<p>正确的反馈信息如下（<em><strong>* 版本可能不一样 *</strong></em>）：</p>\n<pre><code>[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml \n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0\n</code></pre>\n<p><mark>Master01 节点</mark>初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：</p>\n<pre><code>kubeadm init --config /root/new.yaml  --upload-certs\n</code></pre>\n<p>初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：</p>\n<pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n# 不要复制文档当中的，要去使用节点生成的\n  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94\n</code></pre>\n<p><mark>Master01 节点</mark>配置环境变量，用于访问 Kubernetes 集群：</p>\n<pre><code>cat &lt;&lt;EOF &gt;&gt; /root/.bashrc\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nEOF\nsource /root/.bashrc\n</code></pre>\n<p><mark>Master01 节点</mark>查看节点状态：（显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE   VERSION\nk8s-master01   NotReady   control-plane   24s   v1.32.3\n</code></pre>\n<p>采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：</p>\n<pre><code class=\"language-\\\"># kubectl get pods -n kube-system\n</code></pre>\n<h5 id=\"51-初始化失败排查\"><a class=\"anchor\" href=\"#51-初始化失败排查\">#</a> 5.1 初始化失败排查</h5>\n<p>如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：</p>\n<pre><code>kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube\n</code></pre>\n<p>如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：</p>\n<pre><code>tail -f /var/log/messages | grep -v &quot;not found&quot;\n</code></pre>\n<p>经常出错的原因：</p>\n<ol>\n<li>Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对</li>\n<li>new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443</li>\n<li>new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突</li>\n<li>VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错</li>\n</ol>\n<h5 id=\"52-高可用master\"><a class=\"anchor\" href=\"#52-高可用master\">#</a> 5.2 高可用 Master</h5>\n<p><strong>其他 master</strong> 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n</code></pre>\n<p>查看当前状态：（如果显示 NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\n</code></pre>\n<h5 id=\"53-token过期处理\"><a class=\"anchor\" href=\"#53-token过期处理\">#</a> 5.3 Token 过期处理</h5>\n<p>注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。</p>\n<p>Token 过期后生成新的 token：</p>\n<pre><code>kubeadm token create --print-join-command\n</code></pre>\n<p>Master 需要生成 --certificate-key：</p>\n<pre><code>kubeadm init phase upload-certs  --upload-certs\n</code></pre>\n<h4 id=\"6-node节点的配置\"><a class=\"anchor\" href=\"#6-node节点的配置\">#</a> 6. Node 节点的配置</h4>\n<p>Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5\n</code></pre>\n<p>所有节点初始化完成后，查看集群状态（NotReady 不影响）</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\nk8s-node01     NotReady   &lt;none&gt;          13s     v1.32.3\nk8s-node02     NotReady   &lt;none&gt;          10s     v1.32.3\n</code></pre>\n<h4 id=\"7-calico组件的安装\"><a class=\"anchor\" href=\"#7-calico组件的安装\">#</a> 7. Calico 组件的安装</h4>\n<p><mark>所有节点</mark>禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：</p>\n<pre><code>cat &gt;&gt;/etc/NetworkManager/conf.d/calico.conf&lt;&lt;EOF\n[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali\nEOF\nsystemctl daemon-reload\nsystemctl restart NetworkManager\n</code></pre>\n<p>以下步骤只在<mark> master01</mark> 执行（.x 不需要更改）：</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x &amp;&amp; cd calico/\n</code></pre>\n<p>修改 Pod 网段：</p>\n<pre><code>POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '&#123;print $NF&#125;'`\n\nsed -i &quot;s#POD_CIDR#$&#123;POD_SUBNET&#125;#g&quot; calico.yaml\nkubectl apply -f calico.yaml\n</code></pre>\n<p>查看容器和节点状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -n kube-system\nNAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h\ncalico-node-7mzmb                          1/1     Running   0          24h\ncalico-node-ljqnl                          1/1     Running   0          24h\ncalico-node-njqlb                          1/1     Running   0          24h\ncalico-node-ph4m4                          1/1     Running   0          24h\ncalico-node-rx8rl                          1/1     Running   0          24h\ncoredns-76fccbbb6b-76559                   1/1     Running   0          24h\ncoredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h\netcd-k8s-master01                          1/1     Running   0          24h\netcd-k8s-master02                          1/1     Running   0          24h\netcd-k8s-master03                          1/1     Running   0          24h\nkube-apiserver-k8s-master01                1/1     Running   0          24h\nkube-apiserver-k8s-master02                1/1     Running   0          24h\nkube-apiserver-k8s-master03                1/1     Running   0          24h\nkube-controller-manager-k8s-master01       1/1     Running   0          24h\nkube-controller-manager-k8s-master02       1/1     Running   0          24h\nkube-controller-manager-k8s-master03       1/1     Running   0          24h\nkube-proxy-9dtz4                           1/1     Running   0          24h\nkube-proxy-jh7rl                           1/1     Running   0          24h\nkube-proxy-jvvwt                           1/1     Running   0          24h\nkube-proxy-sh89l                           1/1     Running   0          24h\nkube-proxy-t2j49                           1/1     Running   0          24h\nkube-scheduler-k8s-master01                1/1     Running   0          24h\nkube-scheduler-k8s-master02                1/1     Running   0          24h\nkube-scheduler-k8s-master03                1/1     Running   0          24h\nmetrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h\n</code></pre>\n<p>此时节点全部变为 Ready 状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get nodes\nNAME           STATUS   ROLES           AGE   VERSION\nk8s-master01   Ready    control-plane   24h   v1.32.3\nk8s-master02   Ready    control-plane   24h   v1.32.3\nk8s-master03   Ready    control-plane   24h   v1.32.3\nk8s-node01     Ready    &lt;none&gt;          24h   v1.32.3\nk8s-node02     Ready    &lt;none&gt;          24h   v1.32.3\n</code></pre>\n<h4 id=\"8-metrics部署\"><a class=\"anchor\" href=\"#8-metrics部署\">#</a> 8. Metrics 部署</h4>\n<p>在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。</p>\n<p>将<mark> Master01 节点</mark>的 front-proxy-ca.crt 复制到所有 Node 节点</p>\n<pre><code>scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt\n\nscp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt\n</code></pre>\n<p>以下操作均在<mark> master01 节点</mark>执行:</p>\n<p>安装 metrics server</p>\n<pre><code>cd /root/k8s-ha-install/kubeadm-metrics-server\n\n# kubectl  create -f comp.yaml \nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>查看状态：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server\nNAME                              READY   STATUS    RESTARTS   AGE\nmetrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h\n</code></pre>\n<p>等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl top node\nNAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \nk8s-master01   132m         3%       932Mi           5%          \nk8s-master02   131m         3%       845Mi           5%          \nk8s-master03   148m         3%       912Mi           5%          \nk8s-node01     54m          1%       600Mi           3%          \nk8s-node02     49m          1%       602Mi           3%          \n[root@k8s-master01 ~]#  kubectl top po -A\nNAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   \ningress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            \ningress-nginx          ingress-nginx-controller-r978m               1m           104Mi           \nkrm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            \nkrm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             \nkrm                    nginx-574cf48959-vcfjs                       0m           2Mi             \nkube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            \nkube-system            calico-node-7mzmb                            16m          176Mi           \nkube-system            calico-node-ljqnl                            15m          182Mi           \nkube-system            calico-node-njqlb                            19m          180Mi           \nkube-system            calico-node-ph4m4                            15m          178Mi           \nkube-system            calico-node-rx8rl                            17m          180Mi           \nkube-system            coredns-76fccbbb6b-76559                     2m           16Mi            \nkube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            \nkube-system            etcd-k8s-master01                            22m          86Mi            \nkube-system            etcd-k8s-master02                            27m          84Mi            \nkube-system            etcd-k8s-master03                            22m          84Mi            \nkube-system            kube-apiserver-k8s-master01                  22m          267Mi           \nkube-system            kube-apiserver-k8s-master02                  20m          242Mi           \nkube-system            kube-apiserver-k8s-master03                  18m          241Mi           \nkube-system            kube-controller-manager-k8s-master01         6m           69Mi            \nkube-system            kube-controller-manager-k8s-master02         2m           21Mi            \nkube-system            kube-controller-manager-k8s-master03         1m           19Mi            \nkube-system            kube-proxy-9dtz4                             11m          30Mi            \nkube-system            kube-proxy-jh7rl                             1m           27Mi            \nkube-system            kube-proxy-jvvwt                             17m          29Mi            \nkube-system            kube-proxy-sh89l                             1m           29Mi            \nkube-system            kube-proxy-t2j49                             16m          29Mi            \nkube-system            kube-scheduler-k8s-master01                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master02                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master03                  6m           25Mi            \nkube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            \nkubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            \nkubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  \n</code></pre>\n<h4 id=\"9-dashboard部署\"><a class=\"anchor\" href=\"#9-dashboard部署\">#</a> 9. Dashboard 部署</h4>\n<h5 id=\"91-安装dashboard\"><a class=\"anchor\" href=\"#91-安装dashboard\">#</a> 9.1 安装 Dashboard</h5>\n<p>Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"92-登录dashboard\"><a class=\"anchor\" href=\"#92-登录dashboard\">#</a> 9.2 登录 dashboard</h5>\n<p>在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>更改 dashboard 的 svc 为 NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）</em></p>\n<p>查看端口号：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：</p>\n<p>访问 Dashboard：<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>创建登录 Token：</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"10必看一些必须的配置更改\"><a class=\"anchor\" href=\"#10必看一些必须的配置更改\">#</a> 10.【必看】一些必须的配置更改</h4>\n<p>将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：</p>\n<p>在 master01 节点执行：</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\nmode: ipvs\n</code></pre>\n<p>更新 Kube-Proxy 的 Pod：</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>验证 Kube-Proxy 模式:</p>\n<pre><code>[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode\nipvs\n</code></pre>\n<h4 id=\"11必看注意事项\"><a class=\"anchor\" href=\"#11必看注意事项\">#</a> 11.【必看】注意事项</h4>\n<p>注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。</p>\n<p>启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。</p>\n<p>其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。</p>\n<p>kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\n</code></pre>\n<p>进行更改，更改完成后，可以通过 patch 重启 kube-proxy</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：</p>\n<pre><code>[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-\n</code></pre>\n<h4 id=\"12-containerd配置镜像加速\"><a class=\"anchor\" href=\"#12-containerd配置镜像加速\">#</a> 12. Containerd 配置镜像加速</h4>\n<pre><code># vim /etc/containerd/config.toml\n#添加以下配置镜像加速服务\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>所有节点重新启动 Containerd：</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"13-docker配置镜像加速\"><a class=\"anchor\" href=\"#13-docker配置镜像加速\">#</a> 13. Docker 配置镜像加速</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>所有节点重新启动 Docker：</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>本文出自于：<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        }
    ]
}