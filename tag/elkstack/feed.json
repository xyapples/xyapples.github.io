{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre云原生 • All posts by \"elkstack\" tag",
    "description": "专注于 Linux 运维、云计算、云原⽣等技术",
    "home_page_url": "http://ixuyong.cn",
    "items": [
        {
            "id": "http://ixuyong.cn/posts/170066797.html",
            "url": "http://ixuyong.cn/posts/170066797.html",
            "title": "消费租赁项目Kubernetes基于ELK日志分析与实践",
            "date_published": "2025-05-25T06:35:21.000Z",
            "content_html": "<h3 id=\"消费租赁项目kubernetes基于elk日志分析与实践\"><a class=\"anchor\" href=\"#消费租赁项目kubernetes基于elk日志分析与实践\">#</a> 消费租赁项目 Kubernetes 基于 ELK 日志分析与实践</h3>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Og7liF6.jpeg\" alt=\"Snipaste_2025-05-25_13-43-46.jpg\" /></p>\n<h4 id=\"1-elk创建namespace和secrets\"><a class=\"anchor\" href=\"#1-elk创建namespace和secrets\">#</a> 1. ELK 创建 Namespace 和 Secrets</h4>\n<pre><code># kubectl create ns logging\n# kubectl create secret docker-registry harbor-admin -n logging --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xyapples@163.com --docker-password=Talent*19871988\n</code></pre>\n<h4 id=\"2-交付zookeeper集群至k8s\"><a class=\"anchor\" href=\"#2-交付zookeeper集群至k8s\">#</a> 2. 交付 Zookeeper 集群至 K8S</h4>\n<h5 id=\"21-制作zk集群镜像\"><a class=\"anchor\" href=\"#21-制作zk集群镜像\">#</a> 2.1 制作 ZK 集群镜像</h5>\n<h6 id=\"211-dockerfile\"><a class=\"anchor\" href=\"#211-dockerfile\">#</a> 2.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、拷贝Zookeeper压缩包和配置文件\nENV VERSION=3.8.4\nADD ./apache-zookeeper-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./zoo.cfg /apache-zookeeper-$&#123;VERSION&#125;-bin/conf\n\n# 2、对Zookeeper文件夹名称重新命名\nRUN mv /apache-zookeeper-$&#123;VERSION&#125;-bin /zookeeper\n\n# 3、拷贝eentrpoint的启动脚本文件\nADD ./entrypoint.sh /entrypoint.sh\n\n# 4、暴露Zookeeper端口\nEXPOSE 2181 2888 3888\n\n# 5、执行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"212-zoocfg\"><a class=\"anchor\" href=\"#212-zoocfg\">#</a> 2.1.2 zoo.cfg</h6>\n<pre><code># cat zoo.cfg \n# 服务器之间或客户端与服务器之间维持心跳的时间间隔 tickTime以毫秒为单位。\ntickTime=&#123;ZOOK_TICKTIME&#125;\n\n# 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 10* tickTime\ninitLimit=&#123;ZOOK_INIT_LIMIT&#125;\n\n# 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 5 * tickTime\nsyncLimit=&#123;ZOOK_SYNC_LIMIT&#125;\n \n# 数据保存目录\ndataDir=&#123;ZOOK_DATA_DIR&#125;\n\n# 日志保存目录\ndataLogDir=&#123;ZOOK_LOG_DIR&#125;\n\n# 客户端连接端口\nclientPort=&#123;ZOOK_CLIENT_PORT&#125;\n\n# 客户端最大连接数。# 根据自己实际情况设置，默认为60个\nmaxClientCnxns=&#123;ZOOK_MAX_CLIENT_CNXNS&#125;\n\n# 客户端获取 zookeeper 服务的当前状态及相关信息\n4lw.commands.whitelist=*\n\n# 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口\n</code></pre>\n<h6 id=\"213-entrypoint\"><a class=\"anchor\" href=\"#213-entrypoint\">#</a> 2.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n#设定变量\nZOOK_BIN_DIR=/zookeeper/bin\nZOOK_CONF_DIR=/zookeeper/conf/zoo.cfg\n\n# 2、对配置文件中的字符串进行变量替换\nsed -i s@&#123;ZOOK_TICKTIME&#125;@$&#123;ZOOK_TICKTIME:-2000&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_INIT_LIMIT&#125;@$&#123;ZOOK_INIT_LIMIT:-10&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_SYNC_LIMIT&#125;@$&#123;ZOOK_SYNC_LIMIT:-5&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_DATA_DIR&#125;@$&#123;ZOOK_DATA_DIR:-/data&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_LOG_DIR&#125;@$&#123;ZOOK_LOG_DIR:-/logs&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_CLIENT_PORT&#125;@$&#123;ZOOK_CLIENT_PORT:-2181&#125;@g $&#123;ZOOK_CONF_DIR&#125;\nsed -i s@&#123;ZOOK_MAX_CLIENT_CNXNS&#125;@$&#123;ZOOK_MAX_CLIENT_CNXNS:-60&#125;@g $&#123;ZOOK_CONF_DIR&#125;\n\n# 3、准备ZK的集群节点地址，后期肯定是需要通过ENV的方式注入进来\nfor server in $&#123;ZOOK_SERVERS&#125;\ndo\n\techo $&#123;server&#125; &gt;&gt; $&#123;ZOOK_CONF_DIR&#125;\ndone\n\n# 4、在datadir目录中创建myid的文件，并填入对应的编号\nZOOK_MYID=$(( $(hostname | sed 's#.*-##g') + 1 ))\necho $&#123;ZOOK_MYID:-99&#125; &gt; $&#123;ZOOK_DATA_DIR:-/data&#125;/myid\n\n#5、前台运行Zookeeper\ncd $&#123;ZOOK_BIN_DIR&#125;\n./zkServer.sh start-foreground\n</code></pre>\n<h6 id=\"214-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#214-构建镜像并推送仓库\">#</a> 2.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4 .\n# docker push  registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4\n</code></pre>\n<h5 id=\"22-迁移zookeeper至k8s\"><a class=\"anchor\" href=\"#22-迁移zookeeper至k8s\">#</a> 2.2  迁移 zookeeper 至 K8S</h5>\n<h6 id=\"221-zookeeper-headless\"><a class=\"anchor\" href=\"#221-zookeeper-headless\">#</a> 2.2.1 zookeeper-headless</h6>\n<pre><code># cat 01-zookeeper-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: zookeeper\n  ports:\n  - name: client\n    port: 2181\n    targetPort: 2181\n  - name: leader-follwer\n    port: 2888\n    targetPort: 2888\n  - name: selection\n    port: 3888\n    targetPort: 3888\n</code></pre>\n<h6 id=\"222-zookeeper-sts\"><a class=\"anchor\" href=\"#222-zookeeper-sts\">#</a> 2.2.2 zookeeper-sts</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# vim 02-zookeeper-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zookeeper               \n  namespace: logging\nspec:\n  serviceName: &quot;zookeeper-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zookeeper\n  template:\n    metadata:\n      labels:\n        app: zookeeper\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;zookeeper&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: zookeeper\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/zookeeper:3.8.4           \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 2181\n        - name: leader-follwer\n          containerPort: 2888\n        - name: selection\n          containerPort: 3888\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;server.1=zookeeper-0.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.2=zookeeper-1.zookeeper-svc.logging.svc.cluster.local:2888:3888 server.3=zookeeper-2.zookeeper-svc.logging.svc.cluster.local:2888:3888&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          exec:\n            command:\n            - &quot;/bin/bash&quot;\n            - &quot;-c&quot;\n            - '[[ &quot;$(/zookeeper/bin/zkServer.sh status 2&gt;/dev/null|grep 2181)&quot; ]] &amp;&amp; exit 0 || exit 1'\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: data\n        - name: data\n          mountPath: /logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"223-更新资源清单\"><a class=\"anchor\" href=\"#223-更新资源清单\">#</a> 2.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl apply -f 01-zookeeper-headless.yaml \n[root@k8s-master01 01-zookeeper]# kubectl apply -f 02-zookeeper-sts.yaml\n[root@k8s-master01 01-zookeeper]# kubectl get pods -n logging\nNAME          READY   STATUS    RESTARTS   AGE\nzookeeper-0   1/1     Running   0          17m\nzookeeper-1   1/1     Running   0          14m\nzookeeper-2   1/1     Running   0          11m\n</code></pre>\n<h6 id=\"224-检查zookeeper集群状态\"><a class=\"anchor\" href=\"#224-检查zookeeper集群状态\">#</a> 2.2.4 检查 zookeeper 集群状态</h6>\n<pre><code># for i in 0 1 2 ; do kubectl exec zookeeper-$i -n logging -- /zookeeper/bin/zkServer.sh status; done\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: leader\nZooKeeper JMX enabled by default\nUsing config: /zookeeper/bin/../conf/zoo.cfg\nClient port found: 2181. Client address: localhost. Client SSL: false.\nMode: follower\n</code></pre>\n<h6 id=\"225-连接zookeeper集群\"><a class=\"anchor\" href=\"#225-连接zookeeper集群\">#</a> 2.2.5 连接 Zookeeper 集群</h6>\n<pre><code>[root@k8s-master01 01-zookeeper]# kubectl exec -it zookeeper-0 -n logging -- /bin/sh\n# /zookeeper/bin/zkCli.sh -server zookeeper-svc\n[zk: zookeeper-svc(CONNECTED) 0]  create /hello oldxu\nCreated /hello\n[zk: zookeeper-svc(CONNECTED) 1] get /hello\noldxu\n</code></pre>\n<h4 id=\"3-交付kafka集群至k8s\"><a class=\"anchor\" href=\"#3-交付kafka集群至k8s\">#</a> 3. 交付 Kafka 集群至 K8S</h4>\n<h5 id=\"31-制作kafka集群镜像\"><a class=\"anchor\" href=\"#31-制作kafka集群镜像\">#</a> 3.1 制作 Kafka 集群镜像</h5>\n<h6 id=\"311-dockerfile\"><a class=\"anchor\" href=\"#311-dockerfile\">#</a> 3.1.1 Dockerfile</h6>\n<pre><code># cat Dockerfile \nFROM openjdk:8-jre\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=2.12-2.2.0\nADD ./kafka_$&#123;VERSION&#125;.tgz /\nADD ./server.properties /kafka_$&#123;VERSION&#125;/config/server.properties\n\n# 3、修改kafka的名称\nRUN mv /kafka_$&#123;VERSION&#125; /kafka\n\n# 4、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 5、暴露kafka端口 9999是jmx的端口\nEXPOSE 9092 9999\n\n# 6、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"312-serverproperties\"><a class=\"anchor\" href=\"#312-serverproperties\">#</a> 3.1.2 server.properties</h6>\n<pre><code class=\"language-'\"># cat server.properties \n############################# Server Basics ############################# \n# broker的id，值为整数，且必须唯一，在一个集群中不能重复\nbroker.id=&#123;BROKER_ID&#125;\n\n############################# Socket Server Settings ############################# \n# kafka监听端口，默认9092\nlisteners=PLAINTEXT://&#123;LISTENERS&#125;:9092\n\n# 处理网络请求的线程数量，默认为3个\nnum.network.threads=3\n\n# 执行磁盘IO操作的线程数量，默认为8个 \nnum.io.threads=8\n\n# socket服务发送数据的缓冲区大小，默认100KB\nsocket.send.buffer.bytes=102400\n\n# socket服务接受数据的缓冲区大小，默认100KB\nsocket.receive.buffer.bytes=102400\n\n# socket服务所能接受的一个请求的最大大小，默认为100M\nsocket.request.max.bytes=104857600\n\n############################# Log Basics ############################# \n# kafka存储消息数据的目录\nlog.dirs=&#123;KAFKA_DATA_DIR&#125;\n\n# 每个topic默认的partition\nnum.partitions=1\n\n# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。\ndefault.replication.factor=3\n\n# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量\nnum.recovery.threads.per.data.dir=1\n\n############################# Log Flush Policy ############################# \n# 消息刷新到磁盘中的消息条数阈值\nlog.flush.interval.messages=10000\n\n# 消息刷新到磁盘中的最大时间间隔,1s\nlog.flush.interval.ms=1000\n\n############################# Log Retention Policy ############################# \n# 日志保留小时数，超时会自动删除，默认为7天\nlog.retention.hours=168\n\n# 日志保留大小，超出大小会自动删除，默认为1G\n#log.retention.bytes=1073741824\n\n# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件\nlog.segment.bytes=1073741824\n\n# 每隔多长时间检测数据是否达到删除条件,300s\nlog.retention.check.interval.ms=300000\n\n############################# Zookeeper ############################# \n# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开\nzookeeper.connect=&#123;ZOOK_SERVERS&#125;\n\n# 连接zookeeper的超时时间,6s\nzookeeper.connection.timeout.ms=6000\n</code></pre>\n<h6 id=\"313-entrypoint\"><a class=\"anchor\" href=\"#313-entrypoint\">#</a> 3.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 变量\nKAFKA_DIR=/kafka\nKAFKA_CONF=/kafka/config/server.properties\n\n# 1、基于主机名 + 1 获取Broker_id  这个是用来标识集群节点 在整个集群中必须唯一\nBROKER_ID=$(( $(hostname | sed 's#.*-##g') + 1 ))\nLISTENERS=$(hostname -i)\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;BROKER_ID&#125;@$&#123;BROKER_ID&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;LISTENERS&#125;@$&#123;LISTENERS&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;KAFKA_DATA_DIR&#125;@$&#123;KAFKA_DATA_DIR:-/data&#125;@g  $&#123;KAFKA_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;KAFKA_CONF&#125;\n\n# 3、启动Kafka\ncd $&#123;KAFKA_DIR&#125;/bin\nsed -i '/export KAFKA_HEAP_OPTS/a export JMX_PORT=&quot;9999&quot;' kafka-server-start.sh\n./kafka-server-start.sh ../config/server.properties\n</code></pre>\n<h6 id=\"314-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#314-构建镜像并推送仓库\">#</a> 3.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2\n</code></pre>\n<h5 id=\"32-迁移kafka集群至k8s\"><a class=\"anchor\" href=\"#32-迁移kafka集群至k8s\">#</a> 3.2 迁移 Kafka 集群至 K8S</h5>\n<h6 id=\"321-kafka-headless\"><a class=\"anchor\" href=\"#321-kafka-headless\">#</a> 3.2.1 kafka-headless</h6>\n<pre><code># cat 01-kafka-headless.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-svc\n  namespace: logging\nspec:\n  clusterIP: None\n  selector:\n    app: kafka\n  ports:\n  - name: client\n    port: 9092\n    targetPort: 9092\n  - name: jmx\n    port: 9999\n    targetPort: 9999\n</code></pre>\n<h6 id=\"322-kafka-sts\"><a class=\"anchor\" href=\"#322-kafka-sts\">#</a> 3.2.2 kafka-sts</h6>\n<pre><code># cat 02-kafka-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  namespace: logging\nspec:\n  serviceName: &quot;kafka-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kafka\n  template:\n    metadata:\n      labels:\n        app: kafka\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values: [&quot;kafka&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kafka\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kafka:2.12.2 \n        imagePullPolicy: Always\n        ports:\n        - name: client\n          containerPort: 9092\n        - name: jmxport\n          containerPort: 9999\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n        readinessProbe:         # 就绪探针，不就绪则不介入流量\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        livenessProbe:         # 存活探针。如果不存活则根据重启策略进行重启\n          tcpSocket:\n            port: 9092\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"323-更新资源清单\"><a class=\"anchor\" href=\"#323-更新资源清单\">#</a> 3.2.3 更新资源清单</h6>\n<pre><code>[root@k8s-master01 02-kafka]# kubectl apply -f 01-kafka-headless.yaml \n[root@k8s-master01 02-kafka]# kubectl apply -f 02-kafka-sts.yaml\n[root@k8s-master01 02-kafka]# kubectl get pods -n logging \nNAME          READY   STATUS    RESTARTS       AGE\nkafka-0       1/1     Running   0              5m49s\nkafka-1       1/1     Running   0              4m43s\nkafka-2       1/1     Running   0              3m40s\n\n#查看kafka是否注册到zookeeper\n[root@k8s-master01 02-kafka]# kubectl exec -it zookeeper-0 -n logging -- /bin/bash\nroot@zookeeper-0:/# /zookeeper/bin/zkCli.sh \n[zk: localhost:2181(CONNECTED) 2] get /brokers/ids/1\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.85.201:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.85.201&quot;,&quot;timestamp&quot;:&quot;1748162470218&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n[zk: localhost:2181(CONNECTED) 3] get /brokers/ids/2\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.58.205:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.58.205&quot;,&quot;timestamp&quot;:&quot;1748162532658&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n[zk: localhost:2181(CONNECTED) 4] get /brokers/ids/3\n&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://172.16.195.1:9092&quot;],&quot;jmx_port&quot;:9999,&quot;host&quot;:&quot;172.16.195.1&quot;,&quot;timestamp&quot;:&quot;1748162649250&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;\n</code></pre>\n<h6 id=\"324-检查kafka集群\"><a class=\"anchor\" href=\"#324-检查kafka集群\">#</a> 3.2.4 检查 Kafka 集群</h6>\n<pre><code>1.创建一个topic\nroot@kafka-0:/# /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181 --partitions 1 --replication-factor 3 --topic oldxu\n\n2.模拟消息发布\nroot@kafka-1:/# /kafka/bin/kafka-console-producer.sh --broker-list kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu\n&gt;hello kubernetes\n&gt;hello world\n\n3.模拟消息订阅\nroot@kafka-2:/# /kafka/bin/kafka-console-consumer.sh  --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 --topic oldxu --from-beginning\nhello kubernetes\nhello world\n</code></pre>\n<h4 id=\"4-交付efak至k8s\"><a class=\"anchor\" href=\"#4-交付efak至k8s\">#</a> 4. 交付 efak 至 K8S</h4>\n<h5 id=\"41-制作efak镜像\"><a class=\"anchor\" href=\"#41-制作efak镜像\">#</a> 4.1 制作 efak 镜像</h5>\n<h6 id=\"411-dockerfile\"><a class=\"anchor\" href=\"#411-dockerfile\">#</a> 4.1.1 Dockerfile</h6>\n<pre><code>[root@manager 03-efak]# cat Dockerfile \nFROM openjdk:8\n\n# 1、调整时区\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \\\n    echo 'Asia/Shanghai' &gt; /etc/timezone\n\n# 2、拷贝kafka软件以及kafka的配置\nENV VERSION=3.0.1\nADD ./efak-web-$&#123;VERSION&#125;-bin.tar.gz /\nADD ./system-config.properties /efak-web-$&#123;VERSION&#125;/conf/system-config.properties\n\n# 3、修改efak的名称\nRUN mv /efak-web-$&#123;VERSION&#125; /efak\n\n# 4、环境变量\nENV KE_HOME=/efak\nENV PATH=$PATH:$KE_HOME/bin\n\n# 5、启动脚本（修改kafka配置）\nADD ./entrypoint.sh /entrypoint.sh\n\n# 6、暴露kafka端口 9999是jmx的端口\nEXPOSE 8048\n\n# 7、运行启动脚本\nCMD [&quot;/bin/bash&quot;,&quot;/entrypoint.sh&quot;]\n</code></pre>\n<h6 id=\"412-system-config\"><a class=\"anchor\" href=\"#412-system-config\">#</a> 4.1.2 system-config</h6>\n<pre><code># cat system-config.properties \n######################################\n# 填写 zookeeper集群列表\n######################################\nefak.zk.cluster.alias=cluster1\ncluster1.zk.list=&#123;ZOOK_SERVERS&#125;\n\n######################################\n# broker 最大规模数量\n######################################\ncluster1.efak.broker.size=20\n\n######################################\n# zk 客户端线程数\n######################################\nkafka.zk.limit.size=32\n\n######################################\n# EFAK webui 端口\n######################################\nefak.webui.port=8048\n\n######################################\n# kafka offset storage\n######################################\ncluster1.efak.offset.storage=kafka\n\n######################################\n# kafka jmx uri\n######################################\ncluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi\n\n######################################\n# kafka metrics 指标，默认存储15天\n######################################\nefak.metrics.charts=true\nefak.metrics.retain=15\n\n######################################\n# kafka sql topic records max\n######################################\nefak.sql.topic.records.max=5000\nefak.sql.topic.preview.records.max=10\n\n######################################\n# delete kafka topic token\n######################################\nefak.topic.token=keadmin\n\n######################################\n# kafka sqlite 数据库地址（需要修改存储路径）\n######################################\nefak.driver=org.sqlite.JDBC\nefak.url=jdbc:sqlite:&#123;EFAK_DATA_DIR&#125;/db/ke.db\nefak.username=root\nefak.password=www.kafka-eagle.org\n</code></pre>\n<h6 id=\"413-entrypoint\"><a class=\"anchor\" href=\"#413-entrypoint\">#</a> 4.1.3 entrypoint</h6>\n<pre><code># cat entrypoint.sh \n# 1、变量\nEFAK_DIR=/efak\nEFAK_CONF=/efak/conf/system-config.properties\n\n# 2、替换配置文件内容，后期ZK集群的地址通过ENV传递\nsed -i s@&#123;EFAK_DATA_DIR&#125;@$&#123;EFAK_DIR&#125;@g  $&#123;EFAK_CONF&#125;\nsed -i s@&#123;ZOOK_SERVERS&#125;@$&#123;ZOOK_SERVERS&#125;@g  $&#123;EFAK_CONF&#125;\n\n# 3、启动efka\n$&#123;EFAK_DIR&#125;/bin/ke.sh start\ntail -f $&#123;EFAK_DIR&#125;/logs/ke_console.out\n</code></pre>\n<h6 id=\"414-构建镜像并推送仓库\"><a class=\"anchor\" href=\"#414-构建镜像并推送仓库\">#</a> 4.1.4 构建镜像并推送仓库</h6>\n<pre><code># wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz\n# docker build -t registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 .\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0\n</code></pre>\n<h5 id=\"42-迁移efak至k8s\"><a class=\"anchor\" href=\"#42-迁移efak至k8s\">#</a> 4.2 迁移 efak 至 K8S</h5>\n<h6 id=\"421-efak-deploy\"><a class=\"anchor\" href=\"#421-efak-deploy\">#</a> 4.2.1 efak-deploy</h6>\n<pre><code># cat 01-efak-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: efak\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: efak\n  template:\n    metadata:\n      labels:\n        app: efak\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: efak\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/efak:3.0 \n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8048\n        env:\n        - name: ZOOK_SERVERS\n          value: &quot;zookeeper-0.zookeeper-svc:2181,zookeeper-1.zookeeper-svc:2181,zookeeper-2.zookeeper-svc:2181&quot;\n</code></pre>\n<h6 id=\"422-efak-service\"><a class=\"anchor\" href=\"#422-efak-service\">#</a> 4.2.2 efak-service</h6>\n<pre><code># cat 02-efak-service.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: efak-svc\n  namespace: logging\nspec:\n  selector:\n    app: efak\n  ports:\n  - port: 8048\n    targetPort: 8048\n</code></pre>\n<h6 id=\"423-efak-ingress\"><a class=\"anchor\" href=\"#423-efak-ingress\">#</a> 4.2.3 efak-ingress</h6>\n<pre><code># cat 03-efak-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: efak-ingress\n  namespace: logging\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;efak.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: efak-svc\n            port: \n              number: 8048\n</code></pre>\n<h6 id=\"424-更新资源清单\"><a class=\"anchor\" href=\"#424-更新资源清单\">#</a> 4.2.4 更新资源清单</h6>\n<pre><code>[root@k8s-master01 03-efak]# kubectl apply -f 01-efak-deploy.yaml \n[root@k8s-master01 03-efak]# kubectl apply -f 02-efak-service.yaml \n[root@k8s-master01 03-efak]# kubectl apply -f 03-efak-ingress.yaml \n</code></pre>\n<h6 id=\"425-访问efka\"><a class=\"anchor\" href=\"#425-访问efka\">#</a> 4.2.5 访问 efka</h6>\n<p>1、初始用户名密码 admin   123456</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/Nq16u4z.png\" alt=\"1.png\" /></p>\n<p>2、查看 Topics</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/9Bin9cr.png\" alt=\"2.png\" /></p>\n<p>3、查看 kafka 集群状态</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/U76YIck.png\" alt=\"3.png\" /></p>\n<p>4、查看 Zookeeper 集群状态</p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/cY5LeWx.png\" alt=\"4.png\" /></p>\n<h4 id=\"5-交付elastic集群\"><a class=\"anchor\" href=\"#5-交付elastic集群\">#</a> 5. 交付 Elastic 集群</h4>\n<ul>\n<li>ES 集群是由多个节点组成的，通过 <a href=\"http://cluster.name\">cluster.name</a> 设置 ES 集群名称，同时用于区分其它的 ES 集群。</li>\n<li>每个节点通过 <a href=\"http://node.name\">node.name</a> 参数来设定所在集群的节点名称。</li>\n<li>节点使用 discovery.send_hosts 参数来设定集群节点的列表。</li>\n<li>集群在第一次启动时，需要初始化，同时需要指定参与选举的 master 节点 IP，或节点名称。</li>\n<li>每个节点可以通过 node.master:true 设定为 master 角色，通过 node.data:true 设定为 data 角色。</li>\n</ul>\n<pre><code>[root@k8s-master01 ~]# grep &quot;^[a-Z]&quot; /etc/elasticsearch/elasticsearch.yml\n# 集群名称cluster.name: my-oldxu\n# 节点名称node.name: node1\n# 数据存储路径path.data: /var/lib/elasticsearch\n# 日志存储路径path.logs: /var/log/elasticsearch\n# 监听在本地哪个地址上network.host: 10.0.0.100\n# 监听端口http.port: 9200\n# 集群主机列表discovery.seed_hosts: [&quot;ip1&quot;, &quot;ip2&quot;, &quot;ip3&quot;]\n# 仅第一次启动集群时进行选举（可以填写node.name的名称）cluster.initial_master_nodes: [&quot;node01&quot;, &quot;node02&quot;, &quot;node03&quot;]\n</code></pre>\n<h5 id=\"51-下载elastic镜像\"><a class=\"anchor\" href=\"#51-下载elastic镜像\">#</a> 5.1 下载 elastic 镜像</h5>\n<pre><code># docker pull elasticsearch:7.17.6\n# docker tag elasticsearch:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6\n</code></pre>\n<h5 id=\"52-交付es-service\"><a class=\"anchor\" href=\"#52-交付es-service\">#</a> 5.2 交付 ES-Service</h5>\n<p>创建 es-headlessService，为每个 ES Pod 设定固定的 DNS 名称，无论它是 Master 或是 Data，易或是 Coordinating</p>\n<pre><code># cat 01-es-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: es-svc\n  namespace: logging\nspec:\n  selector:\n    app: es\n  clusterIP: None\n  ports:\n  - name: cluster\n    port: 9200\n    targetPort: 9200\n  - name: transport\n    port: 9300\n    targetPort: 9300\n</code></pre>\n<h5 id=\"53-交付es-master节点\"><a class=\"anchor\" href=\"#53-交付es-master节点\">#</a> 5.3 交付 ES-Master 节点</h5>\n<ol>\n<li>\n<p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data ；</p>\n</li>\n<li>\n<p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p>\n</li>\n<li>\n<p>ES 启动是通过 ENV 环境变量传参来完成的；</p>\n<ul>\n<li>\n<p>集群名称、节点名称、角色类型；</p>\n</li>\n<li>\n<p>discovery.seed_hosts 集群地址列表；</p>\n</li>\n<li>\n<p>cluster.initial_master_nodes 初始集群参与选举的 master 节点名称；</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre><code># cat 02-es-master.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-master\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 3           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: master\n  template:\n    metadata:\n      labels:\n        app: es\n        role: master\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;master&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;true&quot;\n        - name: node.data\n          value: &quot;false&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n        - name: cluster.initial_master_nodes\n          value: &quot;es-master-0,es-master-1,es-master-2&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<pre><code>[root@k8s-master01 04-elasticsearch]# cat 03-es-data.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-data\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 2           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: data\n  template:\n    metadata:\n      labels:\n        app: es\n        role: data\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;data&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;false&quot;\n        - name: node.data\n          value: &quot;true&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"54-交付es-data节点\"><a class=\"anchor\" href=\"#54-交付es-data节点\">#</a> 5.4 交付 ES-Data 节点</h5>\n<ol>\n<li>\n<p>ES 无法使用 root 直接启动，需要授权数据目录 UID=1000，同时还需要持久化 /usr/share/elasticsearch/data</p>\n</li>\n<li>\n<p>ES 所有节点都需要设定 vm.max_map_count 内核参数以及 ulimit；</p>\n</li>\n<li>\n<p>ES 启动是通过 ENV 环境变量传参来完成的</p>\n<ul>\n<li>\n<p>集群名称、节点名称、角色类型</p>\n</li>\n<li>\n<p>discovery.seed_hosts 集群地址列表</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre><code># cat 03-es-data.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-data\n  namespace: logging\nspec:\n  serviceName: &quot;es-svc&quot;\n  replicas: 2           # es-pod运行的实例\n  selector:             # 需要管理的ES-Pod标签\n    matchLabels:\n      app: es\n      role: data\n  template:\n    metadata:\n      labels:\n        app: es\n        role: data\n    spec:                       # 定义pod规范\n      imagePullSecrets:         # 镜像拉取使用的认证信息\n      - name: harbor-admin\n      affinity:                 # 设定pod反亲和\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values: [&quot;es&quot;]\n              - key: role\n                operator: In\n                values: [&quot;data&quot;]\n            topologyKey: &quot;kubernetes.io/hostname&quot;       # 每个节点就是一个位置\n      initContainers:           # 初始化容器设定\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;chown -R 1000:1000 /usr/share/elasticsearch/data ; sysctl -w vm.max_map_count=262144; ulimit -n 65536&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n      containers:               # ES主容器\n      - name: es\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/elasticsearch:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n            memory: 4096Mi\n          requests:\n            cpu: 300m\n            memory: 1024Mi\n        ports:\n        - name: cluster\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        env:\n        - name: ES_JAVA_OPTS\n          value: &quot;-Xms1g -Xmx1g&quot;\n        - name: cluster.name\n          value: es-cluster\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: node.master\n          value: &quot;false&quot;\n        - name: node.data\n          value: &quot;true&quot;\n        - name: discovery.seed_hosts\n          value: &quot;es-master-0.es-svc,es-master-1.es-svc,es-master-2.es-svc&quot;\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates: # 动态pvc\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteOnce&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h5 id=\"55-更新资源清单\"><a class=\"anchor\" href=\"#55-更新资源清单\">#</a> 5.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 04-elasticsearch]# kubectl apply -f 01-es-svc.yaml \n[root@k8s-master01 04-elasticsearch]# kubectl apply -f 02-es-master.yaml \n[root@k8s-master01 04-elasticsearch]# kubectl apply -f 03-es-data.yaml \n</code></pre>\n<h5 id=\"56-验证es集群\"><a class=\"anchor\" href=\"#56-验证es集群\">#</a> 5.6 验证 ES 集群</h5>\n<pre><code>#1.解析headlessService获取对应ES集群任一节点的IP地址\n# dig @10.96.0.10 es-svc.logging.svc.cluster.local  +short\n172.16.58.229\n172.16.122.191\n172.16.195.21\n172.16.122.129\n172.16.32.164\n\n#2.通过curl访问ES，检查ES集群是否正常（如果仅交付Master，没有data节点，集群状态可能会Red，因为没有数据节点进行数据存储；）\n# curl -XGET &quot;http://172.16.122.129:9200/_cluster/health?pretty&quot;\n&#123;\n  &quot;cluster_name&quot; : &quot;es-cluster&quot;,\n  &quot;status&quot; : &quot;green&quot;,\n  &quot;timed_out&quot; : false,\n  &quot;number_of_nodes&quot; : 5,\n  &quot;number_of_data_nodes&quot; : 2,\n  &quot;active_primary_shards&quot; : 3,\n  &quot;active_shards&quot; : 6,\n  &quot;relocating_shards&quot; : 0,\n  &quot;initializing_shards&quot; : 0,\n  &quot;unassigned_shards&quot; : 0,\n  &quot;delayed_unassigned_shards&quot; : 0,\n  &quot;number_of_pending_tasks&quot; : 0,\n  &quot;number_of_in_flight_fetch&quot; : 0,\n  &quot;task_max_waiting_in_queue_millis&quot; : 0,\n  &quot;active_shards_percent_as_number&quot; : 100.0\n&#125;\n\n#3.查看ES各个节点详情\n# curl -XGET &quot;http://172.16.122.129:9200/_cat/nodes&quot;\n172.16.122.129 16 33 20 0.38 0.56 0.38 ilmr       - es-master-2\n172.16.58.229  66 33 22 0.64 0.66 0.44 ilmr       * es-master-1\n172.16.122.191 52 34 15 0.38 0.56 0.38 cdfhilrstw - es-data-0\n172.16.195.21  38 35 19 0.38 0.53 0.36 cdfhilrstw - es-data-1\n172.16.32.164  31 33 12 0.28 0.50 0.59 ilmr       - es-master-0\n</code></pre>\n<h4 id=\"6-交付kibana可视化\"><a class=\"anchor\" href=\"#6-交付kibana可视化\">#</a> 6. 交付 Kibana 可视化</h4>\n<h5 id=\"61-下载kibana镜像\"><a class=\"anchor\" href=\"#61-下载kibana镜像\">#</a> 6.1 下载 kibana 镜像</h5>\n<pre><code># docker pull kibana:7.17.6\n# docker tag kibana:7.17.6 registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6\n# docker push registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6\n</code></pre>\n<h5 id=\"62-kibana-deploy\"><a class=\"anchor\" href=\"#62-kibana-deploy\">#</a> 6.2 kibana-deploy</h5>\n<ol>\n<li>Kibana 需要连接 ES 集群，通过 ELASTICSEARCH_HOSTS 变量来传递 ES 集群地址</li>\n<li>kibana 通过 I18N_LOCALE 来传递语言环境</li>\n<li>Kibana 通过 SERVER_PUBLICBASEURL 来传递服务访问的公开地址</li>\n</ol>\n<pre><code># cat 01-kibana-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kibana\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kibana\n  template:\n    metadata:\n      labels:\n        app: kibana\n    spec:\n      imagePullSecrets:\n      - name: harbor-admin\n      containers:\n      - name: kibana\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kibana:7.17.6 \n        resources:\n          limits:\n            cpu: 1000m\n        ports:\n        - containerPort: 5601\n        env:\n        - name: ELASTICSEARCH_HOSTS\n          value: '[&quot;http://es-data-0.es-svc:9200&quot;,&quot;http://es-data-1.es-svc:9200&quot;]'\n        - name: I18N_LOCALE\n          value: &quot;zh-CN&quot;\n        - name: SERVER_PUBLICBASEURL\n          value: &quot;http://kibana.hmallleasing.com&quot;   #kibana访问UI\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n</code></pre>\n<h5 id=\"63-kibana-svc\"><a class=\"anchor\" href=\"#63-kibana-svc\">#</a> 6.3 kibana-svc</h5>\n<pre><code># cat 02-kibana-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: kibana-svc\n  namespace: logging\nspec:\n  selector:\n    app: kibana\n  ports:\n  - name: web\n    port: 5601\n    targetPort: 5601\n</code></pre>\n<h5 id=\"64-kibana-ingress\"><a class=\"anchor\" href=\"#64-kibana-ingress\">#</a> 6.4 kibana-ingress</h5>\n<pre><code># cat 03-kibana-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kibana-ingress\n  namespace: logging\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: &quot;kibana.hmallleasing.com&quot;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kibana-svc\n            port:\n              number: 5601\n</code></pre>\n<h5 id=\"65-更新资源清单\"><a class=\"anchor\" href=\"#65-更新资源清单\">#</a> 6.5 更新资源清单</h5>\n<pre><code>[root@k8s-master01 05-kibana]# kubectl apply -f 01-kibana-deploy.yaml \n[root@k8s-master01 05-kibana]# kubectl apply -f 02-kibana-svc.yaml \n[root@k8s-master01 05-kibana]# kubectl apply -f 03-kibana-ingress.yaml\n\n[root@k8s-master01 05-kibana]# kubectl get pods -n logging\nNAME                      READY   STATUS    RESTARTS   AGE\nefak-5cdc74bf59-nrhb4     1/1     Running   0          5h33m\nes-data-0                 1/1     Running   0          16m\nes-data-1                 1/1     Running   0          15m\nes-master-0               1/1     Running   0          17m\nes-master-1               1/1     Running   0          15m\nes-master-2               1/1     Running   0          12m\nkafka-0                   1/1     Running   0          5h39m\nkafka-1                   1/1     Running   0          5h39m\nkafka-2                   1/1     Running   0          5h38m\nkibana-5ccc46864b-ndzx9   1/1     Running   0          118s\nzookeeper-0               1/1     Running   0          5h42m\nzookeeper-1               1/1     Running   0          5h42m\nzookeeper-2               1/1     Running   0          5h41m\n</code></pre>\n<h5 id=\"66-访问kibana\"><a class=\"anchor\" href=\"#66-访问kibana\">#</a> 6.6 访问 kibana</h5>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/sUXTx1J.png\" alt=\"1.png\" /></p>\n",
            "tags": [
                "ELKStack"
            ]
        }
    ]
}