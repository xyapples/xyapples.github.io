{
    "version": "https://jsonfeed.org/version/1",
    "title": "LinuxSre‰∫ëÂéüÁîü ‚Ä¢ All posts by \"kubernetes\" tag",
    "description": "‰∏ìÊ≥®‰∫é Linux ËøêÁª¥„ÄÅ‰∫ëËÆ°ÁÆó„ÄÅ‰∫ëÂéü‚Ω£Á≠âÊäÄÊúØ",
    "home_page_url": "http://ixuyong.cn",
    "items": [
        {
            "id": "http://ixuyong.cn/posts/626047790.html",
            "url": "http://ixuyong.cn/posts/626047790.html",
            "title": "Ê∂àË¥πÁßüËµÅÁ≥ªÁªüÂæÆÊúçÂä°Â∫îÁî®‰∫§‰ªòÂÆûË∑µ",
            "date_published": "2025-05-18T13:42:46.000Z",
            "content_html": "<h3 id=\"Ê∂àË¥πÁßüËµÅÁ≥ªÁªüÂæÆÊúçÂä°Â∫îÁî®‰∫§‰ªòÂÆûË∑µ\"><a class=\"anchor\" href=\"#Ê∂àË¥πÁßüËµÅÁ≥ªÁªüÂæÆÊúçÂä°Â∫îÁî®‰∫§‰ªòÂÆûË∑µ\">#</a> Ê∂àË¥πÁßüËµÅÁ≥ªÁªüÂæÆÊúçÂä°Â∫îÁî®‰∫§‰ªòÂÆûË∑µ</h3>\n<h4 id=\"‰∏Ä-ÈÉ®ÁΩ≤‰∏≠Èó¥‰ª∂\"><a class=\"anchor\" href=\"#‰∏Ä-ÈÉ®ÁΩ≤‰∏≠Èó¥‰ª∂\">#</a> ‰∏Ä„ÄÇÈÉ®ÁΩ≤‰∏≠Èó¥‰ª∂</h4>\n<h5 id=\"11-ÈÉ®ÁΩ≤mysql\"><a class=\"anchor\" href=\"#11-ÈÉ®ÁΩ≤mysql\">#</a> 1.1 ÈÉ®ÁΩ≤ MySQL</h5>\n<h6 id=\"111-mysql-configmap\"><a class=\"anchor\" href=\"#111-mysql-configmap\">#</a> 1.1.1 MySQL-ConfigMap</h6>\n<pre><code>[root@k8s-master01 01-nf-flms-mysql]# cat 01-mysql-cm.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mysql-cm\n  namespace: prod\ndata:\n  my.cnf: |-\n    [mysqld]\n    #performance setttings\n    lock_wait_timeout = 3600\n    open_files_limit = 65535\n    back_log = 1024\n    max_connections = 1024\n    max_connect_errors = 1000000\n    table_open_cache = 1024\n    table_definition_cache = 1024\n    thread_stack = 512K\n    sort_buffer_size = 4M\n    join_buffer_size = 4M\n    read_buffer_size = 8M\n    read_rnd_buffer_size = 4M\n    bulk_insert_buffer_size = 64M\n    thread_cache_size = 768\n    interactive_timeout = 600\n    wait_timeout = 600\n    tmp_table_size = 32M\n    max_heap_table_size = 32M\n    max_allowed_packet = 128M\n</code></pre>\n<h6 id=\"112-mysql-secret\"><a class=\"anchor\" href=\"#112-mysql-secret\">#</a> <strong>1.1.2 MySQL-Secret</strong></h6>\n<pre><code>[root@k8s-master01 01-nf-flms-mysql]# cat 02-mysql-secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysql-secret\n  namespace: prod\nstringData:\n  MYSQL_ROOT_PASSWORD: Superman*2023\ntype: Opaque\n</code></pre>\n<h6 id=\"113-mysql-statefulset\"><a class=\"anchor\" href=\"#113-mysql-statefulset\">#</a> <strong>1.1.3 MySQL-StatefulSet</strong></h6>\n<pre><code># cat 03-mysql-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql-nf-flms\n  namespace: prod\nspec:\n  serviceName: &quot;mysql-nf-flms-svc&quot;\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql\n      role: nf-flms\n  template:\n    metadata:\n      labels:\n        app: mysql\n        role: nf-flms\n    spec:\n      containers:\n      - name: db\n        image: mysql:8.0\n        args:\n        - &quot;--character-set-server=utf8&quot;\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: MYSQL_ROOT_PASSWORD\n        - name: MYSQL_DATABASE      #Êï∞ÊçÆÂ∫ìÂêçÁß∞\n          value: nf-flms        \n        ports:\n        - name: tcp-3306\n          containerPort: 3306\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4000Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql/\n        - name: config\n          mountPath: /etc/mysql/conf.d/my.cnf\n          subPath: my.cnf\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: config\n        configMap:\n          name: mysql-cm\n          items:\n            - key: my.cnf\n              path: my.cnf\n          defaultMode: 420\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      storageClassName: &quot;nfs-storage&quot;\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"114-mysql-service\"><a class=\"anchor\" href=\"#114-mysql-service\">#</a> <strong>1.1.4 MySQL Service</strong></h6>\n<pre><code># cat 04-mysql-nf-flms-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-nf-flms-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n    role: nf-flms\n  ports:\n  - name: tcp-mysql-svc\n    protocol: TCP\n    port: 3306\n    targetPort: 3306\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: mysql-nf-flms-svc-balance\n  namespace: prod \nspec:\n  selector:\n    app: mysql\n    role: nf-flms\n  ports:\n  - name: tcp-mysql-balance\n    protocol: TCP\n    port: 3306\n    targetPort: 3306\n    nodePort: 32206\n  type: NodePort\n</code></pre>\n<h6 id=\"115-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\"><a class=\"anchor\" href=\"#115-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\">#</a> <strong>1.1.5 Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï</strong></h6>\n<pre><code>[root@k8s-master01 01-nf-flms-mysql]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 01-nf-flms-mysql]# kubectl create ns prod\n[root@k8s-master01 01-nf-flms-mysql]# kubectl apply -f .\n</code></pre>\n<h6 id=\"116-ÂØºÂÖ•Êï∞ÊçÆÂ∫ì\"><a class=\"anchor\" href=\"#116-ÂØºÂÖ•Êï∞ÊçÆÂ∫ì\">#</a> <strong>1.1.6 ÂØºÂÖ•Êï∞ÊçÆÂ∫ì</strong></h6>\n<pre><code>[root@k8s-master01 03-nacos]# dig @10.96.0.10 mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local +short\n172.16.85.213\n[root@k8s-master01 01-nf-flms-mysql]# mysql -h 172.16.85.213 -uroot -p&quot;Superman*2023&quot; -B nf_flms &lt; /root/backup/qzj_db01_192.168.1.143_2023-03-30/nf-flms_mysql_backup_20230330030001.sql\n</code></pre>\n<h5 id=\"12-ÈÉ®ÁΩ≤redis-single\"><a class=\"anchor\" href=\"#12-ÈÉ®ÁΩ≤redis-single\">#</a> 1.2 ÈÉ®ÁΩ≤ Redis-single</h5>\n<h6 id=\"121-redis-configmap\"><a class=\"anchor\" href=\"#121-redis-configmap\">#</a> 1.2.1 Redis-ConfigMap</h6>\n<pre><code># cat 01-redis-cm.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-conf\n  namespace: prod\ndata:\n  redis.conf: |\n    bind 0.0.0.0\n    appendonly yes\n    protected-mode no\n    dir /data\n    port 6379\n    requirepass Superman*2023\n</code></pre>\n<h6 id=\"122-redis-statefulset\"><a class=\"anchor\" href=\"#122-redis-statefulset\">#</a> 1.2.2 Redis-StatefulSet</h6>\n<pre><code># cat 02-redis-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis\n  namespace: prod\nspec:\n  serviceName: redis-svc\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:6.2.7\n        command:\n        - &quot;redis-server&quot;\n        args:\n        - &quot;/etc/redis/redis.conf&quot;\n        ports:\n        - name: redis-6379\n          containerPort: 6379\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 6379\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 6379\n        volumeMounts:\n        - name: config\n          mountPath: /etc/redis\n        - name: data\n          mountPath: /data\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        resources:\n          limits:\n            cpu: '2'\n            memory: 4000Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n      volumes:\n      - name: config\n        configMap:\n          name: redis-conf\n          items:\n          - key: redis.conf\n            path: redis.conf\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 2Gi\n</code></pre>\n<h6 id=\"123-redis-service\"><a class=\"anchor\" href=\"#123-redis-service\">#</a> 1.2.3 Redis-Service</h6>\n<pre><code># cat 03-redis-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-svc\n  namespace: prod\n  labels:\n    app: redis\nspec:\n  ports:\n    - name: redis-6379\n      protocol: TCP\n      port: 6379\n      targetPort: 6379\n  selector:\n    app: redis\n  clusterIP: None\n</code></pre>\n<h6 id=\"124-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\"><a class=\"anchor\" href=\"#124-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\">#</a> 1.2.4 Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï</h6>\n<pre><code>[root@k8s-master01 02-redis]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 02-redis]# kubectl apply -f .\n</code></pre>\n<h5 id=\"14-ÈÉ®ÁΩ≤nacosÈõÜÁæ§\"><a class=\"anchor\" href=\"#14-ÈÉ®ÁΩ≤nacosÈõÜÁæ§\">#</a> 1.4 ÈÉ®ÁΩ≤ Nacos ÈõÜÁæ§</h5>\n<h6 id=\"141-ÈÉ®ÁΩ≤nacos-mysql\"><a class=\"anchor\" href=\"#141-ÈÉ®ÁΩ≤nacos-mysql\">#</a> 1.4.1 ÈÉ®ÁΩ≤ Nacos-MySQL</h6>\n<pre><code># cat 01-mysql-nacos-sts-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-nacos-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n    role: nacos\n  ports:\n  - port: 3306\n    targetPort: 3306\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: mysql-nacos-balance\n  namespace: prod\nspec:\n  selector:\n    app: mysql\n    role: nacos\n  ports:\n  - name: tcp-mysql-balance\n    protocol: TCP\n    port: 3306\n    targetPort: 3306\n    nodePort: 31106\n  type: NodePort\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql-nacos\n  namespace: prod\nspec:\n  serviceName: &quot;mysql-nacos-svc&quot;\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql\n      role: nacos\n  template:\n    metadata:\n      labels:\n        app: mysql\n        role: nacos\n    spec:\n      containers:\n      - name: db\n        image: mysql:8.0\n        args:\n        - &quot;--character-set-server=utf8&quot;\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: Superman*2023\n        - name: MYSQL_DATABASE    #nacosÂ∫ìÂêçÁß∞\n          value: nacos\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: '2'\n            memory: 4000Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql/\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"142-ÂØºÂÖ•Êï∞ÊçÆÂ∫ì\"><a class=\"anchor\" href=\"#142-ÂØºÂÖ•Êï∞ÊçÆÂ∫ì\">#</a> <strong>1.4.2 ÂØºÂÖ•Êï∞ÊçÆÂ∫ì</strong></h6>\n<p>nacos ‰∏ãËΩΩÂú∞ÂùÄÔºö<a href=\"https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK\">https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK</a></p>\n<pre><code>[root@k8s-master01 03-nacos]#  sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 03-nacos]# kubectl apply -f 01-mysql-nacos-sts-svc.yaml\n[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-nacos-svc.prod.svc.cluster.local +short\n172.16.85.255\n[root@k8s-master01 03-nacos]# mysql -h 172.16.85.255  -uroot -p&quot;Superman*2023&quot; -B nacos &lt; nacos/conf/mysql-schema.sql\n</code></pre>\n<h6 id=\"143-ÈÉ®ÁΩ≤nacos-configmap\"><a class=\"anchor\" href=\"#143-ÈÉ®ÁΩ≤nacos-configmap\">#</a> 1.4.3 ÈÉ®ÁΩ≤ Nacos-ConfigMap</h6>\n<pre><code># cat 02-nacos-configmap.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nacos-cm\n  namespace: prod\ndata:\n  mysql.host: &quot;mysql-nacos-svc.prod.svc.cluster.local&quot;\n  mysql.db.name: &quot;nacos&quot;   #nacosÊï∞ÊçÆÂ∫ìÂêçÁß∞\n  mysql.port: &quot;3306&quot;\n  mysql.user: &quot;root&quot;    #nacosÊï∞ÊçÆÂ∫ìÁî®Êà∑Âêç\n  mysql.password: &quot;Superman*2023&quot;   #nacosÊï∞ÊçÆÂ∫ìÂØÜÁ†Å\n</code></pre>\n<h6 id=\"144-ÈÉ®ÁΩ≤nacos-service-statefulset\"><a class=\"anchor\" href=\"#144-ÈÉ®ÁΩ≤nacos-service-statefulset\">#</a> 1.4.4 ÈÉ®ÁΩ≤ Nacos-Service-StatefulSet</h6>\n<pre><code># cat 03-nacos-sts-deploy-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: nacos-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: nacos\n  ports:\n  - name: server\n    port: 8848\n    targetPort: 8848\n  - name: client-rpc\n    port: 9848\n    targetPort: 9848\n  - name: raft-rpc\n    port: 9849\n    targetPort: 9849\n  - name: old-raft-rpc\n    port: 7848\n    targetPort: 7848\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nacos\n  namespace: prod\nspec:\n  serviceName: &quot;nacos-svc&quot;\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nacos\n  template:\n    metadata:\n      labels:\n        app: nacos\n    spec:\n      affinity:                                                 # ÈÅøÂÖçPodËøêË°åÂà∞Âêå‰∏Ä‰∏™ËäÇÁÇπ‰∏ä‰∫Ü\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: app\n                    operator: In\n                    values: [&quot;nacos&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;  \n      initContainers:\n      - name: peer-finder-plugin-install\n        image: nacos/nacos-peer-finder-plugin:1.1\n        imagePullPolicy: Always\n        volumeMounts:\n          - name: data\n            mountPath: /home/nacos/plugins/peer-finder\n            subPath: peer-finder\n      containers:\n      - name: nacos\n        image: nacos/nacos-server:v2.4.3\n        resources:\n          limits:\n            cpu: '2'\n            memory: 4Gi\n          requests:\n            cpu: &quot;100m&quot;\n            memory: &quot;1Gi&quot;\n        ports:\n        - name: client-port\n          containerPort: 8848\n        - name: client-rpc\n          containerPort: 9848\n        - name: raft-rpc\n          containerPort: 9849\n        - name: old-raft-rpc\n          containerPort: 7848\n        env:\n        - name: NACOS_AUTH_ENABLE\n          value: &quot;false&quot;\n        - name: MODE  \n          value: &quot;cluster&quot;\n        - name: NACOS_SERVERS\n          value: &quot;nacos-0.nacos-svc.prod.svc.cluster.local:8848  nacos-1.nacos-svc.prod.svc.cluster.local:8848 nacos-2.nacos-svc.prod.svc.cluster.local:8848&quot;\n        - name: NACOS_VERSION\n          value: 2.4.3\n        - name: SPRING_DATASOURCE_PLATFORM\n          value: &quot;mysql&quot;\n        - name: NACOS_REPLICAS\n          value: &quot;3&quot;\n        - name: SERVICE_NAME \n          value: &quot;nacos-svc&quot;\n        - name: DOMAIN_NAME \n          value: &quot;cluster.local&quot;\n        - name: NACOS_SERVER_PORT   \n          value: &quot;8848&quot;\n        - name: NACOS_APPLICATION_PORT\n          value: &quot;8848&quot;\n        - name: PREFER_HOST_MODE\n          value: &quot;hostname&quot;\n        - name: POD_NAMESPACE      \n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: MYSQL_SERVICE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.host\n        - name: MYSQL_SERVICE_DB_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.db.name\n        - name: MYSQL_SERVICE_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.port\n        - name: MYSQL_SERVICE_USER\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.user\n        - name: MYSQL_SERVICE_PASSWORD\n          valueFrom:\n            configMapKeyRef:\n              name: nacos-cm\n              key: mysql.password\n        volumeMounts:\n        - name: data\n          mountPath: /home/nacos/plugins/peer-finder\n          subPath: peer-finder\n        - name: data\n          mountPath: /home/nacos/data\n          subPath: data\n        - name: data\n          mountPath: /home/nacos/logs\n          subPath: logs\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        storageClassName: &quot;nfs-storage&quot;\n        accessModes: [&quot;ReadWriteMany&quot;]\n        resources:\n          requests:\n            storage: 5Gi\n</code></pre>\n<h6 id=\"145-ÈÉ®ÁΩ≤nacos-ingress\"><a class=\"anchor\" href=\"#145-ÈÉ®ÁΩ≤nacos-ingress\">#</a> 1.4.5 ÈÉ®ÁΩ≤ Nacos-Ingress</h6>\n<pre><code># cat 04-nacos-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nacos-ingress\n  namespace: prod\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: nacos.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nacos-svc\n            port:\n              number: 8848\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h6 id=\"146-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\"><a class=\"anchor\" href=\"#146-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\">#</a> <strong>1.4.6 Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï</strong></h6>\n<pre><code>[root@k8s-master01 03-nacos]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 03-nacos]# kubectl apply -f .\n</code></pre>\n<h6 id=\"147-webËÆøÈóÆnacos\"><a class=\"anchor\" href=\"#147-webËÆøÈóÆnacos\">#</a> <strong>1.4.7 Web ËÆøÈóÆ nacos</strong></h6>\n<pre><code>UrlÔºöhttp://nacos.hmallleasing.com/nacos \nUser: nacos\nPasswd: nacos \n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/iNiUErY.jpeg\" alt=\"Snipaste_2025-05-18_21-13-44.jpg\" /></p>\n<h5 id=\"15-ÈÉ®ÁΩ≤xxl-job\"><a class=\"anchor\" href=\"#15-ÈÉ®ÁΩ≤xxl-job\">#</a> 1.5 ÈÉ®ÁΩ≤ xxl-job</h5>\n<h6 id=\"151-ÈÉ®ÁΩ≤xxl-job-mysql\"><a class=\"anchor\" href=\"#151-ÈÉ®ÁΩ≤xxl-job-mysql\">#</a> 1.5.1 ÈÉ®ÁΩ≤ xxl-job-MySQL</h6>\n<pre><code># cat 01-mysql-xxljob-sts-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-xxljob-svc\n  namespace: prod\nspec:\n  clusterIP: None\n  selector:\n    app: mysql\n    role: xxljob\n  ports:\n    - name: tcp-mysql-svc\n      protocol: TCP\n      port: 3306\n      targetPort: 3306\n---\napiVersion: v1\nkind: Service\napiVersion: v1\nmetadata:\n  name: mysql-xxljob-external\n  namespace: prod\nspec:\n  ports:\n    - name: tcp-mysql-external\n      protocol: TCP\n      port: 3306\n      targetPort: 3306\n      nodePort: 31206\n  selector:\n    app: mysql\n    role: xxljob\n  type: NodePort\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql-xxljob\n  namespace: prod\nspec:\n  serviceName: &quot;mysql-xxljob-svc&quot;\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysql\n      role: xxljob\n  template:\n    metadata:\n      labels:\n        app: mysql\n        role: xxljob\n    spec:\n      containers:\n      - name: db\n        image: mysql:8.0\n        args:\n        - &quot;--character-set-server=utf8&quot;\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: Superman*2023\n        ports:\n        - containerPort: 3306\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4000Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        livenessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        readinessProbe:\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          successThreshold: 1\n          tcpSocket:\n            port: 3306\n          timeoutSeconds: 2\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql/\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [&quot;ReadWriteMany&quot;]\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"152-ÂØºÂÖ•Êï∞ÊçÆÂ∫ì\"><a class=\"anchor\" href=\"#152-ÂØºÂÖ•Êï∞ÊçÆÂ∫ì\">#</a> <strong>1.5.2 ÂØºÂÖ•Êï∞ÊçÆÂ∫ì</strong></h6>\n<p>xxljob Ë°®ÁªìÊûÑ‰∏ãËΩΩÂú∞ÂùÄÔºö<a href=\"https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db\">https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db</a></p>\n<pre><code>[root@k8s-master01 05-xxl-job]# kubectl apply -f 01-mysql-xxljob-sts-svc.yaml\n[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-xxljob-svc.prod.svc.cluster.local +short\n172.16.85.250\n[root@k8s-master01 05-xxl-job]# mysql -h 172.16.85.250  -uroot -p&quot;Superman*2023&quot;  &lt; tables_xxl_job.sql\n</code></pre>\n<h6 id=\"153-ÈÉ®ÁΩ≤xxl-job-service-deployment\"><a class=\"anchor\" href=\"#153-ÈÉ®ÁΩ≤xxl-job-service-deployment\">#</a> 1.5.3 ÈÉ®ÁΩ≤ xxl-job-Service-Deployment</h6>\n<pre><code># cat 02-xxljob-deploy-svc.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: xxl-job\n  namespace: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: xxl-job\n  template:\n    metadata:\n      labels:\n        app: xxl-job\n    spec:\n      containers:\n      - image: xuxueli/xxl-job-admin:3.1.0\n        name: xxl-job\n        ports:\n        - containerPort: 8080\n        env:\n        - name: PARAMS\n          value: &quot;--spring.datasource.url=jdbc:mysql://mysql-xxljob-svc.prod.svc.cluster.local:3306/xxl_job?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;serverTimezone=Asia/Shanghai --spring.datasource.username=root --spring.datasource.password=Superman*2023&quot;\n        volumeMounts:\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n        resources:\n          limits:\n            cpu: '1'\n            memory: 2000Mi\n          requests:\n            cpu: 100m\n            memory: 500Mi\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: xxljob-svc\n  namespace: prod\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: xxl-job\n</code></pre>\n<h6 id=\"154-ÈÉ®ÁΩ≤xxl-job-service\"><a class=\"anchor\" href=\"#154-ÈÉ®ÁΩ≤xxl-job-service\">#</a> 1.5.4 ÈÉ®ÁΩ≤ xxl-job-service</h6>\n<pre><code>[root@k8s-master01 05-xxl-job]# cat 04-xxljob-external.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: xxljob-balancer\n  namespace: prod\nspec:\n  type: NodePort\n  ports:\n    - name: xxljob-balancer\n      protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: xxl-job\n</code></pre>\n<h6 id=\"155-ÈÉ®ÁΩ≤xxl-job-ingress\"><a class=\"anchor\" href=\"#155-ÈÉ®ÁΩ≤xxl-job-ingress\">#</a> 1.5.5 ÈÉ®ÁΩ≤ xxl-job-Ingress</h6>\n<pre><code>[root@k8s-master01 05-xxl-job]# cat 03-xxljob-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: xxljob-ingress\n  namespace: prod\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: xxljob.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: xxljob-svc\n            port:\n              number: 8080\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h6 id=\"156-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\"><a class=\"anchor\" href=\"#156-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\">#</a> 1.5.6 Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï</h6>\n<pre><code>[root@k8s-master01 05-xxl-job]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 05-xxl-job]# kubectl apply -f .\n</code></pre>\n<h6 id=\"157-webËÆøÈóÆxxl-job\"><a class=\"anchor\" href=\"#157-webËÆøÈóÆxxl-job\">#</a> <strong>1.5.7 Web ËÆøÈóÆ xxl-job</strong></h6>\n<pre><code>http://192.168.40.101:30904/xxl-job-admin/\nhttp://xxljob.hmallleasing.com/xxl-job-admin/ \nuser:admin    \npwd:1223456\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/A5NbU2Z.jpeg\" alt=\"Snipaste_2025-05-18_14-54-59.jpg\" /></p>\n<h5 id=\"16-ÈÉ®ÁΩ≤rabbitmqÈõÜÁæ§\"><a class=\"anchor\" href=\"#16-ÈÉ®ÁΩ≤rabbitmqÈõÜÁæ§\">#</a> 1.6 ÈÉ®ÁΩ≤ rabbitmq ÈõÜÁæ§</h5>\n<h6 id=\"161-ÂàõÂª∫rbacÊùÉÈôê\"><a class=\"anchor\" href=\"#161-ÂàõÂª∫rbacÊùÉÈôê\">#</a> 1.6.1 ÂàõÂª∫ RBAC ÊùÉÈôê</h6>\n<pre><code># cat 01-rabbitmq-rbac.yaml \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: rabbitmq-cluster\n  namespace: prod\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: rabbitmq-cluster\n  namespace: prod\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;endpoints&quot;]\n  verbs: [&quot;get&quot;]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: rabbitmq-cluster\n  namespace: prod\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: rabbitmq-cluster\nsubjects:\n- kind: ServiceAccount\n  name: rabbitmq-cluster\n  namespace: prod\n</code></pre>\n<h6 id=\"162-ÂàõÂª∫ÈõÜÁæ§ÁöÑsecret\"><a class=\"anchor\" href=\"#162-ÂàõÂª∫ÈõÜÁæ§ÁöÑsecret\">#</a> 1.6.2 ÂàõÂª∫ÈõÜÁæ§ÁöÑ Secret</h6>\n<pre><code># cat 02-rabbitmq-secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: rabbitmq-secret\n  namespace: prod\nstringData:\n  password: talent\n  url: amqp://RABBITMQ_USER:RABBITMQ_PASS@rmq-cluster-balancer\n  username: superman\ntype: Opaque\n</code></pre>\n<h6 id=\"163-ÂàõÂª∫configmap\"><a class=\"anchor\" href=\"#163-ÂàõÂª∫configmap\">#</a> 1.6.3 ÂàõÂª∫ ConfigMap</h6>\n<pre><code># cat 03-rabbitmq-cm.yaml \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rabbitmq-cluster-config\n  namespace: prod\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\ndata:\n    enabled_plugins: |\n      [rabbitmq_management,rabbitmq_peer_discovery_k8s].\n    rabbitmq.conf: |\n      loopback_users.guest = false\n      default_user = RABBITMQ_USER\n      default_pass = RABBITMQ_PASS\n      ## Cluster \n      cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s\n      cluster_formation.k8s.host = kubernetes.default.svc\n      #cluster_formation.k8s.host = kubernetes.default.svc.cluster.local\n      cluster_formation.k8s.address_type = hostname\n      #################################################\n      # prod is rabbitmq-cluster's namespace#\n      #################################################\n      cluster_formation.k8s.hostname_suffix = .rabbitmq-cluster.prod.svc.cluster.local\n      cluster_formation.node_cleanup.interval = 30\n      cluster_formation.node_cleanup.only_log_warning = true\n      cluster_partition_handling = autoheal\n      ## queue master locator\n      queue_master_locator = min-masters\n      cluster_formation.randomized_startup_delay_range.min = 0\n      cluster_formation.randomized_startup_delay_range.max = 2\n      # memory\n      vm_memory_high_watermark.absolute = 100MB\n      # disk\n      disk_free_limit.absolute = 2GB\n</code></pre>\n<p><em>Ê≥®ÔºöÈÖçÁΩÆÊñá‰ª∂ cluster_formation.k8s.host ËÆæÁΩÆ‰∏∫ kubernetes.default.svc.cluster.localÔºåÁÑ∂ÂêéÂ∞±ÊòØÂêÑÁßçËøû‰∏ç‰∏äÔºåÂêéÊù•Êç¢‰∏ä kubernetes.default.svc Â∞±ÂèØ‰ª•‰∫ÜÔºå‰∏çÁü•ÈÅìÊòØ‰∏çÊòØ k8s Êñ∞ÁâàÊú¨ÁöÑÈóÆÈ¢ò„ÄÇ</em></p>\n<h6 id=\"164-ÂàõÂª∫ÈõÜÁæ§ÁöÑsvc\"><a class=\"anchor\" href=\"#164-ÂàõÂª∫ÈõÜÁæ§ÁöÑsvc\">#</a> 1.6.4 ÂàõÂª∫ÈõÜÁæ§ÁöÑ svc</h6>\n<pre><code># cat 04-rabbitmq-cluster-svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: rabbitmq-cluster\n  name: rabbitmq-cluster\n  namespace: prod\nspec:\n  clusterIP: None\n  ports:\n  - name: rmqport\n    port: 5672\n    targetPort: 5672\n  - name: http\n    port: 15672\n    protocol: TCP\n    targetPort: 15672\n  selector:\n    app: rabbitmq-cluster\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: rabbitmq-cluster-balancer\n  name: rabbitmq-cluster-balancer\n  namespace: prod\nspec:\n  ports:\n  - name: rmqport\n    port: 5672\n    targetPort: 5672\n  - name: http\n    port: 15672\n    protocol: TCP\n    targetPort: 15672\n  selector:\n    app: rabbitmq-cluster\n  type: NodePort\n</code></pre>\n<h6 id=\"165-ÂàõÂª∫statefulset\"><a class=\"anchor\" href=\"#165-ÂàõÂª∫statefulset\">#</a> 1.6.5 ÂàõÂª∫ StatefulSet</h6>\n<pre><code># cat 05-rabbitmq-cluster-sts.yaml \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: rabbitmq-cluster\n  name: rabbitmq-cluster\n  namespace: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: rabbitmq-cluster\n  serviceName: rabbitmq-cluster\n  template:\n    metadata:\n      labels:\n        app: rabbitmq-cluster\n    spec:\n      affinity:                                                 # ÈÅøÂÖçPodËøêË°åÂà∞Âêå‰∏Ä‰∏™ËäÇÁÇπ‰∏ä‰∫Ü\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: app\n                    operator: In\n                    values: [&quot;rabbitmq-cluster&quot;]\n              topologyKey: &quot;kubernetes.io/hostname&quot;  \n      containers:\n      - args:\n        - -c\n        - cp -v /etc/rabbitmq/rabbitmq.conf $&#123;RABBITMQ_CONFIG_FILE&#125;; exec docker-entrypoint.sh rabbitmq-server\n        command:\n        - sh\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: rabbitmq-secret\n        - name: RABBITMQ_DEFAULT_PASS \n          valueFrom:\n            secretKeyRef:\n              key: password \n              name: rabbitmq-secret\n        - name: TZ\n          value: 'Asia/Shanghai'\n        - name: RABBITMQ_ERLANG_COOKIE\n          value: 'SWvCP0Hrqv43NG7GybHC95ntCJKoW8UyNFWnBEWG8TY='\n        - name: K8S_SERVICE_NAME\n          value: rabbitmq-cluster\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RABBITMQ_USE_LONGNAME\n          value: &quot;true&quot;\n        - name: RABBITMQ_NODENAME\n          value: rabbit@$(POD_NAME).$(K8S_SERVICE_NAME).$(POD_NAMESPACE).svc.cluster.local\n        - name: RABBITMQ_CONFIG_FILE\n          value: /var/lib/rabbitmq/rabbitmq.conf\n        image: rabbitmq:3.9-management\n        imagePullPolicy: IfNotPresent\n        name: rabbitmq\n        ports:\n        - containerPort: 15672\n          name: http\n          protocol: TCP\n        - containerPort: 5672\n          name: amqp\n          protocol: TCP\n        livenessProbe:\n          exec:\n            command: [&quot;rabbitmq-diagnostics&quot;, &quot;status&quot;]\n          initialDelaySeconds: 60\n          periodSeconds: 60\n          failureThreshold: 2\n          timeoutSeconds: 10\n        readinessProbe:\n          exec:\n            command: [&quot;rabbitmq-diagnostics&quot;, &quot;status&quot;]\n          failureThreshold: 2\n          initialDelaySeconds: 60\n          periodSeconds: 60\n          timeoutSeconds: 10\n        volumeMounts:\n        - mountPath: /etc/rabbitmq\n          name: config-volume\n          readOnly: false\n        - mountPath: /var/lib/rabbitmq\n          name: rabbitmq-storage\n          readOnly: false\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: timezone\n          mountPath: /etc/timezone\n      serviceAccountName: rabbitmq-cluster\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: config-volume\n        configMap:\n          items:\n          - key: rabbitmq.conf\n            path: rabbitmq.conf\n          - key: enabled_plugins\n            path: enabled_plugins\n          name: rabbitmq-cluster-config\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n  volumeClaimTemplates:\n  - metadata:\n      name: rabbitmq-storage\n    spec:\n      accessModes:\n      - ReadWriteMany\n      storageClassName: &quot;nfs-storage&quot;\n      resources:\n        requests:\n          storage: 5Gi\n</code></pre>\n<h6 id=\"166-ÂàõÂª∫ingress\"><a class=\"anchor\" href=\"#166-ÂàõÂª∫ingress\">#</a> 1.6.6 ÂàõÂª∫ Ingress</h6>\n<pre><code>[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rabbitmq-ingress\n  namespace: prod\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: rabbitmq.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: rabbitmq-cluster\n            port:\n              number: 15672\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h6 id=\"167-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\"><a class=\"anchor\" href=\"#167-Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï\">#</a> <strong>1.6.7 Êõ¥Êñ∞ËµÑÊ∫êÊ∏ÖÂçï</strong></h6>\n<pre><code>[root@k8s-master01 04-rabbitmq]# sed -i &quot;s#dev#prod#g&quot; *.yaml\n[root@k8s-master01 04-rabbitmq]# kubectl apply -f .\n[root@k8s-master01 04-rabbitmq]# kubectl get pods -n prod\nNAME                 READY   STATUS    RESTARTS   AGE\nrabbitmq-cluster-0   1/1     Running   0          9m53s\nrabbitmq-cluster-1   1/1     Running   0          8m47s\nrabbitmq-cluster-2   1/1     Running   0          7m40s\n\n[root@k8s-master01 04-rabbitmq]# kubectl exec -it rabbitmq-cluster-0 -n prod -- /bin/bash\nroot@rabbitmq-cluster-0:/# rabbitmqctl cluster_status\nRABBITMQ_ERLANG_COOKIE env variable support is deprecated and will be REMOVED in a future version. Use the $HOME/.erlang.cookie file or the --erlang-cookie switch instead.\nCluster status of node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local ...\nBasics\n\nCluster name: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\n\nDisk Nodes\n\nrabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local\n\nRunning Nodes\n\nrabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local\nrabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local\n\nVersions\n\nrabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9\nrabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9\nrabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9\n\nMaintenance status\n\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance\n\nAlarms\n\nMemory alarm on node rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local\nMemory alarm on node rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local\nMemory alarm on node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local\n\nNetwork Partitions\n\n(none)\n\nListeners\n\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\nNode: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\nNode: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\nNode: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\n\nFeature flags\n\nFlag: drop_unroutable_metric, state: enabled\nFlag: empty_basic_get_metric, state: enabled\nFlag: implicit_default_bindings, state: enabled\nFlag: maintenance_mode_status, state: enabled\nFlag: quorum_queue, state: enabled\nFlag: stream_queue, state: enabled\nFlag: user_limits, state: enabled\nFlag: virtual_host_metadata, state: enabled\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/XqURJbg.jpeg\" alt=\"Snipaste_2025-05-17_17-42-47.jpg\" /></p>\n<h6 id=\"168-webËÆøÈóÆrabbitmq\"><a class=\"anchor\" href=\"#168-webËÆøÈóÆrabbitmq\">#</a> <strong>1.6.8 Web ËÆøÈóÆ rabbitmq</strong></h6>\n<pre><code>http://rabbitmq.hmallleasing.com/#/\nuser:superman\npwd:talent\n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/7qKxUBf.jpeg\" alt=\"Snipaste_2025-05-17_17-14-54.jpg\" /></p>\n<h6 id=\"168-rabbitmqÂÖ®ÈÉ®ÊåÇ‰∫ÜÊó†Ê≥ïÈáçÂêØËß£ÂÜ≥ÊñπÊ°à\"><a class=\"anchor\" href=\"#168-rabbitmqÂÖ®ÈÉ®ÊåÇ‰∫ÜÊó†Ê≥ïÈáçÂêØËß£ÂÜ≥ÊñπÊ°à\">#</a> 1.6.8 rabbitMQ ÂÖ®ÈÉ®ÊåÇ‰∫ÜÔºåÊó†Ê≥ïÈáçÂêØËß£ÂÜ≥ÊñπÊ°à</h6>\n<p>Kubernetes ÁéØÂ¢É‰∏≠ÔºåÈÅáÂà∞ RabbitMQ ÈõÜÁæ§Êó†Ê≥ïÂêØÂä®ÁöÑÈóÆÈ¢ò„ÄÇÂéüÂõ†ÊòØ RabbitMQ ÊâÄÊúâÂÆû‰æãÂùáÂ§±ÊïàÔºåÈúÄË¶ÅÂú®ÊØè‰∏™ Pod ÂØπÂ∫îÁöÑÊåÅ‰πÖÂåñÂ≠òÂÇ®Ë∑ØÂæÑ‰∏ãÂàõÂª∫ force_load Êñá‰ª∂Êù•Âº∫Âà∂ÂêØÂä®„ÄÇÈÄöËøáËé∑Âèñ PV Â≠òÂÇ®Ë∑ØÂæÑÔºåÂú®ÊåáÂÆöÁõÆÂΩïÂàõÂª∫ËØ•Êñá‰ª∂ÂêéÔºåÈáçÊñ∞ÂêØÂä® RabbitMQ ÊúçÂä°ÔºåÊàêÂäüËß£ÂÜ≥‰∫ÜÈõÜÁæ§ÂêØÂä®ÈóÆÈ¢ò</p>\n<pre><code>[root@k8s-node02 ~# cd /data/dev-rabbitmq-storage-rabbitmq-cluster-0-pvc-3abca920-3c68-44eb-b0fd-406a4358b153/mnesia/rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local\n[root@k8s-node02 rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local]# touch force_load\n</code></pre>\n<h5 id=\"17-ÈÉ®ÁΩ≤rabbitmq-single\"><a class=\"anchor\" href=\"#17-ÈÉ®ÁΩ≤rabbitmq-single\">#</a> 1.7 ÈÉ®ÁΩ≤ rabbitmq-single</h5>\n<pre><code>[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-single.yaml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: rabbitmq-single-data\n  namespace: prod\nspec:\n  storageClassName: &quot;nfs-storage&quot;     # ÊòéÁ°ÆÊåáÂÆö‰ΩøÁî®Âì™‰∏™scÁöÑ‰æõÂ∫îÂïÜÊù•ÂàõÂª∫pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi                      # Ê†πÊçÆ‰∏öÂä°ÂÆûÈôÖÂ§ßÂ∞èËøõË°åËµÑÊ∫êÁî≥ËØ∑\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq-single-svc\n  namespace: prod\n  labels:\n    name: rabbitmq-single-svc\nspec:\n  ports:\n  - port: 5672 \n    protocol: TCP\n    name: web\n    targetPort: 5672\n  - name: http\n    port: 15672\n    protocol: TCP\n    targetPort: 15672\n  selector:\n    app: rabbitmq-single\n\n---\napiVersion: networking.k8s.io/v1 # k8s &gt;= 1.22 ÂøÖÈ°ª v1\nkind: Ingress\nmetadata:\n  name: rabbitmq-single-ingress\n  namespace: prod\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: rabbitmq.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: rabbitmq-single-svc\n            port:\n              number: 15672\n        path: /\n        pathType: Prefix\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rabbitmq-single\n  namespace: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rabbitmq-single\n  template:\n    metadata:\n      labels:\n        app: rabbitmq-single\n    spec:\n      containers:\n      - name: rabbitmq-single\n        image: rabbitmq:3.9-management\n        ports:\n        - containerPort: 5672\n          name: web\n          protocol: TCP\n        - containerPort: 15672\n          name: http\n          protocol: TCP\n        env:\n        - name: RABBITMQ_DEFAULT_USER  # Ëá™ÂÆö‰πâÁéØÂ¢ÉÂèòÈáè\n          value: admin\n        - name: RABBITMQ_DEFAULT_PASS\n          value: Superman*2025\n        resources:\n          requests:\n            memory: &quot;1Gi&quot;\n            cpu: &quot;500m&quot;\n        livenessProbe:\n          exec:\n            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command: [&quot;rabbitmqctl&quot;, &quot;status&quot;]\n          failureThreshold: 2\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: rabbitmq-storage\n          mountPath: /var/lib/rabbitmq\n      volumes:\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n      - name: rabbitmq-storage\n        persistentVolumeClaim:\n          claimName: rabbitmq-single-data\n</code></pre>\n<h4 id=\"‰∫å-ÈÉ®ÁΩ≤ÂæÆÊúçÂä°Â∫îÁî®\"><a class=\"anchor\" href=\"#‰∫å-ÈÉ®ÁΩ≤ÂæÆÊúçÂä°Â∫îÁî®\">#</a> ‰∫å„ÄÇÈÉ®ÁΩ≤ÂæÆÊúçÂä°Â∫îÁî®</h4>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/170573601.html",
            "url": "http://ixuyong.cn/posts/170573601.html",
            "title": "K8sÊúçÂä°ÂèëÂ∏ÉIngress",
            "date_published": "2025-04-26T08:52:06.000Z",
            "content_html": "<h4 id=\"1-ingress-nginx-controller-ÂÆâË£Ö\"><a class=\"anchor\" href=\"#1-ingress-nginx-controller-ÂÆâË£Ö\">#</a> 1. Ingress Nginx Controller ÂÆâË£Ö</h4>\n<table>\n<thead>\n<tr>\n<th>Supported</th>\n<th>Ingress-NGINX version</th>\n<th>k8s supported version</th>\n<th>Alpine Version</th>\n<th>Nginx Version</th>\n<th>Helm Chart Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.12.1</strong></td>\n<td>1.32, 1.31, 1.30, 1.29, 1.28</td>\n<td>3.21.3</td>\n<td>1.25.5</td>\n<td>4.12.1</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.12.0</strong></td>\n<td>1.32, 1.31, 1.30, 1.29, 1.28</td>\n<td>3.21.0</td>\n<td>1.25.5</td>\n<td>4.12.0</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.12.0-beta.0</strong></td>\n<td>1.32, 1.31, 1.30, 1.29, 1.28</td>\n<td>3.20.3</td>\n<td>1.25.5</td>\n<td>4.12.0-beta.0</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.11.5</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.21.3</td>\n<td>1.25.5</td>\n<td>4.11.5</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.11.4</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.21.0</td>\n<td>1.25.5</td>\n<td>4.11.4</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.11.3</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.3</td>\n<td>1.25.5</td>\n<td>4.11.3</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.11.2</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.11.2</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.11.1</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.11.1</td>\n</tr>\n<tr>\n<td>üîÑ</td>\n<td><strong>v1.11.0</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.11.0</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.6</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.21.0</td>\n<td>1.25.5</td>\n<td>4.10.6</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.5</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.3</td>\n<td>1.25.5</td>\n<td>4.10.5</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.4</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.10.4</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.3</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.10.3</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.2</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.20.0</td>\n<td>1.25.5</td>\n<td>4.10.2</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.1</strong></td>\n<td>1.30, 1.29, 1.28, 1.27, 1.26</td>\n<td>3.19.1</td>\n<td>1.25.3</td>\n<td>4.10.1</td>\n</tr>\n<tr>\n<td></td>\n<td><strong>v1.10.0</strong></td>\n<td>1.29, 1.28, 1.27, 1.26</td>\n<td>3.19.1</td>\n<td>1.25.3</td>\n<td>4.10.0</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.6</td>\n<td>1.29, 1.28, 1.27, 1.26, 1.25</td>\n<td>3.19.0</td>\n<td>1.21.6</td>\n<td>4.9.1</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.5</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.9.0</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.4</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.8.3</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.3</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.8.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.1</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.4</td>\n<td>1.21.6</td>\n<td>4.8.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.9.0</td>\n<td>1.28, 1.27, 1.26, 1.25</td>\n<td>3.18.2</td>\n<td>1.21.6</td>\n<td>4.8.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.8.4</td>\n<td>1.27, 1.26, 1.25, 1.24</td>\n<td>3.18.2</td>\n<td>1.21.6</td>\n<td>4.7.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.7.1</td>\n<td>1.27, 1.26, 1.25, 1.24</td>\n<td>3.17.2</td>\n<td>1.21.6</td>\n<td>4.6.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.6.4</td>\n<td>1.26, 1.25, 1.24, 1.23</td>\n<td>3.17.0</td>\n<td>1.21.6</td>\n<td>4.5.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.5.1</td>\n<td>1.25, 1.24, 1.23</td>\n<td>3.16.2</td>\n<td>1.21.6</td>\n<td>4.4.*</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.4.0</td>\n<td>1.25, 1.24, 1.23, 1.22</td>\n<td>3.16.2</td>\n<td>1.19.10‚Ä†</td>\n<td>4.3.0</td>\n</tr>\n<tr>\n<td></td>\n<td>v1.3.1</td>\n<td>1.24, 1.23, 1.22, 1.21, 1.20</td>\n<td>3.16.2</td>\n<td>1.19.10‚Ä†</td>\n<td>4.2.5</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"11-helmÂÆâË£Öingress-nginx-controller\"><a class=\"anchor\" href=\"#11-helmÂÆâË£Öingress-nginx-controller\">#</a> 1.1 Helm ÂÆâË£Ö Ingress Nginx Controller</h5>\n<ol>\n<li>ÂÆâË£Ö Helm</li>\n</ol>\n<pre><code># wget https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz\n# tar xf helm-v3.6.3-linux-amd64.tar.gz\n# mv linux-amd64/helm /usr/local/bin/helm\n# helm version\n</code></pre>\n<ol start=\"2\">\n<li>‰∏ãËΩΩ Ingress Nginx Controller ÂÆâË£ÖÂåÖ</li>\n</ol>\n<pre><code>ÂÆòÊñπÊñáÊ°£Ôºöhttps://github.com/kubernetes/ingress-nginx/tree/helm-chart-4.8.2         #Ê†πÊçÆËá™Â∑±k8sÁâàÊú¨‰∏ãËΩΩ\n# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n# helm repo update\n# helm repo list\n# helm pull ingress-nginx/ingress-nginx --version 4.8.2\n</code></pre>\n<ol start=\"3\">\n<li>ÈÖçÁΩÆ Ingress Nginx Controller</li>\n</ol>\n<pre><code># tar xf ingress-nginx-4.8.2.tgz\n# cd ingress-nginx\n# vim values.yaml\n...\n 16 controller:\n 17   name: controller\n 18   enableAnnotationValidations: false\n 19   image:\n 20     ## Keep false as default for now!\n 21     chroot: false\n 22     registry: registry.cn-hangzhou.aliyuncs.com\n 23     image: kubernetes_public/ingress-nginx-controller\n 24     ## for backwards compatibility consider setting the full image url via the repository value below\n 25     ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml wil    l fail\n 26     ## repository:\n 27     tag: &quot;v1.9.3&quot;\n 28     #digest: sha256:8fd21d59428507671ce0fb47f818b1d859c92d2ad07bb7c947268d433030ba98\n...\n 42   # -- Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configurat    ion/configmap/\n 43   config:\n 44     allow-snippet-annotations: true          #ÂºÄÂêØserver snippetÁöÑÈÖçÁΩÆ\n...\n 67   dnsPolicy: ClusterFirstWithHostNet\n...\n 88   hostNetwork: true\n...\n107   ingressClassResource:\n108     # -- Name of the ingressClass\n109     name: nginx\n110     # -- Is this ingressClass enabled or not\n111     enabled: true\n112     # -- Is this the default ingressClass for the cluster\n113     default: true\n...\n184   kind: DaemonSet\n...\n287   nodeSelector:\n288     kubernetes.io/os: linux\n289     ingress: &quot;true&quot;\n...\n638       image:\n639         registry: registry.cn-hangzhou.aliyuncs.com\n640         image: kubernetes_public/kube-webhook-certgen\n641         ## for backwards compatibility consider setting the full image url via the repository value below\n642         ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml     will fail\n643         ## repository:\n644         tag: v20231011-8b53cabe0\n645         #digest: sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80\n...\n</code></pre>\n<p>4. ÁªôÈúÄË¶ÅÈÉ®ÁΩ≤ ingress ÁöÑËäÇÁÇπ‰∏äÊâìÊ†áÁ≠æ</p>\n<pre><code># kubectl label node k8s-node02 ingress=true\n# kubectl label node k8s-node01 ingress=true\n# kubectl create ns ingress-nginx\n# helm install ingress-nginx -n ingress-nginx .     #ÂÆâË£Ö\n# helm upgrade ingress-nginx -n ingress-nginx .     #Êõ¥Êñ∞\n# kubectl get pods -n ingress-nginx \nNAME                             READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-7nfqn   1/1     Running   0          27s\ningress-nginx-controller-k4p2n   1/1     Running   0          17m\ningress-nginx-controller-kw5jk   1/1     Running   0          24s\n</code></pre>\n<h5 id=\"12-bare-metalÂÆâË£Öingress-nginx-controller\"><a class=\"anchor\" href=\"#12-bare-metalÂÆâË£Öingress-nginx-controller\">#</a> 1.2 Bare metal ÂÆâË£Ö Ingress Nginx Controller</h5>\n<ol>\n<li>‰∏ãËΩΩ Ingress ÈÉ®ÁΩ≤Êñá‰ª∂ÔºåÈìæÊé•Âú∞ÂùÄÔºö<a href=\"https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters\">https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters</a></li>\n</ol>\n<pre><code>[root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.1/deploy/static/provider/baremetal/deploy.yaml\n</code></pre>\n<ol start=\"2\">\n<li>ÈÖçÁΩÆ Ingress</li>\n</ol>\n<pre><code>[root@k8s-master01 ingress-master]# cat deploy.yaml \napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  name: ingress-nginx\n---\napiVersion: v1\nautomountServiceAccountToken: true\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\n  namespace: ingress-nginx\n---\napiVersion: v1\nautomountServiceAccountToken: true\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\n  namespace: ingress-nginx\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - namespaces\n  verbs:\n  - get\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - configmaps\n  - pods\n  - secrets\n  - endpoints\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses/status\n  verbs:\n  - update\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingressclasses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resourceNames:\n  - ingress-nginx-leader\n  resources:\n  - leases\n  verbs:\n  - get\n  - update\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - create\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - events\n  verbs:\n  - create\n  - patch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - list\n  - watch\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - secrets\n  verbs:\n  - get\n  - create\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - configmaps\n  - endpoints\n  - nodes\n  - pods\n  - secrets\n  - namespaces\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - nodes\n  verbs:\n  - get\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - events\n  verbs:\n  - create\n  - patch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses/status\n  verbs:\n  - update\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingressclasses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - list\n  - watch\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\nrules:\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - validatingwebhookconfigurations\n  verbs:\n  - get\n  - update\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\n  namespace: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ingress-nginx\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ingress-nginx-admission\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ingress-nginx\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ingress-nginx-admission\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx-admission\n  namespace: ingress-nginx\n---\napiVersion: v1\ndata: null\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - appProtocol: http\n    name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - appProtocol: https\n    name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  #type: NodePort\n  type: ClusterIP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller-admission\n  namespace: ingress-nginx\nspec:\n  ports:\n  - appProtocol: https\n    name: https-webhook\n    port: 443\n    targetPort: webhook\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  type: ClusterIP\n---\napiVersion: apps/v1\n#kind: Deployment\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/name: ingress-nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/version: 1.12.1\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --election-id=ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/ingress-nginx-controller-v1.12.1:v1.12.1 \n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: controller\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        - containerPort: 8443\n          name: webhook\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsGroup: 82\n          runAsNonRoot: true\n          runAsUser: 101\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /usr/local/certificates/\n          name: webhook-cert\n          readOnly: true\n      hostNetwork: true                         # ‰∏éËäÇÁÇπÂÖ±‰∫´ÁΩëÁªúÂêçÁß∞Á©∫Èó¥\n      #dnsPolicy: ClusterFirst\n      dnsPolicy: ClusterFirstWithHostNet        # dns Á≠ñÁï•\n      nodeSelector:                             # ËäÇÁÇπÈÄâÊã©Âô®\n        kubernetes.io/os: linux\n        ingress: &quot;true&quot;\n      serviceAccountName: ingress-nginx\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: ingress-nginx-admission\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission-create\n  namespace: ingress-nginx\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-webhook\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/version: 1.12.1\n      name: ingress-nginx-admission-create\n    spec:\n      containers:\n      - args:\n        - create\n        - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 \n        imagePullPolicy: IfNotPresent\n        name: create\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      nodeSelector:\n        kubernetes.io/os: linux\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission-patch\n  namespace: ingress-nginx\nspec:\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-webhook\n        app.kubernetes.io/instance: ingress-nginx\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/version: 1.12.1\n      name: ingress-nginx-admission-patch\n    spec:\n      containers:\n      - args:\n        - patch\n        - --webhook-name=ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 \n        imagePullPolicy: IfNotPresent\n        name: patch\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      nodeSelector:\n        kubernetes.io/os: linux\n      restartPolicy: OnFailure\n      serviceAccountName: ingress-nginx-admission\n---\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: nginx\nspec:\n  controller: k8s.io/ingress-nginx\n---\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  labels:\n    app.kubernetes.io/component: admission-webhook\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.1\n  name: ingress-nginx-admission\nwebhooks:\n- admissionReviewVersions:\n  - v1\n  clientConfig:\n    service:\n      name: ingress-nginx-controller-admission\n      namespace: ingress-nginx\n      path: /networking/v1/ingresses\n      port: 443\n  failurePolicy: Fail\n  matchPolicy: Equivalent\n  name: validate.nginx.ingress.kubernetes.io\n  rules:\n  - apiGroups:\n    - networking.k8s.io\n    apiVersions:\n    - v1\n    operations:\n    - CREATE\n    - UPDATE\n    resources:\n    - ingresses\n  sideEffects: None\n</code></pre>\n<ul>\n<li>type: ClusterIP                                              #service Á±ªÂûãÊîπ‰∏∫ ClusterIP</li>\n<li>hostNetwork: true                                      # ‰∏éËäÇÁÇπÂÖ±‰∫´ÁΩëÁªúÂêçÁß∞Á©∫Èó¥</li>\n<li>dnsPolicy: ClusterFirstWithHostNet        # dns Á≠ñÁï•</li>\n<li>nodeSelector:                                             # ËäÇÁÇπÈÄâÊã©Âô®</li>\n<li>kind: DaemonSet                                        # ËµÑÊ∫êÁ±ªÂûã DaemonSet</li>\n</ul>\n<ol start=\"3\">\n<li>Âú®ÊåáÂÆöËäÇÁÇπÈÉ®ÁΩ≤ Ingress-Controller</li>\n</ol>\n<pre><code>[root@k8s-master01 ingress-master]# kubectl apply -f deploy.yaml -n ingress-nginx\n\n[root@k8s-master01 ingress-master]# kubectl label node k8s-node01 ingress=true\n[root@k8s-master01 ingress-master]# kubectl label node k8s-node02 ingress=true\n[root@k8s-master01 ingress-master]# kubectl label node k8s-master03 ingress-     #ÂèñÊ∂àËäÇÁÇπÈÉ®ÁΩ≤\n\n[root@k8s-master01 ingress-master]# kubectl get pods -n ingress-nginx \nNAME                                   READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create-zp6mh   0/1     Completed   0          12m\ningress-nginx-admission-patch-f2bpd    0/1     Completed   0          12m\ningress-nginx-controller-rgtkc         1/1     Running     0          3m59s\ningress-nginx-controller-trmn8         1/1     Running     0          3m59s\n</code></pre>\n<h4 id=\"2-ingress-nginx-ÂÖ•Èó®‰ΩøÁî®\"><a class=\"anchor\" href=\"#2-ingress-nginx-ÂÖ•Èó®‰ΩøÁî®\">#</a> 2. Ingress Nginx ÂÖ•Èó®‰ΩøÁî®</h4>\n<pre><code># cat web-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: test.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h4 id=\"3-ingress-nginx-ÂüüÂêçÈáçÂÆöÂêë-redirect\"><a class=\"anchor\" href=\"#3-ingress-nginx-ÂüüÂêçÈáçÂÆöÂêë-redirect\">#</a> 3. Ingress Nginx ÂüüÂêçÈáçÂÆöÂêë Redirect</h4>\n<pre><code># cat redirect-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: redirect-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/permanent-redirect: https://www.baidu.com\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: redirect.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h4 id=\"4-ingress-nginx-ÂâçÂêéÁ´ØÂàÜÁ¶ª-rewrite\"><a class=\"anchor\" href=\"#4-ingress-nginx-ÂâçÂêéÁ´ØÂàÜÁ¶ª-rewrite\">#</a> 4. Ingress Nginx ÂâçÂêéÁ´ØÂàÜÁ¶ª Rewrite</h4>\n<pre><code># cat rewrite-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rewrite-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: rewrite.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /api(/|$)(.*)\n        pathType: ImplementationSpecif\n</code></pre>\n<h4 id=\"5-ingress-nginx-ÈîôËØØ‰ª£Á†ÅÈáçÂÆöÂêë\"><a class=\"anchor\" href=\"#5-ingress-nginx-ÈîôËØØ‰ª£Á†ÅÈáçÂÆöÂêë\">#</a> 5. Ingress Nginx ÈîôËØØ‰ª£Á†ÅÈáçÂÆöÂêë</h4>\n<pre><code>\n</code></pre>\n<h4 id=\"6-ingress-nginx-ssl\"><a class=\"anchor\" href=\"#6-ingress-nginx-ssl\">#</a> 6. Ingress Nginx SSL</h4>\n<pre><code>1.ÁîüÊàêËØÅ‰π¶\n# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.cert -subj &quot;/CN=s.hmallleasing.com/O=tls.hmallleasing.com&quot;\n\n2.ÂàõÂª∫ËØÅ‰π¶\n# kubectl create secret tls tls.hmallleasig.com --key tls.key --cert tls.cert\n\n3.ingressÈÖçÁΩÆ\n# kubectl create secret tls tls.hmallleasig.com --cert=tls.crt --key=tls.key\n# cat tls-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;    #Á¶ÅÁî®httpsÂº∫Âà∂Ë∑≥ËΩ¨\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: tls.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n  tls:                  #https\n  - hosts:\n    - tls.hmallleasing.com\n    secretName: &quot;tls.hmallleasig.com&quot;\t\n</code></pre>\n<h4 id=\"7-ingress-nginx-ÂåπÈÖçËØ∑Ê±ÇÂ§¥\"><a class=\"anchor\" href=\"#7-ingress-nginx-ÂåπÈÖçËØ∑Ê±ÇÂ§¥\">#</a> 7. Ingress Nginx ÂåπÈÖçËØ∑Ê±ÇÂ§¥</h4>\n<pre><code>1.ÈÉ®ÁΩ≤ÁßªÂä®Á´ØÂ∫îÁî®\n# kubectl create deploy phone --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:phone\n# kubectl expose deploy phone --port 80\n# vim m-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: m-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: m.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: phone\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\n2.ÈÉ®ÁΩ≤PCÁ´ØÂ∫îÁî®\t\t\n# kubectl create deploy laptop --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:laptop\t\n# kubectl expose deploy laptop --port 80\n# vim laptop-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/server-snippet: |\n      set $agentflag 0;\n          if ($http_user_agent ~* &quot;(Android|iPhone|Windows Phone|UC|Kindle)&quot; )&#123;\n              set $agentflag 1;\n          &#125;\n          if ( $agentflag = 1 ) &#123;\n              return 301 http://m.hmallleaing.com;\n          &#125;\n  name: laptop-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: laptop\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\t\n</code></pre>\n<h4 id=\"8ingress-nginx-Âü∫Êú¨ËÆ§ËØÅ\"><a class=\"anchor\" href=\"#8ingress-nginx-Âü∫Êú¨ËÆ§ËØÅ\">#</a> 8.Ingress Nginx Âü∫Êú¨ËÆ§ËØÅ</h4>\n<pre><code># yum install httpd -y\n# htpasswd -c auth superman\n# cat auth \nsuperman:$apr1$AC1pc3dK$RJyWnyDJFNKY6twneGVrA1\t\t\n\n# kubectl create secret generic basic-auth --from-file=auth\n# cat basic-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: basic-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/auth-type: basic  # ËÆ§ËØÅÁ±ªÂûã\n    nginx.ingress.kubernetes.io/auth-secret: basic-auth  # ÂåÖÂê´Áî®Êà∑ÂíåÂØÜÁ†ÅÁöÑ secret ËµÑÊ∫êÂêçÁß∞\n    nginx.ingress.kubernetes.io/auth-realm: 'Please User password'  # Ë¶ÅÊòæÁ§∫ÁöÑ‰ø°ÊÅØ\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: basic.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>\n<h4 id=\"9-ingress-nginx-ÈªëÁôΩÂêçÂçï\"><a class=\"anchor\" href=\"#9-ingress-nginx-ÈªëÁôΩÂêçÂçï\">#</a> 9. Ingress Nginx Èªë / ÁôΩÂêçÂçï</h4>\n<pre><code>ÂÜôÊ≥ï‰∏ÄÔºö\n# cat white-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: white-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/whitelist-source-range: &quot;192.168.40.101&quot;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: white.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\t\n\nÂÜôÊ≥ï‰∫åÔºö\t\t\n[root@k8s-master01 ingress]# cat white-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: white-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/whitelist-source-range: &quot;192.168.40.0/24&quot;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: white.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\n\nÂÜôÊ≥ï‰∏âÔºö\n# cat white-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: white-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/server-snippet: |\n      allow 192.168.40.0/24;\n      deny all;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: white.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\t\t\n\n#Master01ÊµãËØï\t\t\n# curl -H &quot;Host:white.hmallleasing.com&quot; http://192.168.40.103 -I\nHTTP/1.1 200 OK\nDate: Sat, 14 Oct 2023 13:12:03 GMT\nContent-Type: text/html\nContent-Length: 612\nConnection: keep-alive\nLast-Modified: Tue, 16 Apr 2019 13:08:19 GMT\nETag: &quot;5cb5d3c3-264&quot;\nAccept-Ranges: bytes\t\t\n\n#Master02ÊµãËØï\n# curl -H &quot;Host:white.hmallleasing.com&quot; http://192.168.40.103 -I\nHTTP/1.1 403 Forbidden\nDate: Sat, 14 Oct 2023 13:13:34 GMT\nContent-Type: text/html\nContent-Length: 146\nConnection: keep-alive\n</code></pre>\n<h4 id=\"10-ingress-nginx-ÈÄüÁéáÈôêÂà∂\"><a class=\"anchor\" href=\"#10-ingress-nginx-ÈÄüÁéáÈôêÂà∂\">#</a> 10. Ingress Nginx ÈÄüÁéáÈôêÂà∂</h4>\n<pre><code>[root@k8s-master01 ingress]# cat limit-rate-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rate-limit-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/limit-rps: &quot;50&quot;\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: rate-limit.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        path: /\n        pathType: ImplementationSpecific\n\n# ab -c 20 -n 1000 http://rate-limit.hmallleasing.com/ |grep request\nComplete requests:      1000\nFailed requests:        724\nTime per request:       10.301 [ms] (mean)\nTime per request:       0.515 [ms] (mean, across all concurrent requests)\nPercentage of the requests served within a certain time (ms)\n</code></pre>\n<h4 id=\"11‰ΩøÁî®-nginx-ÂÆûÁé∞ÁÅ∞Â∫¶Èáë‰∏ùÈõÄÂèëÂ∏É\"><a class=\"anchor\" href=\"#11‰ΩøÁî®-nginx-ÂÆûÁé∞ÁÅ∞Â∫¶Èáë‰∏ùÈõÄÂèëÂ∏É\">#</a> 11. ‰ΩøÁî® Nginx ÂÆûÁé∞ÁÅ∞Â∫¶ / Èáë‰∏ùÈõÄÂèëÂ∏É</h4>\n<pre><code>1.ÂàõÂª∫ v1 ÁâàÊú¨\n# kubectl create deploy canary-v1 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v1\t\n# kubectl expose deploy canary-v1 --port 8080\n# cat canary-v1-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: canary-v1-ingress\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: canary.hmallleasing.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: canary-v1\n            port:\n              number: 8080\n        path: /\n        pathType: ImplementationSpecific\n\t\t\n# curl -H &quot;Host:canary.hmallleasing.com&quot; http://192.168.40.103 \t\n\n2.ÂàõÂª∫ v2 ÁâàÊú¨\n# kubectl create deploy canary-v2 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v2\n# kubectl expose deploy canary-v2 --port 8080\n# cat canary-v2-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: canary-v2-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: &quot;true&quot;    #ÂêØÂä®ÁÅ∞Â∫¶ÂèëÂ∏É\n    nginx.ingress.kubernetes.io/canary-weight: &quot;20&quot;  #Âü∫‰∫éÊùÉÈáç,50%ÊµÅÈáèË∞ÉÂ∫¶Âà∞Ëøô‰∏™ÁÅ∞Â∫¶ÁöÑÁâàÊú¨‰∏ä\nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: canary.hmallleasing.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: canary-v2\n            port:\n              number: 8080\n\n#ÊµãËØïÁÅ∞Â∫¶ÂèëÂ∏É\n[root@k8s-master01 ingress]# cat canary.sh \n#!/bin/bash\n\nwhile true\ndo\n\tcurl -H &quot;Host:canary.hmallleasing.com&quot; http://192.168.40.103\n\tsleep 0.5\ndone\n</code></pre>\n<h4 id=\"12-kubernetes-dashboardÈÖçÁΩÆËØÅ‰π¶\"><a class=\"anchor\" href=\"#12-kubernetes-dashboardÈÖçÁΩÆËØÅ‰π¶\">#</a> 12. kubernetes-dashboard ÈÖçÁΩÆËØÅ‰π¶</h4>\n<pre><code>1.ÂàõÂª∫ËØÅ‰π¶\nkubectl create secret tls kubernetes-dashboard-certs --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n kubernetes-dashboard\n\n2.‰øÆÊîπkubernetes-dashboardËµÑÊ∫êÊ∏ÖÂçï\nkubectl edit deployment -n kubernetes-dashboard kubernetes-dashboard\n...\n      - args:\n        - --auto-generate-certificates=false\n        - --tls-key-file=_.hmallleasing.com_key.key\n        - --tls-cert-file=_.hmallleasing.com_chain.crt\n        - --token-ttl=21600\n        - --authentication-mode=basic,token\n        - --namespace=kubernetes-dashboard\n...\n\n3.ÂàõÂª∫ingress\n#cat dashboard-ingress.yaml \napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;    \n    nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;    \nspec:\n  ingressClassName: &quot;nginx&quot;\n  rules:\n  - host: dashboard.hmallleasing.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n\n# kubectl apply -f dashboard-ingress.yaml \n</code></pre>\n<h4 id=\"13-ÂÖ•Âè£lbÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#13-ÂÖ•Âè£lbÈÖçÁΩÆ\">#</a> 13. ÂÖ•Âè£ LB ÈÖçÁΩÆ</h4>\n<pre><code>[root@lb nginx]# cat /etc/nginx/conf.d/ingress.conf \nupstream ingress &#123;\n\tserver 192.168.40.103:80 max_conns=2000 max_fails=2 fail_timeout=5s;\n\tserver 192.168.40.104:80 max_conns=2000 max_fails=2 fail_timeout=5s;\n\tserver 192.168.40.105:80 max_conns=2000 max_fails=2 fail_timeout=5s;\n&#125;\n\nserver &#123;\n    listen 443 ssl;\n    server_name test.hmallleasing.com;\n    client_max_body_size 1G; \n    ssl_prefer_server_ciphers on;\n    ssl_certificate  /etc/nginx/sslkey/*.hmallleasing.com_chain.crt;\n    ssl_certificate_key  /etc/nginx/sslkey/*.hmallleasing.com_key.key;\n\n    location / &#123;\n        proxy_pass http://ingress;\n        include proxy_params;\n\t    proxy_next_upstream error timeout http_500 http_502 http_503 http_504;\n\t    proxy_next_upstream_tries 2;\n\t    proxy_next_upstream_timeout 3s;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name test.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n\n[root@lb ~]# mkdir /etc/nginx/sslkey -p\n\n\n[root@lb ~]# cat proxy_params \nproxy_http_version 1.1;\nproxy_set_header Connectin &quot;&quot;;\n\nproxy_set_header Host $http_host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\nproxy_connect_timeout 60;\nproxy_send_timeout 120;\nproxy_read_timeout 120;\n\nproxy_buffering on;\nproxy_buffer_size 32k;\nproxy_buffers 4 128k;\nproxy_temp_file_write_size 10240k;\nproxy_max_temp_file_size 10240k;\n</code></pre>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3030097036.html",
            "url": "http://ixuyong.cn/posts/3030097036.html",
            "title": "K8S‰∫ëÂéüÁîüÂ≠òÂÇ®Rook-Ceph",
            "date_published": "2025-04-24T13:43:19.000Z",
            "content_html": "<h3 id=\"k8s‰∫ëÂéüÁîüÂ≠òÂÇ®rook-ceph\"><a class=\"anchor\" href=\"#k8s‰∫ëÂéüÁîüÂ≠òÂÇ®rook-ceph\">#</a> K8S ‰∫ëÂéüÁîüÂ≠òÂÇ® Rook-Ceph</h3>\n<h4 id=\"1-storageclassÂä®ÊÄÅÂ≠òÂÇ®\"><a class=\"anchor\" href=\"#1-storageclassÂä®ÊÄÅÂ≠òÂÇ®\">#</a> 1. StorageClass Âä®ÊÄÅÂ≠òÂÇ®</h4>\n<p>StorageClassÔºöÂ≠òÂÇ®Á±ªÔºåÁî± K8s ÁÆ°ÁêÜÂëòÂàõÂª∫ÔºåÁî®‰∫éÂä®ÊÄÅ PV ÁöÑÁÆ°ÁêÜÔºåÂèØ‰ª•ÈìæÊé•Ëá≥‰∏çÂêåÁöÑÂêéÁ´ØÂ≠òÂÇ®ÔºåÊØîÂ¶Ç Ceph„ÄÅGlusterFS Á≠â„ÄÇ‰πãÂêéÂØπÂ≠òÂÇ®ÁöÑËØ∑Ê±ÇÂèØ‰ª•ÊåáÂêë StorageClassÔºåÁÑ∂Âêé StorageClass ‰ºöËá™Âä®ÁöÑÂàõÂª∫„ÄÅÂà†Èô§ PV„ÄÇ</p>\n<p>ÂÆûÁé∞ÊñπÂºèÔºö</p>\n<ul>\n<li>in-tree: ÂÜÖÁΩÆ‰∫é K8s Ê†∏ÂøÉ‰ª£Á†ÅÔºåÂØπ‰∫éÂ≠òÂÇ®ÁöÑÁÆ°ÁêÜÔºåÈÉΩÈúÄË¶ÅÁºñÂÜôÁõ∏Â∫îÁöÑ‰ª£Á†Å„ÄÇ</li>\n<li>out-of-treeÔºöÁî±Â≠òÂÇ®ÂéÇÂïÜÊèê‰æõ‰∏Ä‰∏™È©±Âä®ÔºàCSI Êàñ Flex VolumeÔºâÔºåÂÆâË£ÖÂà∞ K8s ÈõÜÁæ§ÔºåÁÑ∂Âêé StorageClass Âè™ÈúÄË¶ÅÈÖçÁΩÆËØ•È©±Âä®Âç≥ÂèØÔºåÈ©±Âä®Âô®‰ºö‰ª£Êõø StorageClass ÁÆ°ÁêÜÂ≠òÂÇ®„ÄÇ</li>\n</ul>\n<p>StorageClass ÂÆòÁΩë‰ªãÁªçÔºö<a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes/\">https://kubernetes.io/docs/concepts/storage/storage-classes/</a></p>\n<h4 id=\"2-‰∫ëÂéüÁîüÂ≠òÂÇ®rook\"><a class=\"anchor\" href=\"#2-‰∫ëÂéüÁîüÂ≠òÂÇ®rook\">#</a> 2. ‰∫ëÂéüÁîüÂ≠òÂÇ® Rook</h4>\n<p>Rook ÊòØ‰∏Ä‰∏™Ëá™ÊàëÁÆ°ÁêÜÁöÑÂàÜÂ∏ÉÂºèÂ≠òÂÇ®ÁºñÊéíÁ≥ªÁªüÔºåÂÆÉÊú¨Ë∫´Âπ∂‰∏çÊòØÂ≠òÂÇ®Á≥ªÁªüÔºåÂú®Â≠òÂÇ®Âíå k8s ‰πãÂâçÊê≠Âª∫‰∫Ü‰∏Ä‰∏™Ê°•Ê¢ÅÔºå‰ΩøÂ≠òÂÇ®Á≥ªÁªüÁöÑÊê≠Âª∫ÊàñËÄÖÁª¥Êä§ÂèòÂæóÁâπÂà´ÁÆÄÂçïÔºåRook Â∞ÜÂàÜÂ∏ÉÂºèÂ≠òÂÇ®Á≥ªÁªüËΩ¨Âèò‰∏∫Ëá™ÊàëÁÆ°ÁêÜ„ÄÅËá™ÊàëÊâ©Â±ï„ÄÅËá™Êàë‰øÆÂ§çÁöÑÂ≠òÂÇ®ÊúçÂä°„ÄÇÂÆÉËÆ©‰∏Ä‰∫õÂ≠òÂÇ®ÁöÑÊìç‰ΩúÔºåÊØîÂ¶ÇÈÉ®ÁΩ≤„ÄÅÈÖçÁΩÆ„ÄÅÊâ©ÂÆπ„ÄÅÂçáÁ∫ß„ÄÅËøÅÁßª„ÄÅÁÅæÈöæÊÅ¢Â§ç„ÄÅÁõëËßÜÂíåËµÑÊ∫êÁÆ°ÁêÜÂèòÂæóËá™Âä®ÂåñÔºåÊó†ÈúÄ‰∫∫Â∑•Â§ÑÁêÜ„ÄÇÂπ∂‰∏î Rook ÊîØÊåÅ CSIÔºåÂèØ‰ª•Âà©Áî® CSI ÂÅö‰∏Ä‰∫õ PVC ÁöÑÂø´ÁÖß„ÄÅÊâ©ÂÆπ„ÄÅÂÖãÈöÜÁ≠âÊìç‰Ωú„ÄÇ</p>\n<p>Rook ÂÆòÁΩë‰ªãÁªçÔºö<a href=\"https://rook.io/\">https://rook.io/</a></p>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/CK4Gn1u.jpeg\" alt=\"Snipaste_2025-05-07_20-15-59.jpg\" /></p>\n<h4 id=\"3-rook-ÂÆâË£Ö\"><a class=\"anchor\" href=\"#3-rook-ÂÆâË£Ö\">#</a> 3. Rook ÂÆâË£Ö</h4>\n<p>ÁéØÂ¢ÉÂáÜÂ§á</p>\n<ul>\n<li>K8s ÈõÜÁæ§Ëá≥Â∞ë‰∫î‰∏™ËäÇÁÇπÔºåÊØè‰∏™ËäÇÁÇπÁöÑÂÜÖÂ≠ò‰∏ç‰Ωé‰∫é 5GÔºåCPU ‰∏ç‰Ωé‰∫é 2 Ê†∏</li>\n<li>ÊâÄÊúâËäÇÁÇπÊó∂Èó¥ÂêåÊ≠•</li>\n<li>Ëá≥Â∞ëÊúâ‰∏â‰∏™Â≠òÂÇ®ËäÇÁÇπÔºåÂπ∂‰∏îÊØè‰∏™ËäÇÁÇπËá≥Â∞ëÊúâ‰∏Ä‰∏™Ë£∏ÁõòÔºåk8s-master03„ÄÅk8s-node01„ÄÅk8s-node02 Â¢ûÂä†Ë£∏Áõò</li>\n</ul>\n<h5 id=\"31-‰∏ãËΩΩ-rook-ÂÆâË£ÖÊñá‰ª∂\"><a class=\"anchor\" href=\"#31-‰∏ãËΩΩ-rook-ÂÆâË£ÖÊñá‰ª∂\">#</a> 3.1 ‰∏ãËΩΩ Rook ÂÆâË£ÖÊñá‰ª∂</h5>\n<pre><code>[root@k8s-master01 ~]# git clone --single-branch --branch v1.17.2 https://github.com/rook/rook.git\n</code></pre>\n<h5 id=\"32-ÈÖçÁΩÆÊõ¥Êîπ\"><a class=\"anchor\" href=\"#32-ÈÖçÁΩÆÊõ¥Êîπ\">#</a> 3.2 ÈÖçÁΩÆÊõ¥Êîπ</h5>\n<pre><code>[root@k8s-master01 ~]# cd rook/deploy/examples\n[root@k8s-master01 ~]# vim operator.yaml\n  ROOK_CSI_CEPH_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephcsi:v3.14.0&quot;\n  ROOK_CSI_REGISTRAR_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-node-driver-registrar:v2.13.0&quot;\n  ROOK_CSI_RESIZER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-resizer:v1.13.1&quot;\n  ROOK_CSI_PROVISIONER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-provisioner:v5.1.0&quot;\n  ROOK_CSI_SNAPSHOTTER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-snapshotter:v8.2.0&quot;\n  ROOK_CSI_ATTACHER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-attacher:v4.8.0&quot;\n\n#ROOK_ENABLE_DISCOVERY_DAEMON ÊîπÊàê true Âç≥ÂèØ\nROOK_ENABLE_DISCOVERY_DAEMON: &quot;true&quot;\n</code></pre>\n<h5 id=\"33-ÈÉ®ÁΩ≤-rook\"><a class=\"anchor\" href=\"#33-ÈÉ®ÁΩ≤-rook\">#</a> 3.3 ÈÉ®ÁΩ≤ rook</h5>\n<pre><code>[root@k8s-master01 ceph]# kubectl create -f crds.yaml -f common.yaml -f operator.yaml\n[root@k8s-master01 examples]# kubectl get pods -n rook-ceph\nNAME                                  READY   STATUS    RESTARTS   AGE\nrook-ceph-operator-84ff77778b-7ww2w   1/1     Running   0          91m\nrook-discover-6j68f                   1/1     Running   0          82m\nrook-discover-9w4kt                   1/1     Running   0          82m\nrook-discover-h2zfm                   1/1     Running   0          82m\nrook-discover-hsz8b                   1/1     Running   0          19m\nrook-discover-rj4t7                   1/1     Running   0          82m\n</code></pre>\n<h4 id=\"4ÂàõÂª∫-ceph-ÈõÜÁæ§\"><a class=\"anchor\" href=\"#4ÂàõÂª∫-ceph-ÈõÜÁæ§\">#</a> 4. ÂàõÂª∫ Ceph ÈõÜÁæ§</h4>\n<h5 id=\"41-ÈÖçÁΩÆÊõ¥Êîπ\"><a class=\"anchor\" href=\"#41-ÈÖçÁΩÆÊõ¥Êîπ\">#</a> 4.1 ÈÖçÁΩÆÊõ¥Êîπ</h5>\n<pre><code>[root@k8s-master01 examples]# vim cluster.yaml\n...\n    image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephv19.2.2:v19.2.2\n...\n  skipUpgradeChecks: true     #Êîπ‰∏∫trueÔºåË∑≥ËøáÂçáÁ∫ß\n....\n  dashboard:\n    enabled: true\n    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)\n    # urlPrefix: /ceph-dashboard\n    # serve the dashboard at the given port.\n    # port: 8443\n    # serve the dashboard using SSL\n    ssl: false          #Êîπ‰∏∫false\n...\n  storage: # cluster level storage configuration and selection\n    useAllNodes: false      #Êîπ‰∏∫false,‰∏ç‰ΩøÁî®ÊâÄÊúâÁöÑËäÇÁÇπÂΩìosd\n    useAllDevices: false    #Êîπ‰∏∫false,‰∏ç‰ΩøÁî®ÊâÄÊúâÁöÑÁ£ÅÁõòÂΩìosd\n...\n    #     deviceFilter: &quot;^sd.&quot;\n    nodes:\n    - name: &quot;k8s-master03&quot;\n      devices:\n      - name: &quot;sdb&quot;\n    - name: &quot;k8s-node01&quot;\n      devices:\n      - name: &quot;sdb&quot;\n    - name: &quot;k8s-node02&quot;\n      devices:\n      - name: &quot;sdb&quot;\n...\n</code></pre>\n<p>Ê≥®ÊÑèÔºöÊñ∞ÁâàÂøÖÈ°ªÈááÁî®Ë£∏ÁõòÔºåÂç≥Êú™Ê†ºÂºèÂåñÁöÑÁ£ÅÁõò„ÄÇÂÖ∂‰∏≠ k8s-master03„ÄÅ k8s-node01„ÄÅ  k8s-node02 ÊúâÊñ∞Âä†ÁöÑ‰∏Ä‰∏™Á£ÅÁõòÔºåÂèØ‰ª•ÈÄöËøá lsblk -f Êü•ÁúãÊñ∞Ê∑ªÂä†ÁöÑÁ£ÅÁõòÂêçÁß∞„ÄÇÂª∫ËÆÆÊúÄÂ∞ë‰∏â‰∏™ËäÇÁÇπÔºåÂê¶ÂàôÂêéÈù¢ÁöÑËØïÈ™åÂèØËÉΩ‰ºöÂá∫Áé∞ÈóÆÈ¢ò</p>\n<h5 id=\"42-ÂàõÂª∫-ceph-ÈõÜÁæ§\"><a class=\"anchor\" href=\"#42-ÂàõÂª∫-ceph-ÈõÜÁæ§\">#</a> 4.2 ÂàõÂª∫ Ceph ÈõÜÁæ§</h5>\n<pre><code>[root@k8s-master01 examples]# kubectl create -f cluster.yaml\n[root@k8s-master01 examples]# kubectl get pods -n rook-ceph\nNAME                                                     READY   STATUS      RESTARTS        AGE\ncsi-cephfsplugin-5nmnl                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-6b6ct                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-8xlnl                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-fh9w5                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-mslst                                   3/3     Running     1 (60m ago)     62m\ncsi-cephfsplugin-provisioner-59bd447c6d-5zwj2            6/6     Running     0               61s\ncsi-cephfsplugin-provisioner-59bd447c6d-7t2kg            6/6     Running     2 (20s ago)     61s\ncsi-rbdplugin-5gvmp                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-dzcs4                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-n82b5                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-provisioner-6856fb8b86-86hw8               6/6     Running     0               19s\ncsi-rbdplugin-provisioner-6856fb8b86-lj9s4               6/6     Running     0               19s\ncsi-rbdplugin-vh8j2                                      3/3     Running     1 (60m ago)     62m\ncsi-rbdplugin-xfgwr                                      3/3     Running     1 (60m ago)     62m\nrook-ceph-crashcollector-k8s-master01-bbc78d496-bzjk8    1/1     Running     0               8m26s\nrook-ceph-crashcollector-k8s-master03-765ff964bb-95wmt   1/1     Running     0               28m\nrook-ceph-crashcollector-k8s-node01-7cf4c4b6b6-r4n84     1/1     Running     0               20m\nrook-ceph-crashcollector-k8s-node02-f887f8cf9-jz2l8      1/1     Running     0               28m\nrook-ceph-detect-version-nsrwj                           0/1     Init:0/1    0               3s\nrook-ceph-exporter-k8s-master01-5cd4577b79-ckd4m         1/1     Running     0               8m26s\nrook-ceph-exporter-k8s-master03-75f4cf6f7-hc9zb          1/1     Running     0               28m\nrook-ceph-exporter-k8s-node01-96fc7cf49-d2r24            1/1     Running     0               20m\nrook-ceph-exporter-k8s-node02-777b9f555b-7j6cz           1/1     Running     0               27m\nrook-ceph-mgr-a-6f46b4b945-q6cjb                         3/3     Running     3 (14m ago)     35m\nrook-ceph-mgr-b-5d4cc5465b-8dfh6                         3/3     Running     0               35m\nrook-ceph-mon-a-7c7b7555c7-nlhwg                         2/2     Running     2 (6m14s ago)   51m\nrook-ceph-mon-c-559bcf95fd-cl62w                         2/2     Running     0               8m27s\nrook-ceph-mon-d-7dbc6b8f5c-8264t                         2/2     Running     0               28m\nrook-ceph-operator-645478ff5b-jdcrp                      1/1     Running     0               102m\nrook-ceph-osd-0-6d9cf78f76-4zhx8                         2/2     Running     0               12m\nrook-ceph-osd-1-88c78bbcb-cn48c                          2/2     Running     0               5m15s\nrook-ceph-osd-2-b464c9fc6-458hv                          2/2     Running     0               4m29s\nrook-ceph-osd-prepare-k8s-master03-pwnrc                 0/1     Completed   0               86s\nrook-ceph-osd-prepare-k8s-node01-xxp2j                   0/1     Completed   0               83s\nrook-ceph-osd-prepare-k8s-node02-8nz7x                   0/1     Completed   0               78s\nrook-discover-jzmkr                                      1/1     Running     0               91m\nrook-discover-k7pxt                                      1/1     Running     0               91m\nrook-discover-vqjh5                                      1/1     Running     0               91m\nrook-discover-wk8jq                                      1/1     Running     0               91m\nrook-discover-x8rsn                                      1/1     Running     0               91m\n\n[root@k8s-master01 examples]# kubectl get cephcluster -n rook-ceph\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH        EXTERNAL   FSID\nrook-ceph   /var/lib/rook     3          63m   Ready   Cluster created successfully   HEALTH_WARN              ca429602-66f4-4a1e-9d5c-a5773a0f594f\n</code></pre>\n<h5 id=\"43-ÂÆâË£Ö-ceph-snapshot-ÊéßÂà∂Âô®\"><a class=\"anchor\" href=\"#43-ÂÆâË£Ö-ceph-snapshot-ÊéßÂà∂Âô®\">#</a> 4.3 ÂÆâË£Ö ceph snapshot ÊéßÂà∂Âô®</h5>\n<pre><code>[root@k8s-master01 ~]# cd /root/k8s-ha-install/\n[root@k8s-master01 k8s-ha-install]# git checkout manual-installation-v1.32.x\n[root@k8s-master01 k8s-ha-install]# kubectl create -f snapshotter/ -n kube-system\n[root@k8s-master01 k8s-ha-install]# kubectl get po -n kube-system -l app=snapshot-controller\nNAME                    READY   STATUS    RESTARTS   AGE\nsnapshot-controller-0   1/1     Running   0          67s\n</code></pre>\n<h4 id=\"5-ÂÆâË£Ö-ceph-ÂÆ¢Êà∑Á´ØÂ∑•ÂÖ∑\"><a class=\"anchor\" href=\"#5-ÂÆâË£Ö-ceph-ÂÆ¢Êà∑Á´ØÂ∑•ÂÖ∑\">#</a> 5. ÂÆâË£Ö ceph ÂÆ¢Êà∑Á´ØÂ∑•ÂÖ∑</h4>\n<pre><code>[root@k8s-master01 k8s-ha-install]# cd /root/rook/deploy/examples/\n[root@k8s-master01 examples]# kubectl create -f toolbox.yaml -n rook-ceph\n[root@k8s-master01 examples]# kubectl get po -n rook-ceph -l app=rook-ceph-tools\nNAME                               READY   STATUS    RESTARTS   AGE\nrook-ceph-tools-7b75b967db-sqddk   1/1     Running   0          8s\n[root@k8s-master01 examples]# kubectl exec -it rook-ceph-tools-7b75b967db-sqddk -n rook-ceph -- bash\nbash-5.1$ ceph status\n  cluster:\n    id:     87b85368-9487-4967-a4e4-5970d2e0ec94\n    health: HEALTH_WARN\n            1 mgr modules have recently crashed\n \n  services:\n    mon: 3 daemons, quorum b,c (age 12s), out of quorum: a\n    mgr: a(active, since 7m), standbys: b\n    osd: 3 osds: 3 up (since 8m), 3 in (since 3h)\n \n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   82 MiB used, 60 GiB / 60 GiB avail\n    pgs: \n\t\nbash-4.4$  ceph osd status\nID  HOST           USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      \n 0  k8s-master03  20.6M  19.9G      0        0       0        0   exists,up  \n 1  k8s-node01    20.6M  19.9G      0        0       0        0   exists,up  \n 2  k8s-node02    20.6M  19.9G      0        0       0        0   exists,up \n\nbash-4.4$ ceph df\n--- RAW STORAGE ---\nCLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED\nhdd    60 GiB  60 GiB  62 MiB    62 MiB       0.10\nTOTAL  60 GiB  60 GiB  62 MiB    62 MiB       0.10\n\n--- POOLS ---\nPOOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL\n.mgr   1    1  449 KiB        2  1.3 MiB      0     19 GiB\n</code></pre>\n<h4 id=\"6-ceph-dashboard\"><a class=\"anchor\" href=\"#6-ceph-dashboard\">#</a> 6. Ceph dashboard</h4>\n<h5 id=\"61-Êö¥Èú≤ÊúçÂä°\"><a class=\"anchor\" href=\"#61-Êö¥Èú≤ÊúçÂä°\">#</a> 6.1 Êö¥Èú≤ÊúçÂä°</h5>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc -n rook-ceph\nNAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nrook-ceph-mgr             ClusterIP   10.96.54.15     &lt;none&gt;        9283/TCP            133m\nrook-ceph-mgr-dashboard   ClusterIP   10.96.97.117    &lt;none&gt;        7000/TCP            133m        #Êö¥Èú≤ingresss‰πüÂèØ\nrook-ceph-mon-a           ClusterIP   10.96.125.216   &lt;none&gt;        6789/TCP,3300/TCP   170m\nrook-ceph-mon-b           ClusterIP   10.96.34.183    &lt;none&gt;        6789/TCP,3300/TCP   133m\nrook-ceph-mon-c           ClusterIP   10.96.232.252   &lt;none&gt;        6789/TCP,3300/TCP   133m\n\n\n[root@k8s-master01 examples]# kubectl create -f dashboard-external-http.yaml           #Êö¥Èú≤nodeport\n[root@k8s-master01 examples]# kubectl get svc -n rook-ceph rook-ceph-mgr-dashboard-external-http\nNAME                                     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\nrook-ceph-mgr-dashboard-external-https   NodePort   10.96.11.120   &lt;none&gt;        8443:32611/TCP   45s\n</code></pre>\n<h5 id=\"62-ÈÖçÁΩÆingressËÆøÈóÆceph\"><a class=\"anchor\" href=\"#62-ÈÖçÁΩÆingressËÆøÈóÆceph\">#</a> 6.2 ÈÖçÁΩÆ ingress ËÆøÈóÆ ceph</h5>\n<pre><code>[root@k8s-master01 examples]# cat dashboard-ingress-https.yaml \n#\n# This example is for Kubernetes running an nginx-ingress\n# and an ACME (e.g. Let's Encrypt) certificate service\n#\n# The nginx-ingress annotations support the dashboard\n# running using HTTPS with a self-signed certificate\n#\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rook-ceph-mgr-dashboard\n  namespace: rook-ceph # namespace:cluster\n#  annotations:\n#    kubernetes.io/ingress.class: &quot;nginx&quot;\n#    kubernetes.io/tls-acme: &quot;true&quot;\n#    nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;\n#    nginx.ingress.kubernetes.io/server-snippet: |\n#      proxy_ssl_verify off;\n\nspec:\n  ingressClassName: &quot;nginx&quot;\n#  tls:\n#    - hosts:\n#        - rook-ceph.hmallleasing.com\n#      secretName: rook-ceph.example.com\n  rules:\n    - host: rook-ceph.hmallleasing.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: rook-ceph-mgr-dashboard\n                port:\n                  name: http-dashboard\n</code></pre>\n<h5 id=\"63-ÁôªÂΩï\"><a class=\"anchor\" href=\"#63-ÁôªÂΩï\">#</a> 6.3 ÁôªÂΩï</h5>\n<pre><code>http://192.168.40.100:32611\nÁî®Êà∑ÂêçÔºöadmin\nÂØÜÁ†ÅÔºökubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;&#123;['data']['password']&#125;&quot; | base64 --decode &amp;&amp; echo\n</code></pre>\n<h4 id=\"7-ceph-ÂùóÂ≠òÂÇ®ÁöÑ‰ΩøÁî®\"><a class=\"anchor\" href=\"#7-ceph-ÂùóÂ≠òÂÇ®ÁöÑ‰ΩøÁî®\">#</a> 7. Ceph ÂùóÂ≠òÂÇ®ÁöÑ‰ΩøÁî®</h4>\n<p>ÂùóÂ≠òÂÇ®‰∏ÄËà¨Áî®‰∫é‰∏Ä‰∏™ Pod ÊåÇËΩΩ‰∏ÄÂùóÂ≠òÂÇ®‰ΩøÁî®ÔºåÁõ∏ÂΩì‰∫é‰∏Ä‰∏™ÊúçÂä°Âô®Êñ∞ÊåÇ‰∫Ü‰∏Ä‰∏™ÁõòÔºåÂè™Áªô‰∏Ä‰∏™Â∫îÁî®‰ΩøÁî®„ÄÇ</p>\n<h5 id=\"71-ÂàõÂª∫-storageclass-Âíå-ceph-ÁöÑÂ≠òÂÇ®Ê±†\"><a class=\"anchor\" href=\"#71-ÂàõÂª∫-storageclass-Âíå-ceph-ÁöÑÂ≠òÂÇ®Ê±†\">#</a> 7.1 ÂàõÂª∫ StorageClass Âíå ceph ÁöÑÂ≠òÂÇ®Ê±†</h5>\n<pre><code>[root@k8s-master01 examples]# kubectl get csidriver\nNAME                            ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE\nrook-ceph.cephfs.csi.ceph.com   true             false            false             &lt;unset&gt;         false               Persistent   15h       #Êñá‰ª∂Â≠òÂÇ®csi\nrook-ceph.rbd.csi.ceph.com      true             false            false             &lt;unset&gt;         false               Persistent   15h       #ÂùóÂ≠òÂÇ®csi\n\n[root@k8s-master01 ~]# cd /root/rook/deploy/examples/\n[root@k8s-master01 examples]# vim csi/rbd/storageclass.yaml\n...\napiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph # namespace:cluster\nspec:\n  failureDomain: host\n  replicated:\n    size: 3                #Êï∞ÊçÆ‰øùÂ≠òÂá†‰ªΩÔºåÊµãËØïÁéØÂ¢ÉÂèØ‰ª•Â∞ÜÂâØÊú¨Êï∞ËÆæÁΩÆÊàê‰∫Ü 2Ôºà‰∏çËÉΩËÆæÁΩÆ‰∏∫ 1ÔºâÔºåÁîü‰∫ßÁéØÂ¢ÉÊúÄÂ∞ë‰∏∫ 3Ôºå‰∏îË¶ÅÂ∞è‰∫éÁ≠â‰∫é osd ÁöÑÊï∞Èáè\n...\nallowVolumeExpansion: true     #ÊòØÂê¶ÂèØ‰ª•Êâ©ÂÆπ\nreclaimPolicy: Delete          #pvÂõûÊî∂Á≠ñÁï•\n\n[root@k8s-master01 examples]# kubectl create -f csi/rbd/storageclass.yaml -n rook-ceph\n\n[root@k8s-master01 examples]# kubectl get cephblockpool -n rook-ceph\nNAME          PHASE\nreplicapool   Ready\n[root@k8s-master01 examples]# kubectl get sc\nNAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nnfs-storage       nfzl.com/nfs                 Delete          Immediate           false                  16h\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   37s\n</code></pre>\n<h5 id=\"72-ÊåÇËΩΩÊµãËØï\"><a class=\"anchor\" href=\"#72-ÊåÇËΩΩÊµãËØï\">#</a> 7.2 ÊåÇËΩΩÊµãËØï</h5>\n<pre><code>[root@k8s-master01 ~]# cat ceph-block-pvc.yaml        #ÂàõÂª∫PVC\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ceph-block-pvc\nspec:\n  storageClassName: &quot;rook-ceph-block&quot;     # ÊòéÁ°ÆÊåáÂÆö‰ΩøÁî®Âì™‰∏™scÁöÑ‰æõÂ∫îÂïÜÊù•ÂàõÂª∫pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi                      # Ê†πÊçÆ‰∏öÂä°ÂÆûÈôÖÂ§ßÂ∞èËøõË°åËµÑÊ∫êÁî≥ËØ∑\n\n[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc.yaml \n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nceph-block-pvc   Bound    pvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            rook-ceph-block   3s\n\n[root@k8s-master01 ~]# kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS      REASON   AGE\npvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            Delete           Bound    default/ceph-block-pvc   rook-ceph-block\n\t  \n[root@k8s-master01 ~]# cat ceph-block-pvc-pod.yaml    #ÊåÇËΩΩPVCÊµãËØï \napiVersion: v1\nkind: Pod\nmetadata:\n  name: ceph-block-pvc-pod\nspec:\n  containers:\n  - name: ceph-block-pvc-pod\n    image: nginx\n    volumeMounts:\n    - name: nginx-page\n      mountPath: /usr/share/nginx/html\n  volumes:\n  - name: nginx-page\n    persistentVolumeClaim:      \n      claimName: ceph-block-pv\n\n[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-pod.yaml\n</code></pre>\n<h5 id=\"73-statefulset-volumeclaimtemplates\"><a class=\"anchor\" href=\"#73-statefulset-volumeclaimtemplates\">#</a> 7.3 StatefulSet volumeClaimTemplates</h5>\n<pre><code>[root@k8s-master01 ~]# cat ceph-block-pvc-sts.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n\n  - port: 80\n    name: web\n      clusterIP: None\n      selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx # ÂøÖÈ°ªÂåπÈÖç .spec.template.metadata.labels\n  serviceName: &quot;nginx&quot;\n  replicas: 3 # ÈªòËÆ§ÂÄºÊòØ 1\n  template:\n    metadata:\n      labels:\n        app: nginx # ÂøÖÈ°ªÂåπÈÖç .spec.selector.matchLabels\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.20\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n    name: www\n    spec:\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      storageClassName: &quot;rook-ceph-block&quot;\n      resources:\n        requests:\n          storage: 1Gi\n    \t  \n[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-sts.yaml \n\n[root@k8s-master01 ~]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS       AGE\nweb-0                                     1/1     Running   0              4m19s\nweb-1                                     1/1     Running   0              4m10s\nweb-2                                     1/1     Running   0              2m21s\n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\nwww-web-0   Bound    pvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            rook-ceph-block   4m23s\nwww-web-1   Bound    pvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            rook-ceph-block   4m14s\nwww-web-2   Bound    pvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            rook-ceph-block   2m25s\n\n[root@k8s-master01 ~]# kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS      REASON   AGE\npvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            Delete           Bound    default/www-web-0   rook-ceph-block            4m25s\npvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            Delete           Bound    default/www-web-2   rook-ceph-block            2m27s\npvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            Delete           Bound    default/www-web-1   rook-ceph-block            4m16s\n</code></pre>\n<h4 id=\"8-ÂÖ±‰∫´Êñá‰ª∂Á≥ªÁªüÁöÑ‰ΩøÁî®\"><a class=\"anchor\" href=\"#8-ÂÖ±‰∫´Êñá‰ª∂Á≥ªÁªüÁöÑ‰ΩøÁî®\">#</a> 8. ÂÖ±‰∫´Êñá‰ª∂Á≥ªÁªüÁöÑ‰ΩøÁî®</h4>\n<p>ÂÖ±‰∫´Êñá‰ª∂Á≥ªÁªü‰∏ÄËà¨Áî®‰∫éÂ§ö‰∏™ Pod ÂÖ±‰∫´‰∏Ä‰∏™Â≠òÂÇ®</p>\n<h5 id=\"81-ÂàõÂª∫ÂÖ±‰∫´Á±ªÂûãÁöÑÊñá‰ª∂Á≥ªÁªü\"><a class=\"anchor\" href=\"#81-ÂàõÂª∫ÂÖ±‰∫´Á±ªÂûãÁöÑÊñá‰ª∂Á≥ªÁªü\">#</a> 8.1 ÂàõÂª∫ÂÖ±‰∫´Á±ªÂûãÁöÑÊñá‰ª∂Á≥ªÁªü</h5>\n<pre><code>[root@k8s-master01 ~]# cd /root/rook/deploy/examples/\n[root@k8s-master01 examples]# kubectl apply -f filesystem.yaml\n[root@k8s-master01 examples]# kubectl get pod -l app=rook-ceph-mds -n rook-ceph\nNAME                                    READY   STATUS    RESTARTS   AGE\nrook-ceph-mds-myfs-a-7d76cb5988-9nz9p   2/2     Running   0          36s\nrook-ceph-mds-myfs-b-76ff7c784c-vs8nm   2/2     Running   0          33s\n</code></pre>\n<h5 id=\"82-ÂàõÂª∫ÂÖ±‰∫´Á±ªÂûãÊñá‰ª∂Á≥ªÁªüÁöÑ-storageclass\"><a class=\"anchor\" href=\"#82-ÂàõÂª∫ÂÖ±‰∫´Á±ªÂûãÊñá‰ª∂Á≥ªÁªüÁöÑ-storageclass\">#</a> 8.2 ÂàõÂª∫ÂÖ±‰∫´Á±ªÂûãÊñá‰ª∂Á≥ªÁªüÁöÑ StorageClass</h5>\n<pre><code>[root@k8s-master01 examples]# cd csi/cephfs\n[root@k8s-master01 cephfs]# kubectl create -f storageclass.yaml\n[root@k8s-master01 cephfs]# kubectl get sc\nNAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nnfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  17h\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   82m\nrook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   13s\n</code></pre>\n<h5 id=\"83-ÊåÇËΩΩÊµãËØï\"><a class=\"anchor\" href=\"#83-ÊåÇËΩΩÊµãËØï\">#</a> 8.3 ÊåÇËΩΩÊµãËØï</h5>\n<pre><code>[root@k8s-master01 ~]# cat cephfs-pvc-deploy.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  selector:\n    app: nginx\n  type: ClusterIP\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: nginx-share-pvc\nspec:\n  storageClassName: rook-cephfs \n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi\n---\napiVersion: apps/v1\nkind: Deployment \nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx # has to match .spec.template.metadata.labels\n  replicas: 3 # by default is 1\n  template:\n    metadata:\n      labels:\n        app: nginx # has to match .spec.selector.matchLabels\n    spec:\n      containers:\n      - name: nginx\n        image: nginx \n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n      volumes:\n        - name: www\n          persistentVolumeClaim:\n            claimName: nginx-share-pvc\n\t\t\t\n[root@k8s-master01 ~]# kubectl apply -f cephfs-pvc-deploy.yaml\n[root@k8s-master01 ~]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS        AGE\ncluster-test-84dfc9c68b-5q4ng             1/1     Running   84 (4m2s ago)   16d\nnfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (123m ago)    18h\nweb-6c59f8559-g5xzb                       1/1     Running   0               46s\nweb-6c59f8559-ns77q                       1/1     Running   0               46s\nweb-6c59f8559-qxb5f                       1/1     Running   0               46s\n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    52s\n\n[root@k8s-master01 ~]# kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE\npvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             53s\n\n[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash\nroot@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# echo &quot;hello cephfs&quot; &gt;&gt; index.html\n\n[root@k8s-master01 ~]# kubectl get svc\nNAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\nkubernetes           ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP    16d\nmysql-svc-external   ClusterIP   None          &lt;none&gt;        3306/TCP   9d\nnginx                ClusterIP   10.96.58.17   &lt;none&gt;        80/TCP     4m34s\n[root@k8s-master01 ~]# curl 10.96.58.17\nhello cephfs\n</code></pre>\n<h4 id=\"9pvc-Êâ©ÂÆπ\"><a class=\"anchor\" href=\"#9pvc-Êâ©ÂÆπ\">#</a> 9.PVC Êâ©ÂÆπ</h4>\n<pre><code>[root@k8s-master01 ~]# kubectl get sc\nNAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nnfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  18h\nrook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   104m     #trueÂÖÅËÆ∏Êâ©ÂÆπ\nrook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   22m      #trueÂÖÅËÆ∏Êâ©ÂÆπ\n\n[root@k8s-master01 ~]# kubectl get pvc\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    13m\n[root@k8s-master01 ~]# kubectl edit pvc nginx-share-pvc\n...\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi         #Êõ¥ÊîπpvcÂ§ßÂ∞è\n  storageClassName: rook-cephfs\n...\n\n[root@k8s-master01 ~]# kubectl get pvc       #Êü•ÁúãPVCÊòØÂê¶Êâ©ÂÆπ\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    15m\n\n[root@k8s-master01 ~]# kubectl get pv         #Êü•ÁúãPVÊòØÂê¶Êâ©ÂÆπ\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE\npvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             15m\n\n[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash       #ËøõÂÖ•ÂÆπÂô®ÔºåÊü•ÁúãpodÊòØÂê¶Êâ©ÂÆπ  \nroot@web-6c59f8559-g5xzb:/# df -h \nFilesystem                                                                                                                                             Size  Used Avail Use% Mounted on\noverlay                                                                                                                                                 17G   13G  4.1G  76% /\ntmpfs                                                                                                                                                   64M     0   64M   0% /dev\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/sda3                                                                                                                                               17G   13G  4.1G  76% /etc/hosts\nshm                                                                                                                                                     64M     0   64M   0% /dev/shm\n10.96.121.140:6789,10.96.131.130:6789,10.96.62.64:6789:/volumes/csi/csi-vol-3b645a11-58f4-475a-9404-5d84964f5291/e4bdf743-eb18-42c8-b04f-41964f76de4f  5.0G     0  5.0G   0% /usr/share/nginx/html\ntmpfs                                                                                                                                                  3.8G   12K  3.8G   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/asound\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/acpi\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/scsi\ntmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/firmware\n</code></pre>\n<h4 id=\"10-pvc-Âø´ÁÖß\"><a class=\"anchor\" href=\"#10-pvc-Âø´ÁÖß\">#</a> 10. PVC Âø´ÁÖß</h4>\n<h5 id=\"101-Êñá‰ª∂ÂÖ±‰∫´Á±ªÂûãÂø´ÁÖß\"><a class=\"anchor\" href=\"#101-Êñá‰ª∂ÂÖ±‰∫´Á±ªÂûãÂø´ÁÖß\">#</a> 10.1 Êñá‰ª∂ÂÖ±‰∫´Á±ªÂûãÂø´ÁÖß</h5>\n<pre><code>[root@k8s-master01 ~]# cd rook/deploy/examples\n[root@k8s-master01 examples]# kubectl create -f csi/cephfs/snapshotclass.yaml \n\n[root@k8s-master01 examples]# kubectl get volumesnapshotclass\nNAME                         DRIVER                          DELETIONPOLICY   AGE\ncsi-cephfsplugin-snapclass   rook-ceph.cephfs.csi.ceph.com   Delete           25s\n\n\n#ÊãçÊëÑÂø´ÁÖß\t\n[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash         #pvcÊñ∞Â¢ûÊï∞ÊçÆ\nroot@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# touch &#123;1..10&#125;\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls\n1  10  2  3  4\t5  6  7  8  9  index.html\n\n[root@k8s-master01 examples]# kubectl get pvc       #Êü•ÁúãpvsÂπ∂ÂØπnginx-share-pvcÊãçÊëÑÂø´ÁÖß\nNAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nnginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h23m\n\n[root@k8s-master01 examples]# cat csi/cephfs/snapshot.yaml         #ÊãçÊëÑÂø´ÁÖß\n---\n# 1.17 &lt;= K8s &lt;= v1.19\n# apiVersion: snapshot.storage.k8s.io/v1beta1\n# K8s &gt;= v1.20\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: cephfs-pvc-snapshot\nspec:\n  volumeSnapshotClassName: csi-cephfsplugin-snapclass\n  source:\n    persistentVolumeClaimName: nginx-share-pvc         #Âü∫‰∫éÈÇ£‰∏™PVCÊãçÊëÑÂø´ÁÖß\n\t\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/snapshot.yaml\n[root@k8s-master01 examples]# kubectl get volumesnapshot\nNAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE\ncephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   4m6s           4m8s\n\n#Âà†Èô§pvcÊï∞ÊçÆ\n[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash\nroot@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls\n1  10  2  3  4\t5  6  7  8  9  index.html\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# rm -rf &#123;1..10&#125;\nroot@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls\nindex.html\n\n#pvcÂõûÊªöÊï∞ÊçÆ\n[root@k8s-master01 examples]# kubectl get volumesnapshot\nNAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE\ncephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   7m39s          7m41s\n\t\n[root@k8s-master01 examples]# cat csi/cephfs/pvc-restore.yaml \n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc-restore\nspec:\n  storageClassName: rook-cephfs       #ÂàõÂª∫pvÁöÑstorageclassÂêçÁß∞Áõ∏Âêå\n  dataSource:\n    name: cephfs-pvc-snapshot         #volumesnapshotÊï∞ÊçÆÊ∫ê\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi      #Â§ßÂ∞èÁ≠â‰∫ésnapshotÂ§ßÂ∞è\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-restore.yaml\n\n[root@k8s-master01 examples]# kubectl get pvc\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ncephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    54s          \nnginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h50m\n\n#ÊåÇËΩΩPVCÊµãËØïÊï∞ÊçÆÊòØÂê¶ÊÅ¢Â§ç\n[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml \n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n    - name: web-server\n      image: nginx\n      volumeMounts:\n        - name: mypvc\n          mountPath: /var/lib/www/html\n  volumes:\n    - name: mypvc\n      persistentVolumeClaim:\n        claimName: cephfs-pvc-restore        #ÊåÇËΩΩÊÅ¢Â§çpvc\n        readOnly: false\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml\n[root@k8s-master01 examples]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS        AGE\ncluster-test-84dfc9c68b-5q4ng             1/1     Running   88 (57m ago)    16d\ncsicephfs-demo-pod                        1/1     Running   0               24s\nnfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (6h57m ago)   23h\nweb-6c59f8559-g5xzb                       1/1     Running   0               4h54m\nweb-6c59f8559-ns77q                       1/1     Running   0               4h54m\nweb-6c59f8559-qxb5f                       1/1     Running   0               4h54m\n[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash\nroot@csicephfs-demo-pod:/# ls /var/lib/www/html/               #sÂà†Èô§Êï∞ÊçÆÂ∑≤ÁªèÊÅ¢Â§ç\n1  10  2  3  4\t5  6  7  8  9  index.html\n</code></pre>\n<h5 id=\"102-pvc-ÂÖãÈöÜ\"><a class=\"anchor\" href=\"#102-pvc-ÂÖãÈöÜ\">#</a> 10.2 PVC ÂÖãÈöÜ</h5>\n<pre><code>[root@k8s-master01 examples]# kubectl get pvc\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ncephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    11m\nnginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h1m\n\n\n[root@k8s-master01 examples]# cat csi/cephfs/pvc-clone.yaml \n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc-clone\nspec:\n  storageClassName: rook-cephfs      # pvc ÁöÑ storageClass ÂêçÁß∞\n  dataSource:\n    name: nginx-share-pvc          #ÂÖãÈöÜÁöÑPVCÂêçÁß∞\n    kind: PersistentVolumeClaim\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi                 #Â§ßÂ∞èÁ≠â‰∫éÊâÄÂÖãÈöÜÁöÑPVCÂ§ßÂ∞è\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-clone.yaml\n\n[root@k8s-master01 examples]# kubectl get pvc\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ncephfs-pvc-clone     Bound    pvc-0a19b65e-cb5e-4379-a7f7-e0783fcf8ddf   5Gi        RWX            rook-cephfs    22s\ncephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    15m\nnginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h4m\n\n#ÊåÇËΩΩÂÖãÈöÜPVCÊµãËØï\n[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml \n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: csicephfs-demo-pod\nspec:\n  containers:\n    - name: web-server\n      image: nginx\n      volumeMounts:\n        - name: mypvc\n          mountPath: /var/lib/www/html\n  volumes:\n    - name: mypvc\n      persistentVolumeClaim:\n        claimName: cephfs-pvc-clone      #ÊåÇËΩΩÂÖãÈöÜÁöÑpvc\n        readOnly: false\n\n[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml          \n[root@k8s-master01 examples]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS         AGE\ncluster-test-84dfc9c68b-5q4ng             1/1     Running   89 (9m54s ago)   16d\ncsicephfs-demo-pod                        1/1     Running   0                17s\nnfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (7h9m ago)     23h\nweb-6c59f8559-g5xzb                       1/1     Running   0                5h6m\nweb-6c59f8559-ns77q                       1/1     Running   0                5h6m\nweb-6c59f8559-qxb5f                       1/1     Running   0                5h6m\n\n[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash\nroot@csicephfs-demo-pod:/# cat /var/lib/www/html/index.html \nhello cephfs\n</code></pre>\n<h4 id=\"11-ÊµãËØïÊï∞ÊçÆÊ∏ÖÁêÜ\"><a class=\"anchor\" href=\"#11-ÊµãËØïÊï∞ÊçÆÊ∏ÖÁêÜ\">#</a> 11. ÊµãËØïÊï∞ÊçÆÊ∏ÖÁêÜ</h4>\n<pre><code>ÂèÇËÄÉÊñáÊ°£Ôºöhttps://rook.io/docs/rook/v1.11/Getting-Started/ceph-teardown/#delete-the-cephcluster-crd\n[root@k8s-master01 ~]# kubectl delete deploy web\n\n[root@k8s-master01 ~]# kubectl delete pods csicephfs-demo-pod\n\n[root@k8s-master01 ~]# kubectl delete pvc --all\n[root@k8s-master01 ~]# kubectl get pvc\nNo resources found in default namespace.\n[root@k8s-master01 ~]# kubectl get pv\nNo resources found\n\n\n[root@k8s-master01 ~]# kubectl get volumesnapshot\nNAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE\ncephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   61m            61m\n[root@k8s-master01 ~]# kubectl delete volumesnapshot cephfs-pvc-snapshot\nvolumesnapshot.snapshot.storage.k8s.io &quot;cephfs-pvc-snapshot&quot; deleted\n\nkubectl delete -n rook-ceph cephblockpool replicapool\nkubectl delete -n rook-ceph cephfilesystem myfs\n\nkubectl delete storageclass rook-ceph-block\nkubectl delete storageclass rook-cephfs\nkubectl delete -f csi/cephfs/kube-registry.yaml\nkubectl delete storageclass csi-cephfs\n\nkubectl -n rook-ceph delete cephcluster rook-ceph\n\nkubectl delete -f operator.yaml\nkubectl delete -f common.yaml\nkubectl delete -f crds.yaml\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3890389502.html",
            "url": "http://ixuyong.cn/posts/3890389502.html",
            "title": "K8SÊåÅ‰πÖÂåñÂ≠òÂÇ®NFS+StorageClass",
            "date_published": "2025-04-23T12:08:26.000Z",
            "content_html": "<h3 id=\"k8sÊåÅ‰πÖÂåñÂ≠òÂÇ®nfsstorageclass\"><a class=\"anchor\" href=\"#k8sÊåÅ‰πÖÂåñÂ≠òÂÇ®nfsstorageclass\">#</a> K8S ÊåÅ‰πÖÂåñÂ≠òÂÇ® NFS+StorageClass</h3>\n<h4 id=\"1-Êê≠Âª∫nfsÊúçÂä°Âô®\"><a class=\"anchor\" href=\"#1-Êê≠Âª∫nfsÊúçÂä°Âô®\">#</a> 1. Êê≠Âª∫ NFS ÊúçÂä°Âô®</h4>\n<pre><code>#ÊâÄÊúâK8SËäÇÁÇπÂÆâË£Önfs-utils\n[root@k8s-node02 ~]# yum install nfs-utils -y    \n\n#K8S-node02ËäÇÁÇπÈÖçÁΩÆnfsÊúçÂä°\n[root@k8s-node02 ~]# mkdir /data/nfs -p\n[root@k8s-node02 ~]# cat /etc/exports\n/data/nfs 192.168.1.0/24(rw,no_root_squash)\n[root@k8s-node02 ~]# exportfs -arv   #NFSÈÖçÁΩÆÁîüÊïà \n[root@k8s-node02 ~]# systemctl start nfs-server &amp;&amp; systemctl enable nfs-server &amp;&amp; systemctl status nfs-server\n</code></pre>\n<h4 id=\"2-ÂàõÂª∫rbac\"><a class=\"anchor\" href=\"#2-ÂàõÂª∫rbac\">#</a> 2.  ÂàõÂª∫ RBAC</h4>\n<pre><code>[root@k8s-node02 ~]# cat 01-rbac.yaml \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: nfs-client-provisioner-runner\nrules:\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;nodes&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;persistentvolumes&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;persistentvolumeclaims&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]\n  - apiGroups: [&quot;storage.k8s.io&quot;]\n    resources: [&quot;storageclasses&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;events&quot;]\n    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: run-nfs-client-provisioner\nsubjects:\n  - kind: ServiceAccount\n    name: nfs-client-provisioner\n    # replace with namespace where provisioner is deployed\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: nfs-client-provisioner-runner\n  apiGroup: rbac.authorization.k8s.io\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: leader-locking-nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\nrules:\n  - apiGroups: [&quot;&quot;]\n    resources: [&quot;endpoints&quot;]\n    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: leader-locking-nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\nsubjects:\n  - kind: ServiceAccount\n    name: nfs-client-provisioner\n    # replace with namespace where provisioner is deployed\n    namespace: default\nroleRef:\n  kind: Role\n  name: leader-locking-nfs-client-provisioner\n  apiGroup: rbac.authorization.k8s.io\n  \n  \n[root@k8s-master01 ~]# kubectl apply -f 01-rbac.yaml \nserviceaccount/nfs-client-provisioner created\nclusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created\nclusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created\nrole.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created\nrolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created\n</code></pre>\n<h4 id=\"3-ÂàõÂª∫nfs-provisioner\"><a class=\"anchor\" href=\"#3-ÂàõÂª∫nfs-provisioner\">#</a> 3. ÂàõÂª∫ nfs-provisioner</h4>\n<pre><code>[root@k8s-master01 ~]# cat 02-nfs-provisioner.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nfs-client-provisioner\n  labels:\n    app: nfs-client-provisioner\n  # replace with namespace where provisioner is deployed\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-client-provisioner\n  template:\n    metadata:\n      labels:\n        app: nfs-client-provisioner\n    spec:\n      serviceAccountName: nfs-client-provisioner\n      containers:\n        - name: nfs-client-provisioner\n          image: registry.cn-hangzhou.aliyuncs.com/old_xu/nfs-subdir-external-provisioner:v4.0.2\n          volumeMounts:\n            - name: nfs-client-root\n              mountPath: /persistentvolumes\n          env:\n            - name: PROVISIONER_NAME\t# nfs-provisionerÁöÑÂêçÁß∞ÔºåÂêéÁª≠storageClassË¶Å‰∏éËØ•ÂêçÁß∞‰∏ÄËá¥\n              value: nfzl.com/nfs\n            - name: NFS_SERVER\t\t# NFSÊúçÂä°ÁöÑIPÂú∞ÂùÄ\n              value: 192.168.1.75\n            - name: NFS_PATH\t\t# NFSÊúçÂä°ÂÖ±‰∫´ÁöÑË∑ØÂæÑ\n              value: /data/nfs\n      volumes:\n        - name: nfs-client-root\n          nfs:\n            server: 192.168.1.75\n            path: /data/nfs\n\n[root@k8s-master01 ~]# kubectl apply -f 02-nfs-provisioner.yaml \n[root@k8s-master01 ~]# kubectl get pods\nNAME                                      READY   STATUS    RESTARTS   AGE\nnfs-client-provisioner-6bcc4587f8-zp8qc   1/1     Running   0          17s\n</code></pre>\n<h4 id=\"4-ÂàõÂª∫storageclass\"><a class=\"anchor\" href=\"#4-ÂàõÂª∫storageclass\">#</a> 4. ÂàõÂª∫ StorageClass</h4>\n<pre><code>[root@k8s-master01 ~]# cat 03-storageClass.yaml \napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-storage \t# pvcÁî≥ËØ∑Êó∂ÈúÄÊòéÁ°ÆÊåáÂÆöÁöÑstorageClassÂêçÁß∞\nprovisioner: nfzl.com/nfs        # ‰æõÂ∫îÂïÜÂêçÁß∞ÔºåÂøÖÈ°ªÂíå‰∏äÈù¢ÂàõÂª∫ÁöÑ&quot;PROVISIONER_NAME&quot;ÂèòÈáèÂÄºËá¥\nparameters:\n  archiveOnDelete: &quot;false&quot;     # Â¶ÇÊûúÂÄº‰∏∫falseÔºåÂà†Èô§PVCÂêé‰πü‰ºöÂà†Èô§ÁõÆÂΩïÂÜÖÂÆπ, &quot;true&quot;Âàô‰ºöÂØπÊï∞ÊçÆËøõË°å‰øùÁïô\n</code></pre>\n<h4 id=\"5-ÂàõÂª∫pvc\"><a class=\"anchor\" href=\"#5-ÂàõÂª∫pvc\">#</a> 5. ÂàõÂª∫ PVC</h4>\n<pre><code>[root@k8s-master01 ~]# cat 04-nginx-pvc.yaml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sc-pvc-001\nspec:\n  storageClassName: &quot;nfs-storage&quot;     # ÊòéÁ°ÆÊåáÂÆö‰ΩøÁî®Âì™‰∏™scÁöÑ‰æõÂ∫îÂïÜÊù•ÂàõÂª∫pv\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi                      # Ê†πÊçÆ‰∏öÂä°ÂÆûÈôÖÂ§ßÂ∞èËøõË°åËµÑÊ∫êÁî≥ËØ∑\n      \n[root@k8s-master01 ~]# kubectl apply -f 04-nginx-pvc.yaml \n</code></pre>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/fgpaP15.png\" alt=\"1.png\" /></p>\n<h4 id=\"6-ÊåÇËΩΩpvcÊµãËØï\"><a class=\"anchor\" href=\"#6-ÊåÇËΩΩpvcÊµãËØï\">#</a> 6. ÊåÇËΩΩ PVC ÊµãËØï</h4>\n<pre><code>[root@k8s-master01 ~]# cat 05-nginx-pod.yaml \napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-sc-001\nspec:\n  containers:\n  - name: nginx-sc-001\n    image: nginx\n    volumeMounts:\n    - name: nginx-page\n      mountPath: /usr/share/nginx/html\n  volumes:\n  - name: nginx-page\n    persistentVolumeClaim:      \n      claimName: sc-pvc-001\n\n[root@k8s-master01 ~]# kubectl apply -f 05-nginx-pod.yaml\n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                                      READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nnginx-sc-001                              1/1     Running   0          15s   172.16.85.244   k8s-node01   &lt;none&gt;           &lt;none&gt;\n\n[root@k8s-master01 ~]# curl 172.16.85.244\nhello world\n</code></pre>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/722512536.html",
            "url": "http://ixuyong.cn/posts/722512536.html",
            "title": "K8sÁªÜÁ≤íÂ∫¶ÊùÉÈôêÊéßÂà∂RBAC",
            "date_published": "2025-04-23T12:04:03.000Z",
            "content_html": "<h3 id=\"k8sÁªÜÁ≤íÂ∫¶ÊùÉÈôêÊéßÂà∂rbac\"><a class=\"anchor\" href=\"#k8sÁªÜÁ≤íÂ∫¶ÊùÉÈôêÊéßÂà∂rbac\">#</a> K8s ÁªÜÁ≤íÂ∫¶ÊùÉÈôêÊéßÂà∂ RBAC</h3>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/KCZPPkv.jpeg\" alt=\"rbac.jpg\" /></p>\n<h4 id=\"1-ÂàõÂª∫‰∏çÂêåÊùÉÈôêÁöÑclusterrole\"><a class=\"anchor\" href=\"#1-ÂàõÂª∫‰∏çÂêåÊùÉÈôêÁöÑclusterrole\">#</a> 1. ÂàõÂª∫‰∏çÂêåÊùÉÈôêÁöÑ clusterrole</h4>\n<h5 id=\"11-ÂëΩ‰ª§Á©∫Èó¥Âè™ËØªnamespace-readonly\"><a class=\"anchor\" href=\"#11-ÂëΩ‰ª§Á©∫Èó¥Âè™ËØªnamespace-readonly\">#</a> 1.1 ÂëΩ‰ª§Á©∫Èó¥Âè™ËØª namespace-readonly</h5>\n<pre><code># cat namespace-readonly.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: namespace-readonly\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - metrics.k8s.io\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>\n<h5 id=\"12-ËµÑÊ∫êÊü•Áúãresource-readonly\"><a class=\"anchor\" href=\"#12-ËµÑÊ∫êÊü•Áúãresource-readonly\">#</a> 1.2 ËµÑÊ∫êÊü•Áúã resource-readonly</h5>\n<pre><code># cat resource-readonly.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: resource-readonly\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - configmaps\n  - endpoints\n  - persistentvolumeclaims\n  - pods\n  - replicationcontrollers\n  - replicationcontrollers/scale\n  - serviceaccounts\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - bindings\n  - events\n  - limitranges\n  - namespaces/status\n  - pods/log\n  - pods/status\n  - replicationcontrollers/status\n  - resourcequotas\n  - resourcequotas/status\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - controllerrevisions\n  - daemonsets\n  - deployments\n  - deployments/scale\n  - replicasets\n  - replicasets/scale\n  - statefulsets\n  - statefulsets/scale\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - deployments/scale\n  - ingresses\n  - networkpolicies\n  - replicasets\n  - replicasets/scale\n  - replicationcontrollers/scale\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - metrics.k8s.io\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>\n<h5 id=\"13-podÊó•ÂøóÊü•Áúã\"><a class=\"anchor\" href=\"#13-podÊó•ÂøóÊü•Áúã\">#</a> 1.3 pod Êó•ÂøóÊü•Áúã</h5>\n<pre><code># cat pod-log.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-log\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods\n  - pods/log\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>\n<h5 id=\"14-podÂà†Èô§\"><a class=\"anchor\" href=\"#14-podÂà†Èô§\">#</a> 1.4 Pod Âà†Èô§</h5>\n<pre><code># cat pod-delete.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-delete\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - delete\n</code></pre>\n<h5 id=\"15-podÊâßË°å\"><a class=\"anchor\" href=\"#15-podÊâßË°å\">#</a> 1.5 Pod ÊâßË°å</h5>\n<pre><code># cat pod-exec.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-exec\nrules:\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n- apiGroups:\n  - &quot;&quot;\n  resources:\n  - pods/exec\n  verbs:\n  - create\n</code></pre>\n<h5 id=\"16-ÂàõÂª∫‰∏çÂêåÊùÉÈôêÁöÑclusterrole\"><a class=\"anchor\" href=\"#16-ÂàõÂª∫‰∏çÂêåÊùÉÈôêÁöÑclusterrole\">#</a> 1.6 ÂàõÂª∫‰∏çÂêåÊùÉÈôêÁöÑ clusterrole</h5>\n<pre><code>[root@k8s-master01 rbac]# kubectl apply -f .\n</code></pre>\n<h4 id=\"2-ÂàõÂª∫serviceaccount\"><a class=\"anchor\" href=\"#2-ÂàõÂª∫serviceaccount\">#</a> 2. ÂàõÂª∫ serviceaccount</h4>\n<pre><code># kubectl create ns kube-users\n\n# kubectl create sa test -n kube-users   \n# kubectl create sa dev -n kube-users    \n# kubectl create sa ops -n kube-users    \n\n# kubectl create token test -n kube-users\n# kubectl create token dev -n kube-users\n# kubectl create token ops -n kube-users\n</code></pre>\n<h4 id=\"3-ÂàõÂª∫clusterrolebinding\"><a class=\"anchor\" href=\"#3-ÂàõÂª∫clusterrolebinding\">#</a> 3. ÂàõÂª∫ ClusterRoleBinding</h4>\n<h5 id=\"31-ÁªëÂÆöÂÖ®Â±ÄÂëΩÂêçÁ©∫Èó¥Êü•ÁúãÊùÉÈôê\"><a class=\"anchor\" href=\"#31-ÁªëÂÆöÂÖ®Â±ÄÂëΩÂêçÁ©∫Èó¥Êü•ÁúãÊùÉÈôê\">#</a> 3.1 ÁªëÂÆöÂÖ®Â±ÄÂëΩÂêçÁ©∫Èó¥Êü•ÁúãÊùÉÈôê</h5>\n<pre><code># cat clusterrolebinding-namespace-readonly.yaml \napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: clusterrolebinding-namespace-readonly \nsubjects:\n- kind: Group\n  name: system:serviceaccounts:kube-users\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: namespace-readonly\n  apiGroup: rbac.authorization.k8s.io\n  \n# kubectl apply -f clusterrolebinding-namespace-readonly.yaml\n</code></pre>\n<h5 id=\"32-ÁªëÂÆöÊó•ÂøóÊü•ÁúãÊùÉÈôê\"><a class=\"anchor\" href=\"#32-ÁªëÂÆöÊó•ÂøóÊü•ÁúãÊùÉÈôê\">#</a> 3.2 ÁªëÂÆöÊó•ÂøóÊü•ÁúãÊùÉÈôê</h5>\n<pre><code># kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<h5 id=\"33-ÁªëÂÆöËµÑÊ∫êÊü•ÁúãÊùÉÈôê\"><a class=\"anchor\" href=\"#33-ÁªëÂÆöËµÑÊ∫êÊü•ÁúãÊùÉÈôê\">#</a> 3.3 ÁªëÂÆöËµÑÊ∫êÊü•ÁúãÊùÉÈôê</h5>\n<pre><code># kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<h5 id=\"34-ÁªëÂÆöpodÊâßË°åÊùÉÈôê\"><a class=\"anchor\" href=\"#34-ÁªëÂÆöpodÊâßË°åÊùÉÈôê\">#</a> 3.4 ÁªëÂÆö Pod ÊâßË°åÊùÉÈôê</h5>\n<pre><code># kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<h5 id=\"35-ÁªëÂÆöpodÂà†Èô§ÊùÉÈôê\"><a class=\"anchor\" href=\"#35-ÁªëÂÆöpodÂà†Èô§ÊùÉÈôê\">#</a> 3.5 ÁªëÂÆö Pod Âà†Èô§ÊùÉÈôê</h5>\n<pre><code># kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectA\n# kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectB\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/176412055.html",
            "url": "http://ixuyong.cn/posts/176412055.html",
            "title": "K8sÂáÜÂÖ•ÊéßÂà∂ResourceQuota„ÄÅLimitRange„ÄÅQoSÊúçÂä°Ë¥®Èáè",
            "date_published": "2025-04-23T11:55:19.000Z",
            "content_html": "<h3 id=\"k8sÂáÜÂÖ•ÊéßÂà∂resourcequota-limitrange-qosÊúçÂä°Ë¥®Èáè\"><a class=\"anchor\" href=\"#k8sÂáÜÂÖ•ÊéßÂà∂resourcequota-limitrange-qosÊúçÂä°Ë¥®Èáè\">#</a> K8s ÂáÜÂÖ•ÊéßÂà∂ ResourceQuota„ÄÅLimitRange„ÄÅQoS ÊúçÂä°Ë¥®Èáè</h3>\n<h4 id=\"1-resourcequotaÈÖçÁΩÆËß£Êûê\"><a class=\"anchor\" href=\"#1-resourcequotaÈÖçÁΩÆËß£Êûê\">#</a> 1. ResourceQuota ÈÖçÁΩÆËß£Êûê</h4>\n<p>ResourceQuotas ÂÆûÁé∞ËµÑÊ∫êÈÖçÈ¢ùÔºåÈÅøÂÖçËøáÂ∫¶ÂàõÂª∫ËµÑÊ∫êÔºåÈíàÂØπ namespace ËøõË°åÈôêÂà∂„ÄÇcpu ÂÜÖÂ≠òÂàôÊòØÊ†πÊçÆ pod ÈÖçÁΩÆÁöÑ resources ÊÄªÈ¢ùËøõË°åÈôêÂà∂ÔºåÂ¶ÇÊûúÊ≤°ÊúâÈÖçÁΩÆ resources ÂèÇÊï∞ÂàôÊó†Ê≥ïÈôêÂà∂„ÄÇ</p>\n<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: resourcequota-test\n  namespace: test\n  labels:\n    app: resourcequota\nspec:\n  hard:\n    pods: 3\n    requests.cpu: 3\n    requests.memory: 512Mi\n    limits.cpu: 8\n    limits.memory: 16Gi\n    configmaps: 201\n    requests.storage: 40Gi\n    persistentvolumeclaims: 20\n    replicationcontrollers: 20\n    secrets: 20\n    services: 50\n    services.loadbalancers: &quot;2&quot;\n    services.nodeports: &quot;10&quot;\n</code></pre>\n<ul>\n<li>podsÔºöÈôêÂà∂ÊúÄÂ§öÂêØÂä® Pod ÁöÑ‰∏™Êï∞</li>\n<li>requests.cpuÔºöÈôêÂà∂ÊúÄÈ´ò CPU ËØ∑Ê±ÇÊï∞</li>\n<li>requests.memoryÔºöÈôêÂà∂ÊúÄÈ´òÂÜÖÂ≠òÁöÑËØ∑Ê±ÇÊï∞</li>\n<li>limits.cpuÔºöÈôêÂà∂ÊúÄÈ´ò CPU ÁöÑ limit ‰∏äÈôê</li>\n<li>limits.memoryÔºöÈôêÂà∂ÊúÄÈ´òÂÜÖÂ≠òÁöÑ limit ‰∏äÈôê</li>\n<li>servicesÔºöÈôêÂà∂ services Êï∞Èáè</li>\n<li>services.nodeportsÔºöÈôêÂà∂ services ‰∏≠ nodeport Á±ªÂûã service Êï∞Èáè</li>\n<li>services.loadbalancersÔºöÈôêÂà∂ services ‰∏≠ loadbalancers Á±ªÂûã service Êï∞Èáè</li>\n</ul>\n<h5 id=\"11-resourcequotaÈÖçÁΩÆÁ§∫‰æã\"><a class=\"anchor\" href=\"#11-resourcequotaÈÖçÁΩÆÁ§∫‰æã\">#</a> 1.1 ResourceQuota ÈÖçÁΩÆÁ§∫‰æã</h5>\n<pre><code>#1.ÈôêÂà∂testÂëΩÂêçÁ©∫Èó¥podsÊï∞ÈáèÈáè‰∏∫3„ÄÅconfigmapÊï∞Èáè‰∏∫2\n[root@k8s-master01 resourcequota]# cat rq-test.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: resourcequota-test\n  namespace: test\n  labels:\n    app: resourcequota\nspec:\n  hard:\n    pods: 3\n#    requests.cpu: 3\n#    requests.memory: 512Mi\n#    limits.cpu: 8\n#    limits.memory: 16Gi\n    configmaps: 2\n#    requests.storage: 40Gi\n#    persistentvolumeclaims: 20\n#    replicationcontrollers: 20\n#    secrets: 20\n#    services: 50\n#    services.loadbalancers: &quot;2&quot;\n#    services.nodeports: &quot;10&quot;\n\n#2.testÂëΩÂêçÁ©∫Èó¥Â∑≤ÂàõÂª∫configmapÊï∞Èáè‰∏∫1,ÈôêÂà∂Êï∞Èáè‰∏∫2\n[root@k8s-master01 resourcequota]# kubectl get resourcequota -n test\nNAME                 AGE   REQUEST                      LIMIT\nresourcequota-test   61s   configmaps: 1/2, pods: 0/3  \n\n#3.testÂëΩÂêçÁ©∫Èó¥ÂàõÂª∫Á¨¨2‰∏™configmapÊó∂Ê≠£Â∏∏ÔºåÂàõÂª∫Á¨¨3‰∏™configmapÊó∂Êä•Èîô\n[root@k8s-master01 resourcequota]# kubectl create cm rq-cm1 -n test --from-literal=key1=value1\n[root@k8s-master01 resourcequota]# kubectl create cm rq-cm2 -n test --from-literal=key2=value2\nerror: failed to create configmap: configmaps &quot;rq-cm2&quot; is forbidden: exceeded quota: resourcequota-test, requested: configmaps=1, used: configmaps=2, limited: configmaps=2\n</code></pre>\n<h4 id=\"2-limitrangeÈÖçÁΩÆËß£Êûê\"><a class=\"anchor\" href=\"#2-limitrangeÈÖçÁΩÆËß£Êûê\">#</a> 2. LimitRange ÈÖçÁΩÆËß£Êûê</h4>\n<p>ËôΩÁÑ∂ ResourceQuota ÂèØ‰ª•ÂÆûÁé∞ËµÑÊ∫êÈÖçÈ¢ùÔºåÂèØ‰ª•ÈôêÂà∂Êüê‰∏™ÂëΩÂêçÁ©∫Èó¥ÂÜÖÂ≠òÂíå CPUÔºå‰ΩÜÊòØÂ¶ÇÊûúÂàõÂª∫ÁöÑ Pod ÈÉΩÊ≤°ÊúâÈÖçÁΩÆ resources ÂèÇÊï∞ÂàôÊó†Ê≥ïÈôêÂà∂„ÄÇÂ¶ÇÊûúÈÖçÁΩÆ LimitRangeÔºåPod Ê≤°ÊúâÈÖçÁΩÆ resources ÊÉÖÂÜµ‰∏ãÔºåÂàõÂª∫ÁöÑ Pod ‰ºöÊ†πÊçÆ LimitRange ÈÖçÁΩÆËá™Âä®Ê∑ªÂä† CPU ÂÜÖÂ≠òÈÖçÁΩÆÔºåÂπ∂‰∏îÂèØ‰ª•ÈôêÂà∂ resources ÂèÇÊï∞ÊúÄÂ§ßÈÖçÁΩÆÂíåÊúÄÂ∞èÈÖçÁΩÆÔºåLimitRange ÈíàÂØπ Pod ËøõË°åÈôêÂà∂„ÄÇ</p>\n<pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§limitsÈÖçÁΩÆ\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§requestÈÖçÁΩÆ\n      cpu: 0.5\n      memory: 256Mi\n    max:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ§ßÈÖçÁΩÆ \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ∞èÈÖçÁΩÆ\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #ÈôêÂà∂pvcÂ§ßÂ∞è\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi\n</code></pre>\n<ul>\n<li>defaultÔºöÈªòËÆ§ limits ÈÖçÁΩÆ</li>\n<li>defaultRequestÔºöÈªòËÆ§ requests ÈÖçÁΩÆ</li>\n</ul>\n<h5 id=\"21-ÈÖçÁΩÆÈªòËÆ§ÁöÑrequestsÂíålimits\"><a class=\"anchor\" href=\"#21-ÈÖçÁΩÆÈªòËÆ§ÁöÑrequestsÂíålimits\">#</a> 2.1 ÈÖçÁΩÆÈªòËÆ§ÁöÑ requests Âíå limits</h5>\n<p>Pod Ê≤°ÊúâÈÖçÁΩÆ resources ÊÉÖÂÜµ‰∏ãÔºåÂàõÂª∫ÁöÑ Pod ‰ºöÊ†πÊçÆ LimitRange ÈÖçÁΩÆËá™Âä®Ê∑ªÂä† CPU ÂÜÖÂ≠òÈÖçÁΩÆ„ÄÇ</p>\n<pre><code>#1.ÂàõÂª∫LimitRange\n[root@k8s-master01 resourcequota]# cat limitrange.yaml \napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§limitsÈÖçÁΩÆ\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§requestÈÖçÁΩÆ\n      cpu: 0.5\n      memory: 256Mi\n    max:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ§ßÈÖçÁΩÆ \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ∞èÈÖçÁΩÆ\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #ÈôêÂà∂pvcÂ§ßÂ∞è\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi  \n      \n[root@k8s-master01 resourcequota]# kubectl apply -f limitrange.yaml\n[root@k8s-master01 resourcequota]# kubectl get limitrange -n test\nNAME                  CREATED AT\ncpu-mem-limit-range   2025-04-23T07:55:03Z\n\n#2.ÂàõÂª∫deployment, Êü•ÁúãÊòØÂê¶‰ºöÊ†πÊçÆLimitRangeËá™Âä®Ê∑ªÂä†CPUÂÜÖÂ≠òÈÖçÁΩÆ\n[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-limirange\n  labels:\n    app: deploy-limirange\n  namespace: test\nspec:\n  selector:\n    matchLabels:\n      app: deploy-limirange\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: deploy-limirange\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: deploy-limirange\n          image: nginx\n          imagePullPolicy: IfNotPresent\n\n[root@k8s-master01 resourcequota]# kubectl get pod -n test\nNAME                                READY   STATUS    RESTARTS   AGE\ndeploy-limirange-854c9545ff-grpxr   1/1     Running   0          39s\n[root@k8s-master01 resourcequota]# kubectl get pod -n test -oyaml\n...\n  spec:\n    containers:\n    - image: nginx\n      imagePullPolicy: IfNotPresent\n      name: deploy-limirange\n      resources:\n        limits:\n          cpu: &quot;1&quot;\n          memory: 512Mi\n        requests:\n          cpu: 500m\n          memory: 256Mi\n...\n</code></pre>\n<h5 id=\"22-ÈôêÂà∂requestsÂíålimitsËåÉÂõ¥\"><a class=\"anchor\" href=\"#22-ÈôêÂà∂requestsÂíålimitsËåÉÂõ¥\">#</a> 2.2 ÈôêÂà∂ requests Âíå limits ËåÉÂõ¥</h5>\n<pre><code>#1.ÂàõÂª∫LimitRange\n[root@k8s-master01 resourcequota]# cat limitrange.yaml \napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§limitsÈÖçÁΩÆ\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§requestÈÖçÁΩÆ\n      cpu: 0.5\n      memory: 256Mi\n    max:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ§ßÈÖçÁΩÆ \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ∞èÈÖçÁΩÆ\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #ÈôêÂà∂pvcÂ§ßÂ∞è\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi  \n\n#2.ÂàõÂª∫deployment, CPUÂÜÖÂ≠òlimitsÂíårequestsÈ´ò‰∫é/‰Ωé‰∫éLimitRangeCPUÂÜÖÂ≠òmax„ÄÅminÈÖçÁΩÆ\n[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-limirange\n  labels:\n    app: deploy-limirange\n  namespace: test\nspec:\n  selector:\n    matchLabels:\n      app: deploy-limirange\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: deploy-limirange\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: deploy-limirange\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 8096Mi\n              cpu: 5\n            requests:\n              memory: 64Mi\n              cpu: 10m\n\n#3.Áî±‰∫éÂàõÂª∫deployment, CPUÂÜÖÂ≠òlimitsÂíårequestsÈ´ò‰∫é/‰Ωé‰∫éLimitRangeCPUÂÜÖÂ≠òmax„ÄÅminÈÖçÁΩÆÔºåpodÊ≤°ÊúâÂàõÂª∫\n[root@k8s-master01 resourcequota]# kubectl create -f deploy-limitrange.yaml \n\n[root@k8s-master01 resourcequota]# kubectl get deploy deploy-limirange -n test\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\ndeploy-limirange   0/1     0            0           2m7s\n[root@k8s-master01 resourcequota]# kubectl get pods -n test\n\n[root@k8s-master01 resourcequota]# kubectl describe rs deploy-limirange-54c5d69b4b -n test\nName:           deploy-limirange-54c5d69b4b\nNamespace:      test\nSelector:       app=deploy-limirange,pod-template-hash=54c5d69b4b\nLabels:         app=deploy-limirange\n                pod-template-hash=54c5d69b4b\nAnnotations:    deployment.kubernetes.io/desired-replicas: 1\n                deployment.kubernetes.io/max-replicas: 2\n                deployment.kubernetes.io/revision: 1\nControlled By:  Deployment/deploy-limirange\nReplicas:       0 current / 1 desired\nPods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=deploy-limirange\n           pod-template-hash=54c5d69b4b\n  Containers:\n   deploy-limirange:\n    Image:      nginx\n    Port:       &lt;none&gt;\n    Host Port:  &lt;none&gt;\n    Limits:\n      cpu:     5\n      memory:  8096Mi\n    Requests:\n      cpu:         10m\n      memory:      64Mi\n    Environment:   &lt;none&gt;\n    Mounts:        &lt;none&gt;\n  Volumes:         &lt;none&gt;\n  Node-Selectors:  &lt;none&gt;\n  Tolerations:     &lt;none&gt;\nConditions:\n  Type             Status  Reason\n  ----             ------  ------\n  ReplicaFailure   True    FailedCreate\nEvents:\n  Type     Reason        Age                 From                   Message\n  ----     ------        ----                ----                   -------\n  Warning  FailedCreate  3m8s                replicaset-controller  Error creating: pods &quot;deploy-limirange-54c5d69b4b-zxhzk&quot; is forbidden: [minimum cpu usage per Container is 100m, but request is 10m, minimum memory usage per Container is 100Mi, but request is 64Mi, maximum cpu usage per Container is 4, but limit is 5, maximum memory usage per Container is 4Gi, but limit is 8096Mi]\n</code></pre>\n<h5 id=\"23-ÈôêÂà∂Â≠òÂÇ®Á©∫Èó¥Â§ßÂ∞è\"><a class=\"anchor\" href=\"#23-ÈôêÂà∂Â≠òÂÇ®Á©∫Èó¥Â§ßÂ∞è\">#</a> 2.3 ÈôêÂà∂Â≠òÂÇ®Á©∫Èó¥Â§ßÂ∞è</h5>\n<pre><code>#1.ÂàõÂª∫LimitRange\n[root@k8s-master01 resourcequota]# cat limitrange.yaml \napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-mem-limit-range\n  namespace: test\nspec:\n  limits:\n  - default:         #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§limitsÈÖçÁΩÆ\n      cpu: 1\n      memory: 512Mi\n    defaultRequest:  #ÈôêÂà∂CPUÂÜÖÂ≠òÈªòËÆ§requestÈÖçÁΩÆ\n      cpu: 0.5\n      memory: 256Mi\n    max:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ§ßÈÖçÁΩÆ \n      cpu: &quot;4000m&quot;\n      memory: 4Gi\n    min:                #ÈôêÂà∂CPUÂÜÖÂ≠òÊúÄÂ∞èÈÖçÁΩÆ\n      cpu: &quot;100m&quot;\n      memory: 100Mi\n    type: Container\n  - type: PersistentVolumeClaim    #ÈôêÂà∂pvcÂ§ßÂ∞è\n    max:\n      storage: 2Gi\n    min:\n      storage: 1Gi  \n  \n#2.Áî±‰∫éÂàõÂª∫ÁöÑpvcÂ§ß‰∫é2GÔºåÊâÄ‰ª•Êä•Èîô  \n[root@k8s-master01 ~]# cat pvc.yaml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sc-pvc-001\nspec:\n  storageClassName: &quot;nfs-storage&quot;     # ÊòéÁ°ÆÊåáÂÆö‰ΩøÁî®Âì™‰∏™scÁöÑ‰æõÂ∫îÂïÜÊù•ÂàõÂª∫pv\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 3Gi                      # Ê†πÊçÆ‰∏öÂä°ÂÆûÈôÖÂ§ßÂ∞èËøõË°åËµÑÊ∫êÁî≥ËØ∑  \n[root@k8s-master01 ~]# kubectl create -f pvc.yaml -n test\nError from server (Forbidden): error when creating &quot;pvc.yaml&quot;: persistentvolumeclaims &quot;sc-pvc-001&quot; is forbidden: maximum storage usage per PersistentVolumeClaim is 2Gi, but request is 3Gi\n</code></pre>\n<h4 id=\"3-ÊúçÂä°Ë¥®Èáè-qos\"><a class=\"anchor\" href=\"#3-ÊúçÂä°Ë¥®Èáè-qos\">#</a> 3. ÊúçÂä°Ë¥®Èáè QoS</h4>\n<ul>\n<li>GuaranteedÔºöÊúÄÈ´òÊúçÂä°Ë¥®ÈáèÔºåÂΩìÂÆø‰∏ªÊú∫ÂÜÖÂ≠ò‰∏çÂ§üÊó∂Ôºå‰ºöÂÖà kill Êéâ QoS ‰∏∫ BestEffort Âíå Burstable ÁöÑ PodÔºåÂ¶ÇÊûúÂÜÖÂ≠òËøòÊòØ‰∏çÂ§üÔºåÊâç‰ºö kill Êéâ QoS ‰∏∫ GuaranteedÔºåËØ•Á∫ßÂà´ Pod ÁöÑËµÑÊ∫êÂç†Áî®Èáè‰∏ÄËà¨ÊØîËæÉÊòéÁ°ÆÔºåÂç≥ requests ÁöÑ cpu Âíå memory Âíå limits ÁöÑ cpu Âíå memory ÈÖçÁΩÆÁöÑ‰∏ÄËá¥„ÄÇ</li>\n<li>BurstableÔºö ÊúçÂä°Ë¥®Èáè‰Ωé‰∫é GuaranteedÔºåÂΩìÂÆø‰∏ªÊú∫ÂÜÖÂ≠ò‰∏çÂ§üÊó∂Ôºå‰ºöÂÖà kill Êéâ QoS ‰∏∫ BestEffort ÁöÑ PodÔºåÂ¶ÇÊûúÂÜÖÂ≠òËøòÊòØ‰∏çÂ§ü‰πãÂêéÂ∞±‰ºö kill Êéâ QoS Á∫ßÂà´‰∏∫ Burstable ÁöÑ PodÔºåÁî®Êù•‰øùËØÅ QoS Ë¥®Èáè‰∏∫ Guaranteed ÁöÑ PodÔºåËØ•Á∫ßÂà´ Pod ‰∏ÄËà¨Áü•ÈÅìÊúÄÂ∞èËµÑÊ∫ê‰ΩøÁî®ÈáèÔºå‰ΩÜÊòØÂΩìÊú∫Âô®ËµÑÊ∫êÂÖÖË∂≥Êó∂ÔºåËøòÊòØÊÉ≥Â∞ΩÂèØËÉΩÁöÑ‰ΩøÁî®Êõ¥Â§öÁöÑËµÑÊ∫êÔºåÂç≥ limits Â≠óÊÆµÁöÑ cpu Âíå memory Â§ß‰∫é requests ÁöÑ cpu Âíå memory ÁöÑÈÖçÁΩÆ„ÄÇ</li>\n<li>BestEffortÔºöÂ∞ΩÂäõËÄå‰∏∫ÔºåÂΩìÂÆø‰∏ªÊú∫ÂÜÖÂ≠ò‰∏çÂ§üÊó∂ÔºåÈ¶ñÂÖà kill ÁöÑÂ∞±ÊòØËØ• QoS ÁöÑ PodÔºåÁî®‰ª•‰øùËØÅ Burstable Âíå Guaranteed Á∫ßÂà´ÁöÑ Pod Ê≠£Â∏∏ËøêË°å„ÄÇ</li>\n</ul>\n<h5 id=\"31-ÂÆûÁé∞qos‰∏∫guaranteedÁöÑpod\"><a class=\"anchor\" href=\"#31-ÂÆûÁé∞qos‰∏∫guaranteedÁöÑpod\">#</a> 3.1 ÂÆûÁé∞ QoS ‰∏∫ Guaranteed ÁöÑ Pod</h5>\n<ol>\n<li>\n<p>Pod ‰∏≠ÁöÑÊØè‰∏™ÂÆπÂô®ÂøÖÈ°ªÊåáÂÆö limits.memory Âíå requests.memoryÔºåÂπ∂‰∏î‰∏§ËÄÖÈúÄË¶ÅÁõ∏Á≠âÔºõ</p>\n</li>\n<li>\n<p>Pod ‰∏≠ÁöÑÊØè‰∏™ÂÆπÂô®ÂøÖÈ°ªÊåáÂÆö limits.cpu Âíå limits.memoryÔºåÂπ∂‰∏î‰∏§ËÄÖÈúÄË¶ÅÁõ∏Á≠â„ÄÇ</p>\n</li>\n</ol>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 1024Mi\n              cpu: 1\n</code></pre>\n<h5 id=\"32-ÂÆûÁé∞qos‰∏∫burstableÁöÑpod\"><a class=\"anchor\" href=\"#32-ÂÆûÁé∞qos‰∏∫burstableÁöÑpod\">#</a> 3.2 ÂÆûÁé∞ QoS ‰∏∫ Burstable ÁöÑ Pod</h5>\n<ol>\n<li>\n<p>Pod ‰∏çÁ¨¶Âêà Guaranteed ÁöÑÈÖçÁΩÆË¶ÅÊ±ÇÔºõ</p>\n</li>\n<li>\n<p>Pod ‰∏≠Ëá≥Â∞ëÊúâ‰∏Ä‰∏™ÂÆπÂô®ÈÖçÁΩÆ‰∫Ü requests.cpu Êàñ requests.memory„ÄÇ</p>\n</li>\n</ol>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<h5 id=\"33-ÂÆûÁé∞qos‰∏∫besteffortÁöÑpod\"><a class=\"anchor\" href=\"#33-ÂÆûÁé∞qos‰∏∫besteffortÁöÑpod\">#</a> 3.3 ÂÆûÁé∞ QoS ‰∏∫ BestEffort ÁöÑ Pod</h5>\n<ol>\n<li>‰∏çËÆæÁΩÆ resources ÂèÇÊï∞</li>\n</ol>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/312010518.html",
            "url": "http://ixuyong.cn/posts/312010518.html",
            "title": "K8s‰∫≤ÂíåÂäõAffinity",
            "date_published": "2025-04-20T09:59:58.000Z",
            "content_html": "<h3 id=\"k8s‰∫≤ÂíåÂäõaffinity\"><a class=\"anchor\" href=\"#k8s‰∫≤ÂíåÂäõaffinity\">#</a> K8s ‰∫≤ÂíåÂäõ Affinity</h3>\n<p>Pod ÂíåËäÇÁÇπ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºö</p>\n<ul>\n<li>Êüê‰∫õ Pod ‰ºòÂÖàÈÄâÊã©Êúâ ssd=true Ê†áÁ≠æÁöÑËäÇÁÇπÔºåÂ¶ÇÊûúÊ≤°ÊúâÂú®ËÄÉËôëÈÉ®ÁΩ≤Âà∞ÂÖ∂ÂÆÉËäÇÁÇπÔºõ</li>\n<li>Êüê‰∫õ Pod ÈúÄË¶ÅÈÉ®ÁΩ≤Âú® ssd=true Âíå type=physical ÁöÑËäÇÁÇπ‰∏äÔºå‰ΩÜÊòØ‰ºòÂÖàÈÉ®ÁΩ≤Âú® ssd=true ÁöÑËäÇÁÇπ‰∏ä„ÄÇ</li>\n</ul>\n<p>Pod Âíå Pod ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºö</p>\n<ul>\n<li>Âêå‰∏Ä‰∏™Â∫îÁî®ÁöÑ Pod ‰∏çÂêåÁöÑÂâØÊú¨ÊàñËÄÖÂêå‰∏Ä‰∏™È°πÁõÆÁöÑÂ∫îÁî®Â∞ΩÈáèÊàñÂøÖÈ°ª‰∏çÈÉ®ÁΩ≤Âú®Âêå‰∏Ä‰∏™ËäÇÁÇπÊàñËÄÖÁ¨¶ÂêàÊüê‰∏™Ê†áÁ≠æÁöÑ‰∏ÄÁ±ªËäÇÁÇπ‰∏äÊàñËÄÖ‰∏çÂêåÁöÑÂå∫ÂüüÔºõ</li>\n<li>Áõ∏‰∫í‰æùËµñÁöÑ‰∏§‰∏™ Pod Â∞ΩÈáèÊàñÂøÖÈ°ªÈÉ®ÁΩ≤Âú®Âêå‰∏Ä‰∏™ËäÇÁÇπ‰∏äÊàñËÄÖÂêå‰∏Ä‰∏™ÂüüÂÜÖ„ÄÇ</li>\n</ul>\n<h4 id=\"1-affinityÂàÜÁ±ª\"><a class=\"anchor\" href=\"#1-affinityÂàÜÁ±ª\">#</a> 1. Affinity ÂàÜÁ±ª</h4>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/hTd0wmD.png\" alt=\"1.png\" /></p>\n<h4 id=\"2-ËäÇÁÇπ‰∫≤ÂíåÂäõÈÖçÁΩÆËØ¶Ëß£\"><a class=\"anchor\" href=\"#2-ËäÇÁÇπ‰∫≤ÂíåÂäõÈÖçÁΩÆËØ¶Ëß£\">#</a> 2. ËäÇÁÇπ‰∫≤ÂíåÂäõÈÖçÁΩÆËØ¶Ëß£</h4>\n<h5 id=\"21-Á°¨‰∫≤ÂíåÂäõrequired\"><a class=\"anchor\" href=\"#21-Á°¨‰∫≤ÂíåÂäõrequired\">#</a> 2.1 Á°¨‰∫≤ÂíåÂäõ required</h5>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: kubernetes.io/hostname\n                    operator: In\n                    values:\n                      - k8s-node01\n                      - k8s-node02\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<ul>\n<li>requiredDuringSchedulingIgnoredDuringExecutionÔºöÁ°¨‰∫≤ÂíåÂäõÈÖçÁΩÆ</li>\n<li>nodeSelectorTermsÔºöËäÇÁÇπÈÄâÊã©Âô®ÈÖçÁΩÆÔºåÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™ matchExpressionsÔºàÊª°Ë∂≥ÂÖ∂‰∏ÄÂç≥ÂèØÔºâ</li>\n<li>matchExpressionsÔºömatchExpressions ‰∏ãÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™ key„ÄÅvaluesÔºàÈÉΩÈúÄË¶ÅÊª°Ë∂≥ÔºâÔºåÂÖ∂‰∏≠ values ÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™ÔºàÊª°Ë∂≥ÂÖ∂‰∏ÄÂç≥ÂèØÔºâ</li>\n<li>operatorÔºö\n<ul>\n<li>IN Áõ∏ÂΩì‰∫é key = value ÁöÑÂΩ¢ÂºèÔºå<strong>NotIn Áõ∏ÂΩì‰∫é key!=value ÁöÑÂΩ¢Âºè (Âèç‰∫≤ÂíåÂäõ)</strong></li>\n<li>Exists: ËäÇÁÇπÂ≠òÂú® label ÁöÑ key ‰∏∫ÊåáÂÆöÁöÑÂÄºÂç≥ÂèØÔºå‰∏çËÉΩÈÖçÁΩÆ values Â≠óÊÆµ</li>\n<li>DoesNotExist: ËäÇÁÇπ‰∏çÂ≠òÂú® label ÁöÑ key ‰∏∫ÊåáÂÆöÁöÑÂÄºÂç≥ÂèØÔºå‰∏çËÉΩÈÖçÁΩÆ values Â≠óÊÆµ</li>\n<li>GtÔºöÂ§ß‰∫é value ÊåáÂÆöÁöÑÂÄº</li>\n<li>LtÔºöÂ∞è‰∫é value ÊåáÂÆöÁöÑÂÄº</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"22-ËΩØ‰∫≤ÂíåÂäõpreferred\"><a class=\"anchor\" href=\"#22-ËΩØ‰∫≤ÂíåÂäõpreferred\">#</a> 2.2 ËΩØ‰∫≤ÂíåÂäõ preferred</h5>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 6\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              preference:\n                matchExpressions:\n                  - key: ssd\n                    operator: In\n                    values:\n                      - 'true'\n            - weight: 50\n              preference:\n                matchExpressions:\n                  - key: kubernetes.io/hostname\n                    operator: In\n                    values:\n                      - k8s-master01\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<ul>\n<li>preferredDuringSchedulingIgnoredDuringExecutionÔºöËΩØ‰∫≤ÂíåÂäõÈÖçÁΩÆ</li>\n<li>weightÔºöËΩØ‰∫≤ÂíåÂäõÁöÑÊùÉÈáçÔºåÊùÉÈáçË∂äÈ´ò‰ºòÂÖàÁ∫ßË∂äÂ§ßÔºåËåÉÂõ¥ 1-100</li>\n<li>matchExpressionsÔºömatchExpressions ‰∏ãÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™ key„ÄÅvaluesÔºàÈÉΩÈúÄË¶ÅÊª°Ë∂≥ÔºâÔºåÂÖ∂‰∏≠ values ÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™ÔºàÊª°Ë∂≥ÂÖ∂‰∏ÄÂç≥ÂèØÔºâ</li>\n<li>operatorÔºö\n<ul>\n<li>IN Áõ∏ÂΩì‰∫é key = value ÁöÑÂΩ¢ÂºèÔºå<strong>NotIn Áõ∏ÂΩì‰∫é key!=value ÁöÑÂΩ¢Âºè (Âèç‰∫≤ÂíåÂäõ)</strong></li>\n<li>Exists: ËäÇÁÇπÂ≠òÂú® label ÁöÑ key ‰∏∫ÊåáÂÆöÁöÑÂÄºÂç≥ÂèØÔºå‰∏çËÉΩÈÖçÁΩÆ values Â≠óÊÆµ</li>\n<li>DoesNotExist: ËäÇÁÇπ‰∏çÂ≠òÂú® label ÁöÑ key ‰∏∫ÊåáÂÆöÁöÑÂÄºÂç≥ÂèØÔºå‰∏çËÉΩÈÖçÁΩÆ values Â≠óÊÆµ</li>\n<li>GtÔºöÂ§ß‰∫é value ÊåáÂÆöÁöÑÂÄº</li>\n<li>LtÔºöÂ∞è‰∫é value ÊåáÂÆöÁöÑÂÄº</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"3-pod‰∫≤ÂíåÂäõËØ¶Ëß£\"><a class=\"anchor\" href=\"#3-pod‰∫≤ÂíåÂäõËØ¶Ëß£\">#</a> 3. Pod ‰∫≤ÂíåÂäõËØ¶Ëß£</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:              \n        podAntiAffinity:   #podÁ°¨Âèç‰∫≤ÂíåÂäõ\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - nginx-deploy\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:       #podËΩØÂèç‰∫≤ÂíåÂäõ\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - nginx-deploy\n              namespaces:     #ÂíåÂì™‰∏™ÂëΩÂêçÁ©∫Èó¥ÁöÑPodËøõË°åÂåπÈÖçÔºå‰∏∫Á©∫‰∏∫ÂΩìÂâçÂëΩÂêçÁ©∫Èó¥\n              - default\n              topologyKey: kubernetes.io/hostname\n</code></pre>\n<ul>\n<li>\n<p>labelSelectorÔºöPod ÈÄâÊã©Âô®ÈÖçÁΩÆÔºåÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™</p>\n</li>\n<li>\n<p>matchExpressionsÔºömatchExpressions ‰∏ãÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™ key„ÄÅvaluesÔºàÈÉΩÈúÄË¶ÅÊª°Ë∂≥ÔºâÔºåÂÖ∂‰∏≠ values ÂèØ‰ª•ÈÖçÁΩÆÂ§ö‰∏™ÔºàÊª°Ë∂≥ÂÖ∂‰∏ÄÂç≥ÂèØÔºâ</p>\n</li>\n<li>\n<p>topologyKeyÔºöÂåπÈÖçÁöÑÊãìÊâëÂüüÁöÑ keyÔºå‰πüÂ∞±ÊòØËäÇÁÇπ‰∏ä label ÁöÑ keyÔºåkey Âíå value Áõ∏ÂêåÁöÑ‰∏∫Âêå‰∏Ä‰∏™ÂüüÔºåÂèØ‰ª•Áî®‰∫éÊ†áÊ≥®‰∏çÂêåÁöÑÊú∫ÊàøÂíåÂú∞Âå∫</p>\n</li>\n<li>\n<p>Namespaces: ÂíåÂì™‰∏™ÂëΩÂêçÁ©∫Èó¥ÁöÑ Pod ËøõË°åÂåπÈÖçÔºå‰∏∫Á©∫‰∏∫ÂΩìÂâçÂëΩÂêçÁ©∫Èó¥</p>\n</li>\n<li>\n<p>operatorÔºöÈÖçÁΩÆÂíåËäÇÁÇπ‰∫≤ÂíåÂäõ‰∏ÄËá¥Ôºå‰ΩÜÊòØÊ≤°Êúâ Gt Âíå Lt</p>\n<ul>\n<li>\n<p>IN Áõ∏ÂΩì‰∫é key = value ÁöÑÂΩ¢ÂºèÔºõ</p>\n</li>\n<li>\n<p>Exists: ËäÇÁÇπÂ≠òÂú® label ÁöÑ key ‰∏∫ÊåáÂÆöÁöÑÂÄºÂç≥ÂèØÔºå‰∏çËÉΩÈÖçÁΩÆ values Â≠óÊÆµÔºõ</p>\n</li>\n<li>\n<p>DoesNotExist: ËäÇÁÇπ‰∏çÂ≠òÂú® label ÁöÑ key ‰∏∫ÊåáÂÆöÁöÑÂÄºÂç≥ÂèØÔºå‰∏çËÉΩÈÖçÁΩÆ values Â≠óÊÆµ</p>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"4-ËäÇÁÇπ‰∫≤ÂíåÂäõÈÖçÁΩÆÁ§∫‰æã\"><a class=\"anchor\" href=\"#4-ËäÇÁÇπ‰∫≤ÂíåÂäõÈÖçÁΩÆÁ§∫‰æã\">#</a> 4. ËäÇÁÇπ‰∫≤ÂíåÂäõÈÖçÁΩÆÁ§∫‰æã</h4>\n<p>Pod Â∞ΩÈáèÈÉ®ÁΩ≤Âú® ssd=true Âíå type=physical ÁöÑËäÇÁÇπ‰∏äÔºå‰ΩÜÊòØ‰ºòÂÖàÈÉ®ÁΩ≤Âú® ssd=true ÁöÑËäÇÁÇπ‰∏äÔºå‰∏çËÉΩÈÉ®ÁΩ≤ label ‰∏∫ gpu=true ÁöÑËäÇÁÇπ„ÄÇ</p>\n<pre><code>[root@k8s-master01 ~]# kubectl label nodes k8s-node01 ssd=true\n[root@k8s-master01 ~]# kubectl label nodes k8s-master01 ssd=true\n[root@k8s-master01 ~]# kubectl label nodes k8s-master01 gpu=true\n[root@k8s-master01 ~]# kubectl label nodes k8s-node02 type=physical\n\n[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n      annotations:\n        app: nginx-deploy\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              preference:\n                matchExpressions:\n                  - key: ssd\n                    operator: In\n                    values:\n                      - 'true'\n                  - key: gpu\n                    operator: NotIn\n                    values:\n                      - 'true'\n            - weight: 50\n              preference:\n                matchExpressions:\n                  - key: type\n                    operator: In\n                    values:\n                      - physical\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n\n\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nnginx-deploy-7d65fbdf-2b4jr   1/1     Running   0          5s    172.16.85.236   k8s-node01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-jjzwr   1/1     Running   0          5s    172.16.58.251   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-kx5lm   1/1     Running   0          5s    172.16.85.237   k8s-node01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-lrmcg   1/1     Running   0          5s    172.16.85.238   k8s-node01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7d65fbdf-n6mlp   1/1     Running   0          5s    172.16.58.250   k8s-node02   &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<h4 id=\"5-pod‰∫≤ÂíåÂäõ-Âèç‰∫≤ÂíåÂäõÈÖçÁΩÆÁ§∫‰æã\"><a class=\"anchor\" href=\"#5-pod‰∫≤ÂíåÂäõ-Âèç‰∫≤ÂíåÂäõÈÖçÁΩÆÁ§∫‰æã\">#</a> 5. Pod ‰∫≤ÂíåÂäõ„ÄÅÂèç‰∫≤ÂíåÂäõÈÖçÁΩÆÁ§∫‰æã</h4>\n<h5 id=\"51-podÂèç‰∫≤ÂíåÂäõrequired\"><a class=\"anchor\" href=\"#51-podÂèç‰∫≤ÂíåÂäõrequired\">#</a> 5.1 Pod Âèç‰∫≤ÂíåÂäõ required</h5>\n<p>Âêå‰∏Ä‰∏™Â∫îÁî®ÈÉ®ÁΩ≤Âú®‰∏çÂêåÁöÑÂÆø‰∏ªÊú∫</p>\n<pre><code>#1.ËäÇÁÇπÂ≠òÂú®Ê±°ÁÇπpodÊó†Ê≥ïË∞ÉÂ∫¶Ëá≥ËØ•ËäÇÁÇπ\n# kubectl describe nodes|grep -i taint\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\n\n#2.podÂèç‰∫≤ÂíåÂäõrequired\n# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: app\n                    operator: In\n                    values:\n                      - nginx-deploy\n              topologyKey: kubernetes.io/hostname\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n\n#3.ÈÉ®ÁΩ≤deployment\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-deploy-5787887b6f-4654b   1/1     Running   0          4s    172.16.85.234    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4s    172.16.122.152   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-fdkft   1/1     Running   0          4s    172.16.58.247    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4s    172.16.32.152    k8s-master01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4s    172.16.195.14    k8s-master03   &lt;none&gt;           &lt;none&gt;\n\n#4.Â∞ÜÂâØÊú¨Êâ©Êàê6‰∏™ÔºåÁî±‰∫éK8sÈõÜÁæ§Âè™Êúâ5‰∏™ËäÇÁÇπÔºåÂç≥5‰∏™topologyKeyÔºàÊãìÊâëÂüüÔºâÔºåÊØè‰∏™ÂüüÂè™ËÉΩÊúâ‰∏Ä‰∏™ÂâØÊú¨ÔºåÊâÄ‰ª•Êúâ‰∏Ä‰∏™pod‰ºöpending\n[root@k8s-master01 ~]# kubectl scale deploy nginx-deploy --replicas=6 \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                            READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-deploy-5787887b6f-4654b   1/1     Running   0          4m44s   172.16.85.234    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4m44s   172.16.122.152   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-fdkft   1/1     Running   0          4m44s   172.16.58.247    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4m44s   172.16.32.152    k8s-master01   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4m44s   172.16.195.14    k8s-master03   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-5787887b6f-sztm7   0/1     Pending   0          9s      &lt;none&gt;           &lt;none&gt;         &lt;none&gt;           &lt;none&gt;\n\n[root@k8s-master01 ~]# kubectl describe pods nginx-deploy-5787887b6f-sztm7\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  102s  default-scheduler  0/5 nodes are available: 5 node(s) didn't match pod anti-affinity rules. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.\n</code></pre>\n<p><strong>Â∞ÜÂâØÊú¨Êâ©Êàê 6 ‰∏™ÔºåÊúâ‰∏Ä‰∏™‰ºö pending Áä∂ÊÄÅÔºåÂéüÂõ† K8s ÈõÜÁæ§Âè™Êúâ 5 ‰∏™ËäÇÁÇπÔºåÂç≥ 5 ‰∏™ topologyKeyÔºàÊãìÊâëÂüüÔºâÔºåÊØè‰∏™ÊãìÊâëÂüüÂè™ËÉΩÊúâ‰∏Ä‰∏™ÂâØÊú¨ÔºåÊâÄ‰ª•Êúâ‰∏Ä‰∏™ pod ‰ºö pending„ÄÇ</strong></p>\n<p><strong>topologyKeyÔºöÂåπÈÖçÁöÑÊãìÊâëÂüüÁöÑ keyÔºå‰πüÂ∞±ÊòØËäÇÁÇπ‰∏ä label ÁöÑ keyÔºåkey Âíå value Áõ∏ÂêåÁöÑ‰∏∫Âêå‰∏Ä‰∏™ÂüüÔºåÂèØ‰ª•Áî®‰∫éÊ†áÊ≥®‰∏çÂêåÁöÑÊú∫ÊàøÂíåÂú∞Âå∫</strong>„ÄÇ</p>\n<h5 id=\"52-podÂèç‰∫≤ÂíåÂäõpreferred\"><a class=\"anchor\" href=\"#52-podÂèç‰∫≤ÂíåÂäõpreferred\">#</a> 5.2 Pod Âèç‰∫≤ÂíåÂäõ preferred</h5>\n<p>Âêå‰∏Ä‰∏™Â∫îÁî®Â∞ΩÈáèÈÉ®ÁΩ≤Âú®‰∏çÂêåÁöÑÂÆø‰∏ªÊú∫</p>\n<pre><code>#1.ËäÇÁÇπÂ≠òÂú®Ê±°ÁÇπpodÊó†Ê≥ïË∞ÉÂ∫¶Ëá≥ËØ•ËäÇÁÇπ\n# kubectl describe nodes|grep -i taint\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\nTaints:             &lt;none&gt;\n\n#2.podÂèç‰∫≤ÂíåÂäõpreferred\n# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 6\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - podAffinityTerm:\n                labelSelector:\n                  matchExpressions:\n                    - key: app\n                      operator: In\n                      values:\n                        - nginx-deploy\n                topologyKey: kubernetes.io/hostname\n              weight: 100\n      restartPolicy: Always\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n      volumes:\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n\n#3.ÈÉ®ÁΩ≤deployment\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-deploy-7c47567b79-97qs5   1/1     Running   0          6s    172.16.122.153   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-g49h4   1/1     Running   0          6s    172.16.85.235    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-g5n2s   1/1     Running   0          6s    172.16.58.248    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-g5v5b   1/1     Running   0          6s    172.16.195.15    k8s-master03   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-pjwws   1/1     Running   0          6s    172.16.58.249    k8s-node02     &lt;none&gt;           &lt;none&gt;\nnginx-deploy-7c47567b79-q2hn5   1/1     Running   0          6s    172.16.32.153    k8s-master01   &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<h5 id=\"53-pod‰∫≤ÂíåÂäõrequired\"><a class=\"anchor\" href=\"#53-pod‰∫≤ÂíåÂäõrequired\">#</a> 5.3 Pod ‰∫≤ÂíåÂäõ required</h5>\n<p>Âêå‰∏Ä‰∏™Â∫îÁî®ÂøÖÈ°ªÈÉ®ÁΩ≤Âú®Âêå‰∏Ä‰∏™ÂÆø‰∏ªÊú∫</p>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 8\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:              \n        podAffinity:   #podÁ°¨‰∫≤ÂíåÂäõ\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - nginx-deploy\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n      volumes:\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n\n[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml \n[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES\nnginx-deploy-dbcc4d65c-2sthn   1/1     Running   0          12s   172.16.58.255   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-78nxf   1/1     Running   0          12s   172.16.58.197   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-82ssq   1/1     Running   0          12s   172.16.58.194   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-986cb   1/1     Running   0          12s   172.16.58.254   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-9rnt7   1/1     Running   0          12s   172.16.58.252   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-knm8q   1/1     Running   0          12s   172.16.58.195   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-kx56f   1/1     Running   0          12s   172.16.58.253   k8s-node02   &lt;none&gt;           &lt;none&gt;\nnginx-deploy-dbcc4d65c-sqlhf   1/1     Running   0          12s   172.16.58.198   k8s-node02   &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<h5 id=\"54-pod‰∫≤ÂíåÂäõpreferre\"><a class=\"anchor\" href=\"#54-pod‰∫≤ÂíåÂäõpreferre\">#</a> 5.4 Pod ‰∫≤ÂíåÂäõ preferre</h5>\n<p>Âêå‰∏Ä‰∏™Â∫îÁî®Â∞ΩÈáèÈÉ®ÁΩ≤Âú®Âêå‰∏Ä‰∏™ÂÆø‰∏ªÊú∫</p>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 20\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      affinity:              \n        podAffinity:       #podËΩØ‰∫≤ÂíåÂäõ\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - nginx-deploy\n              namespaces:     #ÂíåÂì™‰∏™ÂëΩÂêçÁ©∫Èó¥ÁöÑPodËøõË°åÂåπÈÖçÔºå‰∏∫Á©∫‰∏∫ÂΩìÂâçÂëΩÂêçÁ©∫Èó¥\n              - default\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n      volumes:\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3254599477.html",
            "url": "http://ixuyong.cn/posts/3254599477.html",
            "title": "K8sÂÆπÂøçÂíåÊ±°ÁÇπ",
            "date_published": "2025-04-20T07:51:58.000Z",
            "content_html": "<h3 id=\"k8sÂÆπÂøçÂíåÊ±°ÁÇπ\"><a class=\"anchor\" href=\"#k8sÂÆπÂøçÂíåÊ±°ÁÇπ\">#</a> K8s ÂÆπÂøçÂíåÊ±°ÁÇπ</h3>\n<p>Taint ÊåáÂÆöÊúçÂä°Âô®‰∏äÊâì‰∏äÊ±°ÁÇπÔºåËÆ©‰∏çËÉΩÂÆπÂøçËøô‰∏™Ê±°ÁÇπÁöÑ Pod ‰∏çËÉΩÈÉ®ÁΩ≤Âú®Êâì‰∫ÜÊ±°ÁÇπÁöÑÊúçÂä°Âô®‰∏ä„ÄÇToleration ÊòØËÆ© Pod ÂÆπÂøçËäÇÁÇπ‰∏äÈÖçÁΩÆÁöÑÊ±°ÁÇπÔºåÂèØ‰ª•ËÆ©‰∏Ä‰∫õÈúÄË¶ÅÁâπÊÆäÈÖçÁΩÆÁöÑ Pod ËÉΩÂ§üË∞ÉÁî®Âà∞ÂÖ∑ÊúâÊ±°ÁÇπÂíåÁâπÊÆäÈÖçÁΩÆÁöÑËäÇÁÇπ‰∏ä„ÄÇ</p>\n<h4 id=\"1-taintÈÖçÁΩÆËß£Êûê\"><a class=\"anchor\" href=\"#1-taintÈÖçÁΩÆËß£Êûê\">#</a> 1. Taint ÈÖçÁΩÆËß£Êûê</h4>\n<pre><code>#1.TaintËØ≠Ê≥ï\n# kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT\n\n#2.ÂàõÂª∫TaintÁ§∫‰æã\n# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule\n\n#3.Êü•ÁúãÊ±°ÁÇπ\n# kubectl describe node k8s-node01 | grep Taints -A 10\n\n#4.Âà†Èô§Ê±°ÁÇπ\n# kubectl taint nodes k8s-node01 ssd-                   #Âü∫‰∫éKeyÂà†Èô§\n# kubectl taint nodes k8s-node01 ssd:PreferNoSchedule-  #Âü∫‰∫éKey+EffectÂà†Èô§\n\n#5.‰øÆÊîπÊ±°ÁÇπÔºàKeyÂíåEffectÁõ∏ÂêåÔºâ\n# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule --overwrite\n</code></pre>\n<p>EFFECT ÊéíÊñ•Á≠âÁ∫ßÔºö</p>\n<ul>\n<li>NoScheduleÔºöÁ¶ÅÊ≠¢Ë∞ÉÂ∫¶Âà∞ËØ•ËäÇÁÇπÔºåÂ∑≤ÁªèÂú®ËØ•ËäÇÁÇπ‰∏äÁöÑ Pod ‰∏çÂèóÂΩ±Âìç</li>\n<li>NoExecuteÔºöÁ¶ÅÊ≠¢Ë∞ÉÂ∫¶Âà∞ËØ•ËäÇÁÇπÔºåÂ¶ÇÊûú‰∏çÁ¨¶ÂêàËøô‰∏™Ê±°ÁÇπÔºå‰ºöÁ´ãÈ©¨Ë¢´È©±ÈÄêÔºàÊàñÂú®‰∏ÄÊÆµÊó∂Èó¥ÂêéÔºâ</li>\n<li>PreferNoScheduleÔºöÂ∞ΩÈáèÈÅøÂÖçÂ∞Ü Pod Ë∞ÉÂ∫¶Âà∞ÊåáÂÆöÁöÑËäÇÁÇπ‰∏äÔºåÂ¶ÇÊûúÊ≤°ÊúâÊõ¥ÂêàÈÄÇÁöÑËäÇÁÇπÔºåÂèØ‰ª•ÈÉ®ÁΩ≤Âà∞ËØ•ËäÇÁÇπ</li>\n</ul>\n<h4 id=\"2tolerationÈÖçÁΩÆËß£Êûê\"><a class=\"anchor\" href=\"#2tolerationÈÖçÁΩÆËß£Êûê\">#</a> 2.Toleration ÈÖçÁΩÆËß£Êûê</h4>\n<pre><code>#1.ÂÆåÂÖ®ÂåπÈÖç\ntolerations:\n- key: &quot;taintKey&quot;\n  operator: &quot;Equal&quot;\n  value: &quot;taintValue&quot;\n  effect: &quot;NoSchedule\n \n#2.‰∏çÂÆåÂÖ®ÂåπÈÖç \ntolerations:\n- key: &quot;taintKey&quot;\n  operator: &quot;Exists&quot;\n  effect: &quot;NoSchedule&quot;\n  \n#3.Â§ßËåÉÂõ¥ÂåπÈÖçÔºà‰∏çÊé®Ëçêkey‰∏∫ÂÜÖÁΩÆTaintÔºå‰ºöÂØºËá¥ËäÇÁÇπÊïÖÈöúpodÊó†Ê≥ïÊºÇÁßªÔºâ\ntolerations:\n- key: &quot;taintKey&quot;\n  operator: &quot;Exists\n  \n#4.ÂÆπÂøçÊó∂Èó¥ÈÖçÁΩÆ\ntolerations:\n- key: &quot;key1&quot;\n  operator: &quot;Equal&quot;\n  value: &quot;value1&quot;\n  effect: &quot;NoExecute&quot;\n  tolerationSeconds: 3600\n</code></pre>\n<h4 id=\"3-taint-tolerationÈÖçÁΩÆÁ§∫‰æã\"><a class=\"anchor\" href=\"#3-taint-tolerationÈÖçÁΩÆÁ§∫‰æã\">#</a> 3. Taint„ÄÅToleration ÈÖçÁΩÆÁ§∫‰æã</h4>\n<p>Êúâ‰∏Ä‰∏™ K8s ËäÇÁÇπÊòØÁ∫Ø SSD Á°¨ÁõòÁöÑËäÇÁÇπÔºåÁé∞ÈúÄË¶ÅÂè™Êúâ‰∏Ä‰∫õÈúÄË¶ÅÈ´òÊÄßËÉΩÂ≠òÂÇ®ÁöÑ Pod ÊâçËÉΩË∞ÉÂ∫¶Âà∞ËØ•ËäÇÁÇπ‰∏ä„ÄÇ</p>\n<pre><code>#1.ÁªôËäÇÁÇπÊâì‰∏äÊ±°ÁÇπÂíåÊ†áÁ≠æ\n# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule\n# kubectl label node k8s-node01 ssd=true\n\n#2.ÈÖçÁΩÆTolerationÔºö\n# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n      nodeSelector:\n        ssd: 'true'\n      tolerations:\n        - key: ssd\n          operator: Exists\n          effect: NoSchedule\n</code></pre>\n<h4 id=\"4-k8sÂÜÖÁΩÆÊ±°ÁÇπ\"><a class=\"anchor\" href=\"#4-k8sÂÜÖÁΩÆÊ±°ÁÇπ\">#</a> 4. K8s ÂÜÖÁΩÆÊ±°ÁÇπ</h4>\n<ul>\n<li><a href=\"http://node.kubernetes.io/not-ready%EF%BC%9A%E8%8A%82%E7%82%B9%E6%9C%AA%E5%87%86%E5%A4%87%E5%A5%BD%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81Ready%E7%9A%84%E5%80%BC%E4%B8%BAFalse%E3%80%82\">node.kubernetes.io/not-readyÔºöËäÇÁÇπÊú™ÂáÜÂ§áÂ•ΩÔºåÁõ∏ÂΩì‰∫éËäÇÁÇπÁä∂ÊÄÅ Ready ÁöÑÂÄº‰∏∫ False„ÄÇ</a></li>\n<li><a href=\"http://node.kubernetes.io/unreachable%EF%BC%9ANode\">node.kubernetes.io/unreachableÔºöNode</a> Controller ËÆøÈóÆ‰∏çÂà∞ËäÇÁÇπÔºåÁõ∏ÂΩì‰∫éËäÇÁÇπÁä∂ÊÄÅ Ready ÁöÑÂÄº‰∏∫ Unknown„ÄÇ</li>\n<li><a href=\"http://node.kubernetes.io/out-of-disk%EF%BC%9A%E8%8A%82%E7%82%B9%E7%A3%81%E7%9B%98%E8%80%97%E5%B0%BD%E3%80%82\">node.kubernetes.io/out-of-diskÔºöËäÇÁÇπÁ£ÅÁõòËÄóÂ∞Ω„ÄÇ</a></li>\n<li><a href=\"http://node.kubernetes.io/memory-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E5%86%85%E5%AD%98%E5%8E%8B%E5%8A%9B%E3%80%82\">node.kubernetes.io/memory-pressureÔºöËäÇÁÇπÂ≠òÂú®ÂÜÖÂ≠òÂéãÂäõ„ÄÇ</a></li>\n<li><a href=\"http://node.kubernetes.io/disk-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E7%A3%81%E7%9B%98%E5%8E%8B%E5%8A%9B%E3%80%82\">node.kubernetes.io/disk-pressureÔºöËäÇÁÇπÂ≠òÂú®Á£ÅÁõòÂéãÂäõ„ÄÇ</a></li>\n<li><a href=\"http://node.kubernetes.io/network-unavailable%EF%BC%9A%E8%8A%82%E7%82%B9%E7%BD%91%E7%BB%9C%E4%B8%8D%E5%8F%AF%E8%BE%BE%E3%80%82\">node.kubernetes.io/network-unavailableÔºöËäÇÁÇπÁΩëÁªú‰∏çÂèØËææ„ÄÇ</a></li>\n<li><a href=\"http://node.kubernetes.io/unschedulable%EF%BC%9A%E8%8A%82%E7%82%B9%E4%B8%8D%E5%8F%AF%E8%B0%83%E5%BA%A6%E3%80%82\">node.kubernetes.io/unschedulableÔºöËäÇÁÇπ‰∏çÂèØË∞ÉÂ∫¶„ÄÇ</a></li>\n<li><a href=\"http://node.cloudprovider.kubernetes.io/uninitialized%EF%BC%9A%E5%A6%82%E6%9E%9CKubelet%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8C%87%E5%AE%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%A4%96%E9%83%A8%E7%9A%84cloudprovider%EF%BC%8C%E5%AE%83%E5%B0%86%E7%BB%99%E5%BD%93%E5%89%8D%E8%8A%82%E7%82%B9%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AATaint%E5%B0%86%E5%85%B6%E6%A0%87%E8%AE%B0%E4%B8%BA%E4%B8%8D%E5%8F%AF%E7%94%A8%E3%80%82%E5%9C%A8cloud-controller-manager%E7%9A%84%E4%B8%80%E4%B8%AAcontroller%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%E5%90%8E%EF%BC%8CKubelet%E5%B0%86%E5%88%A0%E9%99%A4%E8%BF%99%E4%B8%AATaint%E3%80%82\">node.cloudprovider.kubernetes.io/uninitializedÔºöÂ¶ÇÊûú Kubelet ÂêØÂä®Êó∂ÊåáÂÆö‰∫Ü‰∏Ä‰∏™Â§ñÈÉ®ÁöÑ cloudproviderÔºåÂÆÉÂ∞ÜÁªôÂΩìÂâçËäÇÁÇπÊ∑ªÂä†‰∏Ä‰∏™ Taint Â∞ÜÂÖ∂Ê†áËÆ∞‰∏∫‰∏çÂèØÁî®„ÄÇÂú® cloud-controller-manager ÁöÑ‰∏Ä‰∏™ controller ÂàùÂßãÂåñËøô‰∏™ËäÇÁÇπÂêéÔºåKubelet Â∞ÜÂà†Èô§Ëøô‰∏™ Taint„ÄÇ</a></li>\n</ul>\n<p><img loading=\"lazy\" data-src=\"https://wp-cdn.4ce.cn/v2/vO7kURL.png\" alt=\"1.png\" /></p>\n<p>Deployment ÂàõÂª∫Âêé K8s ÈªòËÆ§‰∏∫ Pod Ê∑ªÂä†ÂÆπÂøçÔºåÂΩì Pod ÊâÄÂú®ÁöÑËäÇÁÇπÂÆïÊú∫Ôºå300 ÁßíÂêé pod ‰ºöÊºÇÁßªÔºåÈªòËÆ§ÂÆπÂøçÊó∂Èó¥ 300 Áßí„ÄÇ</p>\n<h4 id=\"5ËäÇÁÇπÂÆïÊú∫Âø´ÈÄüÊÅ¢Â§ç‰∏öÂä°Â∫îÁî®\"><a class=\"anchor\" href=\"#5ËäÇÁÇπÂÆïÊú∫Âø´ÈÄüÊÅ¢Â§ç‰∏öÂä°Â∫îÁî®\">#</a> 5. ËäÇÁÇπÂÆïÊú∫Âø´ÈÄüÊÅ¢Â§ç‰∏öÂä°Â∫îÁî®</h4>\n<p>ËäÇÁÇπ‰∏çÂÅ•Â∫∑Ôºå180 ÁßíÂêéÂÜçÈ©±ÈÄêÔºàÈªòËÆ§ÊòØ 300 ÁßíÔºâ</p>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 5\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n      tolerations:\n        - key: node.kubernetes.io/unreachable\n          operator: Exists\n          effect: NoExecute\n          tolerationSeconds: 180\n        - key: node.kubernetes.io/not-ready\n          operator: Exists\n          effect: NoExecute\n          tolerationSeconds: 180\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3142072607.html",
            "url": "http://ixuyong.cn/posts/3142072607.html",
            "title": "K8sÂàùÂßãÂåñÂÆπÂô®„ÄÅ‰∏¥Êó∂ÂÆπÂô®",
            "date_published": "2025-04-19T13:07:20.000Z",
            "content_html": "<h3 id=\"k8sÂàùÂßãÂåñÂÆπÂô®-‰∏¥Êó∂ÂÆπÂô®\"><a class=\"anchor\" href=\"#k8sÂàùÂßãÂåñÂÆπÂô®-‰∏¥Êó∂ÂÆπÂô®\">#</a> K8s ÂàùÂßãÂåñÂÆπÂô®„ÄÅ‰∏¥Êó∂ÂÆπÂô®</h3>\n<h4 id=\"1-ÂàùÂßãÂåñÂÆπÂô®\"><a class=\"anchor\" href=\"#1-ÂàùÂßãÂåñÂÆπÂô®\">#</a> 1. ÂàùÂßãÂåñÂÆπÂô®</h4>\n<h5 id=\"1-1-ÂàùÂßãÂåñÂÆπÂô®ÁöÑÁî®ÈÄî\"><a class=\"anchor\" href=\"#1-1-ÂàùÂßãÂåñÂÆπÂô®ÁöÑÁî®ÈÄî\">#</a> 1. 1 ÂàùÂßãÂåñÂÆπÂô®ÁöÑÁî®ÈÄî</h5>\n<p>ÂàùÂßãÂåñÂÆπÂô®‰∏ªË¶ÅÊòØÂú®‰∏ªÂ∫îÁî®ÂêØÂä®‰πãÂâçÔºåÂÅö‰∏Ä‰∫õÂàùÂßãÂåñÁöÑÊìç‰ΩúÔºåÊØîÂ¶ÇÂàõÂª∫Êñá‰ª∂„ÄÅ‰øÆÊîπÂÜÖÊ†∏ÂèÇÊï∞„ÄÅÁ≠âÂæÖ‰æùËµñÁ®ãÂ∫èÂêØÂä®ÊàñÂÖ∂‰ªñÈúÄË¶ÅÂú®‰∏ªÁ®ãÂ∫èÂêØÂä®‰πãÂâçÈúÄË¶ÅÂÅöÁöÑÂ∑•‰Ωú„ÄÇ</p>\n<ul>\n<li>Init ÂÆπÂô®ÂèØ‰ª•ÂåÖÂê´‰∏Ä‰∫õÂÆâË£ÖËøáÁ®ã‰∏≠Â∫îÁî®ÂÆπÂô®‰∏≠‰∏çÂ≠òÂú®ÁöÑÂÆûÁî®Â∑•ÂÖ∑Êàñ‰∏™ÊÄßÂåñ‰ª£Á†ÅÔºõ</li>\n<li>Init ÂÆπÂô®ÂèØ‰ª•ÂÆâÂÖ®Âú∞ËøêË°åËøô‰∫õÂ∑•ÂÖ∑ÔºåÈÅøÂÖçËøô‰∫õÂ∑•ÂÖ∑ÂØºËá¥Â∫îÁî®ÈïúÂÉèÁöÑÂÆâÂÖ®ÊÄßÈôç‰ΩéÔºõ</li>\n<li>Init ÂÆπÂô®ÂèØ‰ª•‰ª• root Ë∫´‰ªΩËøêË°åÔºåÊâßË°å‰∏Ä‰∫õÈ´òÊùÉÈôêÂëΩ‰ª§Ôºõ</li>\n<li>Init ÂÆπÂô®Áõ∏ÂÖ≥Êìç‰ΩúÊâßË°åÂÆåÊàê‰ª•ÂêéÂç≥ÈÄÄÂá∫Ôºå‰∏ç‰ºöÁªô‰∏öÂä°ÂÆπÂô®Â∏¶Êù•ÂÆâÂÖ®ÈöêÊÇ£„ÄÇ</li>\n</ul>\n<h5 id=\"12-ÂàùÂßãÂåñÂÆπÂô®ÂíåpoststartÂå∫Âà´\"><a class=\"anchor\" href=\"#12-ÂàùÂßãÂåñÂÆπÂô®ÂíåpoststartÂå∫Âà´\">#</a> 1.2 ÂàùÂßãÂåñÂÆπÂô®Âíå PostStart Âå∫Âà´</h5>\n<p>PostStartÔºö‰æùËµñ‰∏ªÂ∫îÁî®ÁöÑÁéØÂ¢ÉÔºåËÄå‰∏îÂπ∂‰∏ç‰∏ÄÂÆöÂÖà‰∫é Command ËøêË°å„ÄÇ</p>\n<p>InitContainerÔºö‰∏ç‰æùËµñ‰∏ªÂ∫îÁî®ÁöÑÁéØÂ¢ÉÔºåÂèØ‰ª•ÊúâÊõ¥È´òÁöÑÊùÉÈôêÂíåÊõ¥Â§öÁöÑÂ∑•ÂÖ∑Ôºå‰∏ÄÂÆö‰ºöÂú®‰∏ªÂ∫îÁî®ÂêØÂä®‰πãÂâçÂÆåÊàê</p>\n<h5 id=\"13-ÂàùÂßãÂåñÂÆπÂô®ÂíåÊôÆÈÄöÂÆπÂô®ÁöÑÂå∫Âà´\"><a class=\"anchor\" href=\"#13-ÂàùÂßãÂåñÂÆπÂô®ÂíåÊôÆÈÄöÂÆπÂô®ÁöÑÂå∫Âà´\">#</a> 1.3 ÂàùÂßãÂåñÂÆπÂô®ÂíåÊôÆÈÄöÂÆπÂô®ÁöÑÂå∫Âà´</h5>\n<p>Init ÂÆπÂô®‰∏éÊôÆÈÄöÁöÑÂÆπÂô®ÈùûÂ∏∏ÂÉèÔºåÈô§‰∫ÜÂ¶Ç‰∏ãÂá†ÁÇπÔºö</p>\n<ul>\n<li>\n<p>Á¨¨‰∏Ä‰∏™ Init ÂÆπÂô®ËøêË°åÊàêÂäüÂêéÊâç‰ºöËøêË°å‰∏ã‰∏Ä‰∏™ Init ÂÆπÂô®Ôºõ</p>\n</li>\n<li>\n<p>ÊâÄÊúâÁöÑ Init ÂÆπÂô®ËøêË°åÊàêÂäüÂêéÊâç‰ºöËøêË°å‰∏ªÂÆπÂô®Ôºõ</p>\n</li>\n<li>\n<p>Â¶ÇÊûú Pod ÁöÑ Init ÂÆπÂô®Â§±Ë¥•ÔºåKubernetes ‰ºö‰∏çÊñ≠Âú∞ÈáçÂêØËØ• PodÔºåÁõ¥Âà∞ Init ÂÆπÂô®ÊàêÂäü‰∏∫Ê≠¢Ôºå‰ΩÜÊòØ Pod ÂØπÂ∫îÁöÑ restartPolicy ÂÄº‰∏∫ NeverÔºåKubernetes ‰∏ç‰ºöÈáçÊñ∞ÂêØÂä® Pod„ÄÇ</p>\n</li>\n<li>\n<p>Init ÂÆπÂô®‰∏çÊîØÊåÅ lifecycle„ÄÅlivenessProbe„ÄÅreadinessProbe Âíå startupProbe</p>\n</li>\n</ul>\n<h5 id=\"14-ÂàùÂßãÂåñÂÆπÂô®Á§∫‰æã\"><a class=\"anchor\" href=\"#14-ÂàùÂßãÂåñÂÆπÂô®Á§∫‰æã\">#</a> 1.4 ÂàùÂßãÂåñÂÆπÂô®Á§∫‰æã</h5>\n<pre><code>[root@k8s-master01 ~]# cat init.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      initContainers:           # ÂàùÂßãÂåñÂÆπÂô®ËÆæÂÆö\n      - name: fix-permissions\n        image: busybox\n        command: [&quot;sh&quot;,&quot;-c&quot;,&quot;echo hello kubernetes&gt;/usr/share/nginx/html/index.html&quot;]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: share-volume\n          mountPath: /usr/share/nginx/html\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n        - name: timezone\n          mountPath: /etc/timezone\n        - name: tz-config\n          mountPath: /usr/share/zoneinfo/Asia/Shanghai\n        - name: tz-config\n          mountPath: /etc/localtime\n        - name: share-volume\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: share-volume\n        emptyDir: &#123;&#125;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: File\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: File\n\n[root@k8s-master01 ~]# kubectl create -f init.yaml\n\n[root@k8s-master01 ~]# curl 172.16.32.145\nhello kubernetes\n</code></pre>\n<h4 id=\"2-‰∏¥Êó∂ÂÆπÂô®\"><a class=\"anchor\" href=\"#2-‰∏¥Êó∂ÂÆπÂô®\">#</a> 2. ‰∏¥Êó∂ÂÆπÂô®</h4>\n<h5 id=\"21-Ê≥®ÂÖ•‰∏¥Êó∂ÂÆπÂô®Âà∞pod\"><a class=\"anchor\" href=\"#21-Ê≥®ÂÖ•‰∏¥Êó∂ÂÆπÂô®Âà∞pod\">#</a> 2.1 Ê≥®ÂÖ•‰∏¥Êó∂ÂÆπÂô®Âà∞ Pod</h5>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods\nNAME                            READY   STATUS    RESTARTS      AGE\nnginx-deploy-7dd6cd4b44-ktw5k   1/1     Running   1             16h\nnginx-deploy-7dd6cd4b44-mjcgq   1/1     Running   1 (28m ago)   16h\nnginx-deploy-7dd6cd4b44-wdm6p   1/1     Running   1 (28m ago)   16h\n\n#1.ËøõÂÖ•ÂÆπÂô®ÂèëÁé∞podÊ≤°ÊúâpsÂíånetstatÂëΩ‰ª§\n[root@k8s-master01 ~]# kubectl exec -it nginx-deploy-7dd6cd4b44-ktw5k  -- bash\nroot@nginx-deploy-7dd6cd4b44-ktw5k:/# ps aux\nroot@nginx-deploy-7dd6cd4b44-ktw5k:/# netstat -lntp\n\n#2.Ê≥®ÂÖ•‰∏¥Êó∂ÂÆπÂô®Ëá≥ËØ•Pod\n[root@k8s-master01 ~]# kubectl debug nginx-deploy-7dd6cd4b44-wdm6p -ti --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools\n</code></pre>\n<h5 id=\"21-Ê≥®ÂÖ•‰∏¥Êó∂ÂÆπÂô®Âà∞ËäÇÁÇπ\"><a class=\"anchor\" href=\"#21-Ê≥®ÂÖ•‰∏¥Êó∂ÂÆπÂô®Âà∞ËäÇÁÇπ\">#</a> 2.1 Ê≥®ÂÖ•‰∏¥Êó∂ÂÆπÂô®Âà∞ËäÇÁÇπ</h5>\n<pre><code>kubectl debug node k8s-node01 -it --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3833778957.html",
            "url": "http://ixuyong.cn/posts/3833778957.html",
            "title": "K8sËÆ°Âàí‰ªªÂä°Job„ÄÅCronjob",
            "date_published": "2025-04-19T13:00:21.000Z",
            "content_html": "<h3 id=\"k8sËÆ°Âàí‰ªªÂä°job-cronjob\"><a class=\"anchor\" href=\"#k8sËÆ°Âàí‰ªªÂä°job-cronjob\">#</a> K8s ËÆ°Âàí‰ªªÂä° Job„ÄÅCronjob</h3>\n<h4 id=\"1-jobÈÖçÁΩÆÂèÇÊï∞ËØ¶Ëß£\"><a class=\"anchor\" href=\"#1-jobÈÖçÁΩÆÂèÇÊï∞ËØ¶Ëß£\">#</a> 1. Job ÈÖçÁΩÆÂèÇÊï∞ËØ¶Ëß£</h4>\n<pre><code># cat job.yaml \napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    job-name: echo\n  name: echo\n  namespace: default\nspec:\n  #suspend: true # 1.21+\n  #ttlSecondsAfterFinished: 100\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    spec:\n      containers:\n      - name: echo\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - echo &quot;Hello Job&quot;\n      restartPolicy: Never\n      \n[root@k8s-master01 ~]# kubectl get jobs\nNAME   STATUS     COMPLETIONS   DURATION   AGE\necho   Complete   1/1           70s        2m5s\n\n[root@k8s-master01 ~]# kubectl get pods\nNAME          READY   STATUS      RESTARTS      AGE\necho-564c8    0/1     Completed   0             2m10s\n\n[root@k8s-master01 ~]# kubectl logs echo-564c8\nHello Job\n</code></pre>\n<ul>\n<li>backoffLimit:ÔºöÂ¶ÇÊûú‰ªªÂä°ÊâßË°åÂ§±Ë¥•ÔºåÂ§±Ë¥•Â§öÂ∞ëÊ¨°Âêé‰∏çÂÜçÊâßË°å</li>\n<li>completionsÔºöÊúâÂ§öÂ∞ë‰∏™ Pod ÊâßË°åÊàêÂäüÔºåËÆ§‰∏∫‰ªªÂä°ÊòØÊàêÂäüÁöÑÔºåÈªòËÆ§‰∏∫Á©∫Âíå parallelism Êï∞ÂÄº‰∏ÄÊ†∑</li>\n<li>parallelismÔºöÂπ∂Ë°åÊâßË°å‰ªªÂä°ÁöÑÊï∞ÈáèÔºåÂ¶ÇÊûú parallelism Êï∞ÂÄºÂ§ß‰∫é completions Êï∞ÂÄºÔºåÂè™‰ºöÂàõÂª∫ completions ÁöÑÊï∞ÈáèÔºõÂ¶ÇÊûú completions ÊòØ 4ÔºåÂπ∂ÂèëÊòØ 3ÔºåÁ¨¨‰∏ÄÊ¨°‰ºöÂàõÂª∫ 3 ‰∏™ Pod ÊâßË°å‰ªªÂä°ÔºåÁ¨¨‰∫åÊ¨°Âè™‰ºöÂàõÂª∫‰∏Ä‰∏™ Pod ÊâßË°å‰ªªÂä°</li>\n<li>ttlSecondsAfterFinishedÔºöJob Âú®ÊâßË°åÁªìÊùü‰πãÂêéÔºàÁä∂ÊÄÅ‰∏∫ completed Êàñ FailedÔºâËá™Âä®Ê∏ÖÁêÜ„ÄÇËÆæÁΩÆ‰∏∫ 0 Ë°®Á§∫ÊâßË°åÁªìÊùüÁ´ãÂç≥Âà†Èô§Ôºå‰∏çËÆæÁΩÆÂàô‰∏ç‰ºöÊ∏ÖÈô§ÔºåÈúÄË¶ÅÂºÄÂêØ TTLAfterFinished ÁâπÊÄß</li>\n</ul>\n<h4 id=\"2-cronjobÈÖçÁΩÆÂèÇÊï∞ËØ¶Ëß£\"><a class=\"anchor\" href=\"#2-cronjobÈÖçÁΩÆÂèÇÊï∞ËØ¶Ëß£\">#</a> 2. CronJob ÈÖçÁΩÆÂèÇÊï∞ËØ¶Ëß£</h4>\n<pre><code># cat cronjob.yaml \napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  concurrencyPolicy: Allow   #ÂÖÅËÆ∏ÂêåÊó∂ËøêË°åÂ§ö‰∏™‰ªªÂä°\n  failedJobsHistoryLimit: 10  #‰øùÁïôÂ§öÂ∞ëÂ§±Ë¥•ÁöÑ‰ªªÂä°\n  successfulJobsHistoryLimit: 10  #‰øùÁïôÂ§öÂ∞ëÂ∑≤ÂÆåÊàêÁöÑ‰ªªÂä°\n  #suspend: true             #Â¶ÇÊûútrueÂàôÂèñÊ∂àÂë®ÊúüÊÄßÊâßË°å‰ªªÂä°\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            command:\n            - sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure \n          \n[root@k8s-master01 ~]# kubectl get  cj\nNAME    SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nhello   */1 * * * *   &lt;none&gt;     False     0        6s              81s\n\n[root@k8s-master01 ~]# kubectl get  jobs\nNAME             STATUS     COMPLETIONS   DURATION   AGE\nhello-29084454   Complete   1/1           4s         72s\nhello-29084455   Complete   1/1           5s         12s\n\n[root@k8s-master01 ~]# kubectl get  pods\nNAME                   READY   STATUS      RESTARTS   AGE\nhello-29084454-hwv7p   0/1     Completed   0          78s\nhello-29084455-vf99w   0/1     Completed   0          18s\n\n[root@k8s-master01 ~]# kubectl logs -f hello-29084455-vf99w\nSat Apr 19 12:55:02 UTC 2025\nHello from the Kubernetes cluster\n</code></pre>\n<ul>\n<li>apiVersion: batch/v1beta1   #1.21+ batch/v1</li>\n<li>scheduleÔºöË∞ÉÂ∫¶Âë®ÊúüÔºåÂíå Linux ‰∏ÄËá¥ÔºåÂàÜÂà´ÊòØÂàÜÊó∂Êó•ÊúàÂë®„ÄÇ</li>\n<li>restartPolicyÔºöÈáçÂêØÁ≠ñÁï•ÔºåÂíå Pod ‰∏ÄËá¥„ÄÇ</li>\n<li>concurrencyPolicyÔºöÂπ∂ÂèëË∞ÉÂ∫¶Á≠ñÁï•„ÄÇÂèØÈÄâÂèÇÊï∞Â¶Ç‰∏ãÔºö\n<ul>\n<li>AllowÔºöÂÖÅËÆ∏ÂêåÊó∂ËøêË°åÂ§ö‰∏™‰ªªÂä°„ÄÇ</li>\n<li>ForbidÔºö‰∏çÂÖÅËÆ∏Âπ∂ÂèëËøêË°åÔºåÂ¶ÇÊûú‰πãÂâçÁöÑ‰ªªÂä°Â∞öÊú™ÂÆåÊàêÔºåÊñ∞ÁöÑ‰ªªÂä°‰∏ç‰ºöË¢´ÂàõÂª∫„ÄÇ</li>\n<li>ReplaceÔºöÂ¶ÇÊûú‰πãÂâçÁöÑ‰ªªÂä°Â∞öÊú™ÂÆåÊàêÔºåÊñ∞ÁöÑ‰ªªÂä°‰ºöÊõøÊç¢ÁöÑ‰πãÂâçÁöÑ‰ªªÂä°„ÄÇ</li>\n</ul>\n</li>\n<li>suspendÔºöÂ¶ÇÊûúËÆæÁΩÆ‰∏∫ trueÔºåÂàôÊöÇÂÅúÂêéÁª≠ÁöÑ‰ªªÂä°ÔºåÈªòËÆ§‰∏∫ false„ÄÇ</li>\n<li>successfulJobsHistoryLimitÔºö‰øùÁïôÂ§öÂ∞ëÂ∑≤ÂÆåÊàêÁöÑ‰ªªÂä°ÔºåÊåâÈúÄÈÖçÁΩÆ„ÄÇ</li>\n<li>failedJobsHistoryLimitÔºö‰øùÁïôÂ§öÂ∞ëÂ§±Ë¥•ÁöÑ‰ªªÂä°„ÄÇ</li>\n</ul>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/169153047.html",
            "url": "http://ixuyong.cn/posts/169153047.html",
            "title": "K8sÊåÅ‰πÖÂåñÂ≠òÂÇ®",
            "date_published": "2025-04-18T14:25:17.000Z",
            "content_html": "<h3 id=\"k8sÊåÅ‰πÖÂåñÂ≠òÂÇ®\"><a class=\"anchor\" href=\"#k8sÊåÅ‰πÖÂåñÂ≠òÂÇ®\">#</a> K8s ÊåÅ‰πÖÂåñÂ≠òÂÇ®</h3>\n<h4 id=\"1-volume\"><a class=\"anchor\" href=\"#1-volume\">#</a> 1. Volume</h4>\n<p>ContainerÔºàÂÆπÂô®Ôºâ‰∏≠ÁöÑÁ£ÅÁõòÊñá‰ª∂ÊòØÁü≠ÊöÇÁöÑÔºåÂΩìÂÆπÂô®Â¥©Ê∫ÉÊó∂Ôºåkubelet ‰ºöÈáçÊñ∞ÂêØÂä®ÂÆπÂô®ÔºåContainer ‰ºö‰ª•ÊúÄÂπ≤ÂáÄÁöÑÁä∂ÊÄÅÂêØÂä®ÔºåÊúÄÂàùÁöÑÊñá‰ª∂Â∞Ü‰∏¢Â§±„ÄÇÂè¶Â§ñÔºåÂΩì‰∏Ä‰∏™ Pod ËøêË°åÂ§ö‰∏™ Container Êó∂ÔºåÂêÑ‰∏™ÂÆπÂô®ÂèØËÉΩÈúÄË¶ÅÂÖ±‰∫´‰∏Ä‰∫õÊñá‰ª∂„ÄÇKubernetes Volume ÂèØ‰ª•Ëß£ÂÜ≥Ëøô‰∏§‰∏™ÈóÆÈ¢ò„ÄÇ</p>\n<ul>\n<li>‰∏Ä‰∫õÈúÄË¶ÅÊåÅ‰πÖÂåñÊï∞ÊçÆÁöÑÁ®ãÂ∫èÊâç‰ºöÁî®Âà∞ VolumesÔºåÊàñËÄÖ‰∏Ä‰∫õÈúÄË¶ÅÂÖ±‰∫´Êï∞ÊçÆÁöÑÂÆπÂô®ÈúÄË¶Å volumes„ÄÇ</li>\n<li>Êó•ÂøóÊî∂ÈõÜÁöÑÈúÄÊ±ÇÈúÄË¶ÅÂú®Â∫îÁî®Á®ãÂ∫èÁöÑÂÆπÂô®ÈáåÈù¢Âä†‰∏Ä‰∏™ sidecarÔºåËøô‰∏™ÂÆπÂô®ÊòØ‰∏Ä‰∏™Êî∂ÈõÜÊó•ÂøóÁöÑÂÆπÂô®ÔºåÊØîÂ¶Ç filebeatÔºåÂÆÉÈÄöËøá volumes ÂÖ±‰∫´Â∫îÁî®Á®ãÂ∫èÁöÑÊó•ÂøóÊñá‰ª∂ÁõÆÂΩï„ÄÇ</li>\n</ul>\n<h5 id=\"11-emptydirÂÆûÁé∞Êï∞ÊçÆÂÖ±‰∫´\"><a class=\"anchor\" href=\"#11-emptydirÂÆûÁé∞Êï∞ÊçÆÂÖ±‰∫´\">#</a> 1.1 EmptyDir ÂÆûÁé∞Êï∞ÊçÆÂÖ±‰∫´</h5>\n<p>Âíå‰∏äËø∞ volume ‰∏çÂêåÁöÑÊòØÔºåÂ¶ÇÊûúÂà†Èô§ PodÔºåemptyDir Âç∑‰∏≠ÁöÑÊï∞ÊçÆ‰πüÂ∞ÜË¢´Âà†Èô§Ôºå‰∏ÄËà¨ emptyDir Âç∑Áî®‰∫é Pod ‰∏≠ÁöÑ‰∏çÂêå Container ÂÖ±‰∫´Êï∞ÊçÆ„ÄÇ</p>\n<pre><code># cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n        - name: share-volume\n          emptyDir: &#123;&#125;\n      containers:\n        - name: nginx\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n            - name: share-volume\n              mountPath: /opt\n        - name: nginx2\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          command:\n            - sh\n            - '-c'\n            - sleep 3600\n          volumeMounts:\n            - name: share-volume\n              mountPath: /mnt\n</code></pre>\n<h5 id=\"12-volumes-hostpathÊåÇËΩΩÂÆø‰∏ªÊú∫Ë∑ØÂæÑ\"><a class=\"anchor\" href=\"#12-volumes-hostpathÊåÇËΩΩÂÆø‰∏ªÊú∫Ë∑ØÂæÑ\">#</a> 1.2 Volumes HostPath ÊåÇËΩΩÂÆø‰∏ªÊú∫Ë∑ØÂæÑ</h5>\n<p>hostPath Âç∑ÂèØÂ∞ÜËäÇÁÇπ‰∏äÁöÑÊñá‰ª∂ÊàñÁõÆÂΩïÊåÇËΩΩÂà∞ Pod ‰∏äÔºåÁî®‰∫é Pod Ëá™ÂÆö‰πâÊó•ÂøóËæìÂá∫ÊàñËÆøÈóÆ Docker ÂÜÖÈÉ®ÁöÑÂÆπÂô®Á≠â„ÄÇ</p>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n      - name: share-volume\n        emptyDir: &#123;&#125;\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      containers:\n        - name: nginx\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: share-volume\n            mountPath: /opt\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n        - name: nginx2\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          command:\n            - sh\n            - '-c'\n            - sleep 3600\n          volumeMounts:\n          - name: share-volume\n            mountPath: /mnt\n</code></pre>\n<p>hostPath Âç∑Â∏∏Áî®ÁöÑ typeÔºàÁ±ªÂûãÔºâÂ¶Ç‰∏ãÔºö</p>\n<ul>\n<li>type ‰∏∫Á©∫Â≠óÁ¨¶‰∏≤ÔºöÈªòËÆ§ÈÄâÈ°πÔºåÊÑèÂë≥ÁùÄÊåÇËΩΩ hostPath Âç∑‰πãÂâç‰∏ç‰ºöÊâßË°å‰ªª‰ΩïÊ£ÄÊü•„ÄÇ</li>\n<li>DirectoryOrCreateÔºöÂ¶ÇÊûúÁªôÂÆöÁöÑ path ‰∏çÂ≠òÂú®‰ªª‰Ωï‰∏úË•øÔºåÈÇ£‰πàÂ∞ÜÊ†πÊçÆÈúÄË¶ÅÂàõÂª∫‰∏Ä‰∏™ÊùÉÈôê‰∏∫ 0755 ÁöÑÁ©∫ÁõÆÂΩïÔºåÂíå Kubelet ÂÖ∑ÊúâÁõ∏ÂêåÁöÑÁªÑÂíåÊùÉÈôê„ÄÇ</li>\n<li>DirectoryÔºöÁõÆÂΩïÂøÖÈ°ªÂ≠òÂú®‰∫éÁªôÂÆöÁöÑË∑ØÂæÑ‰∏ã„ÄÇ</li>\n<li>FileOrCreateÔºöÂ¶ÇÊûúÁªôÂÆöÁöÑË∑ØÂæÑ‰∏çÂ≠òÂÇ®‰ªª‰ΩïÂÜÖÂÆπÔºåÂàô‰ºöÊ†πÊçÆÈúÄË¶ÅÂàõÂª∫‰∏Ä‰∏™Á©∫Êñá‰ª∂ÔºåÊùÉÈôêËÆæÁΩÆ‰∏∫ 0644ÔºåÂíå Kubelet ÂÖ∑ÊúâÁõ∏ÂêåÁöÑÁªÑÂíåÊâÄÊúâÊùÉ„ÄÇ</li>\n<li>FileÔºöÊñá‰ª∂ÔºåÂøÖÈ°ªÂ≠òÂú®‰∫éÁªôÂÆöË∑ØÂæÑ‰∏≠„ÄÇ</li>\n<li>SocketÔºöUNIX Â•óÊé•Â≠óÔºåÂøÖÈ°ªÂ≠òÂú®‰∫éÁªôÂÆöË∑ØÂæÑ‰∏≠„ÄÇ</li>\n<li>CharDeviceÔºöÂ≠óÁ¨¶ËÆæÂ§áÔºåÂøÖÈ°ªÂ≠òÂú®‰∫éÁªôÂÆöË∑ØÂæÑ‰∏≠„ÄÇ</li>\n<li>BlockDeviceÔºöÂùóËÆæÂ§áÔºåÂøÖÈ°ªÂ≠òÂú®‰∫éÁªôÂÆöË∑ØÂæÑ‰∏≠„ÄÇ</li>\n</ul>\n<h5 id=\"13-ÊåÇËΩΩnfsËá≥ÂÆπÂô®\"><a class=\"anchor\" href=\"#13-ÊåÇËΩΩnfsËá≥ÂÆπÂô®\">#</a> 1.3 ÊåÇËΩΩ NFS Ëá≥ÂÆπÂô®</h5>\n<pre><code>#1.ÂÆâË£Önfs\n# yum install nfs-utils -y       \n# mkdir /data/nfs -p\n# vim /etc/exports \n/data 192.168.1.0/24(rw,no_root_squash)\n# exportfs -arv   \n# systemctl start nfs-server &amp;&amp; systemctl enable nfs-server &amp;&amp; systemctl status nfs-server \n\n#2.ÊµãËØïÂÆ¢Êà∑Á´ØÊåÇËΩΩ\n# showmount -e 192.168.1.75\n# mount -t nfs 192.168.1.75:/data/nfs /mnt\n\n#3.DeployÊåÇËΩΩNFS\n[root@k8s-master01 ~]# cat nginx-deploy-nfs.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n      annotations:\n        app: nginx-deploy\n    spec:\n      restartPolicy: Always\n      volumes:\n      - name: nfs-volume\n        nfs:\n          server: 192.168.1.75\n          path: /data/nfs\n      - name: tz-config\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n          type: &quot;&quot;\n      - name: timezone\n        hostPath:\n          path: /etc/timezone\n          type: &quot;&quot;\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n          volumeMounts:\n          - name: nfs-volume\n            mountPath: /usr/share/nginx/html\n          - name: tz-config\n            mountPath: /usr/share/zoneinfo/Asia/Shanghai\n          - name: tz-config\n            mountPath: /etc/localtime\n          - name: timezone\n            mountPath: /etc/timezone\n</code></pre>\n<h4 id=\"2-pv-pvc\"><a class=\"anchor\" href=\"#2-pv-pvc\">#</a> 2. PV„ÄÅPVC</h4>\n<p>PersistentVolumeÔºöÁÆÄÁß∞ PVÔºåÊòØÁî± Kubernetes ÁÆ°ÁêÜÂëòËÆæÁΩÆÁöÑÂ≠òÂÇ®ÔºåÂèØ‰ª•ÈÖçÁΩÆ Ceph„ÄÅNFS„ÄÅGlusterFS Á≠âÂ∏∏Áî®Â≠òÂÇ®ÈÖçÁΩÆÔºåÁõ∏ÂØπ‰∫é Volume ÈÖçÁΩÆÔºåÊèê‰æõ‰∫ÜÊõ¥Â§öÁöÑÂäüËÉΩÔºåÊØîÂ¶ÇÁîüÂëΩÂë®ÊúüÁöÑÁÆ°ÁêÜ„ÄÅÂ§ßÂ∞èÁöÑÈôêÂà∂„ÄÇPV ÂàÜ‰∏∫ÈùôÊÄÅÂíåÂä®ÊÄÅ„ÄÇ</p>\n<p>PersistentVolumeClaimÔºöÁÆÄÁß∞ PVCÔºåÊòØÂØπÂ≠òÂÇ® PV ÁöÑËØ∑Ê±ÇÔºåË°®Á§∫ÈúÄË¶Å‰ªÄ‰πàÁ±ªÂûãÁöÑ PVÔºåÈúÄË¶ÅÂ≠òÂÇ®ÁöÑÊäÄÊúØ‰∫∫ÂëòÂè™ÈúÄË¶ÅÈÖçÁΩÆ PVC Âç≥ÂèØ‰ΩøÁî®Â≠òÂÇ®ÔºåÊàñËÄÖ Volume ÈÖçÁΩÆ PVC ÁöÑÂêçÁß∞Âç≥ÂèØ„ÄÇ</p>\n<h5 id=\"21-pvÂõûÊî∂Á≠ñÁï•\"><a class=\"anchor\" href=\"#21-pvÂõûÊî∂Á≠ñÁï•\">#</a> 2.1 PV ÂõûÊî∂Á≠ñÁï•</h5>\n<ul>\n<li>RetainÔºö‰øùÁïôÔºåËØ•Á≠ñÁï•ÂÖÅËÆ∏ÊâãÂä®ÂõûÊî∂ËµÑÊ∫êÔºåÂΩìÂà†Èô§ PVC Êó∂ÔºåPV ‰ªçÁÑ∂Â≠òÂú®ÔºåPV Ë¢´ËßÜ‰∏∫Â∑≤ÈáäÊîæÔºåÁÆ°ÁêÜÂëòÂèØ‰ª•ÊâãÂä®ÂõûÊî∂Âç∑„ÄÇ</li>\n<li>RecycleÔºöÂõûÊî∂ÔºåÂ¶ÇÊûú Volume Êèí‰ª∂ÊîØÊåÅÔºåRecycle Á≠ñÁï•‰ºöÂØπÂç∑ÊâßË°å rm -rf Ê∏ÖÁêÜËØ• PVÔºåÂπ∂‰ΩøÂÖ∂ÂèØÁî®‰∫é‰∏ã‰∏Ä‰∏™Êñ∞ÁöÑ PVCÔºå‰ΩÜÊòØÊú¨Á≠ñÁï•Â∞ÜÊù•‰ºöË¢´ÂºÉÁî®ÔºåÁõÆÂâçÂè™Êúâ NFS Âíå HostPath ÊîØÊåÅËØ•Á≠ñÁï•„ÄÇ</li>\n<li>DeleteÔºöÂà†Èô§ÔºåÂ¶ÇÊûú Volume Êèí‰ª∂ÊîØÊåÅÔºåÂà†Èô§ PVC Êó∂‰ºöÂêåÊó∂Âà†Èô§ PVÔºåÂä®ÊÄÅÂç∑ÈªòËÆ§‰∏∫ DeleteÔºåÁõÆÂâçÊîØÊåÅ Delete ÁöÑÂ≠òÂÇ®ÂêéÁ´ØÂåÖÊã¨ AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder Á≠â„ÄÇ</li>\n<li>ÂèØ‰ª•ÈÄöËøá persistentVolumeReclaimPolicy: Recycle Â≠óÊÆµÈÖçÁΩÆ</li>\n</ul>\n<h5 id=\"22-pvËÆøÈóÆÁ≠ñÁï•\"><a class=\"anchor\" href=\"#22-pvËÆøÈóÆÁ≠ñÁï•\">#</a> 2.2 PV ËÆøÈóÆÁ≠ñÁï•</h5>\n<ul>\n<li>ReadWriteOnceÔºöÂèØ‰ª•Ë¢´ÂçïËäÇÁÇπ‰ª•ËØªÂÜôÊ®°ÂºèÊåÇËΩΩÔºåÂëΩ‰ª§Ë°å‰∏≠ÂèØ‰ª•Ë¢´Áº©ÂÜô‰∏∫ RWO„ÄÇ</li>\n<li>ReadOnlyManyÔºöÂèØ‰ª•Ë¢´Â§ö‰∏™ËäÇÁÇπ‰ª•Âè™ËØªÊ®°ÂºèÊåÇËΩΩÔºåÂëΩ‰ª§Ë°å‰∏≠ÂèØ‰ª•Ë¢´Áº©ÂÜô‰∏∫ ROX„ÄÇ</li>\n<li>ReadWriteManyÔºöÂèØ‰ª•Ë¢´Â§ö‰∏™ËäÇÁÇπ‰ª•ËØªÂÜôÊ®°ÂºèÊåÇËΩΩÔºåÂëΩ‰ª§Ë°å‰∏≠ÂèØ‰ª•Ë¢´Áº©ÂÜô‰∏∫ RWX„ÄÇ</li>\n<li>ReadWriteOncePod ÔºöÂè™ÂÖÅËÆ∏Ë¢´Âçï‰∏™ Pod ËÆøÈóÆÔºåÈúÄË¶Å K8s 1.22 + ‰ª•‰∏äÁâàÊú¨ÔºåÂπ∂‰∏îÊòØ CSI ÂàõÂª∫ÁöÑ PV ÊâçÂèØ‰ΩøÁî®ÔºåÁº©ÂÜô‰∏∫ RWOP</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Volume Plugin</th>\n<th style=\"text-align:center\">ReadWriteOnce</th>\n<th style=\"text-align:center\">ReadOnlyMany</th>\n<th style=\"text-align:center\">ReadWriteMany</th>\n<th style=\"text-align:center\">ReadWriteOncePod</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">AzureFile</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CephFS</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CSI</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">depends on the driver</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FC</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FlexVolume</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">depends on the driver</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">HostPath</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">iSCSI</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">NFS</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">RBD</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">VsphereVolume</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">- (works when Pods are collocated)</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">PortworxVolume</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n<td style=\"text-align:center\">‚úì</td>\n<td style=\"text-align:center\">-</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"23-Â≠òÂÇ®ÂàÜÁ±ª\"><a class=\"anchor\" href=\"#23-Â≠òÂÇ®ÂàÜÁ±ª\">#</a> 2.3 Â≠òÂÇ®ÂàÜÁ±ª</h5>\n<ul>\n<li>Êñá‰ª∂Â≠òÂÇ®Ôºö‰∏Ä‰∫õÊï∞ÊçÆÂèØËÉΩÈúÄË¶ÅË¢´Â§ö‰∏™ËäÇÁÇπ‰ΩøÁî®ÔºåÊØîÂ¶ÇÁî®Êà∑ÁöÑÂ§¥ÂÉè„ÄÅÁî®Êà∑‰∏ä‰º†ÁöÑÊñá‰ª∂Á≠âÔºåÂÆûÁé∞ÊñπÂºèÔºöNFS„ÄÅNAS„ÄÅFTP„ÄÅCephFS Á≠â„ÄÇ</li>\n<li>ÂùóÂ≠òÂÇ®Ôºö‰∏Ä‰∫õÊï∞ÊçÆÂè™ËÉΩË¢´‰∏Ä‰∏™ËäÇÁÇπ‰ΩøÁî®ÔºåÊàñËÄÖÊòØÈúÄË¶ÅÂ∞Ü‰∏ÄÂùóË£∏ÁõòÊï¥‰∏™ÊåÇËΩΩ‰ΩøÁî®ÔºåÊØîÂ¶ÇÊï∞ÊçÆÂ∫ì„ÄÅRedis Á≠âÔºåÂÆûÁé∞ÊñπÂºèÔºöCeph„ÄÅGlusterFS„ÄÅÂÖ¨Êúâ‰∫ë„ÄÇ</li>\n<li>ÂØπË±°Â≠òÂÇ®ÔºöÁî±Á®ãÂ∫è‰ª£Á†ÅÁõ¥Êé•ÂÆûÁé∞ÁöÑ‰∏ÄÁßçÂ≠òÂÇ®ÊñπÂºèÔºå‰∫ëÂéüÁîüÂ∫îÁî®Êó†Áä∂ÊÄÅÂåñÂ∏∏Áî®ÁöÑÂÆûÁé∞ÊñπÂºèÔºåÂÆûÁé∞ÊñπÂºèÔºö‰∏ÄËà¨ÊòØÁ¨¶Âêà S3 ÂçèËÆÆÁöÑ‰∫ëÂ≠òÂÇ®ÔºåÊØîÂ¶Ç AWS ÁöÑ S3 Â≠òÂÇ®„ÄÅMinio„ÄÅ‰∏ÉÁâõ‰∫ëÁ≠â„ÄÇ</li>\n</ul>\n<h5 id=\"24-pvÈÖçÁΩÆÁ§∫‰æãnfs\"><a class=\"anchor\" href=\"#24-pvÈÖçÁΩÆÁ§∫‰æãnfs\">#</a> 2.4 PV ÈÖçÁΩÆÁ§∫‰æã NFS</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv1\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: nfs-slow\n  nfs:\n    path: /data/pv1\n    server: 192.168.1.75\n</code></pre>\n<p>capacityÔºöÂÆπÈáèÈÖçÁΩÆ</p>\n<p>volumeModeÔºöÂç∑ÁöÑÊ®°ÂºèÔºåÁõÆÂâçÊîØÊåÅ FilesystemÔºàÊñá‰ª∂Á≥ªÁªüÔºâ Âíå BlockÔºàÂùóÔºâÔºåÂÖ∂‰∏≠ Block Á±ªÂûãÈúÄË¶ÅÂêéÁ´ØÂ≠òÂÇ®ÊîØÊåÅÔºåÈªòËÆ§‰∏∫Êñá‰ª∂Á≥ªÁªü</p>\n<p>accessModesÔºöËØ• PV ÁöÑËÆøÈóÆÊ®°Âºè</p>\n<p>storageClassNameÔºöPV ÁöÑÁ±ªÔºå‰∏Ä‰∏™ÁâπÂÆöÁ±ªÂûãÁöÑ PV Âè™ËÉΩÁªëÂÆöÂà∞ÁâπÂÆöÁ±ªÂà´ÁöÑ PVCÔºõ</p>\n<p>persistentVolumeReclaimPolicyÔºöÂõûÊî∂Á≠ñÁï•</p>\n<p>mountOptionsÔºöÈùûÂøÖÈ°ªÔºåÊñ∞ÁâàÊú¨‰∏≠Â∑≤ÂºÉÁî®</p>\n<p>nfsÔºöNFS ÊúçÂä°ÈÖçÁΩÆÔºåÂåÖÊã¨‰ª•‰∏ã‰∏§‰∏™ÈÄâÈ°π</p>\n<ul>\n<li>pathÔºöNFS ‰∏äÁöÑÂÖ±‰∫´ÁõÆÂΩï</li>\n<li>serverÔºöNFS ÁöÑ IP Âú∞ÂùÄ</li>\n</ul>\n<h5 id=\"25-pvÈÖçÁΩÆÁ§∫‰æãhostpath\"><a class=\"anchor\" href=\"#25-pvÈÖçÁΩÆÁ§∫‰æãhostpath\">#</a> 2.5 PV ÈÖçÁΩÆÁ§∫‰æã HostPath</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: hostpath\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: hostpath\n  hostPath:\n    path: &quot;/mnt/data&quot;\n</code></pre>\n<p>hostPathÔºöhostPath ÊúçÂä°ÈÖçÁΩÆ</p>\n<ul>\n<li>pathÔºöÂÆø‰∏ªÊú∫Ë∑ØÂæÑ</li>\n</ul>\n<h5 id=\"26-pvÁöÑÁä∂ÊÄÅ\"><a class=\"anchor\" href=\"#26-pvÁöÑÁä∂ÊÄÅ\">#</a> 2.6 PV ÁöÑÁä∂ÊÄÅ</h5>\n<ul>\n<li>AvailableÔºöÂèØÁî®ÔºåÊ≤°ÊúâË¢´ PVC ÁªëÂÆöÁöÑÁ©∫Èó≤ËµÑÊ∫ê„ÄÇ</li>\n<li>BoundÔºöÂ∑≤ÁªëÂÆöÔºåÂ∑≤ÁªèË¢´ PVC ÁªëÂÆö„ÄÇ</li>\n<li>ReleasedÔºöÂ∑≤ÈáäÊîæÔºåPVC Ë¢´Âà†Èô§Ôºå‰ΩÜÊòØËµÑÊ∫êËøòÊú™Ë¢´ÈáçÊñ∞‰ΩøÁî®„ÄÇ</li>\n<li>FailedÔºöÂ§±Ë¥•ÔºåËá™Âä®ÂõûÊî∂Â§±Ë¥•„ÄÇ</li>\n</ul>\n<h5 id=\"27-pvcÁªëÂÆöpv\"><a class=\"anchor\" href=\"#27-pvcÁªëÂÆöpv\">#</a> 2.7 PVC ÁªëÂÆö PV</h5>\n<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  storageClassName: nfs-slow\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi      \n</code></pre>\n<ul>\n<li>PVC ÁöÑÁ©∫Èó¥Áî≥ËØ∑Â§ßÂ∞è‚â§PV ÁöÑÂ§ßÂ∞è</li>\n<li>PVC ÁöÑ StorageClassName Âíå PV ÁöÑ‰∏ÄËá¥</li>\n<li>PVC ÁöÑ accessModes Âíå PV ÁöÑ‰∏ÄËá¥</li>\n</ul>\n<h5 id=\"28-depoymentÊåÇËΩΩpvc\"><a class=\"anchor\" href=\"#28-depoymentÊåÇËΩΩpvc\">#</a> 2.8 Depoyment ÊåÇËΩΩ PVC</h5>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      volumes:\n      - name: nfs-pvc-storage  #volumeÂêçÁß∞\n        persistentVolumeClaim:\n          claimName: nfs-pvc   #PVCÂêçÁß∞\n      containers:\n      - image: nginx\n        name: nginx\n        volumeMounts:\n         - name: nfs-pvc-storage\n          mountPath: /usr/share/nginx/html\n</code></pre>\n<p>ÊåÇËΩΩ PVC ÁöÑ Pod ‰∏ÄÁõ¥Â§Ñ‰∫é PendingÔºö</p>\n<ul>\n<li>PVC Ê≤°ÊúâÂàõÂª∫ÊàêÂäüÊàñ PVC ‰∏çÂ≠òÂú®</li>\n<li>PVC Âíå Pod ‰∏çÂú®Âêå‰∏Ä‰∏™ Namespace</li>\n</ul>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3992668367.html",
            "url": "http://ixuyong.cn/posts/3992668367.html",
            "title": "K8sÈÖçÁΩÆÁÆ°ÁêÜConfigmap",
            "date_published": "2025-04-14T13:47:47.000Z",
            "content_html": "<h3 id=\"k8sÈÖçÁΩÆÁÆ°ÁêÜconfigmap\"><a class=\"anchor\" href=\"#k8sÈÖçÁΩÆÁÆ°ÁêÜconfigmap\">#</a> K8s ÈÖçÁΩÆÁÆ°ÁêÜ Configmap</h3>\n<h4 id=\"1-configmap\"><a class=\"anchor\" href=\"#1-configmap\">#</a> 1. Configmap</h4>\n<h5 id=\"1-1-Âü∫‰∫éfrom-env-fileÂàõÂª∫configmap\"><a class=\"anchor\" href=\"#1-1-Âü∫‰∫éfrom-env-fileÂàõÂª∫configmap\">#</a> 1. 1 Âü∫‰∫é from-env-file ÂàõÂª∫ Configmap</h5>\n<pre><code># cat cm_env.conf \npodname=nf-flms-system\npodip=192.168.1.100\nenv=prod\nnacosaddr=nacos.svc.cluster.local\n\n#kubectl create cm cmenv --from-env-file=./cm_env.conf \n</code></pre>\n<h5 id=\"12-Âü∫‰∫éfrom-literalÂàõÂª∫configmap\"><a class=\"anchor\" href=\"#12-Âü∫‰∫éfrom-literalÂàõÂª∫configmap\">#</a> 1.2 Âü∫‰∫é from-literal ÂàõÂª∫ Configmap</h5>\n<pre><code># kubectl create cm cmliteral --from-literal=level=INFO --from-literal=passwd=Superman*2023\n</code></pre>\n<h5 id=\"13-Âü∫‰∫éfrom-fileÂàõÂª∫configmap\"><a class=\"anchor\" href=\"#13-Âü∫‰∫éfrom-fileÂàõÂª∫configmap\">#</a> 1.3 Âü∫‰∫é from-file ÂàõÂª∫ Configmap</h5>\n<pre><code># cat s.hmallleasing.com.conf \nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    client_max_body_size 1G; \n    location / &#123;\n        proxy_pass http://192.168.1.134;\n        proxy_set_header Host $http_host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        proxy_connect_timeout 30;\n        proxy_send_timeout 60;\n        proxy_read_timeout 60;\n        \n        proxy_buffering on;\n        proxy_buffer_size 32k;\n        proxy_buffers 4 128k;\n        proxy_temp_file_write_size 10240k;\t\t\n        proxy_max_temp_file_size 10240k;\n    &#125;\n&#125;\n\nserver &#123;\n    listen 80;\n    server_name s.hmallleasing.com;\n    return 302 https://$server_name$request_uri;\n&#125;\n\n# kubectl create cm nginxconfig --from-file=./s.hmallleasing.com.conf\n</code></pre>\n<h5 id=\"14-deploymentÊåÇËΩΩconfigmapÁ§∫‰æã\"><a class=\"anchor\" href=\"#14-deploymentÊåÇËΩΩconfigmapÁ§∫‰æã\">#</a> 1.4 Deployment ÊåÇËΩΩ configmap Á§∫‰æã</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.ÊâπÈáèÊåÇËΩΩConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáè\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.Ëá™ÂÆö‰πâÁéØÂ¢ÉÂèòÈáè\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.ÊåÇËΩΩÂçï‰∏™ConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáèÔºåËøôÈáåÂíåConfigMap‰∏≠ÁöÑÈîÆÂêçÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # Ëøô‰∏™ÂÄºÊù•Ëá™ConfigMap\n              key: level            # Êù•Ëá™ConfigMapÁöÑkey\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # Êèê‰æõ‰Ω†ÊÉ≥Ë¶ÅÊåÇËΩΩÁöÑ ConfigMap ÁöÑÂêçÂ≠ó\n</code></pre>\n<h5 id=\"15-ÈáçÂëΩÂêçÊåÇËΩΩÁöÑconfigmaq-keyÁöÑÂêçÁß∞\"><a class=\"anchor\" href=\"#15-ÈáçÂëΩÂêçÊåÇËΩΩÁöÑconfigmaq-keyÁöÑÂêçÁß∞\">#</a> 1.5 ÈáçÂëΩÂêçÊåÇËΩΩÁöÑ configmaq key ÁöÑÂêçÁß∞</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.ÊâπÈáèÊåÇËΩΩConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáè\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.Ëá™ÂÆö‰πâÁéØÂ¢ÉÂèòÈáè\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.ÊåÇËΩΩÂçï‰∏™ConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáèÔºåËøôÈáåÂíåConfigMap‰∏≠ÁöÑÈîÆÂêçÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # Ëøô‰∏™ÂÄºÊù•Ëá™ConfigMap\n              key: level            # Êù•Ëá™ConfigMapÁöÑkey\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # Êèê‰æõ‰Ω†ÊÉ≥Ë¶ÅÊåÇËΩΩÁöÑ ConfigMap ÁöÑÂêçÂ≠ó\n          items:                # ÈáçÂëΩÂêçÊåÇËΩΩÁöÑconfigmaq keyÁöÑÂêçÁß∞‰∏∫nginx.conf\n          - key: s.hmallleasing.com.conf  \n            path: nginx.conf\n \n#Êü•ÁúãÊåÇËΩΩÁöÑconfigmaq keyÁöÑÂêçÁß∞ÈáçÂëΩÂêç‰∏∫nginx.conf\n[root@k8s-master01 cm]# kubectl get pods\nNAME                           READY   STATUS    RESTARTS   AGE\nnginx-deploy-bc476bc56-flln4   1/1     Running   0          10h\nnginx-deploy-bc476bc56-jhsh6   1/1     Running   0          10h\nnginx-deploy-bc476bc56-splv9   1/1     Running   0          10h\n[root@k8s-master01 cm]# kubectl exec -it nginx-deploy-bc476bc56-flln4 -- bash\nroot@nginx-deploy-bc476bc56-flln4:/# ls /etc/nginx/conf.d/\nnginx.conf\n</code></pre>\n<h5 id=\"16-‰øÆÊîπÊåÇËΩΩÁöÑconfigmaq-ÊùÉÈôê\"><a class=\"anchor\" href=\"#16-‰øÆÊîπÊåÇËΩΩÁöÑconfigmaq-ÊùÉÈôê\">#</a> 1.6 ‰øÆÊîπÊåÇËΩΩÁöÑ configmaq ÊùÉÈôê</h5>\n<pre><code>[root@k8s-master01 cm]# cat deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # 1.ÊâπÈáèÊåÇËΩΩConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáè\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # 2.Ëá™ÂÆö‰πâÁéØÂ¢ÉÂèòÈáè\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # 3.ÊåÇËΩΩÂçï‰∏™ConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáèÔºåËøôÈáåÂíåConfigMap‰∏≠ÁöÑÈîÆÂêçÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # Ëøô‰∏™ÂÄºÊù•Ëá™ConfigMap\n              key: level            # Êù•Ëá™ConfigMapÁöÑkey\n        volumeMounts:              \n        - name: nginx-config\n          mountPath: &quot;/etc/nginx/conf.d&quot;\n          readOnly: true\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginxconfig      # Êèê‰æõ‰Ω†ÊÉ≥Ë¶ÅÊåÇËΩΩÁöÑ ConfigMap ÁöÑÂêçÂ≠ó\n          items:                # ÈáçÂëΩÂêçÊåÇËΩΩÁöÑconfigmaq keyÁöÑÂêçÁß∞‰∏∫nginx.conf\n          - key: s.hmallleasing.com.conf  \n            path: nginx.conf\n            mode: 0644        # ÈÖçÁΩÆÊåÇËΩΩÊùÉÈôêÔºåÈíàÂØπÂçï‰∏™keyÁîüÊïà\n          defaultMode: 0666   # ÈÖçÁΩÆÊåÇËΩΩÊùÉÈôêÔºåÈíàÂØπÊï¥‰∏™keyÁîüÊïà\n    \n#Êü•ÁúãÊåÇËΩΩÊùÉÈôê\nroot@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/nginx.conf \nlrwxrwxrwx 1 root root 17 Apr 16 13:37 /etc/nginx/conf.d/nginx.conf -&gt; ..data/nginx.conf\nroot@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/..data/nginx.conf \n-rw-rw-rw- 1 root root 722 Apr 16 13:37 /etc/nginx/conf.d/..data/nginx.conf\n</code></pre>\n<h5 id=\"17-subpathËß£ÂÜ≥ÊåÇËΩΩË¶ÜÁõñÈóÆÈ¢ò\"><a class=\"anchor\" href=\"#17-subpathËß£ÂÜ≥ÊåÇËΩΩË¶ÜÁõñÈóÆÈ¢ò\">#</a> 1.7 subpath Ëß£ÂÜ≥ÊåÇËΩΩË¶ÜÁõñÈóÆÈ¢ò</h5>\n<pre><code>#1.ÂàõÂª∫configmap\n[root@k8s-master01 cm]# cat nginx.conf \n\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents &#123;\n    worker_connections  512;\n&#125;\n\n\nhttp &#123;\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '\n                      '$status $body_bytes_sent &quot;$http_referer&quot; '\n                      '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n&#125;\n\n[root@k8s-master01 cm]# kubectl create cm nginx-config --from-file=./nginx.conf\n\n#subpathËß£ÂÜ≥ÊåÇËΩΩË¶ÜÁõñÈóÆÈ¢ò\n[root@k8s-master01 study]# cat cm-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        envFrom:         # ‚ë†ÊâπÈáèÊåÇËΩΩConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáè\n        - configMapRef:\n            name: cmenv\n        env:\n        - name: MYSQL_ADDR     # ‚ë°Ëá™ÂÆö‰πâÁéØÂ¢ÉÂèòÈáè\n          value: &quot;192.168.40.150&quot;\n        - name: MYSQL_PASSWD\n          value: Superman*2022\n        - name: LOG_LEVEL           # ‚ë¢ÊåÇËΩΩÂçï‰∏™ConfigMapÁîüÊàêÁéØÂ¢ÉÂèòÈáèÔºåËøôÈáåÂíåConfigMap‰∏≠ÁöÑÈîÆÂêçÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ     \n          valueFrom:\n            configMapKeyRef:\n              name: cmliteral       # Ëøô‰∏™ÂÄºÊù•Ëá™ConfigMap\n              key: level            # Êù•Ëá™ConfigMapÁöÑkey\n        volumeMounts:\n        - name: config\n          mountPath: &quot;/etc/nginx/nginx.conf&quot;   #Âè™ÊåÇÂú®nginx.conf‰∏Ä‰∏™Êñá‰ª∂,‰∏çË¶ÜÁõñÁõÆÂΩï\n          subPath: nginx.conf      \n      volumes:\n      - name: config\n        configMap:\n          name: nginx-config      # Êèê‰æõ‰Ω†ÊÉ≥Ë¶ÅÊåÇËΩΩÁöÑConfigMapÁöÑÂêçÂ≠ó\n</code></pre>\n<h4 id=\"2-secret\"><a class=\"anchor\" href=\"#2-secret\">#</a> 2. Secret</h4>\n<h5 id=\"21-secretÊãâÂèñÁßÅÊúâ‰ªìÂ∫ìÈïúÂÉè\"><a class=\"anchor\" href=\"#21-secretÊãâÂèñÁßÅÊúâ‰ªìÂ∫ìÈïúÂÉè\">#</a> 2.1 Secret ÊãâÂèñÁßÅÊúâ‰ªìÂ∫ìÈïúÂÉè</h5>\n<pre><code># kubectl create secret docker-registry harboradmin \\\n--docker-server=s.hmallleasing.com \\\n--docker-username=admin \\\n--docker-password=Superman*2023 \n</code></pre>\n<h5 id=\"22-ÂàõÂª∫ssl-secret\"><a class=\"anchor\" href=\"#22-ÂàõÂª∫ssl-secret\">#</a> 2.2 ÂàõÂª∫ ssl Secret</h5>\n<pre><code># kubectl create secret tls dev.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev\n</code></pre>\n<h5 id=\"23-Âü∫‰∫éÂëΩ‰ª§ÂàõÂª∫generic-secret\"><a class=\"anchor\" href=\"#23-Âü∫‰∫éÂëΩ‰ª§ÂàõÂª∫generic-secret\">#</a> 2.3 Âü∫‰∫éÂëΩ‰ª§ÂàõÂª∫ generic Secret</h5>\n<pre><code>#1.ÈÄöËøáfrom-env-fileÂàõÂª∫\n# cat db.conf \nusername=xuyong\npasswd=Superman*2023\n\n# kubectl create secret generic dbconf --from-env-file=./db.conf\n\n#2.ÈÄöËøáfrom-literalÂàõÂª∫\nkubectl create secret generic db-user-pass \\\n    --from-literal=username=admin \\\n    --from-literal=password='S!B\\*d$zDsb='\n</code></pre>\n<h5 id=\"24-secretÂä†ÂØÜ-Ëß£ÂØÜ\"><a class=\"anchor\" href=\"#24-secretÂä†ÂØÜ-Ëß£ÂØÜ\">#</a> 2.4 Secret Âä†ÂØÜ„ÄÅËß£ÂØÜ</h5>\n<pre><code>1.Âä†ÂØÜ\n# echo -n &quot;Superman*2023&quot; | base64\nU3VwZXJtYW4qMjAyMw==\n\n2.Ëß£ÂØÜ\n# echo &quot;U3VwZXJtYW4qMjAyMw==&quot; | base64 --decode\n</code></pre>\n<h5 id=\"25-Âü∫‰∫éÊñá‰ª∂ÂàõÂª∫ÈùûÂä†ÂØÜgeneric-secret\"><a class=\"anchor\" href=\"#25-Âü∫‰∫éÊñá‰ª∂ÂàõÂª∫ÈùûÂä†ÂØÜgeneric-secret\">#</a> 2.5 Âü∫‰∫éÊñá‰ª∂ÂàõÂª∫ÈùûÂä†ÂØÜ generic Secret</h5>\n<pre><code># kubectl get secret dbconf -oyaml\napiVersion: v1\ndata:\n  passwd: U3VwZXJtYW4qMjAyMw==\n  username: eHV5b25n\nkind: Secret\nmetadata:\n  name: dbconf\n  namespace: default\ntype: Opaque\n</code></pre>\n<h5 id=\"2-6-Âü∫‰∫éyamlÂàõÂª∫Âä†ÂØÜgeneric-secret\"><a class=\"anchor\" href=\"#2-6-Âü∫‰∫éyamlÂàõÂª∫Âä†ÂØÜgeneric-secret\">#</a> 2. 6 Âü∫‰∫é yaml ÂàõÂª∫Âä†ÂØÜ generic Secret</h5>\n<pre><code># cat mysql-secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysql-secret\n  namespace: dev\nstringData:\n  MYSQL_ROOT_PASSWORD: Superman*2023\ntype: Opaque\n</code></pre>\n<h5 id=\"27-deploymentÊåÇËΩΩsecretÁ§∫‰æã\"><a class=\"anchor\" href=\"#27-deploymentÊåÇËΩΩsecretÁ§∫‰æã\">#</a> 2.7 Deployment ÊåÇËΩΩ Secret Á§∫‰æã</h5>\n<pre><code>[root@k8s-master01 study]# cat cm-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx-deploy\n  name: nginx-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      imagePullSecrets:        \n      - name: harboradmin\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: MYSQL_ROOT_PASSWORD  \n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: MYSQL_ROOT_PASSWORD\n</code></pre>\n<h4 id=\"3-configmapsecretÁÉ≠Êõ¥Êñ∞\"><a class=\"anchor\" href=\"#3-configmapsecretÁÉ≠Êõ¥Êñ∞\">#</a> 3. ConfigMap&amp;Secret ÁÉ≠Êõ¥Êñ∞</h4>\n<pre><code># kubectl create cm nginxconfig --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f -\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/858611107.html",
            "url": "http://ixuyong.cn/posts/858611107.html",
            "title": "K8sÊúçÂä°ÂèëÂ∏ÉService",
            "date_published": "2025-04-14T11:25:51.000Z",
            "content_html": "<h3 id=\"k8sÊúçÂä°ÂèëÂ∏Éservice\"><a class=\"anchor\" href=\"#k8sÊúçÂä°ÂèëÂ∏Éservice\">#</a> K8s ÊúçÂä°ÂèëÂ∏É Service</h3>\n<h4 id=\"1-serviceÁ±ªÂûã\"><a class=\"anchor\" href=\"#1-serviceÁ±ªÂûã\">#</a> 1. Service Á±ªÂûã</h4>\n<p>Kubernetes Service TypeÔºàÊúçÂä°Á±ªÂûãÔºâ‰∏ªË¶ÅÂåÖÊã¨‰ª•‰∏ãÂá†ÁßçÔºö</p>\n<ul>\n<li>ClusterIPÔºöÂú®ÈõÜÁæ§ÂÜÖÈÉ®‰ΩøÁî®ÔºåÈªòËÆ§ÂÄºÔºåÂè™ËÉΩ‰ªéÈõÜÁæ§‰∏≠ËÆøÈóÆ„ÄÇ</li>\n<li>NodePortÔºöÂú®ÊâÄÊúâÂÆâË£Ö‰∫Ü Kube-Proxy ÁöÑËäÇÁÇπ‰∏äÊâìÂºÄ‰∏Ä‰∏™Á´ØÂè£ÔºåÊ≠§Á´ØÂè£ÂèØ‰ª•‰ª£ÁêÜËá≥ÂêéÁ´Ø PodÔºåÂèØ‰ª•ÈÄöËøá NodePort ‰ªéÈõÜÁæ§Â§ñÈÉ®ËÆøÈóÆÈõÜÁæ§ÂÜÖÁöÑÊúçÂä°ÔºåÊ†ºÂºè‰∏∫ NodeIP:NodePort„ÄÇ</li>\n<li>LoadBalancerÔºö‰ΩøÁî®‰∫ëÊèê‰æõÂïÜÁöÑË¥üËΩΩÂùáË°°Âô®ÂÖ¨ÂºÄÊúçÂä°ÔºåÊàêÊú¨ËæÉÈ´ò„ÄÇ</li>\n<li>ExternalNameÔºöÈÄöËøáËøîÂõûÂÆö‰πâÁöÑ CNAME Âà´ÂêçÔºåÊ≤°ÊúâËÆæÁΩÆ‰ªª‰ΩïÁ±ªÂûãÁöÑ‰ª£ÁêÜÔºåÈúÄË¶Å 1.7 ÊàñÊõ¥È´òÁâàÊú¨ kube-dns ÊîØÊåÅ„ÄÇ</li>\n</ul>\n<h5 id=\"11-nodeportÁ±ªÂûã\"><a class=\"anchor\" href=\"#11-nodeportÁ±ªÂûã\">#</a> 1.1 NodePort Á±ªÂûã</h5>\n<p>Â¶ÇÊûúÂ∞Ü Service ÁöÑ type Â≠óÊÆµËÆæÁΩÆ‰∏∫ NodePortÔºåÂàô Kubernetes Â∞Ü‰ªé --service-node-port-range ÂèÇÊï∞ÊåáÂÆöÁöÑËåÉÂõ¥ÔºàÈªòËÆ§‰∏∫ 30000-32767Ôºâ‰∏≠Ëá™Âä®ÂàÜÈÖçÁ´ØÂè£Ôºå‰πüÂèØ‰ª•ÊâãÂä®ÊåáÂÆö NodePortÔºåÂàõÂª∫ËØ• Service ÂêéÔºåÈõÜÁæ§ÊØè‰∏™ËäÇÁÇπÈÉΩÂ∞ÜÊö¥Èú≤‰∏Ä‰∏™Á´ØÂè£ÔºåÈÄöËøáÊüê‰∏™ÂÆø‰∏ªÊú∫ÁöÑ IP + Á´ØÂè£Âç≥ÂèØËÆøÈóÆÂà∞ÂêéÁ´ØÁöÑÂ∫îÁî®„ÄÇ</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: NodePort\n</code></pre>\n<h5 id=\"12-clusteripÁ±ªÂûã\"><a class=\"anchor\" href=\"#12-clusteripÁ±ªÂûã\">#</a> 1.2 ClusterIP Á±ªÂûã</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: ClusterIP\n</code></pre>\n<h5 id=\"13-‰ΩøÁî®service‰ª£ÁêÜk8sÂ§ñÈÉ®ÊúçÂä°\"><a class=\"anchor\" href=\"#13-‰ΩøÁî®service‰ª£ÁêÜk8sÂ§ñÈÉ®ÊúçÂä°\">#</a> 1.3 ‰ΩøÁî® Service ‰ª£ÁêÜ K8s Â§ñÈÉ®ÊúçÂä°</h5>\n<p>‰ΩøÁî®Âú∫ÊôØÔºö</p>\n<ul>\n<li>Â∏åÊúõÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠‰ΩøÁî®Êüê‰∏™Âõ∫ÂÆöÁöÑÂêçÁß∞ËÄåÈùû IP Âú∞ÂùÄËÆøÈóÆÂ§ñÈÉ®ÁöÑ‰∏≠Èó¥‰ª∂ÊúçÂä°Ôºõ</li>\n<li>Â∏åÊúõ Service ÊåáÂêëÂè¶‰∏Ä‰∏™ Namespace ‰∏≠ÊàñÂÖ∂‰ªñÈõÜÁæ§‰∏≠ÁöÑÊúçÂä°Ôºõ</li>\n<li>Ê≠£Âú®Â∞ÜÂ∑•‰ΩúË¥üËΩΩËΩ¨ÁßªÂà∞ Kubernetes ÈõÜÁæ§Ôºå‰ΩÜÊòØ‰∏ÄÈÉ®ÂàÜÊúçÂä°‰ªçËøêË°åÂú® Kubernetes ÈõÜÁæ§‰πãÂ§ñÁöÑ backend„ÄÇ</li>\n</ul>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: mysql-svc-external\n  name: mysql-svc-external\nspec:\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306 \n    protocol: TCP\n    targetPort: 3306\n  type: ClusterIP\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  labels:\n    app: mysql-svc-external\n  name: mysql-svc-external\nsubsets:\n- addresses:\n  - ip: 192.168.40.150\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n</code></pre>\n<h5 id=\"14-externalname-service\"><a class=\"anchor\" href=\"#14-externalname-service\">#</a> 1.4 ExternalName Service</h5>\n<p>ExternalName Service ÊòØ Service ÁöÑÁâπ‰æãÔºåÂÆÉÊ≤°Êúâ SelectorÔºå‰πüÊ≤°ÊúâÂÆö‰πâ‰ªª‰ΩïÁ´ØÂè£Âíå EndpointÔºåÂÆÉÈÄöËøáËøîÂõûËØ•Â§ñÈÉ®ÊúçÂä°ÁöÑÂà´ÂêçÊù•Êèê‰æõÊúçÂä°„ÄÇ</p>\n<p>ÊØîÂ¶ÇÂèØ‰ª•ÂÆö‰πâ‰∏Ä‰∏™ ServiceÔºåÂêéÁ´ØËÆæÁΩÆ‰∏∫‰∏Ä‰∏™Â§ñÈÉ®ÂüüÂêçÔºåËøôÊ†∑ÈÄöËøá Service ÁöÑÂêçÁß∞Âç≥ÂèØËÆøÈóÆÂà∞ËØ•ÂüüÂêç„ÄÇ‰ΩøÁî® nslookup Ëß£Êûê‰ª•‰∏ãÊñá‰ª∂ÂÆö‰πâÁöÑ ServiceÔºåÈõÜÁæ§ÁöÑ DNS <a href=\"http://xn--my-uu2cmg2cx7mswf9rko5lsx1a5n3h.database.example.com\">ÊúçÂä°Â∞ÜËøîÂõû‰∏Ä‰∏™ÂÄº‰∏∫ my.database.example.com</a> ÁöÑ CNAME ËÆ∞ÂΩïÔºö</p>\n<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: my-service\n  namespace: prod\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n</code></pre>\n<h5 id=\"15-Â§öÁ´ØÂè£-service\"><a class=\"anchor\" href=\"#15-Â§öÁ´ØÂè£-service\">#</a> 1.5 Â§öÁ´ØÂè£ Service</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  namespace: default\n  labels:\n    app: nginx-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n    - port: 443\n      targetPort: 443\n      protocol: TCP\n      name: https\n  selector:\n    app: nginx\n  type: ClusterIP\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/108692210.html",
            "url": "http://ixuyong.cn/posts/108692210.html",
            "title": "K8sËµÑÊ∫êË∞ÉÂ∫¶deployment„ÄÅstatefulset„ÄÅdaemonset",
            "date_published": "2025-04-14T11:25:00.000Z",
            "content_html": "<h3 id=\"k8sËµÑÊ∫êË∞ÉÂ∫¶deployment-statefulset-daemonset\"><a class=\"anchor\" href=\"#k8sËµÑÊ∫êË∞ÉÂ∫¶deployment-statefulset-daemonset\">#</a> K8s ËµÑÊ∫êË∞ÉÂ∫¶ deployment„ÄÅstatefulset„ÄÅdaemonset</h3>\n<h4 id=\"1-Êó†Áä∂ÊÄÅÂ∫îÁî®ÁÆ°ÁêÜ-deployment\"><a class=\"anchor\" href=\"#1-Êó†Áä∂ÊÄÅÂ∫îÁî®ÁÆ°ÁêÜ-deployment\">#</a> 1. Êó†Áä∂ÊÄÅÂ∫îÁî®ÁÆ°ÁêÜ Deployment</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:1.21.0\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n</code></pre>\n<p>Á§∫‰æãËß£ÊûêÔºö</p>\n<ol>\n<li>\n<p>nginx-deployÔºöDeployment ÁöÑÂêçÁß∞Ôºõ</p>\n</li>\n<li>\n<p>replicasÔºö ÂàõÂª∫ Pod ÁöÑÂâØÊú¨Êï∞Ôºõ</p>\n</li>\n<li>\n<p>selectorÔºöÂÆö‰πâ Deployment Â¶Ç‰ΩïÊâæÂà∞Ë¶ÅÁÆ°ÁêÜÁöÑ PodÔºå‰∏é template ÁöÑ labelÔºàÊ†áÁ≠æÔºâÂØπÂ∫îÔºåapiVersion ‰∏∫ apps/v1 ÂøÖÈ°ªÊåáÂÆöËØ•Â≠óÊÆµÔºõ</p>\n</li>\n<li>\n<p>template Â≠óÊÆµÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµÔºö</p>\n<ul>\n<li>\n<p>app: nginx-deploy ‰ΩøÁî® labelÔºàÊ†áÁ≠æÔºâÊ†áËÆ∞ PodÔºõ</p>\n</li>\n<li>\n<p>specÔºöË°®Á§∫ Pod ËøêË°å‰∏Ä‰∏™ÂêçÂ≠ó‰∏∫ nginx ÁöÑÂÆπÂô®Ôºõ</p>\n</li>\n<li>\n<p>imageÔºöËøêË°åÊ≠§ Pod ‰ΩøÁî®ÁöÑÈïúÂÉèÔºõ</p>\n</li>\n<li>\n<p>PortÔºöÂÆπÂô®Áî®‰∫éÂèëÈÄÅÂíåÊé•Êî∂ÊµÅÈáèÁöÑÁ´ØÂè£„ÄÇ</p>\n</li>\n</ul>\n</li>\n</ol>\n<h5 id=\"11-Êõ¥Êñ∞-deployment\"><a class=\"anchor\" href=\"#11-Êõ¥Êñ∞-deployment\">#</a> 1.1 Êõ¥Êñ∞ Deployment</h5>\n<p>ÂÅáÂ¶ÇÊõ¥Êñ∞ Nginx Pod ÁöÑ image ‰ΩøÁî® nginx:latestÔºåÂπ∂‰ΩøÁî® --record ËÆ∞ÂΩïÂΩìÂâçÊõ¥ÊîπÁöÑÂèÇÊï∞ÔºåÂêéÊúüÂõûÊªöÊó∂ÂèØ‰ª•Êü•ÁúãÂà∞ÂØπÂ∫îÁöÑ‰ø°ÊÅØÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record\n</code></pre>\n<p>Êõ¥Êñ∞ËøáÁ®ã‰∏∫Êñ∞Êóß‰∫§ÊõøÊõ¥Êñ∞ÔºåÈ¶ñÂÖàÊñ∞Âª∫‰∏Ä‰∏™ PodÔºåÂΩì Pod Áä∂ÊÄÅ‰∏∫ Running Êó∂ÔºåÂà†Èô§‰∏Ä‰∏™ÊóßÁöÑ PodÔºåÂêåÊó∂ÂÜçÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑ Pod„ÄÇÂΩìËß¶Âèë‰∏Ä‰∏™Êõ¥Êñ∞ÂêéÔºå‰ºöÊúâÊñ∞ÁöÑ ReplicaSet ‰∫ßÁîüÔºåÊóßÁöÑ ReplicaSet ‰ºöË¢´‰øùÂ≠òÔºåÊü•ÁúãÊ≠§Êó∂ ReplicaSetÔºåÂèØ‰ª•‰ªé AGE Êàñ READY ÁúãÂá∫Êù•Êñ∞Êóß ReplicaSetÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get rs\nNAME                      DESIRED   CURRENT   READY   AGE\nnginx-deploy-65bfb77869   0         0         0       50s\nnginx-deploy-85b94dddb4   3         3         3       8s\n</code></pre>\n<p>ÈÄöËøá describe Êü•Áúã Deployment ÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºö</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl describe deploy nginx-deploy\nName:                   nginx-deploy\nNamespace:              default\nCreationTimestamp:      Mon, 14 Apr 2025 11:28:03 +0800\nLabels:                 app=nginx-deploy\nAnnotations:            app: nginx-deploy\n                        deployment.kubernetes.io/revision: 2\n                        kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\nSelector:               app=nginx-deploy\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=nginx-deploy\n  Containers:\n   nginx-deploy:\n    Image:         nginx:latest\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Environment:   &lt;none&gt;\n    Mounts:        &lt;none&gt;\n  Volumes:         &lt;none&gt;\n  Node-Selectors:  &lt;none&gt;\n  Tolerations:     &lt;none&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  nginx-deploy-65bfb77869 (0/0 replicas created)\nNewReplicaSet:   nginx-deploy-85b94dddb4 (3/3 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  71s   deployment-controller  Scaled up replica set nginx-deploy-65bfb77869 from 0 to 3\n  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 0 to 1\n  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 3 to 2\n  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 1 to 2\n  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 2 to 1\n  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 2 to 3\n  Normal  ScalingReplicaSet  26s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 1 to 0\n</code></pre>\n<p>Âú® describe ‰∏≠ÂèØ‰ª•ÁúãÂá∫ÔºåÁ¨¨‰∏ÄÊ¨°ÂàõÂª∫Êó∂ÔºåÂÆÉÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ nginx-deploy-65bfb77869 ÁöÑ ReplicaSetÔºåÂπ∂Áõ¥Êé•Â∞ÜÂÖ∂Êâ©Â±ï‰∏∫ 3 ‰∏™ÂâØÊú¨„ÄÇÊõ¥Êñ∞ÈÉ®ÁΩ≤Êó∂ÔºåÂÆÉÂàõÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑ ReplicaSetÔºåÂëΩÂêç‰∏∫ nginx-deploy-85b94dddb4ÔºåÂπ∂Â∞ÜÂÖ∂ÂâØÊú¨Êï∞Êâ©Â±ï‰∏∫ 1ÔºåÁÑ∂ÂêéÂ∞ÜÊóßÁöÑ ReplicaSet Áº©Â∞è‰∏∫ 2ÔºåËøôÊ†∑Ëá≥Â∞ëÂèØ‰ª•Êúâ 2 ‰∏™ Pod ÂèØÁî®ÔºåÊúÄÂ§öÂàõÂª∫‰∫Ü 4 ‰∏™ Pod„ÄÇ‰ª•Ê≠§Á±ªÊé®Ôºå‰ΩøÁî®Áõ∏ÂêåÁöÑÊªöÂä®Êõ¥Êñ∞Á≠ñÁï•Âêë‰∏äÂíåÂêë‰∏ãÊâ©Â±ïÊñ∞Êóß ReplicaSetÔºåÊúÄÁªàÊñ∞ÁöÑ ReplicaSet ÂèØ‰ª•Êã•Êúâ 3 ‰∏™ÂâØÊú¨ÔºåÂπ∂Â∞ÜÊóßÁöÑ ReplicaSet Áº©Â∞è‰∏∫ 0„ÄÇ</p>\n<h5 id=\"12-ÂõûÊªö-deployment\"><a class=\"anchor\" href=\"#12-ÂõûÊªö-deployment\">#</a> 1.2 ÂõûÊªö Deployment</h5>\n<p>ÂΩìÊõ¥Êñ∞‰∫ÜÁâàÊú¨‰∏çÁ®≥ÂÆöÊàñÈÖçÁΩÆ‰∏çÂêàÁêÜÊó∂ÔºåÂèØ‰ª•ÂØπÂÖ∂ËøõË°åÂõûÊªöÊìç‰ΩúÔºåÂÅáËÆæÊàë‰ª¨ÂèàËøõË°å‰∫ÜÂá†Ê¨°Êõ¥Êñ∞ÔºàÊ≠§Â§Ñ‰ª•Êõ¥Êñ∞ÈïúÂÉèÁâàÊú¨Ëß¶ÂèëÊõ¥Êñ∞ÔºåÊõ¥ÊîπÈÖçÁΩÆÊïàÊûúÁ±ª‰ººÔºâÔºö</p>\n<pre><code># kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record\n# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record\n</code></pre>\n<p>‰ΩøÁî® kubectl rollout history Êü•ÁúãÊõ¥Êñ∞ÂéÜÂè≤Ôºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy\ndeployment.apps/nginx-deploy \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\n3         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true\n4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n</code></pre>\n<p>Êü•Áúã Deployment ÊüêÊ¨°Êõ¥Êñ∞ÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºå‰ΩøÁî® --revision ÊåáÂÆöÊüêÊ¨°Êõ¥Êñ∞ÁâàÊú¨Âè∑Ôºö</p>\n<pre><code># kubectl rollout history deployment nginx-deploy --revision=4\ndeployment.apps/nginx-deploy with revision #4\nPod Template:\n  Labels:\tapp=nginx-deploy\n\tpod-template-hash=65b576b795\n  Annotations:\tkubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n  Containers:\n   nginx-deploy:\n    Image:\tnginx:1.21.2\n    Port:\t&lt;none&gt;\n    Host Port:\t&lt;none&gt;\n    Environment:\t&lt;none&gt;\n    Mounts:\t&lt;none&gt;\n  Volumes:\t&lt;none&gt;\n  Node-Selectors:\t&lt;none&gt;\n  Tolerations:\t&lt;none&gt;\n</code></pre>\n<p>Â¶ÇÊûúÂè™ÈúÄË¶ÅÂõûÊªöÂà∞‰∏ä‰∏Ä‰∏™Á®≥ÂÆöÁâàÊú¨Ôºå‰ΩøÁî® kubectl rollout undo Âç≥ÂèØÔºö</p>\n<pre><code># kubectl rollout undo deployment nginx-deploy\n</code></pre>\n<p>ÂÜçÊ¨°Êü•ÁúãÊõ¥Êñ∞ÂéÜÂè≤ÔºåÂèëÁé∞ REVISION3 ÂõûÂà∞‰∫Ü nginx:1.21.1Ôºö</p>\n<pre><code># kubectl rollout history deployment nginx-deploy\ndeployment.apps/nginx-deploy \nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true\n4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true\n5         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true\n</code></pre>\n<p>Â¶ÇÊûúË¶ÅÂõûÊªöÂà∞ÊåáÂÆöÁâàÊú¨Ôºå‰ΩøÁî® --to-revision ÂèÇÊï∞Ôºö</p>\n<pre><code># kubectl rollout undo deployment nginx-deploy --to-revision=2\n</code></pre>\n<h5 id=\"13-Êâ©ÂÆπ-deployment\"><a class=\"anchor\" href=\"#13-Êâ©ÂÆπ-deployment\">#</a> 1.3 Êâ©ÂÆπ Deployment</h5>\n<p>ÂΩìÂÖ¨Âè∏ËÆøÈóÆÈáèÂèòÂ§ßÔºåÊàñËÄÖÊúâÈ¢ÑÊúüÂÜÖÁöÑÊ¥ªÂä®Êó∂Ôºå‰∏â‰∏™ Pod ÂèØËÉΩÂ∑≤Êó†Ê≥ïÊîØÊíë‰∏öÂä°Êó∂ÔºåÂèØ‰ª•ÊèêÂâçÂØπÂÖ∂ËøõË°åÊâ©Â±ï„ÄÇ</p>\n<p>‰ΩøÁî® kubectl scale Âä®ÊÄÅË∞ÉÊï¥ Pod ÁöÑÂâØÊú¨Êï∞ÔºåÊØîÂ¶ÇÂ¢ûÂä† Pod ‰∏∫ 5 ‰∏™Ôºö</p>\n<pre><code># kubectl scale deployment nginx-deploy --replicas=5\n</code></pre>\n<p>Êü•Áúã PodÔºåÊ≠§Êó∂ Pod Â∑≤ÁªèÂèòÊàê‰∫Ü 5 ‰∏™Ôºö</p>\n<pre><code># kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nnginx-deploy-85b94dddb4-2qrh6   1/1     Running   0          2m9s\nnginx-deploy-85b94dddb4-gvkqj   1/1     Running   0          2m10s\nnginx-deploy-85b94dddb4-mdfjs   1/1     Running   0          22s\nnginx-deploy-85b94dddb4-rhgpr   1/1     Running   0          2m8s\nnginx-deploy-85b94dddb4-vwjhl   1/1     Running   0          22s\n</code></pre>\n<h5 id=\"14-ÊöÇÂÅúÂíåÊÅ¢Â§ç-deployment-Êõ¥Êñ∞\"><a class=\"anchor\" href=\"#14-ÊöÇÂÅúÂíåÊÅ¢Â§ç-deployment-Êõ¥Êñ∞\">#</a> 1.4 ÊöÇÂÅúÂíåÊÅ¢Â§ç Deployment Êõ¥Êñ∞</h5>\n<p>‰∏äËø∞ÊºîÁ§∫ÁöÑÂùá‰∏∫Êõ¥ÊîπÊüê‰∏ÄÂ§ÑÁöÑÈÖçÁΩÆÔºåÊõ¥ÊîπÂêéÁ´ãÂç≥Ëß¶ÂèëÊõ¥Êñ∞ÔºåÂ§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÂèØËÉΩÈúÄË¶ÅÈíàÂØπ‰∏Ä‰∏™ËµÑÊ∫êÊñá‰ª∂Êõ¥ÊîπÂ§öÂ§ÑÂú∞ÊñπÔºåËÄåÂπ∂‰∏çÈúÄË¶ÅÂ§öÊ¨°Ëß¶ÂèëÊõ¥Êñ∞ÔºåÊ≠§Êó∂ÂèØ‰ª•‰ΩøÁî® Deployment ÊöÇÂÅúÂäüËÉΩÔºå‰∏¥Êó∂Á¶ÅÁî®Êõ¥Êñ∞Êìç‰ΩúÔºåÂØπ Deployment ËøõË°åÂ§öÊ¨°‰øÆÊîπÂêéÂú®ËøõË°åÊõ¥Êñ∞„ÄÇ</p>\n<p>‰ΩøÁî® kubectl rollout pause ÂëΩ‰ª§Âç≥ÂèØÊöÇÂÅú Deployment Êõ¥Êñ∞Ôºö</p>\n<pre><code># kubectl rollout pause deployment nginx-deploy\n</code></pre>\n<p>ÁÑ∂ÂêéÂØπ Deployment ËøõË°åÁõ∏ÂÖ≥Êõ¥Êñ∞Êìç‰ΩúÔºåÊØîÂ¶ÇÂÖàÊõ¥Êñ∞ÈïúÂÉèÔºåÁÑ∂ÂêéÂØπÂÖ∂ËµÑÊ∫êËøõË°åÈôêÂà∂ÔºàÂ¶ÇÊûú‰ΩøÁî®ÁöÑÊòØ kubectl edit ÂëΩ‰ª§ÔºåÂèØ‰ª•Áõ¥Êé•ËøõË°åÂ§öÊ¨°‰øÆÊîπÔºåÊó†ÈúÄÊöÇÂÅúÊõ¥Êñ∞Ôºåkubectlset ÂëΩ‰ª§‰∏ÄËà¨‰ºöÈõÜÊàêÂú® CICD ÊµÅÊ∞¥Á∫ø‰∏≠ÔºâÔºö</p>\n<pre><code># kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.3\n# kubectl set resources deployment nginx-deploy -c=nginx-deploy --limits=cpu=200m,memory=512Mi\n</code></pre>\n<p>ÈÄöËøá rollout history ÂèØ‰ª•ÁúãÂà∞Ê≤°ÊúâÊñ∞ÁöÑÊõ¥Êñ∞Ôºö</p>\n<pre><code>#  kubectl rollout history deployment nginx-deploy\n</code></pre>\n<p>ËøõË°åÂÆåÊúÄÂêé‰∏ÄÂ§ÑÈÖçÁΩÆÊõ¥ÊîπÂêéÔºå‰ΩøÁî® kubectl rollout resume ÊÅ¢Â§ç Deployment Êõ¥Êñ∞Ôºö</p>\n<pre><code># kubectl rollout resume deployment nginx-deploy\n</code></pre>\n<p>ÂèØ‰ª•Êü•ÁúãÂà∞ÊÅ¢Â§çÊõ¥Êñ∞ÁöÑ Deployment ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑ RSÔºàReplicaSet Áº©ÂÜôÔºâÔºö</p>\n<pre><code># kubectl get rs\n</code></pre>\n<p>ÂèØ‰ª•Êü•Áúã Deployment ÁöÑ imageÔºàÈïúÂÉèÔºâÂ∑≤ÁªèÂèò‰∏∫ nginx:1.21.3</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n    - image: nginx:1.21.3\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.3\n      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36\n</code></pre>\n<h5 id=\"15-Êõ¥Êñ∞-deployment-ÁöÑÊ≥®ÊÑè‰∫ãÈ°π\"><a class=\"anchor\" href=\"#15-Êõ¥Êñ∞-deployment-ÁöÑÊ≥®ÊÑè‰∫ãÈ°π\">#</a> 1.5 Êõ¥Êñ∞ Deployment ÁöÑÊ≥®ÊÑè‰∫ãÈ°π</h5>\n<p>Âú®ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºårevision ‰øùÁïô 10 ‰∏™ÊóßÁöÑ ReplicaSetÔºåÂÖ∂‰ΩôÁöÑÂ∞ÜÂú®ÂêéÂè∞ËøõË°åÂûÉÂúæÂõûÊî∂ÔºåÂèØ‰ª•Âú®.spec.revisionHistoryLimit ËÆæÁΩÆ‰øùÁïô ReplicaSet ÁöÑ‰∏™Êï∞„ÄÇÂΩìËÆæÁΩÆ‰∏∫ 0 Êó∂Ôºå‰∏ç‰øùÁïôÂéÜÂè≤ËÆ∞ÂΩï„ÄÇ</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  namespace: default\n  labels:\n    app: nginx-deploy\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:1.21.3\n          resources:\n            limits:\n              cpu: 200m\n              memory: 512Mi\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  revisionHistoryLimit: 10\n</code></pre>\n<p>Êõ¥Êñ∞Á≠ñÁï•Ôºö</p>\n<ul>\n<li>spec.strategy.type==RecreateÔºåË°®Á§∫ÈáçÂª∫ÔºåÂÖàÂà†ÊéâÊóßÁöÑ Pod ÂÜçÂàõÂª∫Êñ∞ÁöÑ PodÔºõ</li>\n</ul>\n<pre><code>  strategy:\n    type: Recreate\n</code></pre>\n<ul>\n<li>\n<p>spec.strategy.type==RollingUpdateÔºåË°®Á§∫ÊªöÂä®Êõ¥Êñ∞ÔºåÂèØ‰ª•ÊåáÂÆö maxUnavailable Âíå maxSurge Êù•ÊéßÂà∂ÊªöÂä®Êõ¥Êñ∞ËøáÁ®ãÔºõ</p>\n<ul>\n<li>\n<p>spec.strategy.rollingUpdate.maxUnavailableÔºåÊåáÂÆöÂú®ÂõûÊªöÊõ¥Êñ∞Êó∂ÊúÄÂ§ß‰∏çÂèØÁî®ÁöÑ Pod Êï∞ÈáèÔºåÂèØÈÄâÂ≠óÊÆµÔºåÈªòËÆ§‰∏∫ 25%ÔºåÂèØ‰ª•ËÆæÁΩÆ‰∏∫Êï∞Â≠óÊàñÁôæÂàÜÊØîÔºåÂ¶ÇÊûú maxSurge ‰∏∫ 0ÔºåÂàôËØ•ÂÄº‰∏çËÉΩ‰∏∫ 0Ôºõ</p>\n</li>\n<li>\n<p>spec.strategy.rollingUpdate.maxSurge ÂèØ‰ª•Ë∂ÖËøáÊúüÊúõÂÄºÁöÑÊúÄÂ§ß Pod Êï∞ÔºåÂèØÈÄâÂ≠óÊÆµÔºåÈªòËÆ§‰∏∫ 25%ÔºåÂèØ‰ª•ËÆæÁΩÆÊàêÊï∞Â≠óÊàñÁôæÂàÜÊØîÔºåÂ¶ÇÊûú maxUnavailable ‰∏∫ 0ÔºåÂàôËØ•ÂÄº‰∏çËÉΩ‰∏∫ 0„ÄÇ</p>\n</li>\n</ul>\n</li>\n</ul>\n<pre><code>  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n</code></pre>\n<h4 id=\"2-ÊúâÁä∂ÊÄÅÂ∫îÁî®ÁÆ°ÁêÜ-statefulset\"><a class=\"anchor\" href=\"#2-ÊúâÁä∂ÊÄÅÂ∫îÁî®ÁÆ°ÁêÜ-statefulset\">#</a> 2. ÊúâÁä∂ÊÄÅÂ∫îÁî®ÁÆ°ÁêÜ StatefulSet</h4>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web\n  namespace: default\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx\n  type: ClusterIP\n  clusterIP: None\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx\n  namespace: default\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          resources:\n            limits:\n              cpu: '1'\n              memory: 1Gi\n            requests:\n              cpu: 100m\n              memory: 128Mi\n      restartPolicy: Always\n  serviceName: web\n</code></pre>\n<ul>\n<li>kind: Service ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂêçÂ≠ó‰∏∫ web ÁöÑ Headless ServiceÔºåÂàõÂª∫ÁöÑ Service Ê†ºÂºè‰∏∫ nginx-0.web.default.svc.cluster.localÔºåÂÖ∂‰ªñÁöÑÁ±ª‰ººÔºåÂõ†‰∏∫Ê≤°ÊúâÊåáÂÆö NamespaceÔºàÂëΩÂêçÁ©∫Èó¥ÔºâÔºåÊâÄ‰ª•ÈªòËÆ§ÈÉ®ÁΩ≤Âú® defaultÔºõ</li>\n<li>kind: StatefulSet ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂêçÂ≠ó‰∏∫ nginx ÁöÑ StatefulSetÔºåreplicas Ë°®Á§∫ÈÉ®ÁΩ≤ Pod ÁöÑÂâØÊú¨Êï∞ÔºåÊú¨ÂÆû‰æã‰∏∫ 3„ÄÇ</li>\n</ul>\n<h5 id=\"21-ÂàõÂª∫-statefulset\"><a class=\"anchor\" href=\"#21-ÂàõÂª∫-statefulset\">#</a> 2.1 ÂàõÂª∫ StatefulSet</h5>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods\nNAME      READY   STATUS    RESTARTS   AGE\nnginx-0   1/1     Running   0          8m51s\nnginx-1   1/1     Running   0          8m50s\nnginx-2   1/1     Running   0          8m48s\n[root@k8s-master01 ~]# kubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   6d1h\nweb          ClusterIP   None         &lt;none&gt;        80/TCP    9m28s\n[root@k8s-master01 ~]# kubectl get sts\nNAME    READY   AGE\nnginx   3/3     8m58s\n</code></pre>\n<h5 id=\"22-statefulsetÂàõÂª∫podÊµÅÁ®ã\"><a class=\"anchor\" href=\"#22-statefulsetÂàõÂª∫podÊµÅÁ®ã\">#</a> 2.2 StatefulSet ÂàõÂª∫ Pod ÊµÅÁ®ã</h5>\n<p>StatefulSet ÁÆ°ÁêÜÁöÑ Pod ÈÉ®ÁΩ≤ÂíåÊâ©Â±ïËßÑÂàôÂ¶Ç‰∏ãÔºö</p>\n<ul>\n<li>ÂØπ‰∫éÂÖ∑Êúâ N ‰∏™ÂâØÊú¨ÁöÑ StatefulSetÔºåÂ∞ÜÊåâÈ°∫Â∫è‰ªé 0 Âà∞ N-1 ÂºÄÂßãÂàõÂª∫ PodÔºõ</li>\n<li>ÂΩìÂà†Èô§ Pod Êó∂ÔºåÂ∞ÜÊåâÁÖß N-1 Âà∞ 0 ÁöÑÂèçÈ°∫Â∫èÁªàÊ≠¢Ôºõ</li>\n<li>Âú®Áº©Êîæ Pod ‰πãÂâçÔºåÂøÖÈ°ª‰øùËØÅÂΩìÂâçÁöÑ Pod ÊòØ RunningÔºàËøêË°å‰∏≠ÔºâÊàñËÄÖ ReadyÔºàÂ∞±Áª™ÔºâÔºõ</li>\n<li>Âú®ÁªàÊ≠¢ Pod ‰πãÂâçÔºåÂÆÉÊâÄÊúâÁöÑÁªß‰ªªËÄÖÂøÖÈ°ªÊòØÂÆåÂÖ®ÂÖ≥Èó≠Áä∂ÊÄÅ„ÄÇ</li>\n</ul>\n<p>StatefulSet ÁöÑ pod.Spec.TerminationGracePeriodSecondsÔºàÁªàÊ≠¢ Pod ÁöÑÁ≠âÂæÖÊó∂Èó¥Ôºâ‰∏çÂ∫îËØ•ÊåáÂÆö‰∏∫ 0ÔºåËÆæÁΩÆ‰∏∫ 0 ÂØπ StatefulSet ÁöÑ Pod ÊòØÊûÅÂÖ∂‰∏çÂÆâÂÖ®ÁöÑÂÅöÊ≥ïÔºå‰ºòÈõÖÂú∞Âà†Èô§ StatefulSet ÁöÑ Pod ÊòØÈùûÂ∏∏ÊúâÂøÖË¶ÅÁöÑÔºåËÄå‰∏îÊòØÂÆâÂÖ®ÁöÑÔºåÂõ†‰∏∫ÂÆÉÂèØ‰ª•Á°Æ‰øùÂú® Kubelet ‰ªé APIServer Âà†Èô§‰πãÂâçÔºåËÆ© Pod Ê≠£Â∏∏ÂÖ≥Èó≠„ÄÇ</p>\n<p>ÂΩìÂàõÂª∫‰∏äÈù¢ÁöÑ Nginx ÂÆû‰æãÊó∂ÔºåPod Â∞ÜÊåâ nginx-0„ÄÅnginx-1„ÄÅnginx-2 ÁöÑÈ°∫Â∫èÈÉ®ÁΩ≤ 3 ‰∏™ Pod„ÄÇÂú® nginx-0 Â§Ñ‰∫é Running ÊàñËÄÖ Ready ‰πãÂâçÔºånginx-1 ‰∏ç‰ºöË¢´ÈÉ®ÁΩ≤ÔºåÁõ∏ÂêåÁöÑÔºånginx-2 Âú® web-1 Êú™Â§Ñ‰∫é Running Âíå Ready ‰πãÂâç‰πü‰∏ç‰ºöË¢´ÈÉ®ÁΩ≤„ÄÇÂ¶ÇÊûúÂú® nginx-1 Â§Ñ‰∫é Running Âíå Ready Áä∂ÊÄÅÊó∂Ôºånginx-0 ÂèòÊàê Failed Â§±Ë¥•ÔºâÁä∂ÊÄÅÔºåÈÇ£‰πà nginx-2 Â∞Ü‰∏ç‰ºöË¢´ÂêØÂä®ÔºåÁõ¥Âà∞ nginx-0 ÊÅ¢Â§ç‰∏∫ Running Âíå Ready Áä∂ÊÄÅ„ÄÇ</p>\n<p>Â¶ÇÊûúÁî®Êà∑Â∞Ü StatefulSet ÁöÑ replicas ËÆæÁΩÆ‰∏∫ 1ÔºåÈÇ£‰πà nginx-2 Â∞ÜÈ¶ñÂÖàË¢´ÁªàÊ≠¢ÔºåÂú®ÂÆåÂÖ®ÂÖ≥Èó≠Âπ∂Âà†Èô§ nginx-2 ‰πãÂâçÔºå‰∏ç‰ºöÂà†Èô§ nginx-1„ÄÇÂ¶ÇÊûú nginx-2 ÁªàÊ≠¢Âπ∂‰∏îÂÆåÂÖ®ÂÖ≥Èó≠ÂêéÔºånginx-0 Á™ÅÁÑ∂Â§±Ë¥•ÔºåÈÇ£‰πàÂú® nginx-0 Êú™ÊÅ¢Â§çÊàê Running ÊàñËÄÖ Ready Êó∂Ôºånginx-1 ‰∏ç‰ºöË¢´Âà†Èô§„ÄÇ</p>\n<h5 id=\"23-tatefulset-Êâ©ÂÆπÂíåÁº©ÂÆπ\"><a class=\"anchor\" href=\"#23-tatefulset-Êâ©ÂÆπÂíåÁº©ÂÆπ\">#</a> 2.3 tatefulSet Êâ©ÂÆπÂíåÁº©ÂÆπ</h5>\n<p>Âíå Deployment Á±ª‰ººÔºåÂèØ‰ª•ÈÄöËøáÊõ¥Êñ∞ replicas Â≠óÊÆµÊâ©ÂÆπ / Áº©ÂÆπ StatefulSetÔºå‰πüÂèØ‰ª•‰ΩøÁî® kubectlscale„ÄÅkubectl edit Âíå kubectl patch Êù•Êâ©ÂÆπ / Áº©ÂÆπ‰∏Ä‰∏™ StatefulSet„ÄÇ</p>\n<pre><code># kubectl scale sts nginx --replicas=5\n</code></pre>\n<h5 id=\"24-statefulset-Êõ¥Êñ∞Á≠ñÁï•\"><a class=\"anchor\" href=\"#24-statefulset-Êõ¥Êñ∞Á≠ñÁï•\">#</a> 2.4 StatefulSet Êõ¥Êñ∞Á≠ñÁï•</h5>\n<p><strong>On Delete Á≠ñÁï•</strong></p>\n<p>OnDelete Êõ¥Êñ∞Á≠ñÁï•ÂÆûÁé∞‰∫Ü‰º†ÁªüÔºà1.7 ÁâàÊú¨‰πãÂâçÔºâÁöÑË°å‰∏∫ÔºåÂÆÉ‰πüÊòØÈªòËÆ§ÁöÑÊõ¥Êñ∞Á≠ñÁï•„ÄÇÂΩìÊàë‰ª¨ÈÄâÊã©Ëøô‰∏™Êõ¥Êñ∞Á≠ñÁï•Âπ∂‰øÆÊîπ StatefulSet ÁöÑ.spec.template Â≠óÊÆµÊó∂ÔºåStatefulSet ÊéßÂà∂Âô®‰∏ç‰ºöËá™Âä®Êõ¥Êñ∞ PodÔºåÂøÖÈ°ªÊâãÂä®Âà†Èô§ Pod ÊâçËÉΩ‰ΩøÊéßÂà∂Âô®ÂàõÂª∫Êñ∞ÁöÑ Pod„ÄÇ</p>\n<pre><code>  updateStrategy:\n    type: OnDelete\n</code></pre>\n<p><strong>RollingUpdate Á≠ñÁï•</strong></p>\n<p>RollingUpdateÔºàÊªöÂä®Êõ¥Êñ∞ÔºâÊõ¥Êñ∞Á≠ñÁï•‰ºöËá™Âä®Êõ¥Êñ∞‰∏Ä‰∏™ StatefulSet ‰∏≠ÊâÄÊúâÁöÑ PodÔºåÈááÁî®‰∏éÂ∫èÂè∑Á¥¢ÂºïÁõ∏ÂèçÁöÑÈ°∫Â∫èËøõË°åÊªöÂä®Êõ¥Êñ∞„ÄÇ</p>\n<pre><code>  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 0\n</code></pre>\n<h5 id=\"25-ÂàÜÊÆµÊõ¥Êñ∞\"><a class=\"anchor\" href=\"#25-ÂàÜÊÆµÊõ¥Êñ∞\">#</a> 2.5 ÂàÜÊÆµÊõ¥Êñ∞</h5>\n<p>Â∞ÜÂàÜÂå∫Êîπ‰∏∫ 2ÔºåÊ≠§Êó∂‰ºöËá™Âä®Êõ¥Êñ∞ nginx-2„ÄÅnginx-3„ÄÅnginx-4ÔºàÂõ†‰∏∫‰πãÂâçÊõ¥Êîπ‰∫ÜÊõ¥Êñ∞Á≠ñÁï•ÔºâÔºå‰ΩÜÊòØ‰∏ç‰ºöÊõ¥Êñ∞ nginx-0 Âíå nginx-1Ôºö</p>\n<pre><code>  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 2\n</code></pre>\n<p>Â∞Ü sts ÈïúÂÉè‰∏∫ nginx:1.21.1</p>\n<pre><code># kubectl set image sts nginx nginx=nginx:1.21.1\n</code></pre>\n<p>ÊåâÁÖß‰∏äËø∞ÊñπÂºèÔºåÂèØ‰ª•ÂÆûÁé∞ÂàÜÈò∂ÊÆµÊõ¥Êñ∞ÔºåÁ±ª‰ºº‰∫éÁÅ∞Â∫¶ / Èáë‰∏ùÈõÄÂèëÂ∏É„ÄÇÊü•ÁúãÊúÄÁªàÁöÑÁªìÊûúÂ¶Ç‰∏ãÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image\n    - image: nginx:latest\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:latest\n      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8\n    - image: nginx:latest\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:latest\n      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n    - image: nginx:1.21.1\n      imagePullPolicy: IfNotPresent\n      image: docker.io/library/nginx:1.21.1\n      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e\n</code></pre>\n<h5 id=\"26-statefulset-ÊåÇËΩΩÂä®ÊÄÅÂ≠òÂÇ®\"><a class=\"anchor\" href=\"#26-statefulset-ÊåÇËΩΩÂä®ÊÄÅÂ≠òÂÇ®\">#</a> 2.6 StatefulSet ÊåÇËΩΩÂä®ÊÄÅÂ≠òÂÇ®</h5>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx \n  serviceName: &quot;nginx&quot;\n  replicas: 3 1\n  template:\n    metadata:\n      labels:\n        app: nginx \n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.20\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ &quot;ReadWriteOnce&quot; ]\n      storageClassName: &quot;rook-ceph-block&quot;\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>\n<h4 id=\"3ÂÆàÊä§ËøõÁ®ãÈõÜ-daemonset\"><a class=\"anchor\" href=\"#3ÂÆàÊä§ËøõÁ®ãÈõÜ-daemonset\">#</a> 3. ÂÆàÊä§ËøõÁ®ãÈõÜ DaemonSet</h4>\n<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ds\n  labels:\n    app: nginx-ds\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-ds\n  template:\n    metadata:\n      labels:\n        app: nginx-ds\n    spec:\n      containers:\n        - name: nginx-ds\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<p>Ê≠§Êó∂‰ºöÂú®ÊØè‰∏™ËäÇÁÇπÂàõÂª∫‰∏Ä‰∏™ PodÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES\nnginx-ds-47dxc   1/1     Running   0          56s   172.16.85.213    k8s-node01     &lt;none&gt;           &lt;none&gt;\nnginx-ds-4m89f   1/1     Running   0          56s   172.16.32.143    k8s-master01   &lt;none&gt;           &lt;none&gt;\nnginx-ds-mtpc2   1/1     Running   0          56s   172.16.195.12    k8s-master03   &lt;none&gt;           &lt;none&gt;\nnginx-ds-t5rxc   1/1     Running   0          56s   172.16.122.142   k8s-master02   &lt;none&gt;           &lt;none&gt;\nnginx-ds-x86kc   1/1     Running   0          56s   172.16.58.222    k8s-node02     &lt;none&gt;           &lt;none&gt;\n</code></pre>\n<p>ÊåáÂÆöËäÇÁÇπÈÉ®ÁΩ≤ Pod</p>\n<pre><code>      nodeSelector:\n        ingress: 'true'\n</code></pre>\n<p>Êõ¥Êñ∞ÂíåÂõûÊªö DaemonSet</p>\n<pre><code># kubectl set image ds nginx-ds nginx-ds=1.21.0 --record=true\n# kubectl rollout undo daemonset &lt;daemonset-name&gt; --to-revision=&lt;revision&gt;\n</code></pre>\n<p>DaemonSet ÁöÑÊõ¥Êñ∞ÂíåÂõûÊªö‰∏é Deployment Á±ª‰ººÔºåÊ≠§Â§Ñ‰∏çÂÜçÊºîÁ§∫„ÄÇ</p>\n<h4 id=\"4-hpa\"><a class=\"anchor\" href=\"#4-hpa\">#</a> 4. HPA</h4>\n<p>ÂàõÂª∫ deployment„ÄÅservice</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-hpa-svc\n  namespace: default\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: nginx-hpa\n  type: ClusterIP\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-hpa\n  labels:\n    app: nginx-hpa\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-hpa\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-hpa\n    spec:\n      restartPolicy: Always\n      containers:\n        - name: nginx-hpa\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              memory: 1024Mi\n              cpu: 1\n            requests:\n              memory: 128Mi\n              cpu: 100m\n</code></pre>\n<p>ÂàõÂª∫ HPA</p>\n<pre><code># kubectl autoscale deployment nginx-hpa --cpu-percent=10 --min=1 --max=10\n# kubectl get hpa\nNAME        REFERENCE              TARGETS       MINPODS   MAXPODS   REPLICAS   AGE\nnginx-hpa   Deployment/nginx-hpa   cpu: 0%/10%   1         10        1          16s\n\n</code></pre>\n<p>ÊµãËØïËá™Âä®Êâ©Áº©ÂÆπ</p>\n<pre><code>while true; do wget -q -O- http://10.96.18.221 &gt; /dev/null; done\n[root@k8s-master01 ~]# kubectl get pods\nNAME                        READY   STATUS    RESTARTS   AGE\nnginx-hpa-d8bcbdf7d-4mkxp   1/1     Running   0          66s\nnginx-hpa-d8bcbdf7d-974q5   1/1     Running   0          6m36s\nnginx-hpa-d8bcbdf7d-g6p2h   1/1     Running   0          66s\nnginx-hpa-d8bcbdf7d-lvvsq   1/1     Running   0          111s\nnginx-hpa-d8bcbdf7d-tgqmr   1/1     Running   0          111s\nnginx-hpa-d8bcbdf7d-tzfbs   1/1     Running   0          21s\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/1771242682.html",
            "url": "http://ixuyong.cn/posts/1771242682.html",
            "title": "K8sÈõ∂ÂÆïÊú∫ÊúçÂä°ÂèëÂ∏É-Êé¢Èíà",
            "date_published": "2025-04-14T11:23:48.000Z",
            "content_html": "<h3 id=\"k8sÈõ∂ÂÆïÊú∫ÊúçÂä°ÂèëÂ∏É-Êé¢Èíà\"><a class=\"anchor\" href=\"#k8sÈõ∂ÂÆïÊú∫ÊúçÂä°ÂèëÂ∏É-Êé¢Èíà\">#</a> K8s Èõ∂ÂÆïÊú∫ÊúçÂä°ÂèëÂ∏É - Êé¢Èíà</h3>\n<h4 id=\"1-podÁä∂ÊÄÅÂèä-pod-ÊïÖÈöúÊéíÊü•ÂëΩ‰ª§\"><a class=\"anchor\" href=\"#1-podÁä∂ÊÄÅÂèä-pod-ÊïÖÈöúÊéíÊü•ÂëΩ‰ª§\">#</a> 1. Pod Áä∂ÊÄÅÂèä Pod ÊïÖÈöúÊéíÊü•ÂëΩ‰ª§</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Áä∂ÊÄÅ</th>\n<th style=\"text-align:left\">ËØ¥Êòé</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">PendingÔºàÊåÇËµ∑Ôºâ</td>\n<td style=\"text-align:left\">Pod Â∑≤Ë¢´ Kubernetes Á≥ªÁªüÊé•Êî∂Ôºå‰ΩÜ‰ªçÊúâ‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÂÆπÂô®Êú™Ë¢´ÂàõÂª∫ÔºåÂèØ‰ª•ÈÄöËøá kubectl describe Êü•ÁúãÂ§Ñ‰∫é Pending Áä∂ÊÄÅÁöÑÂéüÂõ†</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">RunningÔºàËøêË°å‰∏≠Ôºâ</td>\n<td style=\"text-align:left\">Pod Â∑≤ÁªèË¢´ÁªëÂÆöÂà∞‰∏Ä‰∏™ËäÇÁÇπ‰∏äÔºåÂπ∂‰∏îÊâÄÊúâÁöÑÂÆπÂô®ÈÉΩÂ∑≤ÁªèË¢´ÂàõÂª∫ÔºåËÄå‰∏îËá≥Â∞ëÊúâ‰∏Ä‰∏™ÊòØËøêË°åÁä∂ÊÄÅÔºåÊàñËÄÖÊòØÊ≠£Âú®ÂêØÂä®ÊàñËÄÖÈáçÂêØÔºåÂèØ‰ª•ÈÄöËøá kubectl logs Êü•Áúã Pod ÁöÑÊó•Âøó</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">SucceededÔºàÊàêÂäüÔºâ</td>\n<td style=\"text-align:left\">ÊâÄÊúâÂÆπÂô®ÊâßË°åÊàêÂäüÂπ∂ÁªàÊ≠¢ÔºåÂπ∂‰∏î‰∏ç‰ºöÂÜçÊ¨°ÈáçÂêØÔºåÂèØ‰ª•ÈÄöËøá kubectl logs Êü•Áúã Pod Êó•Âøó</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FailedÔºàÂ§±Ë¥•Ôºâ</td>\n<td style=\"text-align:left\">ÊâÄÊúâÂÆπÂô®ÈÉΩÂ∑≤ÁªàÊ≠¢ÔºåÂπ∂‰∏îËá≥Â∞ëÊúâ‰∏Ä‰∏™ÂÆπÂô®‰ª•Â§±Ë¥•ÁöÑÊñπÂºèÁªàÊ≠¢Ôºå‰πüÂ∞±ÊòØËØ¥Ëøô‰∏™ÂÆπÂô®Ë¶Å‰πà‰ª•ÈùûÈõ∂Áä∂ÊÄÅÈÄÄÂá∫ÔºåË¶Å‰πàË¢´Á≥ªÁªüÁªàÊ≠¢ÔºåÂèØ‰ª•ÈÄöËøá logs Âíå describe Êü•Áúã Pod Êó•ÂøóÂíåÁä∂ÊÄÅ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">UnknownÔºàÊú™Áü•Ôºâ</td>\n<td style=\"text-align:left\">ÈÄöÂ∏∏ÊòØÁî±‰∫éÈÄö‰ø°ÈóÆÈ¢òÈÄ†ÊàêÁöÑÊó†Ê≥ïËé∑Âæó Pod ÁöÑÁä∂ÊÄÅ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ImagePullBackOff ErrImagePull</td>\n<td style=\"text-align:left\">ÈïúÂÉèÊãâÂèñÂ§±Ë¥•Ôºå‰∏ÄËà¨ÊòØÁî±‰∫éÈïúÂÉè‰∏çÂ≠òÂú®„ÄÅÁΩëÁªú‰∏çÈÄöÊàñËÄÖÈúÄË¶ÅÁôªÂΩïËÆ§ËØÅÂºïËµ∑ÁöÑÔºåÂèØ‰ª•‰ΩøÁî® describe ÂëΩ‰ª§Êü•ÁúãÂÖ∑‰ΩìÂéüÂõ†</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">CrashLoopBackOff</td>\n<td style=\"text-align:left\">ÂÆπÂô®ÂêØÂä®Â§±Ë¥•ÔºåÂèØ‰ª•ÈÄöËøá logs ÂëΩ‰ª§Êü•ÁúãÂÖ∑‰ΩìÂéüÂõ†Ôºå‰∏ÄËà¨‰∏∫ÂêØÂä®ÂëΩ‰ª§‰∏çÊ≠£Á°ÆÔºåÂÅ•Â∫∑Ê£ÄÊü•‰∏çÈÄöËøáÁ≠â</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">OOMKilled</td>\n<td style=\"text-align:left\">ÂÆπÂô®ÂÜÖÂ≠òÊ∫¢Âá∫Ôºå‰∏ÄËà¨ÊòØÂÆπÂô®ÁöÑÂÜÖÂ≠ò Limit ËÆæÁΩÆÁöÑËøáÂ∞èÔºåÊàñËÄÖÁ®ãÂ∫èÊú¨Ë∫´ÊúâÂÜÖÂ≠òÊ∫¢Âá∫ÔºåÂèØ‰ª•ÈÄöËøá logs Êü•ÁúãÁ®ãÂ∫èÂêØÂä®Êó•Âøó</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Terminating</td>\n<td style=\"text-align:left\">Pod Ê≠£Âú®Ë¢´Âà†Èô§ÔºåÂèØ‰ª•ÈÄöËøá describe Êü•ÁúãÁä∂ÊÄÅ</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">SysctlForbidden</td>\n<td style=\"text-align:left\">Pod Ëá™ÂÆö‰πâ‰∫ÜÂÜÖÊ†∏ÈÖçÁΩÆÔºå‰ΩÜ kubelet Ê≤°ÊúâÊ∑ªÂä†ÂÜÖÊ†∏ÈÖçÁΩÆÊàñÈÖçÁΩÆÁöÑÂÜÖÊ†∏ÂèÇÊï∞‰∏çÊîØÊåÅÔºåÂèØ‰ª•ÈÄöËøá describe Êü•ÁúãÂÖ∑‰ΩìÂéüÂõ†</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">Completed</td>\n<td style=\"text-align:left\">ÂÆπÂô®ÂÜÖÈÉ®‰∏ªËøõÁ®ãÈÄÄÂá∫Ôºå‰∏ÄËà¨ËÆ°Âàí‰ªªÂä°ÊâßË°åÁªìÊùü‰ºöÊòæÁ§∫ËØ•Áä∂ÊÄÅÔºåÊ≠§Êó∂ÂèØ‰ª•ÈÄöËøá logs Êü•ÁúãÂÆπÂô®Êó•Âøó</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">ContainerCreating</td>\n<td style=\"text-align:left\">Pod Ê≠£Âú®ÂàõÂª∫Ôºå‰∏ÄËà¨‰∏∫Ê≠£Âú®‰∏ãËΩΩÈïúÂÉèÔºåÊàñËÄÖÊúâÈÖçÁΩÆ‰∏çÂΩìÁöÑÂú∞ÊñπÔºåÂèØ‰ª•ÈÄöËøá describe Êü•ÁúãÂÖ∑‰ΩìÂéüÂõ†</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"2-podÈïúÂÉèÊãâÂèñÁ≠ñÁï•\"><a class=\"anchor\" href=\"#2-podÈïúÂÉèÊãâÂèñÁ≠ñÁï•\">#</a> 2. Pod ÈïúÂÉèÊãâÂèñÁ≠ñÁï•</h4>\n<p>ÈÄöËøá spec.containers [].imagePullPolicy ÂèÇÊï∞ÂèØ‰ª•ÊåáÂÆöÈïúÂÉèÁöÑÊãâÂèñÁ≠ñÁï•ÔºåÁõÆÂâçÊîØÊåÅÁöÑÁ≠ñÁï•Â¶Ç‰∏ãÔºö</p>\n<table>\n<thead>\n<tr>\n<th>Êìç‰ΩúÊñπÂºè</th>\n<th>ËØ¥Êòé</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Always</td>\n<td>ÊÄªÊòØÊãâÂèñÔºåÂΩìÈïúÂÉè tag ‰∏∫ latest Êó∂Ôºå‰∏î imagePullPolicy Êú™ÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫ Always</td>\n</tr>\n<tr>\n<td>Never</td>\n<td>‰∏çÁÆ°ÊòØÂê¶Â≠òÂú®ÈÉΩ‰∏ç‰ºöÊãâÂèñ</td>\n</tr>\n<tr>\n<td>IfNotPresent</td>\n<td>ÈïúÂÉè‰∏çÂ≠òÂú®Êó∂ÊãâÂèñÈïúÂÉèÔºåÂ¶ÇÊûú tag ‰∏∫Èùû latestÔºå‰∏î imagePullPolicy Êú™ÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫ IfNotPresent</td>\n</tr>\n</tbody>\n</table>\n<p>Êõ¥ÊîπÈïúÂÉèÊãâÂèñÁ≠ñÁï•‰∏∫ IfNotPresentÔºö</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n</code></pre>\n<h4 id=\"3-pod-ÈáçÂêØÁ≠ñÁï•\"><a class=\"anchor\" href=\"#3-pod-ÈáçÂêØÁ≠ñÁï•\">#</a> 3. <strong>Pod</strong> ÈáçÂêØÁ≠ñÁï•</h4>\n<table>\n<thead>\n<tr>\n<th>Êìç‰ΩúÊñπÂºè</th>\n<th>ËØ¥Êòé</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Always</td>\n<td>ÈªòËÆ§Á≠ñÁï•„ÄÇÂÆπÂô®Â§±ÊïàÊó∂ÔºåËá™Âä®ÈáçÂêØËØ•ÂÆπÂô®</td>\n</tr>\n<tr>\n<td>OnFailure</td>\n<td>ÂÆπÂô®‰ª•‰∏ç‰∏∫ 0 ÁöÑÁä∂ÊÄÅÁ†ÅÁªàÊ≠¢ÔºåËá™Âä®ÈáçÂêØËØ•ÂÆπÂô®</td>\n</tr>\n<tr>\n<td>Never</td>\n<td>Êó†ËÆ∫‰ΩïÁßçÁä∂ÊÄÅÔºåÈÉΩ‰∏ç‰ºöÈáçÂêØ</td>\n</tr>\n</tbody>\n</table>\n<p>ÊåáÂÆöÈáçÂêØÁ≠ñÁï•‰∏∫ Always Ôºö</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n      restartPolicy: Always\n</code></pre>\n<h4 id=\"4-podÁöÑ‰∏âÁßçÊé¢Èíà\"><a class=\"anchor\" href=\"#4-podÁöÑ‰∏âÁßçÊé¢Èíà\">#</a> 4. Pod ÁöÑ‰∏âÁßçÊé¢Èíà</h4>\n<table>\n<thead>\n<tr>\n<th>ÁßçÁ±ª</th>\n<th>ËØ¥Êòé</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>startupProbe</td>\n<td>Kubernetes1.16 Êñ∞Âä†ÁöÑÊé¢ÊµãÊñπÂºèÔºåÁî®‰∫éÂà§Êñ≠ÂÆπÂô®ÂÜÖÁöÑÂ∫îÁî®Á®ãÂ∫èÊòØÂê¶Â∑≤ÁªèÂêØÂä®„ÄÇÂ¶ÇÊûúÈÖçÁΩÆ‰∫Ü startupProbeÔºåÂ∞±‰ºöÂÖàÁ¶ÅÁî®ÂÖ∂‰ªñÊé¢ÊµãÔºåÁõ¥Âà∞ÂÆÉÊàêÂäü‰∏∫Ê≠¢„ÄÇÂ¶ÇÊûúÊé¢ÊµãÂ§±Ë¥•ÔºåKubelet ‰ºöÊùÄÊ≠ªÂÆπÂô®Ôºå‰πãÂêéÊ†πÊçÆÈáçÂêØÁ≠ñÁï•ËøõË°åÂ§ÑÁêÜÔºåÂ¶ÇÊûúÊé¢ÊµãÊàêÂäüÔºåÊàñÊ≤°ÊúâÈÖçÁΩÆ startupProbeÔºåÂàôÁä∂ÊÄÅ‰∏∫ÊàêÂäüÔºå‰πãÂêéÂ∞±‰∏çÂÜçÊé¢Êµã„ÄÇ</td>\n</tr>\n<tr>\n<td>livenessProbe</td>\n<td>Áî®‰∫éÊé¢ÊµãÂÆπÂô®ÊòØÂê¶Âú®ËøêË°åÔºåÂ¶ÇÊûúÊé¢ÊµãÂ§±Ë¥•Ôºåkubelet ‰ºö ‚ÄúÊùÄÊ≠ª‚Äù ÂÆπÂô®Âπ∂Ê†πÊçÆÈáçÂêØÁ≠ñÁï•ËøõË°åÁõ∏Â∫îÁöÑÂ§ÑÁêÜ„ÄÇÂ¶ÇÊûúÊú™ÊåáÂÆöËØ•Êé¢ÈíàÔºåÂ∞ÜÈªòËÆ§‰∏∫ Success</td>\n</tr>\n<tr>\n<td>readinessProbe</td>\n<td>‰∏ÄËà¨Áî®‰∫éÊé¢ÊµãÂÆπÂô®ÂÜÖÁöÑÁ®ãÂ∫èÊòØÂê¶ÂÅ•Â∫∑ÔºåÂç≥Âà§Êñ≠ÂÆπÂô®ÊòØÂê¶‰∏∫Â∞±Áª™ÔºàReadyÔºâÁä∂ÊÄÅ„ÄÇÂ¶ÇÊûúÊòØÔºåÂàôÂèØ‰ª•Â§ÑÁêÜËØ∑Ê±ÇÔºåÂèç‰πã Endpoints Controller Â∞Ü‰ªéÊâÄÊúâÁöÑ Service ÁöÑ Endpoints ‰∏≠Âà†Èô§Ê≠§ÂÆπÂô®ÊâÄÂú® Pod ÁöÑ IP Âú∞ÂùÄ„ÄÇÂ¶ÇÊûúÊú™ÊåáÂÆöÔºåÂ∞ÜÈªòËÆ§‰∏∫ Success</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"5-podÊé¢ÈíàÁöÑÂÆûÁé∞ÊñπÂºè\"><a class=\"anchor\" href=\"#5-podÊé¢ÈíàÁöÑÂÆûÁé∞ÊñπÂºè\">#</a> 5. Pod Êé¢ÈíàÁöÑÂÆûÁé∞ÊñπÂºè</h4>\n<table>\n<thead>\n<tr>\n<th>ÂÆûÁé∞ÊñπÂºè</th>\n<th>ËØ¥Êòé</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ExecAction</td>\n<td>Âú®ÂÆπÂô®ÂÜÖÊâßË°å‰∏Ä‰∏™ÊåáÂÆöÁöÑÂëΩ‰ª§ÔºåÂ¶ÇÊûúÂëΩ‰ª§ËøîÂõûÂÄº‰∏∫ 0ÔºåÂàôËÆ§‰∏∫ÂÆπÂô®ÂÅ•Â∫∑</td>\n</tr>\n<tr>\n<td>TCPSocketAction</td>\n<td>ÈÄöËøá TCP ËøûÊé•Ê£ÄÊü•ÂÆπÂô®ÊåáÂÆöÁöÑÁ´ØÂè£ÔºåÂ¶ÇÊûúÁ´ØÂè£ÂºÄÊîæÔºåÂàôËÆ§‰∏∫ÂÆπÂô®ÂÅ•Â∫∑</td>\n</tr>\n<tr>\n<td>HTTPGetAction</td>\n<td>ÂØπÊåáÂÆöÁöÑ URL ËøõË°å Get ËØ∑Ê±ÇÔºåÂ¶ÇÊûúÁä∂ÊÄÅÁ†ÅÂú® 200~400 ‰πãÈó¥ÔºåÂàôËÆ§‰∏∫ÂÆπÂô®ÂÅ•Â∫∑</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"6-ÂÅ•Â∫∑Ê£ÄÊü•ÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#6-ÂÅ•Â∫∑Ê£ÄÊü•ÈÖçÁΩÆ\">#</a> 6. ÂÅ•Â∫∑Ê£ÄÊü•ÈÖçÁΩÆ</h4>\n<p>ÈÖçÁΩÆÂÅ•Â∫∑Ê£ÄÊü•Ôºö</p>\n<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          startupProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          livenessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          readinessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            httpGet:\n              path: /index.html\n              port: 80\n              scheme: HTTP\n      restartPolicy: Always\n</code></pre>\n<h4 id=\"7-prestopÂíå-poststartÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#7-prestopÂíå-poststartÈÖçÁΩÆ\">#</a> 7. PreStop Âíå PostStart ÈÖçÁΩÆ</h4>\n<pre><code>[root@k8s-master01 ~]# cat nginx-deploy.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deploy\n  labels:\n    app: nginx-deploy\n  annotations:\n    app: nginx-deploy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: nginx-deploy\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx-deploy\n    spec:\n      containers:\n        - name: nginx-deploy\n          image: nginx:latest\n          imagePullPolicy: IfNotPresent\n          startupProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          livenessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            tcpSocket:\n              port: 80\n          readinessProbe:\n            initialDelaySeconds: 30\n            timeoutSeconds: 2\n            periodSeconds: 30\n            successThreshold: 1\n            failureThreshold: 2\n            httpGet:\n              path: /index.html\n              port: 80\n              scheme: HTTP\n          lifecycle:\n            postStart:\n              exec:\n                command:\n                  - sh\n                  - '-c'\n                  - mkdir /data\n            preStop:\n              exec:\n                command:\n                  - sh\n                  - '-c'\n                  - sleep 30\n      restartPolicy: Always\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/985149017.html",
            "url": "http://ixuyong.cn/posts/985149017.html",
            "title": "‰∫åËøõÂà∂È´òÂèØÁî®ÂÆâË£ÖK8SÈõÜÁæ§",
            "date_published": "2025-04-10T12:58:40.000Z",
            "content_html": "<h2 id=\"‰∫åËøõÂà∂È´òÂèØÁî®ÂÆâË£Ök8sÈõÜÁæ§\"><a class=\"anchor\" href=\"#‰∫åËøõÂà∂È´òÂèØÁî®ÂÆâË£Ök8sÈõÜÁæ§\">#</a> ‰∫åËøõÂà∂È´òÂèØÁî®ÂÆâË£Ö K8s ÈõÜÁæ§</h2>\n<h4 id=\"1-Âü∫Êú¨ÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#1-Âü∫Êú¨ÈÖçÁΩÆ\">#</a> 1. Âü∫Êú¨ÈÖçÁΩÆ</h4>\n<h5 id=\"11-Âü∫Êú¨ÁéØÂ¢ÉÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#11-Âü∫Êú¨ÁéØÂ¢ÉÈÖçÁΩÆ\">#</a> 1.1 Âü∫Êú¨ÁéØÂ¢ÉÈÖçÁΩÆ</h5>\n<table>\n<thead>\n<tr>\n<th>‰∏ªÊú∫Âêç</th>\n<th>IP Âú∞ÂùÄ</th>\n<th>ËØ¥Êòé</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master ËäÇÁÇπ * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived ËôöÊãü IPÔºà‰∏çÂç†Áî®Êú∫Âô®Ôºâ</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker ËäÇÁÇπ * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>ËØ∑Áªü‰∏ÄÊõøÊç¢Ëøô‰∫õÁΩëÊÆµÔºåPod ÁΩëÊÆµÂíå service ÂíåÂÆø‰∏ªÊú∫ÁΩëÊÆµ‰∏çË¶ÅÈáçÂ§çÔºÅÔºÅÔºÅ</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* ÈÖçÁΩÆ‰ø°ÊÅØ *</strong></em></th>\n<th>Â§áÊ≥®</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Á≥ªÁªüÁâàÊú¨</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod ÁΩëÊÆµ</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service ÁΩëÊÆµ</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Êõ¥Êîπ‰∏ªÊú∫ÂêçÔºàÂÖ∂ÂÆÉËäÇÁÇπÊåâÈúÄ‰øÆÊîπÔºâÔºö</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ hostsÔºå‰øÆÊîπ /etc/hosts Â¶Ç‰∏ãÔºö</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ yum Ê∫êÔºö</p>\n<pre><code># ÈÖçÁΩÆÂü∫Á°ÄÊ∫ê\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂøÖÂ§áÂ∑•ÂÖ∑ÂÆâË£ÖÔºö</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÖ≥Èó≠Èò≤ÁÅ´Â¢ô„ÄÅselinux„ÄÅdnsmasq„ÄÅswap„ÄÅÂºÄÂêØ rsyslog„ÄÇÊúçÂä°Âô®ÈÖçÁΩÆÂ¶Ç‰∏ãÔºö</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÖ≥Èó≠ swap ÂàÜÂå∫Ôºö</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÆâË£Ö ntpdateÔºö</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂêåÊ≠•Êó∂Èó¥Âπ∂ÈÖçÁΩÆ‰∏äÊµ∑Êó∂Âå∫Ôºö</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# Âä†ÂÖ•Âà∞crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ limitÔºö</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# Êú´Â∞æÊ∑ªÂä†Â¶Ç‰∏ãÂÜÖÂÆπ\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂçáÁ∫ßÁ≥ªÁªüÔºö</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÂÖçÂØÜÈí•ÁôªÂΩïÂÖ∂‰ªñËäÇÁÇπÔºåÂÆâË£ÖËøáÁ®ã‰∏≠ÁîüÊàêÈÖçÁΩÆÊñá‰ª∂ÂíåËØÅ‰π¶ÂùáÂú® Master01 ‰∏äÊìç‰ΩúÔºåÈõÜÁæ§ÁÆ°ÁêÜ‰πüÂú® Master01 ‰∏äÊìç‰ΩúÔºö</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>Ê≥®ÊÑèÔºöÂÖ¨Êúâ‰∫ëÁéØÂ¢ÉÔºåÂèØËÉΩÈúÄË¶ÅÊää kubectl ÊîæÂú®‰∏Ä‰∏™Èùû Master ËäÇÁÇπ‰∏ä</em></p>\n<p><mark>Master01 ËäÇÁÇπ</mark>‰∏ãËΩΩÂÆâË£ÖÊâÄÊúâÁöÑÊ∫êÁ†ÅÊñá‰ª∂Ôºö</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-ÂÜÖÊ†∏ÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#12-ÂÜÖÊ†∏ÈÖçÁΩÆ\">#</a> 1.2 ÂÜÖÊ†∏ÈÖçÁΩÆ</h5>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÆâË£Ö ipvsadmÔºö</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ ipvs Ê®°ÂùóÔºö</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂàõÂª∫ ipvs.confÔºåÂπ∂ÈÖçÁΩÆÂºÄÊú∫Ëá™Âä®Âä†ËΩΩÔºö</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# Âä†ÂÖ•‰ª•‰∏ãÂÜÖÂÆπ\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÁÑ∂ÂêéÊâßË°å systemctl enable --now systemd-modules-load.service Âç≥ÂèØÔºàÊä•Èîô‰∏çÁî®ÁÆ°Ôºâ</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÜÖÊ†∏‰ºòÂåñÈÖçÁΩÆÔºö</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Â∫îÁî®ÈÖçÁΩÆÔºö</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆÂÆåÂÜÖÊ†∏ÂêéÔºåÈáçÂêØÊú∫Âô®Ôºå‰πãÂêéÊü•ÁúãÂÜÖÊ†∏Ê®°ÂùóÊòØÂê¶Â∑≤Ëá™Âä®Âä†ËΩΩÔºö</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-È´òÂèØÁî®ÁªÑ‰ª∂ÂÆâË£Ö\"><a class=\"anchor\" href=\"#2-È´òÂèØÁî®ÁªÑ‰ª∂ÂÆâË£Ö\">#</a> 2. È´òÂèØÁî®ÁªÑ‰ª∂ÂÆâË£Ö</h4>\n<p><em>Ê≥®ÊÑèÔºöÂ¶ÇÊûúÂÆâË£ÖÁöÑ‰∏çÊòØÈ´òÂèØÁî®ÈõÜÁæ§Ôºåhaproxy Âíå keepalived Êó†ÈúÄÂÆâË£Ö</em></p>\n<p><em>Ê≥®ÊÑèÔºöÂÖ¨Êúâ‰∫ëË¶ÅÁî®ÂÖ¨Êúâ‰∫ëËá™Â∏¶ÁöÑË¥üËΩΩÂùáË°°ÔºåÊØîÂ¶ÇÈòøÈáå‰∫ëÁöÑ SLB„ÄÅNLBÔºåËÖæËÆØ‰∫ëÁöÑ ELBÔºåÁî®Êù•Êõø‰ª£ haproxy Âíå keepalivedÔºåÂõ†‰∏∫ÂÖ¨Êúâ‰∫ëÂ§ßÈÉ®ÂàÜÈÉΩÊòØ‰∏çÊîØÊåÅ keepalived ÁöÑ„ÄÇ</em></p>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÈÄöËøá yum ÂÆâË£Ö HAProxy Âíå KeepAlivedÔºö</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÈÖçÁΩÆ HAProxyÔºåÈúÄË¶ÅÊ≥®ÊÑèÈªÑËâ≤ÈÉ®ÂàÜÁöÑ IPÔºö</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:8443       #HAProxyÁõëÂê¨Á´ØÂè£\n  bind 127.0.0.1:8443     #HAProxyÁõëÂê¨Á´ØÂè£\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IPÂú∞ÂùÄ\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IPÂú∞ÂùÄ\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IPÂú∞ÂùÄ\n</code></pre>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÈÖçÁΩÆ KeepAlivedÔºåÈúÄË¶ÅÊ≥®ÊÑèÈªÑËâ≤ÈÉ®ÂàÜÁöÑÈÖçÁΩÆ„ÄÇ</p>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÁöÑÈÖçÁΩÆÔºö</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #ÁΩëÂç°ÂêçÁß∞\n    mcast_src_ip 192.168.1.71      #K8s-master01 IPÂú∞ÂùÄ\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIPÂú∞ÂùÄ\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 ËäÇÁÇπ</mark>ÁöÑÈÖçÁΩÆÔºö</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #ÁΩëÂç°ÂêçÁß∞\n    mcast_src_ip 192.168.1.72       #K8s-master02 IPÂú∞ÂùÄ\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIPÂú∞ÂùÄ\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 ËäÇÁÇπ</mark>ÁöÑÈÖçÁΩÆÔºö</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #ÁΩëÂç°ÂêçÁß∞\n    mcast_src_ip 192.168.1.73        #K8s-master03 IPÂú∞ÂùÄ\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIPÂú∞ÂùÄ\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>ÊâÄÊúâ master ËäÇÁÇπ</mark>ÈÖçÁΩÆ KeepAlived ÂÅ•Â∫∑Ê£ÄÊü•Êñá‰ª∂Ôºö</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>ÊâÄÊúâ master ËäÇÁÇπ</mark>ÈÖçÁΩÆÂÅ•Â∫∑Ê£ÄÊü•Êñá‰ª∂Ê∑ªÂä†ÊâßË°åÊùÉÈôêÔºö</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>ÊâÄÊúâ master ËäÇÁÇπ</mark>ÂêØÂä® haproxy Âíå keepalivedÔºö</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>ÈáçË¶ÅÔºöÂ¶ÇÊûúÂÆâË£Ö‰∫Ü keepalived Âíå haproxyÔºåÈúÄË¶ÅÊµãËØï keepalived ÊòØÂê¶ÊòØÊ≠£Â∏∏ÁöÑ</p>\n<pre><code>ÊâÄÊúâËäÇÁÇπÊµãËØïVIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>Â¶ÇÊûú ping ‰∏çÈÄö‰∏î telnet Ê≤°ÊúâÂá∫Áé∞ ] ÔºåÂàôËÆ§‰∏∫ VIP ‰∏çÂèØ‰ª•Ôºå‰∏çÂèØÂú®ÁªßÁª≠ÂæÄ‰∏ãÊâßË°åÔºåÈúÄË¶ÅÊéíÊü• keepalived ÁöÑÈóÆÈ¢òÔºåÊØîÂ¶ÇÈò≤ÁÅ´Â¢ôÂíå selinuxÔºåhaproxy Âíå keepalived ÁöÑÁä∂ÊÄÅÔºåÁõëÂê¨Á´ØÂè£Á≠â</p>\n<ul>\n<li>ÊâÄÊúâËäÇÁÇπÊü•ÁúãÈò≤ÁÅ´Â¢ôÁä∂ÊÄÅÂøÖÈ°ª‰∏∫ disable Âíå inactiveÔºösystemctl status firewalld</li>\n<li>ÊâÄÊúâËäÇÁÇπÊü•Áúã selinux Áä∂ÊÄÅÔºåÂøÖÈ°ª‰∏∫ disableÔºögetenforce</li>\n<li>master ËäÇÁÇπÊü•Áúã haproxy Âíå keepalived Áä∂ÊÄÅÔºösystemctl status keepalived haproxy</li>\n<li>master ËäÇÁÇπÊü•ÁúãÁõëÂê¨Á´ØÂè£Ôºönetstat -lntp</li>\n</ul>\n<p>Â¶ÇÊûú‰ª•‰∏äÈÉΩÊ≤°ÊúâÈóÆÈ¢òÔºåÈúÄË¶ÅÁ°ÆËÆ§Ôºö</p>\n<ol>\n<li>\n<p>ÊòØÂê¶ÊòØÂÖ¨Êúâ‰∫ëÊú∫Âô®</p>\n</li>\n<li>\n<p>ÊòØÂê¶ÊòØÁßÅÊúâ‰∫ëÊú∫Âô®ÔºàÁ±ª‰ºº OpenStackÔºâ</p>\n</li>\n</ol>\n<p>‰∏äËø∞ÂÖ¨Êúâ‰∫ë‰∏ÄËà¨ÈÉΩÊòØ‰∏çÊîØÊåÅ keepalivedÔºåÁßÅÊúâ‰∫ëÂèØËÉΩ‰πüÊúâÈôêÂà∂ÔºåÈúÄË¶ÅÂíåËá™Â∑±ÁöÑÁßÅÊúâ‰∫ëÁÆ°ÁêÜÂëòÂí®ËØ¢</p>\n<h4 id=\"3-runtimeÂÆâË£Ö\"><a class=\"anchor\" href=\"#3-runtimeÂÆâË£Ö\">#</a> 3. Runtime ÂÆâË£Ö</h4>\n<p>Â¶ÇÊûúÂÆâË£ÖÁöÑÁâàÊú¨‰Ωé‰∫é 1.24ÔºåÈÄâÊã© Docker Âíå Containerd ÂùáÂèØÔºåÈ´ò‰∫é 1.24 Âª∫ËÆÆÈÄâÊã© Containerd ‰Ωú‰∏∫ RuntimeÔºå‰∏çÂÜçÊé®Ëçê‰ΩøÁî® Docker ‰Ωú‰∏∫ Runtime„ÄÇ</p>\n<h5 id=\"31-ÂÆâË£Öcontainerd\"><a class=\"anchor\" href=\"#31-ÂÆâË£Öcontainerd\">#</a> 3.1 ÂÆâË£Ö Containerd</h5>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆÂÆâË£ÖÊ∫êÔºö</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÆâË£Ö docker-ceÔºàÂ¶ÇÊûúÂú®‰ª•ÂâçÂ∑≤ÁªèÂÆâË£ÖËøáÔºåÈúÄË¶ÅÈáçÊñ∞ÂÆâË£ÖÊõ¥Êñ∞‰∏Ä‰∏ãÔºâÔºö</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>ÂèØ‰ª•Êó†ÈúÄÂêØÂä® DockerÔºåÂè™ÈúÄË¶ÅÈÖçÁΩÆÂíåÂêØÂä® Containerd Âç≥ÂèØ„ÄÇ</em></p>\n<p>È¶ñÂÖàÈÖçÁΩÆ Containerd ÊâÄÈúÄÁöÑÊ®°ÂùóÔºà<mark>ÊâÄÊúâËäÇÁÇπ</mark>ÔºâÔºö</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Âä†ËΩΩÊ®°ÂùóÔºö</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÔºåÈÖçÁΩÆ Containerd ÊâÄÈúÄÁöÑÂÜÖÊ†∏Ôºö</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Âä†ËΩΩÂÜÖÊ†∏Ôºö</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÁîüÊàê Containerd ÁöÑÈÖçÁΩÆÊñá‰ª∂Ôºö</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Êõ¥Êîπ Containerd ÁöÑ Cgroup Âíå Pause ÈïúÂÉèÈÖçÁΩÆÔºö</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂêØÂä® ContainerdÔºåÂπ∂ÈÖçÁΩÆÂºÄÊú∫Ëá™ÂêØÂä®Ôºö</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ crictl ÂÆ¢Êà∑Á´ØËøûÊé•ÁöÑËøêË°åÊó∂‰ΩçÁΩÆÔºàÂèØÈÄâÔºâÔºö</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-k8sÂèäetcdÂÆâË£Ö\"><a class=\"anchor\" href=\"#4-k8sÂèäetcdÂÆâË£Ö\">#</a> 4 . K8S Âèä etcd ÂÆâË£Ö</h4>\n<p><mark>Master01</mark> ‰∏ãËΩΩ kubernetes ÂÆâË£ÖÂåÖÔºà1.32.3 ÈúÄË¶ÅÊõ¥Êîπ‰∏∫‰Ω†ÁúãÂà∞ÁöÑÊúÄÊñ∞ÁâàÊú¨ÔºâÔºö</p>\n<pre><code>[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz\n</code></pre>\n<p>ÊúÄÊñ∞ÁâàËé∑ÂèñÂú∞ÂùÄÔºö<a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md\">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/</a></p>\n<p><mark>‰ª•‰∏ãÊìç‰ΩúÈÉΩÂú® master01 ÊâßË°å</mark></p>\n<p>‰∏ãËΩΩ etcd ÂÆâË£ÖÂåÖÔºö<a href=\"https://github.com/etcd-io/etcd/releases/\">https://github.com/etcd-io/etcd/releases/</a></p>\n<pre><code>[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz\n</code></pre>\n<p>Ëß£Âéã kubernetes ÂÆâË£ÖÊñá‰ª∂Ôºö</p>\n<pre><code>[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125;\n</code></pre>\n<p>Ëß£Âéã etcd ÂÆâË£ÖÊñá‰ª∂Ôºö</p>\n<pre><code>[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&#123;,ctl&#125;\n</code></pre>\n<p>ÁâàÊú¨Êü•ÁúãÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubelet --version\nKubernetes v1.32.3\n[root@k8s-master01 ~]# etcdctl version\netcdctl version: 3.5.16\nAPI version: 3.5\n</code></pre>\n<p>Â∞ÜÁªÑ‰ª∂ÂèëÈÄÅÂà∞ÂÖ∂‰ªñËäÇÁÇπ</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\nWorkNodes='k8s-node01 k8s-node02'\nfor NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done\nfor NODE in $WorkNodes; do     scp /usr/local/bin/kube&#123;let,-proxy&#125; $NODE:/usr/local/bin/ ; done\n</code></pre>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÂàáÊç¢Âà∞ 1.32.x ÂàÜÊîØÔºàÂÖ∂‰ªñÁâàÊú¨ÂèØ‰ª•ÂàáÊç¢Âà∞ÂÖ∂‰ªñÂàÜÊîØÔºå.x Âç≥ÂèØÔºå‰∏çÈúÄË¶ÅÊõ¥Êîπ‰∏∫ÂÖ∑‰ΩìÁöÑÂ∞èÁâàÊú¨ÔºâÔºö</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x\n</code></pre>\n<h4 id=\"5-ÁîüÊàêËØÅ‰π¶\"><a class=\"anchor\" href=\"#5-ÁîüÊàêËØÅ‰π¶\">#</a> 5 . ÁîüÊàêËØÅ‰π¶</h4>\n<p><em><mark>‰∫åËøõÂà∂ÂÆâË£ÖÊúÄÂÖ≥ÈîÆÊ≠•È™§Ôºå‰∏ÄÊ≠•ÈîôËØØÂÖ®ÁõòÁöÜËæìÔºå‰∏ÄÂÆöË¶ÅÊ≥®ÊÑèÊØè‰∏™Ê≠•È™§ÈÉΩË¶ÅÊòØÊ≠£Á°ÆÁöÑ</mark></em></p>\n<p><mark>Master01</mark> ‰∏ãËΩΩÁîüÊàêËØÅ‰π¶Â∑•ÂÖ∑Ôºà‰∏ãËΩΩ‰∏çÊàêÂäüÂèØ‰ª•ÂéªÁôæÂ∫¶ÁΩëÁõòÔºâ</p>\n<pre><code>wget &quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl\nwget &quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson\nchmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson\n</code></pre>\n<h5 id=\"51-etcdËØÅ‰π¶\"><a class=\"anchor\" href=\"#51-etcdËØÅ‰π¶\">#</a> 5.1 Etcd ËØÅ‰π¶</h5>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÂàõÂª∫ etcd ËØÅ‰π¶ÁõÆÂΩïÔºö</p>\n<pre><code>mkdir /etc/etcd/ssl -p\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂàõÂª∫ kubernetes Áõ∏ÂÖ≥ÁõÆÂΩïÔºö</p>\n<pre><code>mkdir -p /etc/kubernetes/pki\n</code></pre>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÁîüÊàê etcd ËØÅ‰π¶</p>\n<p>ÁîüÊàêËØÅ‰π¶ÁöÑ CSRÔºàËØÅ‰π¶Á≠æÂêçËØ∑Ê±ÇÊñá‰ª∂ÔºåÈÖçÁΩÆ‰∫Ü‰∏Ä‰∫õÂüüÂêç„ÄÅÂÖ¨Âè∏„ÄÅÂçï‰ΩçÔºâÊñá‰ª∂Ôºö</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\n# ÁîüÊàêetcd CAËØÅ‰π¶ÂíåCAËØÅ‰π¶ÁöÑkey\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca\n\n\ncfssl gencert \\\n   -ca=/etc/etcd/ssl/etcd-ca.pem \\\n   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \\\n   -config=ca-config.json \\\n   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \\\n   -profile=kubernetes \\\n   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd\n\nÊâßË°åÁªìÊûú\n[INFO] generate received request\n \t[INFO] received CSR\n     [INFO] generating key: rsa-2048\n     [INFO] encoded CSR\n     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033\n</code></pre>\n<p>Â∞ÜËØÅ‰π¶Â§çÂà∂Âà∞ÂÖ∂‰ªñ master ËäÇÁÇπ</p>\n<pre><code>MasterNodes='k8s-master02 k8s-master03'\n\nfor NODE in $MasterNodes; do\n     ssh $NODE &quot;mkdir -p /etc/etcd/ssl&quot;\n     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do\n       scp /etc/etcd/ssl/$&#123;FILE&#125; $NODE:/etc/etcd/ssl/$&#123;FILE&#125;\n     done\n done\n</code></pre>\n<h5 id=\"52-k8sÁªÑ‰ª∂ËØÅ‰π¶\"><a class=\"anchor\" href=\"#52-k8sÁªÑ‰ª∂ËØÅ‰π¶\">#</a> 5.2 K8s ÁªÑ‰ª∂ËØÅ‰π¶</h5>\n<p><mark>Master01</mark> ÁîüÊàê kubernetes CA ËØÅ‰π¶Ôºö</p>\n<pre><code>[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca\n</code></pre>\n<h6 id=\"521-apiserverËØÅ‰π¶\"><a class=\"anchor\" href=\"#521-apiserverËØÅ‰π¶\">#</a> 5.2.1 APIServer ËØÅ‰π¶</h6>\n<p>Ê≥®ÊÑèÔºö10.96.0. ÊòØ k8s service ÁöÑÁΩëÊÆµÔºåÂ¶ÇÊûúËØ¥ÈúÄË¶ÅÊõ¥Êîπ k8s service ÁΩëÊÆµÔºåÈÇ£Â∞±ÈúÄË¶ÅÊõ¥Êîπ 10.96.0.1</p>\n<pre><code>cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver\n</code></pre>\n<p>ÁîüÊàê apiserver ÁöÑËÅöÂêàËØÅ‰π¶ÔºöÔºö</p>\n<pre><code>cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca \n\ncfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client\n</code></pre>\n<p>ËøîÂõûÁªìÊûúÔºàÂøΩÁï•Ë≠¶ÂëäÔºâÔºö</p>\n<pre><code>2020/12/11 18:15:28 [INFO] generate received request\n2020/12/11 18:15:28 [INFO] received CSR\n2020/12/11 18:15:28 [INFO] generating key: rsa-2048\n\n2020/12/11 18:15:28 [INFO] encoded CSR\n2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845\n2020/12/11 18:15:28 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for\nwebsites. For more information see the Baseline Requirements for the Issuance and Management\nof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);\nspecifically, section 10.2.3 (&quot;Information Requirements&quot;).\n</code></pre>\n<h6 id=\"522-controllermanager\"><a class=\"anchor\" href=\"#522-controllermanager\">#</a> 5.2.2 ControllerManager</h6>\n<p>ÁîüÊàê controller-manage ÁöÑËØÅ‰π¶Ôºö</p>\n<pre><code class=\"language-\\\">cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager\n\nÊ≥®ÊÑèÔºö‰øÆÊîπÈªÑËâ≤ÈÉ®ÂàÜÁöÑIPÂú∞ÂùÄ\n# set-clusterÔºöËÆæÁΩÆ‰∏Ä‰∏™ÈõÜÁæ§È°πÔºå\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# ËÆæÁΩÆ‰∏Ä‰∏™ÁéØÂ¢ÉÈ°πÔºå‰∏Ä‰∏™‰∏ä‰∏ãÊñá\nkubectl config set-context system:kube-controller-manager@kubernetes \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n# set-credentials ËÆæÁΩÆ‰∏Ä‰∏™Áî®Êà∑È°π\n\nkubectl config set-credentials system:kube-controller-manager \\\n     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \\\n     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n\n\n# ‰ΩøÁî®Êüê‰∏™ÁéØÂ¢ÉÂΩìÂÅöÈªòËÆ§ÁéØÂ¢É\n\nkubectl config use-context system:kube-controller-manager@kubernetes \\\n     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig\n</code></pre>\n<h6 id=\"523-schedulerËØÅ‰π¶\"><a class=\"anchor\" href=\"#523-schedulerËØÅ‰π¶\">#</a> 5.2.3 Scheduler ËØÅ‰π¶</h6>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler\n\nÊ≥®ÊÑèÔºö‰øÆÊîπÈªÑËâ≤ÈÉ®ÂàÜÁöÑIPÂú∞ÂùÄ\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\n\nkubectl config set-credentials system:kube-scheduler \\\n     --client-certificate=/etc/kubernetes/pki/scheduler.pem \\\n     --client-key=/etc/kubernetes/pki/scheduler-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config set-context system:kube-scheduler@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-scheduler \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nkubectl config use-context system:kube-scheduler@kubernetes \\\n     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n</code></pre>\n<h6 id=\"524-ÁîüÊàêÁÆ°ÁêÜÂëòËØÅ‰π¶\"><a class=\"anchor\" href=\"#524-ÁîüÊàêÁÆ°ÁêÜÂëòËØÅ‰π¶\">#</a> 5.2.4 ÁîüÊàêÁÆ°ÁêÜÂëòËØÅ‰π¶</h6>\n<p>Kubectl /etc/Kubernetes/admin.conf ~/.kube/config</p>\n<pre><code>cfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin\n\nÊ≥®ÊÑèÔºö‰øÆÊîπÈªÑËâ≤ÈÉ®ÂàÜÁöÑIP\n\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig\nkubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n\nkubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig\n</code></pre>\n<h6 id=\"525-ÂàõÂª∫serviceaccountËØÅ‰π¶\"><a class=\"anchor\" href=\"#525-ÂàõÂª∫serviceaccountËØÅ‰π¶\">#</a> 5.2.5 ÂàõÂª∫ ServiceAccount ËØÅ‰π¶</h6>\n<p>ÂàõÂª∫‰∏ÄÂØπÂÖ¨Èí•ÔºåÁî®Êù•Á≠æÂèë ServiceAccount ÁöÑ TokenÔºö</p>\n<pre><code>openssl genrsa -out /etc/kubernetes/pki/sa.key 2048\n</code></pre>\n<p>ËøîÂõûÁªìÊûúÔºö</p>\n<pre><code>Generating RSA private key, 2048 bit long modulus (2 primes)\n...................................................................................+++++\n...............+++++\ne is 65537 (0x010001)\n</code></pre>\n<pre><code> openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub\n</code></pre>\n<p>ÂèëÈÄÅËØÅ‰π¶Ëá≥ÂÖ∂‰ªñËäÇÁÇπÔºö</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do \n  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do \n    scp /etc/kubernetes/pki/$&#123;FILE&#125; $NODE:/etc/kubernetes/pki/$&#123;FILE&#125;;\n  done; \n  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do \n    scp /etc/kubernetes/$&#123;FILE&#125; $NODE:/etc/kubernetes/$&#123;FILE&#125;;\n  done;\ndone\n</code></pre>\n<p>Êü•ÁúãËØÅ‰π¶Êñá‰ª∂Ôºö</p>\n<pre><code>[root@k8s-master01 pki]# ls /etc/kubernetes/pki/\nadmin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem\nadmin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem\nadmin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr\n[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l\n23\n</code></pre>\n<h4 id=\"6-kubernetesÁªÑ‰ª∂ÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#6-kubernetesÁªÑ‰ª∂ÈÖçÁΩÆ\">#</a> 6. Kubernetes ÁªÑ‰ª∂ÈÖçÁΩÆ</h4>\n<h5 id=\"61-ecdÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#61-ecdÈÖçÁΩÆ\">#</a> 6.1 Ecd ÈÖçÁΩÆ</h5>\n<p>Etcd ÈÖçÁΩÆÂ§ßËá¥Áõ∏ÂêåÔºåÊ≥®ÊÑè‰øÆÊîπÊØè‰∏™ Master ËäÇÁÇπÁöÑ etcd ÈÖçÁΩÆÁöÑ‰∏ªÊú∫ÂêçÂíå IP Âú∞ÂùÄ</p>\n<h6 id=\"611-master01\"><a class=\"anchor\" href=\"#611-master01\">#</a> 6.1.1 Master01</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master01'     # k8s-master01ÂêçÁß∞\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.71:2380'            # k8s-master01 IP\nlisten-client-urls: 'https://192.168.1.71:2379,http://127.0.0.1:2379'   # k8s-master01 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.71:2380'  # k8s-master01 IP\nadvertise-client-urls: 'https://192.168.1.71:2379'        # k8s-master01 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'     # k8s-master01„ÄÅk8s-master02„ÄÅk8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"612-master02\"><a class=\"anchor\" href=\"#612-master02\">#</a> 6.1.2 Master02</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\t\nname: 'k8s-master02'   # k8s-master02ÂêçÁß∞\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.72:2380'      # k8s-master02 IP\nlisten-client-urls: 'https://192.168.1.72:2379,http://127.0.0.1:2379'    # k8s-master02 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.72:2380'    # k8s-master02 IP\nadvertise-client-urls: 'https://192.168.1.72:2379'     # k8s-master02 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'             # k8s-master01„ÄÅk8s-master02„ÄÅk8s-master03 IP \ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"613-master03\"><a class=\"anchor\" href=\"#613-master03\">#</a> 6.1.3 Master03</h6>\n<pre><code># vim /etc/etcd/etcd.config.yml\nname: 'k8s-master03'           # k8s-master03ÂêçÁß∞\ndata-dir: /var/lib/etcd\nwal-dir: /var/lib/etcd/wal\nsnapshot-count: 5000\nheartbeat-interval: 100\nelection-timeout: 1000\nquota-backend-bytes: 0\nlisten-peer-urls: 'https://192.168.1.73:2380'           # k8s-master03 IP\nlisten-client-urls: 'https://192.168.1.73:2379,http://127.0.0.1:2379'       # k8s-master03 IP\nmax-snapshots: 3\nmax-wals: 5\ncors:\ninitial-advertise-peer-urls: 'https://192.168.1.73:2380'      # k8s-master03 IP\nadvertise-client-urls: 'https://192.168.1.73:2379'            # k8s-master03 IP\ndiscovery:\ndiscovery-fallback: 'proxy'\ndiscovery-proxy:\ndiscovery-srv:\ninitial-cluster: 'k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380'                # k8s-master01„ÄÅk8s-master02„ÄÅk8s-master03 IP\ninitial-cluster-token: 'etcd-k8s-cluster'\ninitial-cluster-state: 'new'\nstrict-reconfig-check: false\nenable-v2: true\nenable-pprof: true\nproxy: 'off'\nproxy-failure-wait: 5000\nproxy-refresh-interval: 30000\nproxy-dial-timeout: 1000\nproxy-write-timeout: 5000\nproxy-read-timeout: 0\nclient-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\npeer-transport-security:\n  cert-file: '/etc/kubernetes/pki/etcd/etcd.pem'\n  key-file: '/etc/kubernetes/pki/etcd/etcd-key.pem'\n  peer-client-cert-auth: true\n  trusted-ca-file: '/etc/kubernetes/pki/etcd/etcd-ca.pem'\n  auto-tls: true\ndebug: false\nlog-package-levels:\nlog-outputs: [default]\nforce-new-cluster: false\n</code></pre>\n<h6 id=\"614-ÂêØÂä®etcd\"><a class=\"anchor\" href=\"#614-ÂêØÂä®etcd\">#</a> 6.1.4 ÂêØÂä® Etcd</h6>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÂàõÂª∫ etcd service Âπ∂ÂêØÂä®</p>\n<pre><code># vim /usr/lib/systemd/system/etcd.service\n[Unit]\nDescription=Etcd Service\nDocumentation=https://coreos.com/etcd/docs/latest/\nAfter=network.target\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml\nRestart=on-failure\nRestartSec=10\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nAlias=etcd3.service\n</code></pre>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÂàõÂª∫ etcd ÁöÑËØÅ‰π¶ÁõÆÂΩïÔºö</p>\n<pre><code>mkdir /etc/kubernetes/pki/etcd\nln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/\nsystemctl daemon-reload\nsystemctl enable --now etcd\n</code></pre>\n<p>Êü•Áúã etcd Áä∂ÊÄÅÔºö</p>\n<pre><code>export ETCDCTL_API=3\netcdctl --endpoints=&quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table\n</code></pre>\n<h5 id=\"62-apiserverÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#62-apiserverÈÖçÁΩÆ\">#</a> 6.2 APIServer ÈÖçÁΩÆ</h5>\n<h6 id=\"621-master01\"><a class=\"anchor\" href=\"#621-master01\">#</a> 6.2.1 Master01</h6>\n<p>Ê≥®ÊÑèÔºöÊú¨ÊñáÊ°£‰ΩøÁî®ÁöÑ k8s service ÁΩëÊÆµ‰∏∫ 10.96.0.0/16ÔºåËØ•ÁΩëÊÆµ‰∏çËÉΩÂíåÂÆø‰∏ªÊú∫ÁöÑÁΩëÊÆµ„ÄÅPod ÁΩëÊÆµÁöÑÈáçÂ§çÔºåËØ∑ÊåâÈúÄ‰øÆÊîπÔºö</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.71 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"622-master02\"><a class=\"anchor\" href=\"#622-master02\">#</a> 6.2.2 Master02</h6>\n<p>Ê≥®ÊÑèÔºöÊú¨ÊñáÊ°£‰ΩøÁî®ÁöÑ k8s service ÁΩëÊÆµ‰∏∫ 10.96.0.0/16ÔºåËØ•ÁΩëÊÆµ‰∏çËÉΩÂíåÂÆø‰∏ªÊú∫ÁöÑÁΩëÊÆµ„ÄÅPod ÁΩëÊÆµÁöÑÈáçÂ§çÔºåËØ∑ÊåâÈúÄ‰øÆÊîπÔºö</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.72 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"623-master03\"><a class=\"anchor\" href=\"#623-master03\">#</a> 6.2.3 Master03</h6>\n<p>Ê≥®ÊÑèÔºöÊú¨ÊñáÊ°£‰ΩøÁî®ÁöÑ k8s service ÁΩëÊÆµ‰∏∫ 10.96.0.0/16ÔºåËØ•ÁΩëÊÆµ‰∏çËÉΩÂíåÂÆø‰∏ªÊú∫ÁöÑÁΩëÊÆµ„ÄÅPod ÁΩëÊÆµÁöÑÈáçÂ§çÔºåËØ∑ÊåâÈúÄ‰øÆÊîπÔºö</p>\n<pre><code>[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service \n\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\n      --v=2  \\\n      --allow-privileged=true  \\\n      --bind-address=0.0.0.0  \\\n      --secure-port=6443  \\\n      --advertise-address=192.168.1.73 \\\n      --service-cluster-ip-range=10.96.0.0/16  \\\n      --service-node-port-range=30000-32767  \\\n      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \\\n      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \\\n      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \\\n      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \\\n      --client-ca-file=/etc/kubernetes/pki/ca.pem  \\\n      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \\\n      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \\\n      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \\\n      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \\\n      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \\\n      --service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\\n      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\\n      --authorization-mode=Node,RBAC  \\\n      --enable-bootstrap-token-auth=true  \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \\\n      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \\\n      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \\\n      --requestheader-allowed-names=aggregator  \\\n      --requestheader-group-headers=X-Remote-Group  \\\n      --requestheader-extra-headers-prefix=X-Remote-Extra-  \\\n      --requestheader-username-headers=X-Remote-User\n      # --token-auth-file=/etc/kubernetes/token.csv\n\nRestart=on-failure\nRestartSec=10s\nLimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<h6 id=\"624-ÂêØÂä®apiserver\"><a class=\"anchor\" href=\"#624-ÂêØÂä®apiserver\">#</a> 6.2.4 ÂêØÂä® apiserver</h6>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÂºÄÂêØ kube-apiserverÔºö</p>\n<pre><code>systemctl daemon-reload &amp;&amp; systemctl enable --now kube-apiserver\n</code></pre>\n<p>Ê£ÄÊµã kube-server Áä∂ÊÄÅÔºö</p>\n<pre><code># systemctl status kube-apiserver\n\n‚óè kube-apiserver.service ‚Äì Kubernetes API Server\n   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)\n   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s ago¬†\n</code></pre>\n<p>Â¶ÇÊûúÁ≥ªÁªüÊó•ÂøóÊúâËøô‰∫õÊèêÁ§∫ÂèØ‰ª•ÂøΩÁï•:</p>\n<pre><code>Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to ‚Äúpick_first‚Äù\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;CONNECTING &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &#123;READY &lt;nil&gt;&#125;\nDec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = ‚Äútransport is closing‚Äù\n</code></pre>\n<h5 id=\"63-controllermanage\"><a class=\"anchor\" href=\"#63-controllermanage\">#</a> 6.3 ControllerManage</h5>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÈÖçÁΩÆ kube-controller-manager serviceÔºàÊâÄÊúâ master ËäÇÁÇπÈÖçÁΩÆ‰∏ÄÊ†∑Ôºâ</p>\n<p>Ê≥®ÊÑèÔºöÊú¨ÊñáÊ°£‰ΩøÁî®ÁöÑ k8s Pod ÁΩëÊÆµ‰∏∫ 172.16.0.0/16ÔºåËØ•ÁΩëÊÆµ‰∏çËÉΩÂíåÂÆø‰∏ªÊú∫ÁöÑÁΩëÊÆµ„ÄÅk8s Service ÁΩëÊÆµÁöÑÈáçÂ§çÔºåËØ∑ÊåâÈúÄ‰øÆÊîπÔºö</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\n      --v=2 \\\n      --root-ca-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\\n      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\\n      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\\n      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\\n      --leader-elect=true \\\n      --use-service-account-credentials=true \\\n      --node-monitor-grace-period=40s \\\n      --node-monitor-period=5s \\\n      --controllers=*,bootstrapsigner,tokencleaner \\\n      --allocate-node-cidrs=true \\\n      --cluster-cidr=172.16.0.0/16 \\\n      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\\n      --node-cidr-mask-size=24\n      \nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÂêØÂä® kube-controller-manager</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service ‚Üí /usr/lib/systemd/system/kube-controller-manager.service.\n</code></pre>\n<p>Êü•ÁúãÂêØÂä®Áä∂ÊÄÅ</p>\n<pre><code>[root@k8s-master01 pki]# systemctl  status kube-controller-manager\n‚óè kube-controller-manager.service ‚Äì Kubernetes Controller Manager\n   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)\n Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago\n     Docs: https://github.com/  ubernetes/  ubernetes\n Main PID: 7518 (kube-controller)\n</code></pre>\n<h5 id=\"64-scheduler\"><a class=\"anchor\" href=\"#64-scheduler\">#</a> 6.4 Scheduler</h5>\n<p>ÊâÄÊúâ Master ËäÇÁÇπÈÖçÁΩÆ kube-scheduler serviceÔºàÊâÄÊúâ master ËäÇÁÇπÈÖçÁΩÆ‰∏ÄÊ†∑Ôºâ</p>\n<pre><code>[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service \n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\n      --v=2 \\\n      --leader-elect=true \\\n      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\\n      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>ÂêØÂä® schedulerÔºö</p>\n<pre><code>[root@k8s-master01 pki]# systemctl daemon-reload\n\n[root@k8s-master01 pki]# systemctl enable --now kube-scheduler\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service ‚Üí /usr/lib/systemd/system/kube-scheduler.service.\n[root@k8s-master01 pki]# systemctl status kube-scheduler\n‚óè kube-scheduler.service - Kubernetes Scheduler\n   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)\n   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago\n     Docs: https://github.com/kubernetes/kubernetes\n Main PID: 5815 (kube-scheduler)\n    Tasks: 9\n   Memory: 19.8M\n</code></pre>\n<h4 id=\"7-tls-bootstrappingÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#7-tls-bootstrappingÈÖçÁΩÆ\">#</a> 7. TLS Bootstrapping ÈÖçÁΩÆ</h4>\n<p>Âè™ÈúÄË¶ÅÂú®<mark> Master01</mark> ÂàõÂª∫ bootstrap</p>\n<p>Ê≥®ÊÑèÔºö ‰øÆÊîπÈªÑËâ≤ÈÉ®ÂàÜÁöÑ IP Âú∞ÂùÄ</p>\n<pre><code>cd /root/k8s-ha-install/bootstrap\nkubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\nkubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig\n\n[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config\n</code></pre>\n<p>ÂèØ‰ª•Ê≠£Â∏∏Êü•ËØ¢ÈõÜÁæ§Áä∂ÊÄÅÔºåÊâçÂèØ‰ª•ÁªßÁª≠ÂæÄ‰∏ãÔºåÂê¶Âàô‰∏çË°åÔºåÈúÄË¶ÅÊéíÊü• k8s ÁªÑ‰ª∂ÊòØÂê¶ÊúâÊïÖÈöúÔºàÂè™Ë¶ÅÊúâÁªìÊûúÂç≥ÂèØÔºåÂ¶ÇÊûúËøîÂõû‰∏ç‰∏ÄÊ†∑‰∏çÂΩ±ÂìçÔºâ</p>\n<pre><code># kubectl get cs\nWarning: v1 ComponentStatus is deprecated in v1.19+\nNAME                 STATUS    MESSAGE   ERROR\ncontroller-manager   Healthy   ok        \nscheduler            Healthy   ok        \netcd-0               Healthy   ok\n</code></pre>\n<p>ÂàõÂª∫ bootstrap Áõ∏ÂÖ≥ËµÑÊ∫êÔºö</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml \nsecret/bootstrap-token-c8ad9c created\nclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created\nclusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created\nclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created\nclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created\n</code></pre>\n<h4 id=\"8-nodeËäÇÁÇπÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#8-nodeËäÇÁÇπÈÖçÁΩÆ\">#</a> 8. Node ËäÇÁÇπÈÖçÁΩÆ</h4>\n<h5 id=\"81-Â§çÂà∂ËØÅ‰π¶\"><a class=\"anchor\" href=\"#81-Â§çÂà∂ËØÅ‰π¶\">#</a> 8.1 Â§çÂà∂ËØÅ‰π¶</h5>\n<p><mark>Master01 ËäÇÁÇπ</mark>Â§çÂà∂ËØÅ‰π¶Ëá≥ÂÖ∂‰ªñËäÇÁÇπÔºö</p>\n<pre><code>cd /etc/kubernetes/\n\nfor NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do\n     ssh $NODE mkdir -p /etc/kubernetes/pki\n     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do\n       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&#123;FILE&#125;\n done\n done\n</code></pre>\n<p>ÊâßË°åÁªìÊûúÔºö</p>\n<pre><code>ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    \n‚Ä¶\nbootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00\n</code></pre>\n<h5 id=\"82-kubeletÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#82-kubeletÈÖçÁΩÆ\">#</a> 8.2 Kubelet ÈÖçÁΩÆ</h5>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂàõÂª∫ Kubelet ÈÖçÁΩÆÁõÆÂΩï</p>\n<pre><code>mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ kubelet service</p>\n<pre><code>[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service\n\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kubelet\n\nRestart=always\nStartLimitInterval=0\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ kubelet service ÁöÑÈÖçÁΩÆÊñá‰ª∂Ôºà‰πüÂèØ‰ª•ÂÜôÂà∞ kubelet.serviceÔºâÔºö</p>\n<pre><code># Runtime‰∏∫Containerd\n# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf\n\n[Service]\nEnvironment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&quot;\nEnvironment=&quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&quot;\nEnvironment=&quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&quot;\nEnvironment=&quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' &quot;\nExecStart=\nExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂàõÂª∫ kubelet ÁöÑÈÖçÁΩÆÊñá‰ª∂</p>\n<p><em>Ê≥®ÊÑèÔºöÂ¶ÇÊûúÊõ¥Êîπ‰∫Ü k8s ÁöÑ service ÁΩëÊÆµÔºåÈúÄË¶ÅÊõ¥Êîπ kubelet-conf.yml ÁöÑ clusterDNS: ÈÖçÁΩÆÔºåÊîπÊàê k8s Service ÁΩëÊÆµÁöÑÁ¨¨ÂçÅ‰∏™Âú∞ÂùÄÔºåÊØîÂ¶Ç 10.96.0.10</em></p>\n<pre><code>[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml\n\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 2m0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.pem\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\ncgroupDriver: systemd\ncgroupsPerQOS: true\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerLogMaxFiles: 5\ncontainerLogMaxSize: 10Mi\ncontentType: application/vnd.kubernetes.protobuf\ncpuCFSQuota: true\ncpuManagerPolicy: none\ncpuManagerReconcilePeriod: 10s\nenableControllerAttachDetach: true\nenableDebuggingHandlers: true\nenforceNodeAllocatable:\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhairpinMode: promiscuous-bridge\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\niptablesDropBit: 15\niptablesMasqueradeBit: 14\nkubeAPIBurst: 10\nkubeAPIQPS: 5\nmakeIPTablesUtilChains: true\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nregistryBurst: 10\nregistryPullQPS: 5\nresolvConf: /etc/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 2m0s\nserializeImagePulls: true\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h0m0s\nsyncFrequency: 1m0s\nvolumeStatsAggPeriod: 1m0s\n</code></pre>\n<p>ÂêØÂä®<mark>ÊâÄÊúâËäÇÁÇπ</mark> kubelet</p>\n<pre><code>systemctl daemon-reload\nsystemctl enable --now kubelet\n</code></pre>\n<p>Ê≠§Êó∂Á≥ªÁªüÊó•Âøó /var/log/messages**** ÊòæÁ§∫Âè™ÊúâÂ¶Ç‰∏ã‰∏§Áßç‰ø°ÊÅØ‰∏∫Ê≠£Â∏∏ ****ÔºåÂÆâË£Ö calico ÂêéÂç≥ÂèØÊÅ¢Â§ç</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<p><em>Â¶ÇÊûúÊúâÂæàÂ§öÊä•ÈîôÊó•ÂøóÔºåÊàñËÄÖÊúâÂ§ßÈáèÁúã‰∏çÊáÇÁöÑÊä•ÈîôÔºåËØ¥Êòé kubelet ÁöÑÈÖçÁΩÆÊúâËØØÔºåÈúÄË¶ÅÊ£ÄÊü• kubelet ÈÖçÁΩÆ</em></p>\n<p>Master01 Êü•ÁúãÈõÜÁæ§Áä∂ÊÄÅ (Ready Êàñ NotReady ÈÉΩÊ≠£Â∏∏)</p>\n<pre><code>[root@k8s-master01 bootstrap]# kubectl get node\n</code></pre>\n<h5 id=\"83-kube-proxyÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#83-kube-proxyÈÖçÁΩÆ\">#</a> 8.3 kube-proxy ÈÖçÁΩÆ</h5>\n<p><em>Ê≥®ÊÑèÔºåÂ¶ÇÊûú‰∏çÊòØÈ´òÂèØÁî®ÈõÜÁæ§Ôºå192.168.1.70:8443 Êîπ‰∏∫ master01 ÁöÑÂú∞ÂùÄÔºå8443 Êîπ‰∏∫ apiserver ÁöÑÁ´ØÂè£ÔºåÈªòËÆ§ÊòØ 6443</em></p>\n<p>ÁîüÊàê kube-proxy ÁöÑËØÅ‰π¶Ôºå‰ª•‰∏ãÊìç‰ΩúÂè™Âú®<mark> Master01</mark> ÊâßË°å</p>\n<pre><code>cd /root/k8s-ha-install/pki\ncfssl gencert \\\n   -ca=/etc/kubernetes/pki/ca.pem \\\n   -ca-key=/etc/kubernetes/pki/ca-key.pem \\\n   -config=ca-config.json \\\n   -profile=kubernetes \\\n   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy\n\nkubectl config set-cluster kubernetes \\\n     --certificate-authority=/etc/kubernetes/pki/ca.pem \\\n     --embed-certs=true \\\n     --server=https://192.168.1.70:8443 \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config set-credentials system:kube-proxy \\\n     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \\\n     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \\\n     --embed-certs=true \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\nkubectl config set-context system:kube-proxy@kubernetes \\\n     --cluster=kubernetes \\\n     --user=system:kube-proxy \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n\n\nkubectl config use-context system:kube-proxy@kubernetes \\\n     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig\n</code></pre>\n<p>Â∞Ü kubeconfig ÂèëÈÄÅËá≥ÂÖ∂‰ªñËäÇÁÇπ</p>\n<pre><code>for NODE in k8s-master02 k8s-master03; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n\nfor NODE in k8s-node01 k8s-node02; do\n     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig\n done\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Ê∑ªÂä† kube-proxy ÁöÑÈÖçÁΩÆÂíå service Êñá‰ª∂Ôºö</p>\n<pre><code>vim /usr/lib/systemd/system/kube-proxy.service\n\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\n  --config=/etc/kubernetes/kube-proxy.yaml \\\n  --v=2\n\nRestart=always\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<p>Â¶ÇÊûúÊõ¥Êîπ‰∫ÜÈõÜÁæ§ Pod ÁöÑÁΩëÊÆµÔºåÈúÄË¶ÅÊõ¥Êîπ kube-proxy.yaml ÁöÑ clusterCIDR ‰∏∫Ëá™Â∑±ÁöÑ Pod ÁΩëÊÆµÔºö</p>\n<pre><code>vim /etc/kubernetes/kube-proxy.yaml\n\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nclientConnection:\n  acceptContentTypes: &quot;&quot;\n  burst: 10\n  contentType: application/vnd.kubernetes.protobuf\n  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig\n  qps: 5\nclusterCIDR: 172.16.0.0/16 \nconfigSyncPeriod: 15m0s\nconntrack:\n  max: null\n  maxPerCore: 32768\n  min: 131072\n  tcpCloseWaitTimeout: 1h0m0s\n  tcpEstablishedTimeout: 24h0m0s\nenableProfiling: false\nhealthzBindAddress: 0.0.0.0:10256\nhostnameOverride: &quot;&quot;\niptables:\n  masqueradeAll: false\n  masqueradeBit: 14\n  minSyncPeriod: 0s\n  syncPeriod: 30s\nipvs:\n  masqueradeAll: true\n  minSyncPeriod: 5s\n  scheduler: &quot;rr&quot;\n  syncPeriod: 30s\nkind: KubeProxyConfiguration\nmetricsBindAddress: 127.0.0.1:10249\nmode: &quot;ipvs&quot;\nnodePortAddresses: null\noomScoreAdj: -999\nportRange: &quot;&quot;\nudpIdleTimeout: 250ms\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂêØÂä® kube-proxy</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload\n[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy\nCreated symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service ‚Üí /usr/lib/systemd/system/kube-proxy.service.\n</code></pre>\n<p>Ê≠§Êó∂Á≥ªÁªüÊó•Âøó /var/log/messages**** ÊòæÁ§∫Âè™ÊúâÂ¶Ç‰∏ã‰∏§Áßç‰ø°ÊÅØ‰∏∫Ê≠£Â∏∏ ****ÔºåÂÆâË£Ö calico ÂêéÂç≥ÂèØÊÅ¢Â§ç</p>\n<pre><code>Unable to update cni config: no networks found in /etc/cni/net.d\n</code></pre>\n<p><a href=\"https://imgse.com/i/pE2ZkVK\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/10/pE2ZkVK.png\" alt=\"pE2ZkVK.png\" /></a></p>\n<h4 id=\"9-calicoÁªÑ‰ª∂ÁöÑÂÆâË£Ö\"><a class=\"anchor\" href=\"#9-calicoÁªÑ‰ª∂ÁöÑÂÆâË£Ö\">#</a> 9. Calico ÁªÑ‰ª∂ÁöÑÂÆâË£Ö</h4>\n<p>‰ª•‰∏ãÊ≠•È™§Âè™Âú® master01 ÊâßË°åÔºö</p>\n<pre><code>cd /root/k8s-ha-install/calico/\n</code></pre>\n<p>Êõ¥Êîπ calico ÁöÑÁΩëÊÆµÔºå‰∏ªË¶ÅÈúÄË¶ÅÂ∞ÜÁ∫¢Ëâ≤ÈÉ®ÂàÜÁöÑÁΩëÊÆµÔºåÊîπ‰∏∫Ëá™Â∑±ÁöÑ Pod ÁΩëÊÆµ</p>\n<pre><code>sed -i &quot;s#POD_CIDR#172.16.0.0/16#g&quot; calico.yaml\n</code></pre>\n<p><em>Ê£ÄÊü•ÁΩëÊÆµÊòØËá™Â∑±ÁöÑ Pod ÁΩëÊÆµÔºå grep &quot;IPV4POOL_CIDR&quot; calico.yaml  -A 1</em></p>\n<p>Êü•ÁúãÂÆπÂô®ÂíåËäÇÁÇπÁä∂ÊÄÅÔºö</p>\n<pre><code>[root@k8s-master01 calico]# kubectl get po -n kube-system\nNAME                                       READY   STATUS    RESTARTS      AGE\ncalico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s\ncalico-node-8fxqp                          1/1     Running   0             85s\ncalico-node-8nkfl                          1/1     Running   0             86s\ncalico-node-pmpf4                          1/1     Running   0             86s\ncalico-node-vnlk7                          1/1     Running   0             86s\ncalico-node-xpchb                          1/1     Running   0             85s\ncalico-typha-67c6dc57d6-259t8              1/1     Running   0             86s\n</code></pre>\n<p><em>Â¶ÇÊûúÂÆπÂô®Áä∂ÊÄÅÂºÇÂ∏∏ÂèØ‰ª•‰ΩøÁî® kubectl describe ÊàñËÄÖ kubectl logs Êü•ÁúãÂÆπÂô®ÁöÑÊó•Âøó</em></p>\n<ol>\n<li>Kubectl logs -f POD_NAME -n kube-system</li>\n<li>Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system</li>\n</ol>\n<h4 id=\"10-ÂÆâË£Öcoredns\"><a class=\"anchor\" href=\"#10-ÂÆâË£Öcoredns\">#</a> 10. ÂÆâË£Ö CoreDNS</h4>\n<pre><code>cd /root/k8s-ha-install/\n</code></pre>\n<p>Â¶ÇÊûúÊõ¥Êîπ‰∫Ü k8s service ÁöÑÁΩëÊÆµÈúÄË¶ÅÂ∞Ü coredns ÁöÑ serviceIP ÊîπÊàê k8s service ÁΩëÊÆµÁöÑÁ¨¨ÂçÅ‰∏™ IP</p>\n<pre><code>COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk '&#123;print $3&#125;'`0\nsed -i &quot;s#KUBEDNS_SERVICE_IP#$&#123;COREDNS_SERVICE_IP&#125;#g&quot; CoreDNS/coredns.yaml\n</code></pre>\n<p>ÂÆâË£Ö coredns</p>\n<pre><code>[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml \nserviceaccount/coredns created\nclusterrole.rbac.authorization.k8s.io/system:coredns created\nclusterrolebinding.rbac.authorization.k8s.io/system:coredns created\nconfigmap/coredns created\ndeployment.apps/coredns created\nservice/kube-dns created\n</code></pre>\n<h4 id=\"11-metricsÈÉ®ÁΩ≤\"><a class=\"anchor\" href=\"#11-metricsÈÉ®ÁΩ≤\">#</a> 11. Metrics ÈÉ®ÁΩ≤</h4>\n<p>Âú®Êñ∞ÁâàÁöÑ Kubernetes ‰∏≠Á≥ªÁªüËµÑÊ∫êÁöÑÈááÈõÜÂùá‰ΩøÁî® Metrics-serverÔºåÂèØ‰ª•ÈÄöËøá Metrics ÈááÈõÜËäÇÁÇπÂíå Pod ÁöÑÂÜÖÂ≠ò„ÄÅÁ£ÅÁõò„ÄÅCPU ÂíåÁΩëÁªúÁöÑ‰ΩøÁî®Áéá„ÄÇ</p>\n<p>‰ª•‰∏ãÊìç‰ΩúÂùáÂú®<mark> master01 ËäÇÁÇπ</mark>ÊâßË°åÔºåÂÆâË£Ö metrics server:</p>\n<pre><code>cd /root/k8s-ha-install/metrics-server\nkubectl  create -f . \n\nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>Á≠âÂæÖ metrics server ÂêØÂä®ÁÑ∂ÂêéÊü•ÁúãÁä∂ÊÄÅÔºö</p>\n<pre><code># kubectl  top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-master01   231m         5%     1620Mi          42%       \nk8s-master02   274m         6%     1203Mi          31%       \nk8s-master03   202m         5%     1251Mi          32%       \nk8s-node01     69m          1%     667Mi           17%       \nk8s-node02     73m          1%     650Mi           16%\n</code></pre>\n<p>Â¶ÇÊûúÊúâÂ¶Ç‰∏ãÊä•ÈîôÔºåÂèØ‰ª•Á≠âÂæÖ 10 ÂàÜÈíüÂêéÔºåÂÜçÊ¨°Êü•ÁúãÔºö</p>\n<pre><code>Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)\n</code></pre>\n<h4 id=\"12-dashboardÈÉ®ÁΩ≤\"><a class=\"anchor\" href=\"#12-dashboardÈÉ®ÁΩ≤\">#</a> 12. Dashboard ÈÉ®ÁΩ≤</h4>\n<h5 id=\"121-ÂÆâË£Ödashboard\"><a class=\"anchor\" href=\"#121-ÂÆâË£Ödashboard\">#</a> 12.1 ÂÆâË£Ö Dashboard</h5>\n<p>Dashboard Áî®‰∫éÂ±ïÁ§∫ÈõÜÁæ§‰∏≠ÁöÑÂêÑÁ±ªËµÑÊ∫êÔºåÂêåÊó∂‰πüÂèØ‰ª•ÈÄöËøá Dashboard ÂÆûÊó∂Êü•Áúã Pod ÁöÑÊó•ÂøóÂíåÂú®ÂÆπÂô®‰∏≠ÊâßË°å‰∏Ä‰∫õÂëΩ‰ª§Á≠â„ÄÇ</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"122-ÁôªÂΩïdashboard\"><a class=\"anchor\" href=\"#122-ÁôªÂΩïdashboard\">#</a> 12.2 ÁôªÂΩï dashboard</h5>\n<p>Âú®Ë∞∑Ê≠åÊµèËßàÂô®ÔºàChromeÔºâÂêØÂä®Êñá‰ª∂‰∏≠Âä†ÂÖ•ÂêØÂä®ÂèÇÊï∞ÔºåÁî®‰∫éËß£ÂÜ≥Êó†Ê≥ïËÆøÈóÆ Dashboard ÁöÑÈóÆÈ¢òÔºåÂèÇËÄÉ‰∏ãÂõæÔºö</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>Êõ¥Êîπ dashboard ÁöÑ svc ‰∏∫ NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>Â∞Ü ClusterIP Êõ¥Êîπ‰∏∫ NodePortÔºàÂ¶ÇÊûúÂ∑≤Áªè‰∏∫ NodePort ÂøΩÁï•Ê≠§Ê≠•È™§Ôºâ</em></p>\n<p>Êü•ÁúãÁ´ØÂè£Âè∑Ôºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>Ê†πÊçÆËá™Â∑±ÁöÑÂÆû‰æãÁ´ØÂè£Âè∑ÔºåÈÄöËøá‰ªªÊÑèÂÆâË£Ö‰∫Ü kube-proxy ÁöÑÂÆø‰∏ªÊú∫ÁöÑ IP + Á´ØÂè£Âç≥ÂèØËÆøÈóÆÂà∞ dashboardÔºö</p>\n<p>ËÆøÈóÆ DashboardÔºö<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> ÔºàÊää IP Âú∞ÂùÄÂíåÁ´ØÂè£ÊîπÊàê‰Ω†Ëá™Â∑±ÁöÑÔºâÈÄâÊã©ÁôªÂΩïÊñπÂºè‰∏∫‰ª§ÁâåÔºàÂç≥ token ÊñπÂºèÔºâÔºåÂèÇËÄÉ‰∏ãÂõæÔºö</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>ÂàõÂª∫ÁôªÂΩï TokenÔºö</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>Â∞Ü token ÂÄºËæìÂÖ•Âà∞‰ª§ÁâåÂêéÔºåÂçïÂáªÁôªÂΩïÂç≥ÂèØËÆøÈóÆ DashboardÔºåÂèÇËÄÉ‰∏ãÂõæÔºö</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"14-containerdÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\"><a class=\"anchor\" href=\"#14-containerdÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\">#</a> 14. Containerd ÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü</h4>\n<pre><code># vim /etc/containerd/config.toml\n#Ê∑ªÂä†‰ª•‰∏ãÈÖçÁΩÆÈïúÂÉèÂä†ÈÄüÊúçÂä°\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>ÊâÄÊúâËäÇÁÇπÈáçÊñ∞ÂêØÂä® ContainerdÔºö</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"15-dockerÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\"><a class=\"anchor\" href=\"#15-dockerÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\">#</a> 15. Docker ÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>ÊâÄÊúâËäÇÁÇπÈáçÊñ∞ÂêØÂä® DockerÔºö</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/2771271649.html",
            "url": "http://ixuyong.cn/posts/2771271649.html",
            "title": "‰∫ëÂéüÁîüK8sÂÆâÂÖ®‰∏ìÂÆ∂CKSËÆ§ËØÅËÄÉÈ¢òËØ¶Ëß£",
            "date_published": "2025-04-09T13:38:39.000Z",
            "content_html": "<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Êä±Ê≠â, Ëøô‰∏™ÂØÜÁ†ÅÁúãÁùÄ‰∏çÂ§™ÂØπ, ËØ∑ÂÜçËØïËØï„ÄÇ\" data-whm=\"Êä±Ê≠â, Ëøô‰∏™ÊñáÁ´†‰∏çËÉΩË¢´Ê†°È™å, ‰∏çËøáÊÇ®ËøòÊòØËÉΩÁúãÁúãËß£ÂØÜÂêéÁöÑÂÜÖÂÆπ„ÄÇ\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-xray\">\n      <input class=\"hbe hbe-input-field hbe-input-field-xray\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-xray\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-xray\">ÊÇ®Â•Ω, ËøôÈáåÈúÄË¶ÅËæìÂÖ•ÂØÜÁ†Å„ÄÇ</span>\n      </label>\n      <svg class=\"hbe hbe-graphic hbe-graphic-xray\" width=\"300%\" height=\"100%\" viewBox=\"0 0 1200 60\" preserveAspectRatio=\"none\">\n        <path d=\"M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0\"></path>\n        <path d=\"M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0\"></path>\n      </svg>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">",
            "tags": [
                "Kubernetes"
            ]
        },
        {
            "id": "http://ixuyong.cn/posts/3166738000.html",
            "url": "http://ixuyong.cn/posts/3166738000.html",
            "title": "KubeadmÈ´òÂèØÁî®ÂÆâË£ÖK8sÈõÜÁæ§",
            "date_published": "2025-04-09T10:28:34.000Z",
            "content_html": "<h2 id=\"kubeadmÈ´òÂèØÁî®ÂÆâË£Ök8sÈõÜÁæ§\"><a class=\"anchor\" href=\"#kubeadmÈ´òÂèØÁî®ÂÆâË£Ök8sÈõÜÁæ§\">#</a> Kubeadm È´òÂèØÁî®ÂÆâË£Ö K8s ÈõÜÁæ§</h2>\n<h4 id=\"1-Âü∫Êú¨ÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#1-Âü∫Êú¨ÈÖçÁΩÆ\">#</a> 1. Âü∫Êú¨ÈÖçÁΩÆ</h4>\n<h5 id=\"11-Âü∫Êú¨ÁéØÂ¢ÉÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#11-Âü∫Êú¨ÁéØÂ¢ÉÈÖçÁΩÆ\">#</a> 1.1 Âü∫Êú¨ÁéØÂ¢ÉÈÖçÁΩÆ</h5>\n<table>\n<thead>\n<tr>\n<th>‰∏ªÊú∫Âêç</th>\n<th>IP Âú∞ÂùÄ</th>\n<th>ËØ¥Êòé</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k8s-master01 ~ 03</td>\n<td>192.168.1.71 ~ 73</td>\n<td>master ËäÇÁÇπ * 3</td>\n</tr>\n<tr>\n<td>/</td>\n<td>192.168.1.70</td>\n<td>keepalived ËôöÊãü IPÔºà‰∏çÂç†Áî®Êú∫Âô®Ôºâ</td>\n</tr>\n<tr>\n<td>k8s-node01 ~ 02</td>\n<td>192.168.1.74/75</td>\n<td>worker ËäÇÁÇπ * 2</td>\n</tr>\n</tbody>\n</table>\n<p><em>ËØ∑Áªü‰∏ÄÊõøÊç¢Ëøô‰∫õÁΩëÊÆµÔºåPod ÁΩëÊÆµÂíå service ÂíåÂÆø‰∏ªÊú∫ÁΩëÊÆµ‰∏çË¶ÅÈáçÂ§çÔºÅÔºÅÔºÅ</em></p>\n<table>\n<thead>\n<tr>\n<th><em><strong>* ÈÖçÁΩÆ‰ø°ÊÅØ *</strong></em></th>\n<th>Â§áÊ≥®</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Á≥ªÁªüÁâàÊú¨</td>\n<td>Rocky Linux 8/9</td>\n</tr>\n<tr>\n<td>Containerd</td>\n<td>latest</td>\n</tr>\n<tr>\n<td>Pod ÁΩëÊÆµ</td>\n<td>172.16.0.0/16</td>\n</tr>\n<tr>\n<td>Service ÁΩëÊÆµ</td>\n<td>10.96.0.0/16</td>\n</tr>\n</tbody>\n</table>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Êõ¥Êîπ‰∏ªÊú∫ÂêçÔºàÂÖ∂ÂÆÉËäÇÁÇπÊåâÈúÄ‰øÆÊîπÔºâÔºö</p>\n<pre><code>hostnamectl set-hostname k8s-master01 \n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ hostsÔºå‰øÆÊîπ /etc/hosts Â¶Ç‰∏ãÔºö</p>\n<pre><code>[root@k8s-master01 ~]# cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.1.71 k8s-master01\n192.168.1.72 k8s-master02\n192.168.1.73 k8s-master03\n192.168.1.74 k8s-node01\n192.168.1.75 k8s-node02\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ yum Ê∫êÔºö</p>\n<pre><code># ÈÖçÁΩÆÂü∫Á°ÄÊ∫ê\nsed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/*.repo\n\nyum makecache\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂøÖÂ§áÂ∑•ÂÖ∑ÂÆâË£ÖÔºö</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÖ≥Èó≠Èò≤ÁÅ´Â¢ô„ÄÅselinux„ÄÅdnsmasq„ÄÅswap„ÄÅÂºÄÂêØ rsyslog„ÄÇÊúçÂä°Âô®ÈÖçÁΩÆÂ¶Ç‰∏ãÔºö</p>\n<pre><code>systemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsetenforce 0\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux\nsed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config\nsystemctl enable --now rsyslog\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÖ≥Èó≠ swap ÂàÜÂå∫Ôºö</p>\n<pre><code>swapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÆâË£Ö ntpdateÔºö</p>\n<pre><code>sudo dnf install epel-release -y\nsudo dnf config-manager --set-enabled epel\nsudo dnf install ntpsec\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂêåÊ≠•Êó∂Èó¥Âπ∂ÈÖçÁΩÆ‰∏äÊµ∑Êó∂Âå∫Ôºö</p>\n<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' &gt;/etc/timezone\nntpdate time2.aliyun.com\n# Âä†ÂÖ•Âà∞crontab\ncrontab -e\n*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ limitÔºö</p>\n<pre><code>ulimit -SHn 65535\nvim /etc/security/limits.conf\n# Êú´Â∞æÊ∑ªÂä†Â¶Ç‰∏ãÂÜÖÂÆπ\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 65535\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂçáÁ∫ßÁ≥ªÁªüÔºö</p>\n<pre><code>yum update -y\n</code></pre>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÂÖçÂØÜÈí•ÁôªÂΩïÂÖ∂‰ªñËäÇÁÇπÔºåÂÆâË£ÖËøáÁ®ã‰∏≠ÁîüÊàêÈÖçÁΩÆÊñá‰ª∂ÂíåËØÅ‰π¶ÂùáÂú® Master01 ‰∏äÊìç‰ΩúÔºåÈõÜÁæ§ÁÆ°ÁêÜ‰πüÂú® Master01 ‰∏äÊìç‰ΩúÔºö</p>\n<pre><code>ssh-keygen -t rsa\nfor i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done\n</code></pre>\n<p><em>Ê≥®ÊÑèÔºöÂÖ¨Êúâ‰∫ëÁéØÂ¢ÉÔºåÂèØËÉΩÈúÄË¶ÅÊää kubectl ÊîæÂú®‰∏Ä‰∏™Èùû Master ËäÇÁÇπ‰∏ä</em></p>\n<p><mark>Master01 ËäÇÁÇπ</mark>‰∏ãËΩΩÂÆâË£ÖÊâÄÊúâÁöÑÊ∫êÁ†ÅÊñá‰ª∂Ôºö</p>\n<pre><code>cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install\n</code></pre>\n<h5 id=\"12-ÂÜÖÊ†∏ÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#12-ÂÜÖÊ†∏ÈÖçÁΩÆ\">#</a> 1.2 ÂÜÖÊ†∏ÈÖçÁΩÆ</h5>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÆâË£Ö ipvsadmÔºö</p>\n<pre><code>yum install ipvsadm ipset sysstat conntrack libseccomp -y\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ ipvs Ê®°ÂùóÔºö</p>\n<pre><code>modprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂàõÂª∫ ipvs.confÔºåÂπ∂ÈÖçÁΩÆÂºÄÊú∫Ëá™Âä®Âä†ËΩΩÔºö</p>\n<pre><code>vim /etc/modules-load.d/ipvs.conf \n# Âä†ÂÖ•‰ª•‰∏ãÂÜÖÂÆπ\nip_vs\nip_vs_lc\nip_vs_wlc\nip_vs_rr\nip_vs_wrr\nip_vs_lblc\nip_vs_lblcr\nip_vs_dh\nip_vs_sh\nip_vs_fo\nip_vs_nq\nip_vs_sed\nip_vs_ftp\nip_vs_sh\nnf_conntrack\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÁÑ∂ÂêéÊâßË°å systemctl enable --now systemd-modules-load.service Âç≥ÂèØÔºàÊä•Èîô‰∏çÁî®ÁÆ°Ôºâ</p>\n<pre><code>systemctl enable --now systemd-modules-load.service\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÜÖÊ†∏‰ºòÂåñÈÖçÁΩÆÔºö</p>\n<pre><code>cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nfs.may_detach_mounts = 1\nnet.ipv4.conf.all.route_localnet = 1\nvm.overcommit_memory=1\nvm.panic_on_oom=0\nfs.inotify.max_user_watches=89100\nfs.file-max=52706963\nfs.nr_open=52706963\nnet.netfilter.nf_conntrack_max=2310720\n\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_probes = 3\nnet.ipv4.tcp_keepalive_intvl =15\nnet.ipv4.tcp_max_tw_buckets = 36000\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_max_orphans = 327680\nnet.ipv4.tcp_orphan_retries = 3\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.ip_conntrack_max = 65536\nnet.ipv4.tcp_max_syn_backlog = 16384\nnet.ipv4.tcp_timestamps = 0\nnet.core.somaxconn = 16384\nEOF\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Â∫îÁî®ÈÖçÁΩÆÔºö</p>\n<pre><code>sysctl --system\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆÂÆåÂÜÖÊ†∏ÂêéÔºåÈáçÂêØÊú∫Âô®Ôºå‰πãÂêéÊü•ÁúãÂÜÖÊ†∏Ê®°ÂùóÊòØÂê¶Â∑≤Ëá™Âä®Âä†ËΩΩÔºö</p>\n<pre><code>reboot\nlsmod | grep --color=auto -e ip_vs -e nf_conntrack\n</code></pre>\n<h4 id=\"2-È´òÂèØÁî®ÁªÑ‰ª∂ÂÆâË£Ö\"><a class=\"anchor\" href=\"#2-È´òÂèØÁî®ÁªÑ‰ª∂ÂÆâË£Ö\">#</a> 2. È´òÂèØÁî®ÁªÑ‰ª∂ÂÆâË£Ö</h4>\n<p><em>Ê≥®ÊÑèÔºöÂ¶ÇÊûúÂÆâË£ÖÁöÑ‰∏çÊòØÈ´òÂèØÁî®ÈõÜÁæ§Ôºåhaproxy Âíå keepalived Êó†ÈúÄÂÆâË£Ö</em></p>\n<p><em>Ê≥®ÊÑèÔºöÂÖ¨Êúâ‰∫ëË¶ÅÁî®ÂÖ¨Êúâ‰∫ëËá™Â∏¶ÁöÑË¥üËΩΩÂùáË°°ÔºåÊØîÂ¶ÇÈòøÈáå‰∫ëÁöÑ SLB„ÄÅNLBÔºåËÖæËÆØ‰∫ëÁöÑ ELBÔºåÁî®Êù•Êõø‰ª£ haproxy Âíå keepalivedÔºåÂõ†‰∏∫ÂÖ¨Êúâ‰∫ëÂ§ßÈÉ®ÂàÜÈÉΩÊòØ‰∏çÊîØÊåÅ keepalived ÁöÑ„ÄÇ</em></p>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÈÄöËøá yum ÂÆâË£Ö HAProxy Âíå KeepAlivedÔºö</p>\n<pre><code>yum install keepalived haproxy -y\n</code></pre>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÈÖçÁΩÆ HAProxyÔºåÈúÄË¶ÅÊ≥®ÊÑèÈªÑËâ≤ÈÉ®ÂàÜÁöÑ IPÔºö</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/haproxy\n[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg \nglobal\n  maxconn  2000\n  ulimit-n  16384\n  log  127.0.0.1 local0 err\n  stats timeout 30s\n\ndefaults\n  log global\n  mode  http\n  option  httplog\n  timeout connect 5000\n  timeout client  50000\n  timeout server  50000\n  timeout http-request 15s\n  timeout http-keep-alive 15s\n\nfrontend monitor-in\n  bind *:33305\n  mode http\n  option httplog\n  monitor-uri /monitor\n\nfrontend k8s-master\n  bind 0.0.0.0:16443       #HAProxyÁõëÂê¨Á´ØÂè£\n  bind 127.0.0.1:16443     #HAProxyÁõëÂê¨Á´ØÂè£\n  mode tcp\n  option tcplog\n  tcp-request inspect-delay 5s\n  default_backend k8s-master\n\nbackend k8s-master\n  mode tcp\n  option tcplog\n  option tcp-check\n  balance roundrobin\n  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n  server k8s-master01\t192.168.1.71:6443  check       #API Server IPÂú∞ÂùÄ\n  server k8s-master02\t192.168.1.72:6443  check       #API Server IPÂú∞ÂùÄ\n  server k8s-master03\t192.168.1.73:6443  check       #API Server IPÂú∞ÂùÄ\n</code></pre>\n<p><mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÈÖçÁΩÆ KeepAlivedÔºåÈúÄË¶ÅÊ≥®ÊÑèÈªÑËâ≤ÈÉ®ÂàÜÁöÑÈÖçÁΩÆ„ÄÇ</p>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÁöÑÈÖçÁΩÆÔºö</p>\n<pre><code>[root@k8s-master01 etc]# mkdir /etc/keepalived\n\n[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n    interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state MASTER\n    interface ens160               #ÁΩëÂç°ÂêçÁß∞\n    mcast_src_ip 192.168.1.71      #K8s-master01 IPÂú∞ÂùÄ\n    virtual_router_id 51\n    priority 101\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70        #VIPÂú∞ÂùÄ\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\t\n</code></pre>\n<p><mark>Master02 ËäÇÁÇπ</mark>ÁöÑÈÖçÁΩÆÔºö</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n   interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                #ÁΩëÂç°ÂêçÁß∞\n    mcast_src_ip 192.168.1.72       #K8s-master02 IPÂú∞ÂùÄ\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70              #VIPÂú∞ÂùÄ\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>Master03 ËäÇÁÇπ</mark>ÁöÑÈÖçÁΩÆÔºö</p>\n<pre><code># vim /etc/keepalived/keepalived.conf \n\n! Configuration File for keepalived\nglobal_defs &#123;\n    router_id LVS_DEVEL\nscript_user root\n    enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n    script &quot;/etc/keepalived/check_apiserver.sh&quot;\n interval 5\n    weight -5\n    fall 2  \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n    state BACKUP\n    interface ens160                 #ÁΩëÂç°ÂêçÁß∞\n    mcast_src_ip 192.168.1.73        #K8s-master03 IPÂú∞ÂùÄ\n    virtual_router_id 51\n    priority 100\n    advert_int 2\n    authentication &#123;\n        auth_type PASS\n        auth_pass K8SHA_KA_AUTH\n    &#125;\n    virtual_ipaddress &#123;\n        192.168.1.70          #VIPÂú∞ÂùÄ\n    &#125;\n    track_script &#123;\n       chk_apiserver\n    &#125;\n&#125;\n</code></pre>\n<p><mark>ÊâÄÊúâ master ËäÇÁÇπ</mark>ÈÖçÁΩÆ KeepAlived ÂÅ•Â∫∑Ê£ÄÊü•Êñá‰ª∂Ôºö</p>\n<pre><code>[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh \n#!/bin/bash\n\nerr=0\nfor k in $(seq 1 3)\ndo\n    check_code=$(pgrep haproxy)\n    if [[ $check_code == &quot;&quot; ]]; then\n        err=$(expr $err + 1)\n        sleep 1\n        continue\n    else\n        err=0\n        break\n    fi\ndone\n\nif [[ $err != &quot;0&quot; ]]; then\n    echo &quot;systemctl stop keepalived&quot;\n    /usr/bin/systemctl stop keepalived\n    exit 1\nelse\n    exit 0\nfi\n</code></pre>\n<p><mark>ÊâÄÊúâ master ËäÇÁÇπ</mark>ÈÖçÁΩÆÂÅ•Â∫∑Ê£ÄÊü•Êñá‰ª∂Ê∑ªÂä†ÊâßË°åÊùÉÈôêÔºö</p>\n<pre><code>chmod +x /etc/keepalived/check_apiserver.sh\n</code></pre>\n<p><mark>ÊâÄÊúâ master ËäÇÁÇπ</mark>ÂêØÂä® haproxy Âíå keepalivedÔºö</p>\n<pre><code>[root@k8s-master01 keepalived]# systemctl daemon-reload\n[root@k8s-master01 keepalived]# systemctl enable --now haproxy\n[root@k8s-master01 keepalived]# systemctl enable --now keepalived\n</code></pre>\n<p>ÈáçË¶ÅÔºöÂ¶ÇÊûúÂÆâË£Ö‰∫Ü keepalived Âíå haproxyÔºåÈúÄË¶ÅÊµãËØï keepalived ÊòØÂê¶ÊòØÊ≠£Â∏∏ÁöÑ</p>\n<pre><code>ÊâÄÊúâËäÇÁÇπÊµãËØïVIP\n[root@k8s-master01 ~]# ping 192.168.1.70 -c 4\nPING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.\n64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms\n64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms\n64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms\n64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms\n\n[root@k8s-master01 ~]# telnet 192.168.1.70 16443\nTrying 192.168.1.70...\nConnected to 192.168.1.70.\nEscape character is '^]'.\nConnection closed by foreign host.\n</code></pre>\n<p>Â¶ÇÊûú ping ‰∏çÈÄö‰∏î telnet Ê≤°ÊúâÂá∫Áé∞ ] ÔºåÂàôËÆ§‰∏∫ VIP ‰∏çÂèØ‰ª•Ôºå‰∏çÂèØÂú®ÁªßÁª≠ÂæÄ‰∏ãÊâßË°åÔºåÈúÄË¶ÅÊéíÊü• keepalived ÁöÑÈóÆÈ¢òÔºåÊØîÂ¶ÇÈò≤ÁÅ´Â¢ôÂíå selinuxÔºåhaproxy Âíå keepalived ÁöÑÁä∂ÊÄÅÔºåÁõëÂê¨Á´ØÂè£Á≠â</p>\n<ul>\n<li>ÊâÄÊúâËäÇÁÇπÊü•ÁúãÈò≤ÁÅ´Â¢ôÁä∂ÊÄÅÂøÖÈ°ª‰∏∫ disable Âíå inactiveÔºösystemctl status firewalld</li>\n<li>ÊâÄÊúâËäÇÁÇπÊü•Áúã selinux Áä∂ÊÄÅÔºåÂøÖÈ°ª‰∏∫ disableÔºögetenforce</li>\n<li>master ËäÇÁÇπÊü•Áúã haproxy Âíå keepalived Áä∂ÊÄÅÔºösystemctl status keepalived haproxy</li>\n<li>master ËäÇÁÇπÊü•ÁúãÁõëÂê¨Á´ØÂè£Ôºönetstat -lntp</li>\n</ul>\n<p>Â¶ÇÊûú‰ª•‰∏äÈÉΩÊ≤°ÊúâÈóÆÈ¢òÔºåÈúÄË¶ÅÁ°ÆËÆ§Ôºö</p>\n<ol>\n<li>\n<p>ÊòØÂê¶ÊòØÂÖ¨Êúâ‰∫ëÊú∫Âô®</p>\n</li>\n<li>\n<p>ÊòØÂê¶ÊòØÁßÅÊúâ‰∫ëÊú∫Âô®ÔºàÁ±ª‰ºº OpenStackÔºâ</p>\n</li>\n</ol>\n<p>‰∏äËø∞ÂÖ¨Êúâ‰∫ë‰∏ÄËà¨ÈÉΩÊòØ‰∏çÊîØÊåÅ keepalivedÔºåÁßÅÊúâ‰∫ëÂèØËÉΩ‰πüÊúâÈôêÂà∂ÔºåÈúÄË¶ÅÂíåËá™Â∑±ÁöÑÁßÅÊúâ‰∫ëÁÆ°ÁêÜÂëòÂí®ËØ¢</p>\n<h4 id=\"3-runtimeÂÆâË£Ö\"><a class=\"anchor\" href=\"#3-runtimeÂÆâË£Ö\">#</a> 3. Runtime ÂÆâË£Ö</h4>\n<p>Â¶ÇÊûúÂÆâË£ÖÁöÑÁâàÊú¨‰Ωé‰∫é 1.24ÔºåÈÄâÊã© Docker Âíå Containerd ÂùáÂèØÔºåÈ´ò‰∫é 1.24 Âª∫ËÆÆÈÄâÊã© Containerd ‰Ωú‰∏∫ RuntimeÔºå‰∏çÂÜçÊé®Ëçê‰ΩøÁî® Docker ‰Ωú‰∏∫ Runtime„ÄÇ</p>\n<h5 id=\"31-ÂÆâË£Öcontainerd\"><a class=\"anchor\" href=\"#31-ÂÆâË£Öcontainerd\">#</a> 3.1 ÂÆâË£Ö Containerd</h5>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆÂÆâË£ÖÊ∫êÔºö</p>\n<pre><code>yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÆâË£Ö docker-ceÔºàÂ¶ÇÊûúÂú®‰ª•ÂâçÂ∑≤ÁªèÂÆâË£ÖËøáÔºåÈúÄË¶ÅÈáçÊñ∞ÂÆâË£ÖÊõ¥Êñ∞‰∏Ä‰∏ãÔºâÔºö</p>\n<pre><code># yum install docker-ce containerd -y\n</code></pre>\n<p><em>ÂèØ‰ª•Êó†ÈúÄÂêØÂä® DockerÔºåÂè™ÈúÄË¶ÅÈÖçÁΩÆÂíåÂêØÂä® Containerd Âç≥ÂèØ„ÄÇ</em></p>\n<p>È¶ñÂÖàÈÖçÁΩÆ Containerd ÊâÄÈúÄÁöÑÊ®°ÂùóÔºà<mark>ÊâÄÊúâËäÇÁÇπ</mark>ÔºâÔºö</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Âä†ËΩΩÊ®°ÂùóÔºö</p>\n<pre><code># modprobe -- overlay\n# modprobe -- br_netfilter\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÔºåÈÖçÁΩÆ Containerd ÊâÄÈúÄÁöÑÂÜÖÊ†∏Ôºö</p>\n<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Âä†ËΩΩÂÜÖÊ†∏Ôºö</p>\n<pre><code># sysctl --system\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÁîüÊàê Containerd ÁöÑÈÖçÁΩÆÊñá‰ª∂Ôºö</p>\n<pre><code># mkdir -p /etc/containerd\n# containerd config default | tee /etc/containerd/config.toml\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Êõ¥Êîπ Containerd ÁöÑ Cgroup Âíå Pause ÈïúÂÉèÈÖçÁΩÆÔºö</p>\n<pre><code>sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml\nsed -i 's#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\nsed -i 's#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g'  /etc/containerd/config.toml\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂêØÂä® ContainerdÔºåÂπ∂ÈÖçÁΩÆÂºÄÊú∫Ëá™ÂêØÂä®Ôºö</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now containerd\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆ crictl ÂÆ¢Êà∑Á´ØËøûÊé•ÁöÑËøêË°åÊó∂‰ΩçÁΩÆÔºàÂèØÈÄâÔºâÔºö</p>\n<pre><code># cat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre>\n<h4 id=\"4-ÂÆâË£ÖkubernetesÁªÑ‰ª∂\"><a class=\"anchor\" href=\"#4-ÂÆâË£ÖkubernetesÁªÑ‰ª∂\">#</a> 4 . ÂÆâË£Ö Kubernetes ÁªÑ‰ª∂</h4>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÈÖçÁΩÆÊ∫êÔºàÊ≥®ÊÑèÊõ¥ÊîπÁâàÊú¨Âè∑ÔºâÔºö</p>\n<pre><code>cat &lt;&lt;EOF | tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key\nEOF\n</code></pre>\n<p>È¶ñÂÖàÂú®<mark> Master01 ËäÇÁÇπ</mark>Êü•ÁúãÊúÄÊñ∞ÁöÑ Kubernetes ÁâàÊú¨ÊòØÂ§öÂ∞ëÔºö</p>\n<pre><code># yum list kubeadm.x86_64 --showduplicates | sort -r\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ÂÆâË£Ö 1.32 ÊúÄÊñ∞ÁâàÊú¨ kubeadm„ÄÅkubelet Âíå kubectlÔºö</p>\n<pre><code># yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y\n</code></pre>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>ËÆæÁΩÆ Kubelet ÂºÄÊú∫Ëá™ÂêØÂä®ÔºàÁî±‰∫éËøòÊú™ÂàùÂßãÂåñÔºåÊ≤°Êúâ kubelet ÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÊ≠§Êó∂ kubelet Êó†Ê≥ïÂêØÂä®ÔºåÊó†ÈúÄÂÖ≥ÂøÉÔºâÔºö</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now kubelet\n</code></pre>\n<p><em>Ê≠§Êó∂ kubelet ÊòØËµ∑‰∏çÊù•ÁöÑÔºåÊó•Âøó‰ºöÊúâÊä•Èîô‰∏çÂΩ±ÂìçÔºÅ</em></p>\n<h4 id=\"5-ÈõÜÁæ§ÂàùÂßãÂåñ\"><a class=\"anchor\" href=\"#5-ÈõÜÁæ§ÂàùÂßãÂåñ\">#</a> 5 . ÈõÜÁæ§ÂàùÂßãÂåñ</h4>\n<p>‰ª•‰∏ãÊìç‰ΩúÂú®<mark> master01</mark>ÔºàÊ≥®ÊÑèÈªÑËâ≤ÈÉ®ÂàÜÔºâÔºö</p>\n<pre><code>vim kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta3\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 7t2weq.bjbawausm0jaxury\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.1.71\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  name: k8s-master01\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n---\napiServer:\n  certSANs:\n  - 192.168.1.70               # Â¶ÇÊûúÊê≠Âª∫ÁöÑ‰∏çÊòØÈ´òÂèØÁî®ÈõÜÁæ§ÔºåÊääÊ≠§Â§ÑÊîπ‰∏∫masterÁöÑIP\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta3\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: 192.168.1.70:16443 # Â¶ÇÊûúÊê≠Âª∫ÁöÑ‰∏çÊòØÈ´òÂèØÁî®ÈõÜÁæ§ÔºåÊääÊ≠§Â§ÑIPÊîπ‰∏∫masterÁöÑIPÔºåÁ´ØÂè£ÊîπÊàê6443\ncontrollerManager: &#123;&#125;\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.32.3    # Êõ¥ÊîπÊ≠§Â§ÑÁöÑÁâàÊú¨Âè∑Âíåkubeadm version‰∏ÄËá¥\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 172.16.0.0/16    # Ê≥®ÊÑèÊ≠§Â§ÑÁöÑÁΩëÊÆµÔºå‰∏çË¶Å‰∏éserviceÂíåËäÇÁÇπÁΩëÊÆµÂÜ≤Á™Å\n  serviceSubnet: 10.96.0.0/16 # Ê≥®ÊÑèÊ≠§Â§ÑÁöÑÁΩëÊÆµÔºå‰∏çË¶Å‰∏épodÂíåËäÇÁÇπÁΩëÊÆµÂÜ≤Á™Å\nscheduler: &#123;&#125;\n</code></pre>\n<p><mark>master01 ËäÇÁÇπ</mark>Êõ¥Êñ∞ kubeadm Êñá‰ª∂Ôºö</p>\n<pre><code>kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml\n</code></pre>\n<p>Â∞Ü new.yaml Êñá‰ª∂Â§çÂà∂Âà∞<mark>ÂÖ∂‰ªñ master ËäÇÁÇπ</mark>:</p>\n<pre><code>for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done\n</code></pre>\n<p>‰πãÂêé<mark>ÊâÄÊúâ Master ËäÇÁÇπ</mark>ÊèêÂâç‰∏ãËΩΩÈïúÂÉèÔºåÂèØ‰ª•ËäÇÁúÅÂàùÂßãÂåñÊó∂Èó¥ÔºàÂÖ∂‰ªñËäÇÁÇπ‰∏çÈúÄË¶ÅÊõ¥Êîπ‰ªª‰ΩïÈÖçÁΩÆÔºåÂåÖÊã¨ IP Âú∞ÂùÄ‰πü‰∏çÈúÄË¶ÅÊõ¥ÊîπÔºâÔºö</p>\n<pre><code>kubeadm config images pull --config /root/new.yaml \n</code></pre>\n<p>Ê≠£Á°ÆÁöÑÂèçÈ¶à‰ø°ÊÅØÂ¶Ç‰∏ãÔºà<em><strong>* ÁâàÊú¨ÂèØËÉΩ‰∏ç‰∏ÄÊ†∑ *</strong></em>ÔºâÔºö</p>\n<pre><code>[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml \n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10\n[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0\n</code></pre>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÂàùÂßãÂåñÔºåÂàùÂßãÂåñ‰ª•Âêé‰ºöÂú® /etc/kubernetes ÁõÆÂΩï‰∏ãÁîüÊàêÂØπÂ∫îÁöÑËØÅ‰π¶ÂíåÈÖçÁΩÆÊñá‰ª∂Ôºå‰πãÂêéÂÖ∂‰ªñ Master ËäÇÁÇπÂä†ÂÖ• Master01 Âç≥ÂèØÔºö</p>\n<pre><code>kubeadm init --config /root/new.yaml  --upload-certs\n</code></pre>\n<p>ÂàùÂßãÂåñÊàêÂäü‰ª•ÂêéÔºå‰ºö‰∫ßÁîü Token ÂÄºÔºåÁî®‰∫éÂÖ∂‰ªñËäÇÁÇπÂä†ÂÖ•Êó∂‰ΩøÁî®ÔºåÂõ†Ê≠§Ë¶ÅËÆ∞ÂΩï‰∏ãÂàùÂßãÂåñÊàêÂäüÁîüÊàêÁöÑ token ÂÄºÔºà‰ª§ÁâåÂÄºÔºâÔºö</p>\n<pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n# ‰∏çË¶ÅÂ§çÂà∂ÊñáÊ°£ÂΩì‰∏≠ÁöÑÔºåË¶ÅÂéª‰ΩøÁî®ËäÇÁÇπÁîüÊàêÁöÑ\n  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94\n</code></pre>\n<p><mark>Master01 ËäÇÁÇπ</mark>ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºåÁî®‰∫éËÆøÈóÆ Kubernetes ÈõÜÁæ§Ôºö</p>\n<pre><code>cat &lt;&lt;EOF &gt;&gt; /root/.bashrc\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nEOF\nsource /root/.bashrc\n</code></pre>\n<p><mark>Master01 ËäÇÁÇπ</mark>Êü•ÁúãËäÇÁÇπÁä∂ÊÄÅÔºöÔºàÊòæÁ§∫ NotReady ‰∏çÂΩ±ÂìçÔºâ</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE   VERSION\nk8s-master01   NotReady   control-plane   24s   v1.32.3\n</code></pre>\n<p>ÈááÁî®ÂàùÂßãÂåñÂÆâË£ÖÊñπÂºèÔºåÊâÄÊúâÁöÑÁ≥ªÁªüÁªÑ‰ª∂Âùá‰ª•ÂÆπÂô®ÁöÑÊñπÂºèËøêË°åÂπ∂‰∏îÂú® kube-system ÂëΩÂêçÁ©∫Èó¥ÂÜÖÔºåÊ≠§Êó∂ÂèØ‰ª•Êü•Áúã Pod Áä∂ÊÄÅÔºàÊòæÁ§∫ pending ‰∏çÂΩ±ÂìçÔºâÔºö</p>\n<pre><code class=\"language-\\\"># kubectl get pods -n kube-system\n</code></pre>\n<h5 id=\"51-ÂàùÂßãÂåñÂ§±Ë¥•ÊéíÊü•\"><a class=\"anchor\" href=\"#51-ÂàùÂßãÂåñÂ§±Ë¥•ÊéíÊü•\">#</a> 5.1 ÂàùÂßãÂåñÂ§±Ë¥•ÊéíÊü•</h5>\n<p>Â¶ÇÊûúÂàùÂßãÂåñÂ§±Ë¥•ÔºåÈáçÁΩÆÂêéÂÜçÊ¨°ÂàùÂßãÂåñÔºåÂëΩ‰ª§Â¶Ç‰∏ãÔºàÊ≤°ÊúâÂ§±Ë¥•‰∏çË¶ÅÊâßË°åÔºâÔºö</p>\n<pre><code>kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube\n</code></pre>\n<p>Â¶ÇÊûúÂ§öÊ¨°Â∞ùËØïÈÉΩÊòØÂàùÂßãÂåñÂ§±Ë¥•ÔºåÈúÄË¶ÅÁúãÁ≥ªÁªüÊó•ÂøóÔºåCentOS/RockyLinux Êó•ÂøóË∑ØÂæÑ:/var/log/messagesÔºåUbuntu Á≥ªÂàóÊó•ÂøóË∑ØÂæÑ:/var/log/syslogÔºö</p>\n<pre><code>tail -f /var/log/messages | grep -v &quot;not found&quot;\n</code></pre>\n<p>ÁªèÂ∏∏Âá∫ÈîôÁöÑÂéüÂõ†Ôºö</p>\n<ol>\n<li>Containerd ÁöÑÈÖçÁΩÆÊñá‰ª∂‰øÆÊîπÁöÑ‰∏çÂØπÔºåËá™Ë°åÂèÇËÄÉ„ÄäÂÆâË£Ö containerd„ÄãÂ∞èËäÇÊ†∏ÂØπ</li>\n<li>new.yaml ÈÖçÁΩÆÈóÆÈ¢òÔºåÊØîÂ¶ÇÈùûÈ´òÂèØÁî®ÈõÜÁæ§ÂøòËÆ∞‰øÆÊîπ 16443 Á´ØÂè£‰∏∫ 6443</li>\n<li>new.yaml ÈÖçÁΩÆÈóÆÈ¢òÔºå‰∏â‰∏™ÁΩëÊÆµÊúâ‰∫§ÂèâÔºåÂá∫Áé∞ IP Âú∞ÂùÄÂÜ≤Á™Å</li>\n<li>VIP ‰∏çÈÄöÂØºËá¥Êó†Ê≥ïÂàùÂßãÂåñÊàêÂäüÔºåÊ≠§Êó∂ messages Êó•Âøó‰ºöÊúâ VIP Ë∂ÖÊó∂ÁöÑÊä•Èîô</li>\n</ol>\n<h5 id=\"52-È´òÂèØÁî®master\"><a class=\"anchor\" href=\"#52-È´òÂèØÁî®master\">#</a> 5.2 È´òÂèØÁî® Master</h5>\n<p><strong>ÂÖ∂‰ªñ master</strong> Âä†ÂÖ•ÈõÜÁæ§Ôºåmaster02 Âíå master03 ÂàÜÂà´ÊâßË°å (ÂçÉ‰∏á‰∏çË¶ÅÂú® master01 ÂÜçÊ¨°ÊâßË°åÔºå‰∏çËÉΩÁõ¥Êé•Â§çÂà∂ÊñáÊ°£ÂΩì‰∏≠ÁöÑÂëΩ‰ª§ÔºåËÄåÊòØ‰Ω†Ëá™Â∑±ÂàöÊâç master01 ÂàùÂßãÂåñ‰πãÂêé‰∫ßÁîüÁöÑÂëΩ‰ª§)</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \\\n\t--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c\n</code></pre>\n<p>Êü•ÁúãÂΩìÂâçÁä∂ÊÄÅÔºöÔºàÂ¶ÇÊûúÊòæÁ§∫ NotReady ‰∏çÂΩ±ÂìçÔºâ</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\n</code></pre>\n<h5 id=\"53-tokenËøáÊúüÂ§ÑÁêÜ\"><a class=\"anchor\" href=\"#53-tokenËøáÊúüÂ§ÑÁêÜ\">#</a> 5.3 Token ËøáÊúüÂ§ÑÁêÜ</h5>\n<p>Ê≥®ÊÑèÔºö‰ª•‰∏ãÊ≠•È™§ÊòØ‰∏äËø∞ init ÂëΩ‰ª§‰∫ßÁîüÁöÑ Token ËøáÊúü‰∫ÜÊâçÈúÄË¶ÅÊâßË°å‰ª•‰∏ãÊ≠•È™§ÔºåÂ¶ÇÊûúÊ≤°ÊúâËøáÊúü‰∏çÈúÄË¶ÅÊâßË°åÔºåÁõ¥Êé• join Âç≥ÂèØ„ÄÇ</p>\n<p>Token ËøáÊúüÂêéÁîüÊàêÊñ∞ÁöÑ tokenÔºö</p>\n<pre><code>kubeadm token create --print-join-command\n</code></pre>\n<p>Master ÈúÄË¶ÅÁîüÊàê --certificate-keyÔºö</p>\n<pre><code>kubeadm init phase upload-certs  --upload-certs\n</code></pre>\n<h4 id=\"6-nodeËäÇÁÇπÁöÑÈÖçÁΩÆ\"><a class=\"anchor\" href=\"#6-nodeËäÇÁÇπÁöÑÈÖçÁΩÆ\">#</a> 6. Node ËäÇÁÇπÁöÑÈÖçÁΩÆ</h4>\n<p>Node ËäÇÁÇπ‰∏ä‰∏ªË¶ÅÈÉ®ÁΩ≤ÂÖ¨Âè∏ÁöÑ‰∏Ä‰∫õ‰∏öÂä°Â∫îÁî®ÔºåÁîü‰∫ßÁéØÂ¢É‰∏≠‰∏çÂª∫ËÆÆ Master ËäÇÁÇπÈÉ®ÁΩ≤Á≥ªÁªüÁªÑ‰ª∂‰πãÂ§ñÁöÑÂÖ∂‰ªñ PodÔºåÊµãËØïÁéØÂ¢ÉÂèØ‰ª•ÂÖÅËÆ∏ Master ËäÇÁÇπÈÉ®ÁΩ≤ Pod ‰ª•ËäÇÁúÅÁ≥ªÁªüËµÑÊ∫ê„ÄÇ</p>\n<pre><code>kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \\\n\t--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5\n</code></pre>\n<p>ÊâÄÊúâËäÇÁÇπÂàùÂßãÂåñÂÆåÊàêÂêéÔºåÊü•ÁúãÈõÜÁæ§Áä∂ÊÄÅÔºàNotReady ‰∏çÂΩ±ÂìçÔºâ</p>\n<pre><code># kubectl get node\nNAME           STATUS     ROLES           AGE     VERSION\nk8s-master01   NotReady   control-plane   4m23s   v1.32.3\nk8s-master02   NotReady   control-plane   66s     v1.32.3\nk8s-master03   NotReady   control-plane   14s     v1.32.3\nk8s-node01     NotReady   &lt;none&gt;          13s     v1.32.3\nk8s-node02     NotReady   &lt;none&gt;          10s     v1.32.3\n</code></pre>\n<h4 id=\"7-calicoÁªÑ‰ª∂ÁöÑÂÆâË£Ö\"><a class=\"anchor\" href=\"#7-calicoÁªÑ‰ª∂ÁöÑÂÆâË£Ö\">#</a> 7. Calico ÁªÑ‰ª∂ÁöÑÂÆâË£Ö</h4>\n<p><mark>ÊâÄÊúâËäÇÁÇπ</mark>Á¶ÅÊ≠¢ NetworkManager ÁÆ°ÁêÜ Calico ÁöÑÁΩëÁªúÊé•Âè£ÔºåÈò≤Ê≠¢ÊúâÂÜ≤Á™ÅÊàñÂπ≤Êâ∞Ôºö</p>\n<pre><code>cat &gt;&gt;/etc/NetworkManager/conf.d/calico.conf&lt;&lt;EOF\n[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali\nEOF\nsystemctl daemon-reload\nsystemctl restart NetworkManager\n</code></pre>\n<p>‰ª•‰∏ãÊ≠•È™§Âè™Âú®<mark> master01</mark> ÊâßË°åÔºà.x ‰∏çÈúÄË¶ÅÊõ¥ÊîπÔºâÔºö</p>\n<pre><code>cd /root/k8s-ha-install &amp;&amp; git checkout manual-installation-v1.32.x &amp;&amp; cd calico/\n</code></pre>\n<p>‰øÆÊîπ Pod ÁΩëÊÆµÔºö</p>\n<pre><code>POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '&#123;print $NF&#125;'`\n\nsed -i &quot;s#POD_CIDR#$&#123;POD_SUBNET&#125;#g&quot; calico.yaml\nkubectl apply -f calico.yaml\n</code></pre>\n<p>Êü•ÁúãÂÆπÂô®ÂíåËäÇÁÇπÁä∂ÊÄÅÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -n kube-system\nNAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h\ncalico-node-7mzmb                          1/1     Running   0          24h\ncalico-node-ljqnl                          1/1     Running   0          24h\ncalico-node-njqlb                          1/1     Running   0          24h\ncalico-node-ph4m4                          1/1     Running   0          24h\ncalico-node-rx8rl                          1/1     Running   0          24h\ncoredns-76fccbbb6b-76559                   1/1     Running   0          24h\ncoredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h\netcd-k8s-master01                          1/1     Running   0          24h\netcd-k8s-master02                          1/1     Running   0          24h\netcd-k8s-master03                          1/1     Running   0          24h\nkube-apiserver-k8s-master01                1/1     Running   0          24h\nkube-apiserver-k8s-master02                1/1     Running   0          24h\nkube-apiserver-k8s-master03                1/1     Running   0          24h\nkube-controller-manager-k8s-master01       1/1     Running   0          24h\nkube-controller-manager-k8s-master02       1/1     Running   0          24h\nkube-controller-manager-k8s-master03       1/1     Running   0          24h\nkube-proxy-9dtz4                           1/1     Running   0          24h\nkube-proxy-jh7rl                           1/1     Running   0          24h\nkube-proxy-jvvwt                           1/1     Running   0          24h\nkube-proxy-sh89l                           1/1     Running   0          24h\nkube-proxy-t2j49                           1/1     Running   0          24h\nkube-scheduler-k8s-master01                1/1     Running   0          24h\nkube-scheduler-k8s-master02                1/1     Running   0          24h\nkube-scheduler-k8s-master03                1/1     Running   0          24h\nmetrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h\n</code></pre>\n<p>Ê≠§Êó∂ËäÇÁÇπÂÖ®ÈÉ®Âèò‰∏∫ Ready Áä∂ÊÄÅÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get nodes\nNAME           STATUS   ROLES           AGE   VERSION\nk8s-master01   Ready    control-plane   24h   v1.32.3\nk8s-master02   Ready    control-plane   24h   v1.32.3\nk8s-master03   Ready    control-plane   24h   v1.32.3\nk8s-node01     Ready    &lt;none&gt;          24h   v1.32.3\nk8s-node02     Ready    &lt;none&gt;          24h   v1.32.3\n</code></pre>\n<h4 id=\"8-metricsÈÉ®ÁΩ≤\"><a class=\"anchor\" href=\"#8-metricsÈÉ®ÁΩ≤\">#</a> 8. Metrics ÈÉ®ÁΩ≤</h4>\n<p>Âú®Êñ∞ÁâàÁöÑ Kubernetes ‰∏≠Á≥ªÁªüËµÑÊ∫êÁöÑÈááÈõÜÂùá‰ΩøÁî® Metrics-serverÔºåÂèØ‰ª•ÈÄöËøá Metrics ÈááÈõÜËäÇÁÇπÂíå Pod ÁöÑÂÜÖÂ≠ò„ÄÅÁ£ÅÁõò„ÄÅCPU ÂíåÁΩëÁªúÁöÑ‰ΩøÁî®Áéá„ÄÇ</p>\n<p>Â∞Ü<mark> Master01 ËäÇÁÇπ</mark>ÁöÑ front-proxy-ca.crt Â§çÂà∂Âà∞ÊâÄÊúâ Node ËäÇÁÇπ</p>\n<pre><code>scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt\n\nscp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(ÂÖ∂‰ªñËäÇÁÇπËá™Ë°åÊã∑Ë¥ù):/etc/kubernetes/pki/front-proxy-ca.crt\n</code></pre>\n<p>‰ª•‰∏ãÊìç‰ΩúÂùáÂú®<mark> master01 ËäÇÁÇπ</mark>ÊâßË°å:</p>\n<p>ÂÆâË£Ö metrics server</p>\n<pre><code>cd /root/k8s-ha-install/kubeadm-metrics-server\n\n# kubectl  create -f comp.yaml \nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre>\n<p>Êü•ÁúãÁä∂ÊÄÅÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server\nNAME                              READY   STATUS    RESTARTS   AGE\nmetrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h\n</code></pre>\n<p>Á≠â Pod ÂèòÊàê 1/1   Running ÂêéÔºåÊü•ÁúãËäÇÁÇπÂíå Pod ËµÑÊ∫ê‰ΩøÁî®ÁéáÔºö</p>\n<pre><code>[root@k8s-master01 ~]#  kubectl top node\nNAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \nk8s-master01   132m         3%       932Mi           5%          \nk8s-master02   131m         3%       845Mi           5%          \nk8s-master03   148m         3%       912Mi           5%          \nk8s-node01     54m          1%       600Mi           3%          \nk8s-node02     49m          1%       602Mi           3%          \n[root@k8s-master01 ~]#  kubectl top po -A\nNAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   \ningress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            \ningress-nginx          ingress-nginx-controller-r978m               1m           104Mi           \nkrm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            \nkrm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             \nkrm                    nginx-574cf48959-vcfjs                       0m           2Mi             \nkube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            \nkube-system            calico-node-7mzmb                            16m          176Mi           \nkube-system            calico-node-ljqnl                            15m          182Mi           \nkube-system            calico-node-njqlb                            19m          180Mi           \nkube-system            calico-node-ph4m4                            15m          178Mi           \nkube-system            calico-node-rx8rl                            17m          180Mi           \nkube-system            coredns-76fccbbb6b-76559                     2m           16Mi            \nkube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            \nkube-system            etcd-k8s-master01                            22m          86Mi            \nkube-system            etcd-k8s-master02                            27m          84Mi            \nkube-system            etcd-k8s-master03                            22m          84Mi            \nkube-system            kube-apiserver-k8s-master01                  22m          267Mi           \nkube-system            kube-apiserver-k8s-master02                  20m          242Mi           \nkube-system            kube-apiserver-k8s-master03                  18m          241Mi           \nkube-system            kube-controller-manager-k8s-master01         6m           69Mi            \nkube-system            kube-controller-manager-k8s-master02         2m           21Mi            \nkube-system            kube-controller-manager-k8s-master03         1m           19Mi            \nkube-system            kube-proxy-9dtz4                             11m          30Mi            \nkube-system            kube-proxy-jh7rl                             1m           27Mi            \nkube-system            kube-proxy-jvvwt                             17m          29Mi            \nkube-system            kube-proxy-sh89l                             1m           29Mi            \nkube-system            kube-proxy-t2j49                             16m          29Mi            \nkube-system            kube-scheduler-k8s-master01                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master02                  6m           25Mi            \nkube-system            kube-scheduler-k8s-master03                  6m           25Mi            \nkube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            \nkubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            \nkubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  \n</code></pre>\n<h4 id=\"9-dashboardÈÉ®ÁΩ≤\"><a class=\"anchor\" href=\"#9-dashboardÈÉ®ÁΩ≤\">#</a> 9. Dashboard ÈÉ®ÁΩ≤</h4>\n<h5 id=\"91-ÂÆâË£Ödashboard\"><a class=\"anchor\" href=\"#91-ÂÆâË£Ödashboard\">#</a> 9.1 ÂÆâË£Ö Dashboard</h5>\n<p>Dashboard Áî®‰∫éÂ±ïÁ§∫ÈõÜÁæ§‰∏≠ÁöÑÂêÑÁ±ªËµÑÊ∫êÔºåÂêåÊó∂‰πüÂèØ‰ª•ÈÄöËøá Dashboard ÂÆûÊó∂Êü•Áúã Pod ÁöÑÊó•ÂøóÂíåÂú®ÂÆπÂô®‰∏≠ÊâßË°å‰∏Ä‰∫õÂëΩ‰ª§Á≠â„ÄÇ</p>\n<pre><code>cd /root/k8s-ha-install/dashboard/\n\n[root@k8s-master01 dashboard]# kubectl  create -f .\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n</code></pre>\n<h5 id=\"92-ÁôªÂΩïdashboard\"><a class=\"anchor\" href=\"#92-ÁôªÂΩïdashboard\">#</a> 9.2 ÁôªÂΩï dashboard</h5>\n<p>Âú®Ë∞∑Ê≠åÊµèËßàÂô®ÔºàChromeÔºâÂêØÂä®Êñá‰ª∂‰∏≠Âä†ÂÖ•ÂêØÂä®ÂèÇÊï∞ÔºåÁî®‰∫éËß£ÂÜ≥Êó†Ê≥ïËÆøÈóÆ Dashboard ÁöÑÈóÆÈ¢òÔºåÂèÇËÄÉ‰∏ãÂõæÔºö</p>\n<pre><code>--test-type --ignore-certificate-errors\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgWfHJ\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgWfHJ.png\" alt=\"pEgWfHJ.png\" /></a></p>\n<p>Êõ¥Êîπ dashboard ÁöÑ svc ‰∏∫ NodePort:</p>\n<pre><code>kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard\n</code></pre>\n<p><a href=\"https://imgse.com/i/pEgW5NR\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW5NR.png\" alt=\"pEgW5NR.png\" /></a></p>\n<p><em>Â∞Ü ClusterIP Êõ¥Êîπ‰∏∫ NodePortÔºàÂ¶ÇÊûúÂ∑≤Áªè‰∏∫ NodePort ÂøΩÁï•Ê≠§Ê≠•È™§Ôºâ</em></p>\n<p>Êü•ÁúãÁ´ØÂè£Âè∑Ôºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.96.139.11   &lt;none&gt;        443:32409/TCP   24h\n</code></pre>\n<p>Ê†πÊçÆËá™Â∑±ÁöÑÂÆû‰æãÁ´ØÂè£Âè∑ÔºåÈÄöËøá‰ªªÊÑèÂÆâË£Ö‰∫Ü kube-proxy ÁöÑÂÆø‰∏ªÊú∫ÁöÑ IP + Á´ØÂè£Âç≥ÂèØËÆøÈóÆÂà∞ dashboardÔºö</p>\n<p>ËÆøÈóÆ DashboardÔºö<a href=\"https://192.168.181.129:31106\">https://192.168.1.71:32409</a> ÔºàÊää IP Âú∞ÂùÄÂíåÁ´ØÂè£ÊîπÊàê‰Ω†Ëá™Â∑±ÁöÑÔºâÈÄâÊã©ÁôªÂΩïÊñπÂºè‰∏∫‰ª§ÁâåÔºàÂç≥ token ÊñπÂºèÔºâÔºåÂèÇËÄÉ‰∏ãÂõæÔºö</p>\n<p><a href=\"https://imgse.com/i/pEgW736\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgW736.png\" alt=\"pEgW736.png\" /></a></p>\n<p>ÂàõÂª∫ÁôªÂΩï TokenÔºö</p>\n<pre><code>kubectl create token admin-user -n kube-system\n</code></pre>\n<p>Â∞Ü token ÂÄºËæìÂÖ•Âà∞‰ª§ÁâåÂêéÔºåÂçïÂáªÁôªÂΩïÂç≥ÂèØËÆøÈóÆ DashboardÔºåÂèÇËÄÉ‰∏ãÂõæÔºö</p>\n<p><a href=\"https://imgse.com/i/pEgfPv8\"><img loading=\"lazy\" data-src=\"https://s21.ax1x.com/2025/04/09/pEgfPv8.png\" alt=\"pEgfPv8.png\" /></a></p>\n<h4 id=\"10ÂøÖÁúã‰∏Ä‰∫õÂøÖÈ°ªÁöÑÈÖçÁΩÆÊõ¥Êîπ\"><a class=\"anchor\" href=\"#10ÂøÖÁúã‰∏Ä‰∫õÂøÖÈ°ªÁöÑÈÖçÁΩÆÊõ¥Êîπ\">#</a> 10.„ÄêÂøÖÁúã„Äë‰∏Ä‰∫õÂøÖÈ°ªÁöÑÈÖçÁΩÆÊõ¥Êîπ</h4>\n<p>Â∞Ü Kube-proxy Êîπ‰∏∫ ipvs Ê®°ÂºèÔºåÂõ†‰∏∫Âú®ÂàùÂßãÂåñÈõÜÁæ§ÁöÑÊó∂ÂÄôÊ≥®Èáä‰∫Ü ipvs ÈÖçÁΩÆÔºåÊâÄ‰ª•ÈúÄË¶ÅËá™Ë°å‰øÆÊîπ‰∏Ä‰∏ãÔºö</p>\n<p>Âú® master01 ËäÇÁÇπÊâßË°åÔºö</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\nmode: ipvs\n</code></pre>\n<p>Êõ¥Êñ∞ Kube-Proxy ÁöÑ PodÔºö</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>È™åËØÅ Kube-Proxy Ê®°Âºè:</p>\n<pre><code>[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode\nipvs\n</code></pre>\n<h4 id=\"11ÂøÖÁúãÊ≥®ÊÑè‰∫ãÈ°π\"><a class=\"anchor\" href=\"#11ÂøÖÁúãÊ≥®ÊÑè‰∫ãÈ°π\">#</a> 11.„ÄêÂøÖÁúã„ÄëÊ≥®ÊÑè‰∫ãÈ°π</h4>\n<p>Ê≥®ÊÑèÔºökubeadm ÂÆâË£ÖÁöÑÈõÜÁæ§ÔºåËØÅ‰π¶ÊúâÊïàÊúüÈªòËÆ§ÊòØ‰∏ÄÂπ¥„ÄÇmaster ËäÇÁÇπÁöÑ kube-apiserver„ÄÅkube-scheduler„ÄÅkube-controller-manager„ÄÅetcd ÈÉΩÊòØ‰ª•ÂÆπÂô®ËøêË°åÁöÑ„ÄÇÂèØ‰ª•ÈÄöËøá kubectl get po -n kube-system Êü•Áúã„ÄÇ</p>\n<p>ÂêØÂä®Âíå‰∫åËøõÂà∂‰∏çÂêåÁöÑÊòØÔºåkubelet ÁöÑÈÖçÁΩÆÊñá‰ª∂Âú® /etc/sysconfig/kubelet Âíå /var/lib/kubelet/config.yamlÔºå‰øÆÊîπÂêéÈúÄË¶ÅÈáçÂêØ kubelet ËøõÁ®ã„ÄÇ</p>\n<p>ÂÖ∂‰ªñÁªÑ‰ª∂ÁöÑÈÖçÁΩÆÊñá‰ª∂Âú® /etc/kubernetes/manifests ÁõÆÂΩï‰∏ãÔºåÊØîÂ¶Ç kube-apiserver.yamlÔºåËØ• yaml Êñá‰ª∂Êõ¥ÊîπÂêéÔºåkubelet ‰ºöËá™Âä®Âà∑Êñ∞ÈÖçÁΩÆÔºå‰πüÂ∞±ÊòØ‰ºöÈáçÂêØ pod„ÄÇ‰∏çËÉΩÂÜçÊ¨°ÂàõÂª∫ËØ•Êñá‰ª∂„ÄÇ</p>\n<p>kube-proxy ÁöÑÈÖçÁΩÆÂú® kube-system ÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑ configmap ‰∏≠ÔºåÂèØ‰ª•ÈÄöËøá</p>\n<pre><code>kubectl edit cm kube-proxy -n kube-system\n</code></pre>\n<p>ËøõË°åÊõ¥ÊîπÔºåÊõ¥ÊîπÂÆåÊàêÂêéÔºåÂèØ‰ª•ÈÄöËøá patch ÈáçÂêØ kube-proxy</p>\n<pre><code>kubectl patch daemonset kube-proxy -p &quot;&#123;\\&quot;spec\\&quot;:&#123;\\&quot;template\\&quot;:&#123;\\&quot;metadata\\&quot;:&#123;\\&quot;annotations\\&quot;:&#123;\\&quot;date\\&quot;:\\&quot;`date +'%s'`\\&quot;&#125;&#125;&#125;&#125;&#125;&quot; -n kube-system\n</code></pre>\n<p>Kubeadm ÂÆâË£ÖÂêéÔºåmaster ËäÇÁÇπÈªòËÆ§‰∏çÂÖÅËÆ∏ÈÉ®ÁΩ≤ podÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÊñπÂºèÂà†Èô§ TaintÔºåÂç≥ÂèØÈÉ®ÁΩ≤ PodÔºö</p>\n<pre><code>[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-\n</code></pre>\n<h4 id=\"12-containerdÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\"><a class=\"anchor\" href=\"#12-containerdÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\">#</a> 12. Containerd ÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü</h4>\n<pre><code># vim /etc/containerd/config.toml\n#Ê∑ªÂä†‰ª•‰∏ãÈÖçÁΩÆÈïúÂÉèÂä†ÈÄüÊúçÂä°\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://registry-1.docker.io&quot;, &quot;https://hbv0b596.mirror.aliyuncs.com&quot;]\n       [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;]\n        endpoint=[&quot;https://dockerproxy.com&quot;, &quot;https://mirror.baidubce.com&quot;,&quot;https://ccr.ccs.tencentyun.com&quot;,&quot;https://docker.m.daocloud.io&quot;,&quot;https://docker.nju.edu.cn&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hbv0b596.mirror.aliyuncs.com&quot;, &quot;https://k8s.m.daocloud.io&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;]\n</code></pre>\n<p>ÊâÄÊúâËäÇÁÇπÈáçÊñ∞ÂêØÂä® ContainerdÔºö</p>\n<pre><code># systemctl daemon-reload\n# systemctl restart containerd\n</code></pre>\n<h4 id=\"13-dockerÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\"><a class=\"anchor\" href=\"#13-dockerÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü\">#</a> 13. Docker ÈÖçÁΩÆÈïúÂÉèÂä†ÈÄü</h4>\n<pre><code># sudo mkdir -p /etc/docker\n# sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'\n&#123;\n  &quot;registry-mirrors&quot;: [\n\t  &quot;https://docker.credclouds.com&quot;,\n\t  &quot;https://k8s.credclouds.com&quot;,\n\t  &quot;https://quay.credclouds.com&quot;,\n\t  &quot;https://gcr.credclouds.com&quot;,\n\t  &quot;https://k8s-gcr.credclouds.com&quot;,\n\t  &quot;https://ghcr.credclouds.com&quot;,\n\t  &quot;https://do.nark.eu.org&quot;,\n\t  &quot;https://docker.m.daocloud.io&quot;,\n\t  &quot;https://docker.nju.edu.cn&quot;,\n\t  &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,\n\t  &quot;https://docker.1panel.live&quot;,\n\t  &quot;https://docker.rainbond.cc&quot;\n  ], \n  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;] \n&#125;\nEOF\n</code></pre>\n<p>ÊâÄÊúâËäÇÁÇπÈáçÊñ∞ÂêØÂä® DockerÔºö</p>\n<pre><code># systemctl daemon-reload\n# systemctl enable --now docker\n</code></pre>\n<p><em>Êú¨ÊñáÂá∫Ëá™‰∫éÔºö<a href=\"https://edu.51cto.com/course/23845.html\">https://edu.51cto.com/course/23845.html</a></em></p>\n",
            "tags": [
                "Kubernetes"
            ]
        }
    ]
}