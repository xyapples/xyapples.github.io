<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://ixuyong.cn</id>
    <title>LinuxSreäº‘åŸç”Ÿ â€¢ Posts by &#34;kubernetes&#34; tag</title>
    <link href="http://ixuyong.cn" />
    <updated>2025-05-18T13:42:46.000Z</updated>
    <category term="Docker" />
    <category term="Harbor" />
    <category term="ELKStack" />
    <category term="rsync" />
    <category term="MySQL" />
    <category term="Kubernetes" />
    <category term="Redis" />
    <category term="Windows" />
    <entry>
        <id>http://ixuyong.cn/posts/626047790.html</id>
        <title>æ¶ˆè´¹ç§Ÿèµç³»ç»Ÿå¾®æœåŠ¡åº”ç”¨äº¤ä»˜å®è·µ</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/626047790.html"/>
        <content type="html">&lt;h3 id=&#34;æ¶ˆè´¹ç§Ÿèµç³»ç»Ÿå¾®æœåŠ¡åº”ç”¨äº¤ä»˜å®è·µ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#æ¶ˆè´¹ç§Ÿèµç³»ç»Ÿå¾®æœåŠ¡åº”ç”¨äº¤ä»˜å®è·µ&#34;&gt;#&lt;/a&gt; æ¶ˆè´¹ç§Ÿèµç³»ç»Ÿå¾®æœåŠ¡åº”ç”¨äº¤ä»˜å®è·µ&lt;/h3&gt;
&lt;h4 id=&#34;ä¸€-éƒ¨ç½²ä¸­é—´ä»¶&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ä¸€-éƒ¨ç½²ä¸­é—´ä»¶&#34;&gt;#&lt;/a&gt; ä¸€ã€éƒ¨ç½²ä¸­é—´ä»¶&lt;/h4&gt;
&lt;h5 id=&#34;11-éƒ¨ç½²mysql&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-éƒ¨ç½²mysql&#34;&gt;#&lt;/a&gt; 1.1 éƒ¨ç½² MySQL&lt;/h5&gt;
&lt;h6 id=&#34;111-mysql-configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#111-mysql-configmap&#34;&gt;#&lt;/a&gt; 1.1.1 MySQL-ConfigMap&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 01-nf-flms-mysql]# cat 01-mysql-cm.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-cm
  namespace: prod
data:
  my.cnf: |-
    [mysqld]
    #performance setttings
    lock_wait_timeout = 3600
    open_files_limit = 65535
    back_log = 1024
    max_connections = 1024
    max_connect_errors = 1000000
    table_open_cache = 1024
    table_definition_cache = 1024
    thread_stack = 512K
    sort_buffer_size = 4M
    join_buffer_size = 4M
    read_buffer_size = 8M
    read_rnd_buffer_size = 4M
    bulk_insert_buffer_size = 64M
    thread_cache_size = 768
    interactive_timeout = 600
    wait_timeout = 600
    tmp_table_size = 32M
    max_heap_table_size = 32M
    max_allowed_packet = 128M
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;112-mysql-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#112-mysql-secret&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.1.2 MySQL-Secret&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 01-nf-flms-mysql]# cat 02-mysql-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
  namespace: prod
stringData:
  MYSQL_ROOT_PASSWORD: Superman*2023
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;113-mysql-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#113-mysql-statefulset&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.1.3 MySQL-StatefulSet&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 03-mysql-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-nf-flms
  namespace: prod
spec:
  serviceName: &amp;quot;mysql-nf-flms-svc&amp;quot;
  replicas: 1
  selector:
    matchLabels:
      app: mysql
      role: nf-flms
  template:
    metadata:
      labels:
        app: mysql
        role: nf-flms
    spec:
      containers:
      - name: db
        image: mysql:8.0
        args:
        - &amp;quot;--character-set-server=utf8&amp;quot;
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_ROOT_PASSWORD
        - name: MYSQL_DATABASE      #æ•°æ®åº“åç§°
          value: nf-flms        
        ports:
        - name: tcp-3306
          containerPort: 3306
          protocol: TCP
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        resources:
          limits:
            cpu: 2000m
            memory: 4000Mi
          requests:
            cpu: 200m
            memory: 500Mi
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql/
        - name: config
          mountPath: /etc/mysql/conf.d/my.cnf
          subPath: my.cnf
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: config
        configMap:
          name: mysql-cm
          items:
            - key: my.cnf
              path: my.cnf
          defaultMode: 420
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      storageClassName: &amp;quot;nfs-storage&amp;quot;
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      resources:
        requests:
          storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;114-mysql-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#114-mysql-service&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.1.4 MySQL Service&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 04-mysql-nf-flms-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-nf-flms-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: mysql
    role: nf-flms
  ports:
  - name: tcp-mysql-svc
    protocol: TCP
    port: 3306
    targetPort: 3306
---
kind: Service
apiVersion: v1
metadata:
  name: mysql-nf-flms-svc-balance
  namespace: prod 
spec:
  selector:
    app: mysql
    role: nf-flms
  ports:
  - name: tcp-mysql-balance
    protocol: TCP
    port: 3306
    targetPort: 3306
    nodePort: 32206
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;115-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#115-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.1.5 æ›´æ–°èµ„æºæ¸…å•&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 01-nf-flms-mysql]# sed -i &amp;quot;s#dev#prod#g&amp;quot; *.yaml
[root@k8s-master01 01-nf-flms-mysql]# kubectl create ns prod
[root@k8s-master01 01-nf-flms-mysql]# kubectl apply -f .
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;116-å¯¼å…¥æ•°æ®åº“&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#116-å¯¼å…¥æ•°æ®åº“&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.1.6 å¯¼å…¥æ•°æ®åº“&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 03-nacos]# dig @10.96.0.10 mysql-nf-flms-0.mysql-nf-flms-svc.prod.svc.cluster.local +short
172.16.85.213
[root@k8s-master01 01-nf-flms-mysql]# mysql -h 172.16.85.213 -uroot -p&amp;quot;Superman*2023&amp;quot; -B nf_flms &amp;lt; /root/backup/qzj_db01_192.168.1.143_2023-03-30/nf-flms_mysql_backup_20230330030001.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-éƒ¨ç½²redis-single&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-éƒ¨ç½²redis-single&#34;&gt;#&lt;/a&gt; 1.2 éƒ¨ç½² Redis-single&lt;/h5&gt;
&lt;h6 id=&#34;121-redis-configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#121-redis-configmap&#34;&gt;#&lt;/a&gt; 1.2.1 Redis-ConfigMap&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 01-redis-cm.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-conf
  namespace: prod
data:
  redis.conf: |
    bind 0.0.0.0
    appendonly yes
    protected-mode no
    dir /data
    port 6379
    requirepass Superman*2023
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;122-redis-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#122-redis-statefulset&#34;&gt;#&lt;/a&gt; 1.2.2 Redis-StatefulSet&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 02-redis-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: prod
spec:
  serviceName: redis-svc
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:6.2.7
        command:
        - &amp;quot;redis-server&amp;quot;
        args:
        - &amp;quot;/etc/redis/redis.conf&amp;quot;
        ports:
        - name: redis-6379
          containerPort: 6379
          protocol: TCP
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 6379
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 6379
        volumeMounts:
        - name: config
          mountPath: /etc/redis
        - name: data
          mountPath: /data
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        resources:
          limits:
            cpu: &#39;2&#39;
            memory: 4000Mi
          requests:
            cpu: 100m
            memory: 500Mi
      volumes:
      - name: config
        configMap:
          name: redis-conf
          items:
          - key: redis.conf
            path: redis.conf
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: &amp;quot;nfs-storage&amp;quot;
      resources:
        requests:
          storage: 2Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;123-redis-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#123-redis-service&#34;&gt;#&lt;/a&gt; 1.2.3 Redis-Service&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 03-redis-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: redis-svc
  namespace: prod
  labels:
    app: redis
spec:
  ports:
    - name: redis-6379
      protocol: TCP
      port: 6379
      targetPort: 6379
  selector:
    app: redis
  clusterIP: None
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;124-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#124-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;#&lt;/a&gt; 1.2.4 æ›´æ–°èµ„æºæ¸…å•&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 02-redis]# sed -i &amp;quot;s#dev#prod#g&amp;quot; *.yaml
[root@k8s-master01 02-redis]# kubectl apply -f .
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-éƒ¨ç½²nacosé›†ç¾¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-éƒ¨ç½²nacosé›†ç¾¤&#34;&gt;#&lt;/a&gt; 1.4 éƒ¨ç½² Nacos é›†ç¾¤&lt;/h5&gt;
&lt;h6 id=&#34;141-éƒ¨ç½²nacos-mysql&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#141-éƒ¨ç½²nacos-mysql&#34;&gt;#&lt;/a&gt; 1.4.1 éƒ¨ç½² Nacos-MySQL&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 01-mysql-nacos-sts-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-nacos-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: mysql
    role: nacos
  ports:
  - port: 3306
    targetPort: 3306
---
kind: Service
apiVersion: v1
metadata:
  name: mysql-nacos-balance
  namespace: prod
spec:
  selector:
    app: mysql
    role: nacos
  ports:
  - name: tcp-mysql-balance
    protocol: TCP
    port: 3306
    targetPort: 3306
    nodePort: 31106
  type: NodePort
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-nacos
  namespace: prod
spec:
  serviceName: &amp;quot;mysql-nacos-svc&amp;quot;
  replicas: 1
  selector:
    matchLabels:
      app: mysql
      role: nacos
  template:
    metadata:
      labels:
        app: mysql
        role: nacos
    spec:
      containers:
      - name: db
        image: mysql:8.0
        args:
        - &amp;quot;--character-set-server=utf8&amp;quot;
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: Superman*2023
        - name: MYSQL_DATABASE    #nacosåº“åç§°
          value: nacos
        ports:
        - containerPort: 3306
        resources:
          limits:
            cpu: &#39;2&#39;
            memory: 4000Mi
          requests:
            cpu: 100m
            memory: 500Mi
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql/
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&amp;quot;ReadWriteMany&amp;quot;]
      storageClassName: &amp;quot;nfs-storage&amp;quot;
      resources:
        requests:
          storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;142-å¯¼å…¥æ•°æ®åº“&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#142-å¯¼å…¥æ•°æ®åº“&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.4.2 å¯¼å…¥æ•°æ®åº“&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;nacos ä¸‹è½½åœ°å€ï¼š&lt;a href=&#34;https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK&#34;&gt;https://nacos.io/download/release-history/?spm=5238cd80.7a4232a8.0.0.f834e7559caaaK&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 03-nacos]#  sed -i &amp;quot;s#dev#prod#g&amp;quot; *.yaml
[root@k8s-master01 03-nacos]# kubectl apply -f 01-mysql-nacos-sts-svc.yaml
[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-nacos-svc.prod.svc.cluster.local +short
172.16.85.255
[root@k8s-master01 03-nacos]# mysql -h 172.16.85.255  -uroot -p&amp;quot;Superman*2023&amp;quot; -B nacos &amp;lt; nacos/conf/mysql-schema.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;143-éƒ¨ç½²nacos-configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#143-éƒ¨ç½²nacos-configmap&#34;&gt;#&lt;/a&gt; 1.4.3 éƒ¨ç½² Nacos-ConfigMap&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 02-nacos-configmap.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: nacos-cm
  namespace: prod
data:
  mysql.host: &amp;quot;mysql-nacos-svc.prod.svc.cluster.local&amp;quot;
  mysql.db.name: &amp;quot;nacos&amp;quot;   #nacosæ•°æ®åº“åç§°
  mysql.port: &amp;quot;3306&amp;quot;
  mysql.user: &amp;quot;root&amp;quot;    #nacosæ•°æ®åº“ç”¨æˆ·å
  mysql.password: &amp;quot;Superman*2023&amp;quot;   #nacosæ•°æ®åº“å¯†ç 
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;144-éƒ¨ç½²nacos-service-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#144-éƒ¨ç½²nacos-service-statefulset&#34;&gt;#&lt;/a&gt; 1.4.4 éƒ¨ç½² Nacos-Service-StatefulSet&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 03-nacos-sts-deploy-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nacos-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: nacos
  ports:
  - name: server
    port: 8848
    targetPort: 8848
  - name: client-rpc
    port: 9848
    targetPort: 9848
  - name: raft-rpc
    port: 9849
    targetPort: 9849
  - name: old-raft-rpc
    port: 7848
    targetPort: 7848
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nacos
  namespace: prod
spec:
  serviceName: &amp;quot;nacos-svc&amp;quot;
  replicas: 3
  selector:
    matchLabels:
      app: nacos
  template:
    metadata:
      labels:
        app: nacos
    spec:
      affinity:                                                 # é¿å…Podè¿è¡Œåˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šäº†
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values: [&amp;quot;nacos&amp;quot;]
              topologyKey: &amp;quot;kubernetes.io/hostname&amp;quot;  
      initContainers:
      - name: peer-finder-plugin-install
        image: nacos/nacos-peer-finder-plugin:1.1
        imagePullPolicy: IfNotPresent 
        volumeMounts:
          - name: data
            mountPath: /home/nacos/plugins/peer-finder
            subPath: peer-finder
      containers:
      - name: nacos
        image: nacos/nacos-server:v2.4.3
        resources:
          limits:
            cpu: &#39;2&#39;
            memory: 4Gi
          requests:
            cpu: &amp;quot;100m&amp;quot;
            memory: &amp;quot;1Gi&amp;quot;
        ports:
        - name: client-port
          containerPort: 8848
        - name: client-rpc
          containerPort: 9848
        - name: raft-rpc
          containerPort: 9849
        - name: old-raft-rpc
          containerPort: 7848
        env:
        - name: NACOS_REPLICAS
          value: &amp;quot;3&amp;quot;
        - name: SERVICE_NAME
          value: &amp;quot;nacos-svc&amp;quot;
        - name: DOMAIN_NAME
          value: &amp;quot;cluster.local&amp;quot;
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: MYSQL_SERVICE_HOST
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.host
        - name: MYSQL_SERVICE_DB_NAME
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.db.name
        - name: MYSQL_SERVICE_PORT
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.port
        - name: MYSQL_SERVICE_USER
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.user
        - name: MYSQL_SERVICE_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: nacos-cm
              key: mysql.password
        - name: SPRING_DATASOURCE_PLATFORM
          value: &amp;quot;mysql&amp;quot;
        - name: NACOS_SERVER_PORT
          value: &amp;quot;8848&amp;quot;
        - name: NACOS_APPLICATION_PORT
          value: &amp;quot;8848&amp;quot;
        - name: PREFER_HOST_MODE
          value: &amp;quot;hostname&amp;quot;
        - name: NACOS_AUTH_ENABLE
          value: &amp;quot;true&amp;quot;
        - name: NACOS_AUTH_IDENTITY_KEY
          value: &amp;quot;nacosAuthKey&amp;quot;
        - name: NACOS_AUTH_IDENTITY_VALUE
          value: &amp;quot;nacosSecurtyValue&amp;quot;
        - name: NACOS_AUTH_TOKEN
          value: &amp;quot;SecretKey012345678901234567890123456789012345678901234567890123456789&amp;quot;
        - name: NACOS_AUTH_TOKEN_EXPIRE_SECONDS
          value: &amp;quot;18000&amp;quot;
        volumeMounts:
        - name: data
          mountPath: /home/nacos/plugins/peer-finder
          subPath: peer-finder
        - name: data
          mountPath: /home/nacos/data
          subPath: data
        - name: data
          mountPath: /home/nacos/logs
          subPath: logs
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        storageClassName: &amp;quot;nfs-storage&amp;quot;
        accessModes: [&amp;quot;ReadWriteMany&amp;quot;]
        resources:
          requests:
            storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;145-éƒ¨ç½²nacos-ingress&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#145-éƒ¨ç½²nacos-ingress&#34;&gt;#&lt;/a&gt; 1.4.5 éƒ¨ç½² Nacos-Ingress&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 04-nacos-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nacos-ingress
  namespace: prod
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: nacos.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nacos-svc
            port:
              number: 8848
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;146-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#146-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.4.6 æ›´æ–°èµ„æºæ¸…å•&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 03-nacos]# sed -i &amp;quot;s#dev#prod#g&amp;quot; *.yaml
[root@k8s-master01 03-nacos]# kubectl apply -f .

#æ£€æŸ¥clusteræ˜¯å¦ä¸€è‡´
[root@k8s-master01 03-nacos]# for i in &amp;#123;0..2&amp;#125;; do echo nacos-$i; kubectl exec nacos-$i -c nacos -n prod -- cat conf/cluster.conf; donenacos-0
#2025-05-21T10:43:12.668
nacos-0.nacos-svc.prod.svc.cluster.local:8848
nacos-1.nacos-svc.prod.svc.cluster.local:8848
nacos-2.nacos-svc.prod.svc.cluster.local:8848
nacos-1
#2025-05-21T10:43:14.879
nacos-0.nacos-svc.prod.svc.cluster.local:8848
nacos-1.nacos-svc.prod.svc.cluster.local:8848
nacos-2.nacos-svc.prod.svc.cluster.local:8848
nacos-2
#2025-05-21T10:43:17.299
nacos-0.nacos-svc.prod.svc.cluster.local:8848
nacos-1.nacos-svc.prod.svc.cluster.local:8848
nacos-2.nacos-svc.prod.svc.cluster.local:8848
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;147-webè®¿é—®nacos&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#147-webè®¿é—®nacos&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.4.7 Web è®¿é—® nacos&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;Urlï¼šhttp://nacos.hmallleasing.com/nacos 
User: nacos
Passwd: nacos 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/iNiUErY.jpeg&#34; alt=&#34;Snipaste_2025-05-18_21-13-44.jpg&#34; /&gt;&lt;/p&gt;
&lt;h5 id=&#34;15-éƒ¨ç½²xxl-job&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-éƒ¨ç½²xxl-job&#34;&gt;#&lt;/a&gt; 1.5 éƒ¨ç½² xxl-job&lt;/h5&gt;
&lt;h6 id=&#34;151-éƒ¨ç½²xxl-job-mysql&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#151-éƒ¨ç½²xxl-job-mysql&#34;&gt;#&lt;/a&gt; 1.5.1 éƒ¨ç½² xxl-job-MySQL&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 01-mysql-xxljob-sts-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-xxljob-svc
  namespace: prod
spec:
  clusterIP: None
  selector:
    app: mysql
    role: xxljob
  ports:
    - name: tcp-mysql-svc
      protocol: TCP
      port: 3306
      targetPort: 3306
---
apiVersion: v1
kind: Service
apiVersion: v1
metadata:
  name: mysql-xxljob-external
  namespace: prod
spec:
  ports:
    - name: tcp-mysql-external
      protocol: TCP
      port: 3306
      targetPort: 3306
      nodePort: 31206
  selector:
    app: mysql
    role: xxljob
  type: NodePort
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-xxljob
  namespace: prod
spec:
  serviceName: &amp;quot;mysql-xxljob-svc&amp;quot;
  replicas: 1
  selector:
    matchLabels:
      app: mysql
      role: xxljob
  template:
    metadata:
      labels:
        app: mysql
        role: xxljob
    spec:
      containers:
      - name: db
        image: mysql:8.0
        args:
        - &amp;quot;--character-set-server=utf8&amp;quot;
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: Superman*2023
        ports:
        - containerPort: 3306
        resources:
          limits:
            cpu: 2000m
            memory: 4000Mi
          requests:
            cpu: 200m
            memory: 500Mi
        livenessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          tcpSocket:
            port: 3306
          timeoutSeconds: 2
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql/
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&amp;quot;ReadWriteMany&amp;quot;]
      storageClassName: &amp;quot;nfs-storage&amp;quot;
      resources:
        requests:
          storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;152-å¯¼å…¥æ•°æ®åº“&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#152-å¯¼å…¥æ•°æ®åº“&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.5.2 å¯¼å…¥æ•°æ®åº“&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;xxljob è¡¨ç»“æ„ä¸‹è½½åœ°å€ï¼š&lt;a href=&#34;https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db&#34;&gt;https://gitee.com/xuxueli0323/xxl-job/tree/3.1.0-release/doc/db&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 05-xxl-job]# kubectl apply -f 01-mysql-xxljob-sts-svc.yaml
[root@k8s-master01 05-xxl-job]# dig @10.96.0.10 mysql-xxljob-svc.prod.svc.cluster.local +short
172.16.85.250
[root@k8s-master01 05-xxl-job]# mysql -h 172.16.85.250  -uroot -p&amp;quot;Superman*2023&amp;quot;  &amp;lt; tables_xxl_job.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;153-éƒ¨ç½²xxl-job-service-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#153-éƒ¨ç½²xxl-job-service-deployment&#34;&gt;#&lt;/a&gt; 1.5.3 éƒ¨ç½² xxl-job-Service-Deployment&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 02-xxljob-deploy-svc.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: xxl-job
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: xxl-job
  template:
    metadata:
      labels:
        app: xxl-job
    spec:
      containers:
      - image: xuxueli/xxl-job-admin:3.1.0
        name: xxl-job
        ports:
        - containerPort: 8080
        env:
        - name: PARAMS
          value: &amp;quot;--spring.datasource.url=jdbc:mysql://mysql-xxljob-svc.prod.svc.cluster.local:3306/xxl_job?useUnicode=true&amp;amp;characterEncoding=UTF-8&amp;amp;autoReconnect=true&amp;amp;serverTimezone=Asia/Shanghai --spring.datasource.username=root --spring.datasource.password=Superman*2023&amp;quot;
        volumeMounts:
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
        resources:
          limits:
            cpu: &#39;1&#39;
            memory: 2000Mi
          requests:
            cpu: 100m
            memory: 500Mi
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
---
apiVersion: v1
kind: Service
metadata:
  name: xxljob-svc
  namespace: prod
spec:
  ports:
  - port: 8080
    protocol: TCP
    name: http
  selector:
    app: xxl-job
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;154-éƒ¨ç½²xxl-job-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#154-éƒ¨ç½²xxl-job-service&#34;&gt;#&lt;/a&gt; 1.5.4 éƒ¨ç½² xxl-job-service&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 05-xxl-job]# cat 04-xxljob-external.yaml 
apiVersion: v1
kind: Service
metadata:
  name: xxljob-balancer
  namespace: prod
spec:
  type: NodePort
  ports:
    - name: xxljob-balancer
      protocol: TCP
      port: 8080
      targetPort: 8080
  selector:
    app: xxl-job
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;155-éƒ¨ç½²xxl-job-ingress&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#155-éƒ¨ç½²xxl-job-ingress&#34;&gt;#&lt;/a&gt; 1.5.5 éƒ¨ç½² xxl-job-Ingress&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 05-xxl-job]# cat 03-xxljob-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: xxljob-ingress
  namespace: prod
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: xxljob.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: xxljob-svc
            port:
              number: 8080
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;156-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#156-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;#&lt;/a&gt; 1.5.6 æ›´æ–°èµ„æºæ¸…å•&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 05-xxl-job]# sed -i &amp;quot;s#dev#prod#g&amp;quot; *.yaml
[root@k8s-master01 05-xxl-job]# kubectl apply -f .
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;157-webè®¿é—®xxl-job&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#157-webè®¿é—®xxl-job&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.5.7 Web è®¿é—® xxl-job&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;http://192.168.40.101:30904/xxl-job-admin/
http://xxljob.hmallleasing.com/xxl-job-admin/ 
user:admin    
pwd:1223456
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/A5NbU2Z.jpeg&#34; alt=&#34;Snipaste_2025-05-18_14-54-59.jpg&#34; /&gt;&lt;/p&gt;
&lt;h5 id=&#34;16-éƒ¨ç½²rabbitmqé›†ç¾¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-éƒ¨ç½²rabbitmqé›†ç¾¤&#34;&gt;#&lt;/a&gt; 1.6 éƒ¨ç½² rabbitmq é›†ç¾¤&lt;/h5&gt;
&lt;h6 id=&#34;161-åˆ›å»ºrbacæƒé™&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#161-åˆ›å»ºrbacæƒé™&#34;&gt;#&lt;/a&gt; 1.6.1 åˆ›å»º RBAC æƒé™&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 01-rabbitmq-rbac.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rabbitmq-cluster
  namespace: prod
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rabbitmq-cluster
  namespace: prod
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;endpoints&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rabbitmq-cluster
  namespace: prod
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rabbitmq-cluster
subjects:
- kind: ServiceAccount
  name: rabbitmq-cluster
  namespace: prod
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;162-åˆ›å»ºé›†ç¾¤çš„secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#162-åˆ›å»ºé›†ç¾¤çš„secret&#34;&gt;#&lt;/a&gt; 1.6.2 åˆ›å»ºé›†ç¾¤çš„ Secret&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 02-rabbitmq-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: rabbitmq-secret
  namespace: prod
stringData:
  password: talent
  url: amqp://RABBITMQ_USER:RABBITMQ_PASS@rmq-cluster-balancer
  username: superman
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;163-åˆ›å»ºconfigmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#163-åˆ›å»ºconfigmap&#34;&gt;#&lt;/a&gt; 1.6.3 åˆ›å»º ConfigMap&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 03-rabbitmq-cm.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: rabbitmq-cluster-config
  namespace: prod
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
data:
    enabled_plugins: |
      [rabbitmq_management,rabbitmq_peer_discovery_k8s].
    rabbitmq.conf: |
      loopback_users.guest = false
      default_user = RABBITMQ_USER
      default_pass = RABBITMQ_PASS
      ## Cluster 
      cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s
      cluster_formation.k8s.host = kubernetes.default.svc
      #cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
      cluster_formation.k8s.address_type = hostname
      #################################################
      # prod is rabbitmq-cluster&#39;s namespace#
      #################################################
      cluster_formation.k8s.hostname_suffix = .rabbitmq-cluster.prod.svc.cluster.local
      cluster_formation.node_cleanup.interval = 30
      cluster_formation.node_cleanup.only_log_warning = true
      cluster_partition_handling = autoheal
      ## queue master locator
      queue_master_locator = min-masters
      cluster_formation.randomized_startup_delay_range.min = 0
      cluster_formation.randomized_startup_delay_range.max = 2
      # memory
      vm_memory_high_watermark.absolute = 100MB
      # disk
      disk_free_limit.absolute = 2GB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æ³¨ï¼šé…ç½®æ–‡ä»¶ cluster_formation.k8s.host è®¾ç½®ä¸º kubernetes.default.svc.cluster.localï¼Œç„¶åå°±æ˜¯å„ç§è¿ä¸ä¸Šï¼Œåæ¥æ¢ä¸Š kubernetes.default.svc å°±å¯ä»¥äº†ï¼Œä¸çŸ¥é“æ˜¯ä¸æ˜¯ k8s æ–°ç‰ˆæœ¬çš„é—®é¢˜ã€‚&lt;/em&gt;&lt;/p&gt;
&lt;h6 id=&#34;164-åˆ›å»ºé›†ç¾¤çš„svc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#164-åˆ›å»ºé›†ç¾¤çš„svc&#34;&gt;#&lt;/a&gt; 1.6.4 åˆ›å»ºé›†ç¾¤çš„ svc&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 04-rabbitmq-cluster-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: rabbitmq-cluster
  name: rabbitmq-cluster
  namespace: prod
spec:
  clusterIP: None
  ports:
  - name: rmqport
    port: 5672
    targetPort: 5672
  - name: http
    port: 15672
    protocol: TCP
    targetPort: 15672
  selector:
    app: rabbitmq-cluster
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: rabbitmq-cluster-balancer
  name: rabbitmq-cluster-balancer
  namespace: prod
spec:
  ports:
  - name: rmqport
    port: 5672
    targetPort: 5672
  - name: http
    port: 15672
    protocol: TCP
    targetPort: 15672
  selector:
    app: rabbitmq-cluster
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;165-åˆ›å»ºstatefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#165-åˆ›å»ºstatefulset&#34;&gt;#&lt;/a&gt; 1.6.5 åˆ›å»º StatefulSet&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# cat 05-rabbitmq-cluster-sts.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: rabbitmq-cluster
  name: rabbitmq-cluster
  namespace: prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rabbitmq-cluster
  serviceName: rabbitmq-cluster
  template:
    metadata:
      labels:
        app: rabbitmq-cluster
    spec:
      affinity:                                                 # é¿å…Podè¿è¡Œåˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šäº†
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values: [&amp;quot;rabbitmq-cluster&amp;quot;]
              topologyKey: &amp;quot;kubernetes.io/hostname&amp;quot;  
      containers:
      - args:
        - -c
        - cp -v /etc/rabbitmq/rabbitmq.conf $&amp;#123;RABBITMQ_CONFIG_FILE&amp;#125;; exec docker-entrypoint.sh rabbitmq-server
        command:
        - sh
        env:
        - name: RABBITMQ_DEFAULT_USER
          valueFrom:
            secretKeyRef:
              key: username
              name: rabbitmq-secret
        - name: RABBITMQ_DEFAULT_PASS 
          valueFrom:
            secretKeyRef:
              key: password 
              name: rabbitmq-secret
        - name: TZ
          value: &#39;Asia/Shanghai&#39;
        - name: RABBITMQ_ERLANG_COOKIE
          value: &#39;SWvCP0Hrqv43NG7GybHC95ntCJKoW8UyNFWnBEWG8TY=&#39;
        - name: K8S_SERVICE_NAME
          value: rabbitmq-cluster
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: RABBITMQ_USE_LONGNAME
          value: &amp;quot;true&amp;quot;
        - name: RABBITMQ_NODENAME
          value: rabbit@$(POD_NAME).$(K8S_SERVICE_NAME).$(POD_NAMESPACE).svc.cluster.local
        - name: RABBITMQ_CONFIG_FILE
          value: /var/lib/rabbitmq/rabbitmq.conf
        image: rabbitmq:3.9-management
        imagePullPolicy: IfNotPresent
        name: rabbitmq
        ports:
        - containerPort: 15672
          name: http
          protocol: TCP
        - containerPort: 5672
          name: amqp
          protocol: TCP
        livenessProbe:
          exec:
            command: [&amp;quot;rabbitmq-diagnostics&amp;quot;, &amp;quot;status&amp;quot;]
          initialDelaySeconds: 60
          periodSeconds: 60
          failureThreshold: 2
          timeoutSeconds: 10
        readinessProbe:
          exec:
            command: [&amp;quot;rabbitmq-diagnostics&amp;quot;, &amp;quot;status&amp;quot;]
          failureThreshold: 2
          initialDelaySeconds: 60
          periodSeconds: 60
          timeoutSeconds: 10
        volumeMounts:
        - mountPath: /etc/rabbitmq
          name: config-volume
          readOnly: false
        - mountPath: /var/lib/rabbitmq
          name: rabbitmq-storage
          readOnly: false
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: timezone
          mountPath: /etc/timezone
      serviceAccountName: rabbitmq-cluster
      terminationGracePeriodSeconds: 30
      volumes:
      - name: config-volume
        configMap:
          items:
          - key: rabbitmq.conf
            path: rabbitmq.conf
          - key: enabled_plugins
            path: enabled_plugins
          name: rabbitmq-cluster-config
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-storage
    spec:
      accessModes:
      - ReadWriteMany
      storageClassName: &amp;quot;nfs-storage&amp;quot;
      resources:
        requests:
          storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;166-åˆ›å»ºingress&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#166-åˆ›å»ºingress&#34;&gt;#&lt;/a&gt; 1.6.6 åˆ›å»º Ingress&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rabbitmq-ingress
  namespace: prod
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: rabbitmq.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: rabbitmq-cluster
            port:
              number: 15672
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;167-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#167-æ›´æ–°èµ„æºæ¸…å•&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.6.7 æ›´æ–°èµ„æºæ¸…å•&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 04-rabbitmq]# sed -i &amp;quot;s#dev#prod#g&amp;quot; *.yaml
[root@k8s-master01 04-rabbitmq]# kubectl apply -f .
[root@k8s-master01 04-rabbitmq]# kubectl get pods -n prod
NAME                 READY   STATUS    RESTARTS   AGE
rabbitmq-cluster-0   1/1     Running   0          9m53s
rabbitmq-cluster-1   1/1     Running   0          8m47s
rabbitmq-cluster-2   1/1     Running   0          7m40s

[root@k8s-master01 04-rabbitmq]# kubectl exec -it rabbitmq-cluster-0 -n prod -- /bin/bash
root@rabbitmq-cluster-0:/# rabbitmqctl cluster_status
RABBITMQ_ERLANG_COOKIE env variable support is deprecated and will be REMOVED in a future version. Use the $HOME/.erlang.cookie file or the --erlang-cookie switch instead.
Cluster status of node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local ...
Basics

Cluster name: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local

Disk Nodes

rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local

Running Nodes

rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local
rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local

Versions

rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9
rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9
rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local: RabbitMQ 3.9.29 on Erlang 25.3.2.9

Maintenance status

Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, status: not under maintenance

Alarms

Memory alarm on node rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local
Memory alarm on node rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local
Memory alarm on node rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local

Network Partitions

(none)

Listeners

Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@rabbitmq-cluster-0.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@rabbitmq-cluster-1.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@rabbitmq-cluster-2.rabbitmq-cluster.prod.svc.cluster.local, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0

Feature flags

Flag: drop_unroutable_metric, state: enabled
Flag: empty_basic_get_metric, state: enabled
Flag: implicit_default_bindings, state: enabled
Flag: maintenance_mode_status, state: enabled
Flag: quorum_queue, state: enabled
Flag: stream_queue, state: enabled
Flag: user_limits, state: enabled
Flag: virtual_host_metadata, state: enabled
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/XqURJbg.jpeg&#34; alt=&#34;Snipaste_2025-05-17_17-42-47.jpg&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;168-webè®¿é—®rabbitmq&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#168-webè®¿é—®rabbitmq&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.6.8 Web è®¿é—® rabbitmq&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;http://rabbitmq.hmallleasing.com/#/
user:superman
pwd:talent
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/7qKxUBf.jpeg&#34; alt=&#34;Snipaste_2025-05-17_17-14-54.jpg&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;168-rabbitmqå…¨éƒ¨æŒ‚äº†æ— æ³•é‡å¯è§£å†³æ–¹æ¡ˆ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#168-rabbitmqå…¨éƒ¨æŒ‚äº†æ— æ³•é‡å¯è§£å†³æ–¹æ¡ˆ&#34;&gt;#&lt;/a&gt; 1.6.8 rabbitMQ å…¨éƒ¨æŒ‚äº†ï¼Œæ— æ³•é‡å¯è§£å†³æ–¹æ¡ˆ&lt;/h6&gt;
&lt;p&gt;Kubernetes ç¯å¢ƒä¸­ï¼Œé‡åˆ° RabbitMQ é›†ç¾¤æ— æ³•å¯åŠ¨çš„é—®é¢˜ã€‚åŸå› æ˜¯ RabbitMQ æ‰€æœ‰å®ä¾‹å‡å¤±æ•ˆï¼Œéœ€è¦åœ¨æ¯ä¸ª Pod å¯¹åº”çš„æŒä¹…åŒ–å­˜å‚¨è·¯å¾„ä¸‹åˆ›å»º force_load æ–‡ä»¶æ¥å¼ºåˆ¶å¯åŠ¨ã€‚é€šè¿‡è·å– PV å­˜å‚¨è·¯å¾„ï¼Œåœ¨æŒ‡å®šç›®å½•åˆ›å»ºè¯¥æ–‡ä»¶åï¼Œé‡æ–°å¯åŠ¨ RabbitMQ æœåŠ¡ï¼ŒæˆåŠŸè§£å†³äº†é›†ç¾¤å¯åŠ¨é—®é¢˜&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-node02 ~# cd /data/dev-rabbitmq-storage-rabbitmq-cluster-0-pvc-3abca920-3c68-44eb-b0fd-406a4358b153/mnesia/rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local
[root@k8s-node02 rabbit@rabbitmq-cluster-0.rabbitmq-cluster.dev.svc.cluster.local]# touch force_load
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;17-éƒ¨ç½²rabbitmq-single&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#17-éƒ¨ç½²rabbitmq-single&#34;&gt;#&lt;/a&gt; 1.7 éƒ¨ç½² rabbitmq-single&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 04-rabbitmq]# cat 06-rabbitmq-single.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rabbitmq-single-data
  namespace: prod
spec:
  storageClassName: &amp;quot;nfs-storage&amp;quot;     # æ˜ç¡®æŒ‡å®šä½¿ç”¨å“ªä¸ªscçš„ä¾›åº”å•†æ¥åˆ›å»ºpv
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi                      # æ ¹æ®ä¸šåŠ¡å®é™…å¤§å°è¿›è¡Œèµ„æºç”³è¯·
---

apiVersion: v1
kind: Service
metadata:
  name: rabbitmq-single-svc
  namespace: prod
  labels:
    name: rabbitmq-single-svc
spec:
  ports:
  - port: 5672 
    protocol: TCP
    name: web
    targetPort: 5672
  - name: http
    port: 15672
    protocol: TCP
    targetPort: 15672
  selector:
    app: rabbitmq-single

---
apiVersion: networking.k8s.io/v1 # k8s &amp;gt;= 1.22 å¿…é¡» v1
kind: Ingress
metadata:
  name: rabbitmq-single-ingress
  namespace: prod
spec:
  ingressClassName: nginx
  rules:
  - host: rabbitmq.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: rabbitmq-single-svc
            port:
              number: 15672
        path: /
        pathType: Prefix

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rabbitmq-single
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq-single
  template:
    metadata:
      labels:
        app: rabbitmq-single
    spec:
      containers:
      - name: rabbitmq-single
        image: rabbitmq:3.9-management
        ports:
        - containerPort: 5672
          name: web
          protocol: TCP
        - containerPort: 15672
          name: http
          protocol: TCP
        env:
        - name: RABBITMQ_DEFAULT_USER  # è‡ªå®šä¹‰ç¯å¢ƒå˜é‡
          value: admin
        - name: RABBITMQ_DEFAULT_PASS
          value: Superman*2025
        resources:
          requests:
            memory: &amp;quot;1Gi&amp;quot;
            cpu: &amp;quot;500m&amp;quot;
        livenessProbe:
          exec:
            command: [&amp;quot;rabbitmqctl&amp;quot;, &amp;quot;status&amp;quot;]
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command: [&amp;quot;rabbitmqctl&amp;quot;, &amp;quot;status&amp;quot;]
          failureThreshold: 2
          initialDelaySeconds: 30
          periodSeconds: 10
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: rabbitmq-storage
          mountPath: /var/lib/rabbitmq
      volumes:
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File
      - name: rabbitmq-storage
        persistentVolumeClaim:
          claimName: rabbitmq-single-data
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;äºŒ-éƒ¨ç½²å¾®æœåŠ¡åº”ç”¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#äºŒ-éƒ¨ç½²å¾®æœåŠ¡åº”ç”¨&#34;&gt;#&lt;/a&gt; äºŒã€éƒ¨ç½²å¾®æœåŠ¡åº”ç”¨&lt;/h4&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-05-18T13:42:46.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/170573601.html</id>
        <title>K8sæœåŠ¡å‘å¸ƒIngress</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/170573601.html"/>
        <content type="html">&lt;h4 id=&#34;1-ingress-nginx-controller-å®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-ingress-nginx-controller-å®‰è£…&#34;&gt;#&lt;/a&gt; 1. Ingress Nginx Controller å®‰è£…&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Supported&lt;/th&gt;
&lt;th&gt;Ingress-NGINX version&lt;/th&gt;
&lt;th&gt;k8s supported version&lt;/th&gt;
&lt;th&gt;Alpine Version&lt;/th&gt;
&lt;th&gt;Nginx Version&lt;/th&gt;
&lt;th&gt;Helm Chart Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.12.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.32, 1.31, 1.30, 1.29, 1.28&lt;/td&gt;
&lt;td&gt;3.21.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.12.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.12.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.32, 1.31, 1.30, 1.29, 1.28&lt;/td&gt;
&lt;td&gt;3.21.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.12.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.12.0-beta.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.32, 1.31, 1.30, 1.29, 1.28&lt;/td&gt;
&lt;td&gt;3.20.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.12.0-beta.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.21.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.21.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ğŸ”„&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.11.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.11.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.6&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.21.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.3&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.20.0&lt;/td&gt;
&lt;td&gt;1.25.5&lt;/td&gt;
&lt;td&gt;4.10.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.30, 1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.19.1&lt;/td&gt;
&lt;td&gt;1.25.3&lt;/td&gt;
&lt;td&gt;4.10.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;v1.10.0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.29, 1.28, 1.27, 1.26&lt;/td&gt;
&lt;td&gt;3.19.1&lt;/td&gt;
&lt;td&gt;1.25.3&lt;/td&gt;
&lt;td&gt;4.10.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.6&lt;/td&gt;
&lt;td&gt;1.29, 1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.19.0&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.9.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.5&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.9.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.4&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.3&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.1&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.4&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.9.0&lt;/td&gt;
&lt;td&gt;1.28, 1.27, 1.26, 1.25&lt;/td&gt;
&lt;td&gt;3.18.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.8.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.8.4&lt;/td&gt;
&lt;td&gt;1.27, 1.26, 1.25, 1.24&lt;/td&gt;
&lt;td&gt;3.18.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.7.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.7.1&lt;/td&gt;
&lt;td&gt;1.27, 1.26, 1.25, 1.24&lt;/td&gt;
&lt;td&gt;3.17.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.6.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.6.4&lt;/td&gt;
&lt;td&gt;1.26, 1.25, 1.24, 1.23&lt;/td&gt;
&lt;td&gt;3.17.0&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.5.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.5.1&lt;/td&gt;
&lt;td&gt;1.25, 1.24, 1.23&lt;/td&gt;
&lt;td&gt;3.16.2&lt;/td&gt;
&lt;td&gt;1.21.6&lt;/td&gt;
&lt;td&gt;4.4.*&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.4.0&lt;/td&gt;
&lt;td&gt;1.25, 1.24, 1.23, 1.22&lt;/td&gt;
&lt;td&gt;3.16.2&lt;/td&gt;
&lt;td&gt;1.19.10â€ &lt;/td&gt;
&lt;td&gt;4.3.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;v1.3.1&lt;/td&gt;
&lt;td&gt;1.24, 1.23, 1.22, 1.21, 1.20&lt;/td&gt;
&lt;td&gt;3.16.2&lt;/td&gt;
&lt;td&gt;1.19.10â€ &lt;/td&gt;
&lt;td&gt;4.2.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;11-helmå®‰è£…ingress-nginx-controller&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-helmå®‰è£…ingress-nginx-controller&#34;&gt;#&lt;/a&gt; 1.1 Helm å®‰è£… Ingress Nginx Controller&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;å®‰è£… Helm&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# wget https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz
# tar xf helm-v3.6.3-linux-amd64.tar.gz
# mv linux-amd64/helm /usr/local/bin/helm
# helm version
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;ä¸‹è½½ Ingress Nginx Controller å®‰è£…åŒ…&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;å®˜æ–¹æ–‡æ¡£ï¼šhttps://github.com/kubernetes/ingress-nginx/tree/helm-chart-4.8.2         #æ ¹æ®è‡ªå·±k8sç‰ˆæœ¬ä¸‹è½½
# helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
# helm repo update
# helm repo list
# helm pull ingress-nginx/ingress-nginx --version 4.8.2
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;é…ç½® Ingress Nginx Controller&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# tar xf ingress-nginx-4.8.2.tgz
# cd ingress-nginx
# vim values.yaml
...
 16 controller:
 17   name: controller
 18   enableAnnotationValidations: false
 19   image:
 20     ## Keep false as default for now!
 21     chroot: false
 22     registry: registry.cn-hangzhou.aliyuncs.com
 23     image: kubernetes_public/ingress-nginx-controller
 24     ## for backwards compatibility consider setting the full image url via the repository value below
 25     ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml wil    l fail
 26     ## repository:
 27     tag: &amp;quot;v1.9.3&amp;quot;
 28     #digest: sha256:8fd21d59428507671ce0fb47f818b1d859c92d2ad07bb7c947268d433030ba98
...
 42   # -- Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configurat    ion/configmap/
 43   config:
 44     allow-snippet-annotations: true          #å¼€å¯server snippetçš„é…ç½®
...
 67   dnsPolicy: ClusterFirstWithHostNet
...
 88   hostNetwork: true
...
107   ingressClassResource:
108     # -- Name of the ingressClass
109     name: nginx
110     # -- Is this ingressClass enabled or not
111     enabled: true
112     # -- Is this the default ingressClass for the cluster
113     default: true
...
184   kind: DaemonSet
...
287   nodeSelector:
288     kubernetes.io/os: linux
289     ingress: &amp;quot;true&amp;quot;
...
638       image:
639         registry: registry.cn-hangzhou.aliyuncs.com
640         image: kubernetes_public/kube-webhook-certgen
641         ## for backwards compatibility consider setting the full image url via the repository value below
642         ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml     will fail
643         ## repository:
644         tag: v20231011-8b53cabe0
645         #digest: sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;4. ç»™éœ€è¦éƒ¨ç½² ingress çš„èŠ‚ç‚¹ä¸Šæ‰“æ ‡ç­¾&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl label node k8s-node02 ingress=true
# kubectl label node k8s-node01 ingress=true
# kubectl create ns ingress-nginx
# helm install ingress-nginx -n ingress-nginx .     #å®‰è£…
# helm upgrade ingress-nginx -n ingress-nginx .     #æ›´æ–°
# kubectl get pods -n ingress-nginx 
NAME                             READY   STATUS    RESTARTS   AGE
ingress-nginx-controller-7nfqn   1/1     Running   0          27s
ingress-nginx-controller-k4p2n   1/1     Running   0          17m
ingress-nginx-controller-kw5jk   1/1     Running   0          24s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-bare-metalå®‰è£…ingress-nginx-controller&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-bare-metalå®‰è£…ingress-nginx-controller&#34;&gt;#&lt;/a&gt; 1.2 Bare metal å®‰è£… Ingress Nginx Controller&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ä¸‹è½½ Ingress éƒ¨ç½²æ–‡ä»¶ï¼Œé“¾æ¥åœ°å€ï¼š&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters&#34;&gt;https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.1/deploy/static/provider/baremetal/deploy.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;é…ç½® Ingress&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ingress-master]# cat deploy.yaml 
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  name: ingress-nginx
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
  namespace: ingress-nginx
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - pods
  - secrets
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resourceNames:
  - ingress-nginx-leader
  resources:
  - leases
  verbs:
  - get
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
  namespace: ingress-nginx
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - secrets
  verbs:
  - get
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - endpoints
  - nodes
  - pods
  - secrets
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - nodes
  verbs:
  - get
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
rules:
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  verbs:
  - get
  - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: v1
data: null
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - appProtocol: http
    name: http
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  #type: NodePort
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller-admission
  namespace: ingress-nginx
spec:
  ports:
  - appProtocol: https
    name: https-webhook
    port: 443
    targetPort: webhook
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: ClusterIP
---
apiVersion: apps/v1
#kind: Deployment
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.1
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/ingress-nginx-controller-v1.12.1:v1.12.1 
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          readOnlyRootFilesystem: false
          runAsGroup: 82
          runAsNonRoot: true
          runAsUser: 101
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      hostNetwork: true                         # ä¸èŠ‚ç‚¹å…±äº«ç½‘ç»œåç§°ç©ºé—´
      #dnsPolicy: ClusterFirst
      dnsPolicy: ClusterFirstWithHostNet        # dns ç­–ç•¥
      nodeSelector:                             # èŠ‚ç‚¹é€‰æ‹©å™¨
        kubernetes.io/os: linux
        ingress: &amp;quot;true&amp;quot;
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission-create
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.1
      name: ingress-nginx-admission-create
    spec:
      containers:
      - args:
        - create
        - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
        - --namespace=$(POD_NAMESPACE)
        - --secret-name=ingress-nginx-admission
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 
        imagePullPolicy: IfNotPresent
        name: create
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
          seccompProfile:
            type: RuntimeDefault
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      serviceAccountName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission-patch
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.1
      name: ingress-nginx-admission-patch
    spec:
      containers:
      - args:
        - patch
        - --webhook-name=ingress-nginx-admission
        - --namespace=$(POD_NAMESPACE)
        - --patch-mutating=false
        - --secret-name=ingress-nginx-admission
        - --patch-failure-policy=Fail
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/kube-webhook-certgen-v1.5.2:v1.5.2 
        imagePullPolicy: IfNotPresent
        name: patch
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
          seccompProfile:
            type: RuntimeDefault
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      serviceAccountName: ingress-nginx-admission
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: nginx
spec:
  controller: k8s.io/ingress-nginx
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.1
  name: ingress-nginx-admission
webhooks:
- admissionReviewVersions:
  - v1
  clientConfig:
    service:
      name: ingress-nginx-controller-admission
      namespace: ingress-nginx
      path: /networking/v1/ingresses
      port: 443
  failurePolicy: Fail
  matchPolicy: Equivalent
  name: validate.nginx.ingress.kubernetes.io
  rules:
  - apiGroups:
    - networking.k8s.io
    apiVersions:
    - v1
    operations:
    - CREATE
    - UPDATE
    resources:
    - ingresses
  sideEffects: None
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;type: ClusterIP                                              #service ç±»å‹æ”¹ä¸º ClusterIP&lt;/li&gt;
&lt;li&gt;hostNetwork: true                                      # ä¸èŠ‚ç‚¹å…±äº«ç½‘ç»œåç§°ç©ºé—´&lt;/li&gt;
&lt;li&gt;dnsPolicy: ClusterFirstWithHostNet        # dns ç­–ç•¥&lt;/li&gt;
&lt;li&gt;nodeSelector:                                             # èŠ‚ç‚¹é€‰æ‹©å™¨&lt;/li&gt;
&lt;li&gt;kind: DaemonSet                                        # èµ„æºç±»å‹ DaemonSet&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;åœ¨æŒ‡å®šèŠ‚ç‚¹éƒ¨ç½² Ingress-Controller&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ingress-master]# kubectl apply -f deploy.yaml -n ingress-nginx

[root@k8s-master01 ingress-master]# kubectl label node k8s-node01 ingress=true
[root@k8s-master01 ingress-master]# kubectl label node k8s-node02 ingress=true
[root@k8s-master01 ingress-master]# kubectl label node k8s-master03 ingress-     #å–æ¶ˆèŠ‚ç‚¹éƒ¨ç½²

[root@k8s-master01 ingress-master]# kubectl get pods -n ingress-nginx 
NAME                                   READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-zp6mh   0/1     Completed   0          12m
ingress-nginx-admission-patch-f2bpd    0/1     Completed   0          12m
ingress-nginx-controller-rgtkc         1/1     Running     0          3m59s
ingress-nginx-controller-trmn8         1/1     Running     0          3m59s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-ingress-nginx-å…¥é—¨ä½¿ç”¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-ingress-nginx-å…¥é—¨ä½¿ç”¨&#34;&gt;#&lt;/a&gt; 2. Ingress Nginx å…¥é—¨ä½¿ç”¨&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat web-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: test.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-ingress-nginx-åŸŸåé‡å®šå‘-redirect&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-ingress-nginx-åŸŸåé‡å®šå‘-redirect&#34;&gt;#&lt;/a&gt; 3. Ingress Nginx åŸŸåé‡å®šå‘ Redirect&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat redirect-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: redirect-ingress
  annotations:
    nginx.ingress.kubernetes.io/permanent-redirect: https://www.baidu.com
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: redirect.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-ingress-nginx-å‰åç«¯åˆ†ç¦»-rewrite&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-ingress-nginx-å‰åç«¯åˆ†ç¦»-rewrite&#34;&gt;#&lt;/a&gt; 4. Ingress Nginx å‰åç«¯åˆ†ç¦» Rewrite&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat rewrite-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rewrite-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: rewrite.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /api(/|$)(.*)
        pathType: ImplementationSpecif
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-ingress-nginx-é”™è¯¯ä»£ç é‡å®šå‘&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-ingress-nginx-é”™è¯¯ä»£ç é‡å®šå‘&#34;&gt;#&lt;/a&gt; 5. Ingress Nginx é”™è¯¯ä»£ç é‡å®šå‘&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-ingress-nginx-ssl&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-ingress-nginx-ssl&#34;&gt;#&lt;/a&gt; 6. Ingress Nginx SSL&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.ç”Ÿæˆè¯ä¹¦
# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.cert -subj &amp;quot;/CN=s.hmallleasing.com/O=tls.hmallleasing.com&amp;quot;

2.åˆ›å»ºè¯ä¹¦
# kubectl create secret tls tls.hmallleasig.com --key tls.key --cert tls.cert

3.ingressé…ç½®
# kubectl create secret tls tls.hmallleasig.com --cert=tls.crt --key=tls.key
# cat tls-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;    #ç¦ç”¨httpså¼ºåˆ¶è·³è½¬
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: tls.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
  tls:                  #https
  - hosts:
    - tls.hmallleasing.com
    secretName: &amp;quot;tls.hmallleasig.com&amp;quot;	
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-ingress-nginx-åŒ¹é…è¯·æ±‚å¤´&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-ingress-nginx-åŒ¹é…è¯·æ±‚å¤´&#34;&gt;#&lt;/a&gt; 7. Ingress Nginx åŒ¹é…è¯·æ±‚å¤´&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.éƒ¨ç½²ç§»åŠ¨ç«¯åº”ç”¨
# kubectl create deploy phone --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:phone
# kubectl expose deploy phone --port 80
# vim m-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: m-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: m.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: phone
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific

2.éƒ¨ç½²PCç«¯åº”ç”¨		
# kubectl create deploy laptop --image=registry.cn-beijing.aliyuncs.com/dotbalo/nginx:laptop	
# kubectl expose deploy laptop --port 80
# vim laptop-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/server-snippet: |
      set $agentflag 0;
          if ($http_user_agent ~* &amp;quot;(Android|iPhone|Windows Phone|UC|Kindle)&amp;quot; )&amp;#123;
              set $agentflag 1;
          &amp;#125;
          if ( $agentflag = 1 ) &amp;#123;
              return 301 http://m.hmallleaing.com;
          &amp;#125;
  name: laptop-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: laptop
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific	
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8ingress-nginx-åŸºæœ¬è®¤è¯&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8ingress-nginx-åŸºæœ¬è®¤è¯&#34;&gt;#&lt;/a&gt; 8.Ingress Nginx åŸºæœ¬è®¤è¯&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# yum install httpd -y
# htpasswd -c auth superman
# cat auth 
superman:$apr1$AC1pc3dK$RJyWnyDJFNKY6twneGVrA1		

# kubectl create secret generic basic-auth --from-file=auth
# cat basic-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: basic-ingress
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic  # è®¤è¯ç±»å‹
    nginx.ingress.kubernetes.io/auth-secret: basic-auth  # åŒ…å«ç”¨æˆ·å’Œå¯†ç çš„ secret èµ„æºåç§°
    nginx.ingress.kubernetes.io/auth-realm: &#39;Please User password&#39;  # è¦æ˜¾ç¤ºçš„ä¿¡æ¯
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: basic.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9-ingress-nginx-é»‘ç™½åå•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-ingress-nginx-é»‘ç™½åå•&#34;&gt;#&lt;/a&gt; 9. Ingress Nginx é»‘ / ç™½åå•&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;å†™æ³•ä¸€ï¼š
# cat white-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: white-ingress
  annotations:
    nginx.ingress.kubernetes.io/whitelist-source-range: &amp;quot;192.168.40.101&amp;quot;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: white.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific	

å†™æ³•äºŒï¼š		
[root@k8s-master01 ingress]# cat white-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: white-ingress
  annotations:
    nginx.ingress.kubernetes.io/whitelist-source-range: &amp;quot;192.168.40.0/24&amp;quot;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: white.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific


å†™æ³•ä¸‰ï¼š
# cat white-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: white-ingress
  annotations:
    nginx.ingress.kubernetes.io/server-snippet: |
      allow 192.168.40.0/24;
      deny all;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: white.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
		

#Master01æµ‹è¯•		
# curl -H &amp;quot;Host:white.hmallleasing.com&amp;quot; http://192.168.40.103 -I
HTTP/1.1 200 OK
Date: Sat, 14 Oct 2023 13:12:03 GMT
Content-Type: text/html
Content-Length: 612
Connection: keep-alive
Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT
ETag: &amp;quot;5cb5d3c3-264&amp;quot;
Accept-Ranges: bytes		

#Master02æµ‹è¯•
# curl -H &amp;quot;Host:white.hmallleasing.com&amp;quot; http://192.168.40.103 -I
HTTP/1.1 403 Forbidden
Date: Sat, 14 Oct 2023 13:13:34 GMT
Content-Type: text/html
Content-Length: 146
Connection: keep-alive
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;10-ingress-nginx-é€Ÿç‡é™åˆ¶&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-ingress-nginx-é€Ÿç‡é™åˆ¶&#34;&gt;#&lt;/a&gt; 10. Ingress Nginx é€Ÿç‡é™åˆ¶&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ingress]# cat limit-rate-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rate-limit-ingress
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: &amp;quot;50&amp;quot;
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: rate-limit.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: nginx
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific

# ab -c 20 -n 1000 http://rate-limit.hmallleasing.com/ |grep request
Complete requests:      1000
Failed requests:        724
Time per request:       10.301 [ms] (mean)
Time per request:       0.515 [ms] (mean, across all concurrent requests)
Percentage of the requests served within a certain time (ms)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11ä½¿ç”¨-nginx-å®ç°ç°åº¦é‡‘ä¸é›€å‘å¸ƒ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11ä½¿ç”¨-nginx-å®ç°ç°åº¦é‡‘ä¸é›€å‘å¸ƒ&#34;&gt;#&lt;/a&gt; 11. ä½¿ç”¨ Nginx å®ç°ç°åº¦ / é‡‘ä¸é›€å‘å¸ƒ&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.åˆ›å»º v1 ç‰ˆæœ¬
# kubectl create deploy canary-v1 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v1	
# kubectl expose deploy canary-v1 --port 8080
# cat canary-v1-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-v1-ingress
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: canary.hmallleasing.com
    http:
      paths:
      - backend:
          service:
            name: canary-v1
            port:
              number: 8080
        path: /
        pathType: ImplementationSpecific
		
# curl -H &amp;quot;Host:canary.hmallleasing.com&amp;quot; http://192.168.40.103 	

2.åˆ›å»º v2 ç‰ˆæœ¬
# kubectl create deploy canary-v2 --image=registry.cn-beijing.aliyuncs.com/dotbalo/canary:v2
# kubectl expose deploy canary-v2 --port 8080
# cat canary-v2-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-v2-ingress
  annotations:
    nginx.ingress.kubernetes.io/canary: &amp;quot;true&amp;quot;    #å¯åŠ¨ç°åº¦å‘å¸ƒ
    nginx.ingress.kubernetes.io/canary-weight: &amp;quot;20&amp;quot;  #åŸºäºæƒé‡,50%æµé‡è°ƒåº¦åˆ°è¿™ä¸ªç°åº¦çš„ç‰ˆæœ¬ä¸Š
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: canary.hmallleasing.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: canary-v2
            port:
              number: 8080

#æµ‹è¯•ç°åº¦å‘å¸ƒ
[root@k8s-master01 ingress]# cat canary.sh 
#!/bin/bash

while true
do
	curl -H &amp;quot;Host:canary.hmallleasing.com&amp;quot; http://192.168.40.103
	sleep 0.5
done
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-kubernetes-dashboardé…ç½®è¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-kubernetes-dashboardé…ç½®è¯ä¹¦&#34;&gt;#&lt;/a&gt; 12. kubernetes-dashboard é…ç½®è¯ä¹¦&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;1.åˆ›å»ºè¯ä¹¦
kubectl create secret tls kubernetes-dashboard-certs --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n kubernetes-dashboard

2.ä¿®æ”¹kubernetes-dashboardèµ„æºæ¸…å•
kubectl edit deployment -n kubernetes-dashboard kubernetes-dashboard
...
      - args:
        - --auto-generate-certificates=false
        - --tls-key-file=_.hmallleasing.com_key.key
        - --tls-cert-file=_.hmallleasing.com_chain.crt
        - --token-ttl=21600
        - --authentication-mode=basic,token
        - --namespace=kubernetes-dashboard
...

3.åˆ›å»ºingress
#cat dashboard-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/ssl-passthrough: &amp;quot;true&amp;quot;    
    nginx.ingress.kubernetes.io/backend-protocol: &amp;quot;HTTPS&amp;quot;    
spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
  rules:
  - host: dashboard.hmallleasing.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443

# kubectl apply -f dashboard-ingress.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;13-å…¥å£lbé…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-å…¥å£lbé…ç½®&#34;&gt;#&lt;/a&gt; 13. å…¥å£ LB é…ç½®&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@lb nginx]# cat /etc/nginx/conf.d/ingress.conf 
upstream ingress &amp;#123;
	server 192.168.40.103:80 max_conns=2000 max_fails=2 fail_timeout=5s;
	server 192.168.40.104:80 max_conns=2000 max_fails=2 fail_timeout=5s;
	server 192.168.40.105:80 max_conns=2000 max_fails=2 fail_timeout=5s;
&amp;#125;

server &amp;#123;
    listen 443 ssl;
    server_name test.hmallleasing.com;
    client_max_body_size 1G; 
    ssl_prefer_server_ciphers on;
    ssl_certificate  /etc/nginx/sslkey/*.hmallleasing.com_chain.crt;
    ssl_certificate_key  /etc/nginx/sslkey/*.hmallleasing.com_key.key;

    location / &amp;#123;
        proxy_pass http://ingress;
        include proxy_params;
	    proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
	    proxy_next_upstream_tries 2;
	    proxy_next_upstream_timeout 3s;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name test.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;

[root@lb ~]# mkdir /etc/nginx/sslkey -p


[root@lb ~]# cat proxy_params 
proxy_http_version 1.1;
proxy_set_header Connectin &amp;quot;&amp;quot;;

proxy_set_header Host $http_host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

proxy_connect_timeout 60;
proxy_send_timeout 120;
proxy_read_timeout 120;

proxy_buffering on;
proxy_buffer_size 32k;
proxy_buffers 4 128k;
proxy_temp_file_write_size 10240k;
proxy_max_temp_file_size 10240k;
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-26T08:52:06.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3030097036.html</id>
        <title>K8Säº‘åŸç”Ÿå­˜å‚¨Rook-Ceph</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3030097036.html"/>
        <content type="html">&lt;h3 id=&#34;k8säº‘åŸç”Ÿå­˜å‚¨rook-ceph&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8säº‘åŸç”Ÿå­˜å‚¨rook-ceph&#34;&gt;#&lt;/a&gt; K8S äº‘åŸç”Ÿå­˜å‚¨ Rook-Ceph&lt;/h3&gt;
&lt;h4 id=&#34;1-storageclassåŠ¨æ€å­˜å‚¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-storageclassåŠ¨æ€å­˜å‚¨&#34;&gt;#&lt;/a&gt; 1. StorageClass åŠ¨æ€å­˜å‚¨&lt;/h4&gt;
&lt;p&gt;StorageClassï¼šå­˜å‚¨ç±»ï¼Œç”± K8s ç®¡ç†å‘˜åˆ›å»ºï¼Œç”¨äºåŠ¨æ€ PV çš„ç®¡ç†ï¼Œå¯ä»¥é“¾æ¥è‡³ä¸åŒçš„åç«¯å­˜å‚¨ï¼Œæ¯”å¦‚ Cephã€GlusterFS ç­‰ã€‚ä¹‹åå¯¹å­˜å‚¨çš„è¯·æ±‚å¯ä»¥æŒ‡å‘ StorageClassï¼Œç„¶å StorageClass ä¼šè‡ªåŠ¨çš„åˆ›å»ºã€åˆ é™¤ PVã€‚&lt;/p&gt;
&lt;p&gt;å®ç°æ–¹å¼ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in-tree: å†…ç½®äº K8s æ ¸å¿ƒä»£ç ï¼Œå¯¹äºå­˜å‚¨çš„ç®¡ç†ï¼Œéƒ½éœ€è¦ç¼–å†™ç›¸åº”çš„ä»£ç ã€‚&lt;/li&gt;
&lt;li&gt;out-of-treeï¼šç”±å­˜å‚¨å‚å•†æä¾›ä¸€ä¸ªé©±åŠ¨ï¼ˆCSI æˆ– Flex Volumeï¼‰ï¼Œå®‰è£…åˆ° K8s é›†ç¾¤ï¼Œç„¶å StorageClass åªéœ€è¦é…ç½®è¯¥é©±åŠ¨å³å¯ï¼Œé©±åŠ¨å™¨ä¼šä»£æ›¿ StorageClass ç®¡ç†å­˜å‚¨ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StorageClass å®˜ç½‘ä»‹ç»ï¼š&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/&#34;&gt;https://kubernetes.io/docs/concepts/storage/storage-classes/&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-äº‘åŸç”Ÿå­˜å‚¨rook&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-äº‘åŸç”Ÿå­˜å‚¨rook&#34;&gt;#&lt;/a&gt; 2. äº‘åŸç”Ÿå­˜å‚¨ Rook&lt;/h4&gt;
&lt;p&gt;Rook æ˜¯ä¸€ä¸ªè‡ªæˆ‘ç®¡ç†çš„åˆ†å¸ƒå¼å­˜å‚¨ç¼–æ’ç³»ç»Ÿï¼Œå®ƒæœ¬èº«å¹¶ä¸æ˜¯å­˜å‚¨ç³»ç»Ÿï¼Œåœ¨å­˜å‚¨å’Œ k8s ä¹‹å‰æ­å»ºäº†ä¸€ä¸ªæ¡¥æ¢ï¼Œä½¿å­˜å‚¨ç³»ç»Ÿçš„æ­å»ºæˆ–è€…ç»´æŠ¤å˜å¾—ç‰¹åˆ«ç®€å•ï¼ŒRook å°†åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿè½¬å˜ä¸ºè‡ªæˆ‘ç®¡ç†ã€è‡ªæˆ‘æ‰©å±•ã€è‡ªæˆ‘ä¿®å¤çš„å­˜å‚¨æœåŠ¡ã€‚å®ƒè®©ä¸€äº›å­˜å‚¨çš„æ“ä½œï¼Œæ¯”å¦‚éƒ¨ç½²ã€é…ç½®ã€æ‰©å®¹ã€å‡çº§ã€è¿ç§»ã€ç¾éš¾æ¢å¤ã€ç›‘è§†å’Œèµ„æºç®¡ç†å˜å¾—è‡ªåŠ¨åŒ–ï¼Œæ— éœ€äººå·¥å¤„ç†ã€‚å¹¶ä¸” Rook æ”¯æŒ CSIï¼Œå¯ä»¥åˆ©ç”¨ CSI åšä¸€äº› PVC çš„å¿«ç…§ã€æ‰©å®¹ã€å…‹éš†ç­‰æ“ä½œã€‚&lt;/p&gt;
&lt;p&gt;Rook å®˜ç½‘ä»‹ç»ï¼š&lt;a href=&#34;https://rook.io/&#34;&gt;https://rook.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/CK4Gn1u.jpeg&#34; alt=&#34;Snipaste_2025-05-07_20-15-59.jpg&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;3-rook-å®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-rook-å®‰è£…&#34;&gt;#&lt;/a&gt; 3. Rook å®‰è£…&lt;/h4&gt;
&lt;p&gt;ç¯å¢ƒå‡†å¤‡&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K8s é›†ç¾¤è‡³å°‘äº”ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„å†…å­˜ä¸ä½äº 5Gï¼ŒCPU ä¸ä½äº 2 æ ¸&lt;/li&gt;
&lt;li&gt;æ‰€æœ‰èŠ‚ç‚¹æ—¶é—´åŒæ­¥&lt;/li&gt;
&lt;li&gt;è‡³å°‘æœ‰ä¸‰ä¸ªå­˜å‚¨èŠ‚ç‚¹ï¼Œå¹¶ä¸”æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘æœ‰ä¸€ä¸ªè£¸ç›˜ï¼Œk8s-master03ã€k8s-node01ã€k8s-node02 å¢åŠ è£¸ç›˜&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;31-ä¸‹è½½-rook-å®‰è£…æ–‡ä»¶&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-ä¸‹è½½-rook-å®‰è£…æ–‡ä»¶&#34;&gt;#&lt;/a&gt; 3.1 ä¸‹è½½ Rook å®‰è£…æ–‡ä»¶&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# git clone --single-branch --branch v1.17.2 https://github.com/rook/rook.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;32-é…ç½®æ›´æ”¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-é…ç½®æ›´æ”¹&#34;&gt;#&lt;/a&gt; 3.2 é…ç½®æ›´æ”¹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd rook/deploy/examples
[root@k8s-master01 ~]# vim operator.yaml
  ROOK_CSI_CEPH_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephcsi:v3.14.0&amp;quot;
  ROOK_CSI_REGISTRAR_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-node-driver-registrar:v2.13.0&amp;quot;
  ROOK_CSI_RESIZER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-resizer:v1.13.1&amp;quot;
  ROOK_CSI_PROVISIONER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-provisioner:v5.1.0&amp;quot;
  ROOK_CSI_SNAPSHOTTER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-snapshotter:v8.2.0&amp;quot;
  ROOK_CSI_ATTACHER_IMAGE: &amp;quot;registry.cn-hangzhou.aliyuncs.com/kubernetes_public/csi-attacher:v4.8.0&amp;quot;

#ROOK_ENABLE_DISCOVERY_DAEMON æ”¹æˆ true å³å¯
ROOK_ENABLE_DISCOVERY_DAEMON: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-éƒ¨ç½²-rook&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-éƒ¨ç½²-rook&#34;&gt;#&lt;/a&gt; 3.3 éƒ¨ç½² rook&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ceph]# kubectl create -f crds.yaml -f common.yaml -f operator.yaml
[root@k8s-master01 examples]# kubectl get pods -n rook-ceph
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-operator-84ff77778b-7ww2w   1/1     Running   0          91m
rook-discover-6j68f                   1/1     Running   0          82m
rook-discover-9w4kt                   1/1     Running   0          82m
rook-discover-h2zfm                   1/1     Running   0          82m
rook-discover-hsz8b                   1/1     Running   0          19m
rook-discover-rj4t7                   1/1     Running   0          82m
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4åˆ›å»º-ceph-é›†ç¾¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4åˆ›å»º-ceph-é›†ç¾¤&#34;&gt;#&lt;/a&gt; 4. åˆ›å»º Ceph é›†ç¾¤&lt;/h4&gt;
&lt;h5 id=&#34;41-é…ç½®æ›´æ”¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#41-é…ç½®æ›´æ”¹&#34;&gt;#&lt;/a&gt; 4.1 é…ç½®æ›´æ”¹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# vim cluster.yaml
...
    image: registry.cn-hangzhou.aliyuncs.com/kubernetes_public/cephv19.2.2:v19.2.2
...
  skipUpgradeChecks: true     #æ”¹ä¸ºtrueï¼Œè·³è¿‡å‡çº§
....
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # serve the dashboard at the given port.
    # port: 8443
    # serve the dashboard using SSL
    ssl: false          #æ”¹ä¸ºfalse
...
  storage: # cluster level storage configuration and selection
    useAllNodes: false      #æ”¹ä¸ºfalse,ä¸ä½¿ç”¨æ‰€æœ‰çš„èŠ‚ç‚¹å½“osd
    useAllDevices: false    #æ”¹ä¸ºfalse,ä¸ä½¿ç”¨æ‰€æœ‰çš„ç£ç›˜å½“osd
...
    #     deviceFilter: &amp;quot;^sd.&amp;quot;
    nodes:
    - name: &amp;quot;k8s-master03&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;k8s-node01&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;k8s-node02&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ³¨æ„ï¼šæ–°ç‰ˆå¿…é¡»é‡‡ç”¨è£¸ç›˜ï¼Œå³æœªæ ¼å¼åŒ–çš„ç£ç›˜ã€‚å…¶ä¸­ k8s-master03ã€ k8s-node01ã€  k8s-node02 æœ‰æ–°åŠ çš„ä¸€ä¸ªç£ç›˜ï¼Œå¯ä»¥é€šè¿‡ lsblk -f æŸ¥çœ‹æ–°æ·»åŠ çš„ç£ç›˜åç§°ã€‚å»ºè®®æœ€å°‘ä¸‰ä¸ªèŠ‚ç‚¹ï¼Œå¦åˆ™åé¢çš„è¯•éªŒå¯èƒ½ä¼šå‡ºç°é—®é¢˜&lt;/p&gt;
&lt;h5 id=&#34;42-åˆ›å»º-ceph-é›†ç¾¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#42-åˆ›å»º-ceph-é›†ç¾¤&#34;&gt;#&lt;/a&gt; 4.2 åˆ›å»º Ceph é›†ç¾¤&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# kubectl create -f cluster.yaml
[root@k8s-master01 examples]# kubectl get pods -n rook-ceph
NAME                                                     READY   STATUS      RESTARTS        AGE
csi-cephfsplugin-5nmnl                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-6b6ct                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-8xlnl                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-fh9w5                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-mslst                                   3/3     Running     1 (60m ago)     62m
csi-cephfsplugin-provisioner-59bd447c6d-5zwj2            6/6     Running     0               61s
csi-cephfsplugin-provisioner-59bd447c6d-7t2kg            6/6     Running     2 (20s ago)     61s
csi-rbdplugin-5gvmp                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-dzcs4                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-n82b5                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-provisioner-6856fb8b86-86hw8               6/6     Running     0               19s
csi-rbdplugin-provisioner-6856fb8b86-lj9s4               6/6     Running     0               19s
csi-rbdplugin-vh8j2                                      3/3     Running     1 (60m ago)     62m
csi-rbdplugin-xfgwr                                      3/3     Running     1 (60m ago)     62m
rook-ceph-crashcollector-k8s-master01-bbc78d496-bzjk8    1/1     Running     0               8m26s
rook-ceph-crashcollector-k8s-master03-765ff964bb-95wmt   1/1     Running     0               28m
rook-ceph-crashcollector-k8s-node01-7cf4c4b6b6-r4n84     1/1     Running     0               20m
rook-ceph-crashcollector-k8s-node02-f887f8cf9-jz2l8      1/1     Running     0               28m
rook-ceph-detect-version-nsrwj                           0/1     Init:0/1    0               3s
rook-ceph-exporter-k8s-master01-5cd4577b79-ckd4m         1/1     Running     0               8m26s
rook-ceph-exporter-k8s-master03-75f4cf6f7-hc9zb          1/1     Running     0               28m
rook-ceph-exporter-k8s-node01-96fc7cf49-d2r24            1/1     Running     0               20m
rook-ceph-exporter-k8s-node02-777b9f555b-7j6cz           1/1     Running     0               27m
rook-ceph-mgr-a-6f46b4b945-q6cjb                         3/3     Running     3 (14m ago)     35m
rook-ceph-mgr-b-5d4cc5465b-8dfh6                         3/3     Running     0               35m
rook-ceph-mon-a-7c7b7555c7-nlhwg                         2/2     Running     2 (6m14s ago)   51m
rook-ceph-mon-c-559bcf95fd-cl62w                         2/2     Running     0               8m27s
rook-ceph-mon-d-7dbc6b8f5c-8264t                         2/2     Running     0               28m
rook-ceph-operator-645478ff5b-jdcrp                      1/1     Running     0               102m
rook-ceph-osd-0-6d9cf78f76-4zhx8                         2/2     Running     0               12m
rook-ceph-osd-1-88c78bbcb-cn48c                          2/2     Running     0               5m15s
rook-ceph-osd-2-b464c9fc6-458hv                          2/2     Running     0               4m29s
rook-ceph-osd-prepare-k8s-master03-pwnrc                 0/1     Completed   0               86s
rook-ceph-osd-prepare-k8s-node01-xxp2j                   0/1     Completed   0               83s
rook-ceph-osd-prepare-k8s-node02-8nz7x                   0/1     Completed   0               78s
rook-discover-jzmkr                                      1/1     Running     0               91m
rook-discover-k7pxt                                      1/1     Running     0               91m
rook-discover-vqjh5                                      1/1     Running     0               91m
rook-discover-wk8jq                                      1/1     Running     0               91m
rook-discover-x8rsn                                      1/1     Running     0               91m

[root@k8s-master01 examples]# kubectl get cephcluster -n rook-ceph
NAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH        EXTERNAL   FSID
rook-ceph   /var/lib/rook     3          63m   Ready   Cluster created successfully   HEALTH_WARN              ca429602-66f4-4a1e-9d5c-a5773a0f594f
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;43-å®‰è£…-ceph-snapshot-æ§åˆ¶å™¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#43-å®‰è£…-ceph-snapshot-æ§åˆ¶å™¨&#34;&gt;#&lt;/a&gt; 4.3 å®‰è£… ceph snapshot æ§åˆ¶å™¨&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd /root/k8s-ha-install/
[root@k8s-master01 k8s-ha-install]# git checkout manual-installation-v1.32.x
[root@k8s-master01 k8s-ha-install]# kubectl create -f snapshotter/ -n kube-system
[root@k8s-master01 k8s-ha-install]# kubectl get po -n kube-system -l app=snapshot-controller
NAME                    READY   STATUS    RESTARTS   AGE
snapshot-controller-0   1/1     Running   0          67s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-å®‰è£…-ceph-å®¢æˆ·ç«¯å·¥å…·&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-å®‰è£…-ceph-å®¢æˆ·ç«¯å·¥å…·&#34;&gt;#&lt;/a&gt; 5. å®‰è£… ceph å®¢æˆ·ç«¯å·¥å…·&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# cd /root/rook/deploy/examples/
[root@k8s-master01 examples]# kubectl create -f toolbox.yaml -n rook-ceph
[root@k8s-master01 examples]# kubectl get po -n rook-ceph -l app=rook-ceph-tools
NAME                               READY   STATUS    RESTARTS   AGE
rook-ceph-tools-7b75b967db-sqddk   1/1     Running   0          8s
[root@k8s-master01 examples]# kubectl exec -it rook-ceph-tools-7b75b967db-sqddk -n rook-ceph -- bash
bash-5.1$ ceph status
  cluster:
    id:     87b85368-9487-4967-a4e4-5970d2e0ec94
    health: HEALTH_WARN
            1 mgr modules have recently crashed
 
  services:
    mon: 3 daemons, quorum b,c (age 12s), out of quorum: a
    mgr: a(active, since 7m), standbys: b
    osd: 3 osds: 3 up (since 8m), 3 in (since 3h)
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   82 MiB used, 60 GiB / 60 GiB avail
    pgs: 
	
bash-4.4$  ceph osd status
ID  HOST           USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      
 0  k8s-master03  20.6M  19.9G      0        0       0        0   exists,up  
 1  k8s-node01    20.6M  19.9G      0        0       0        0   exists,up  
 2  k8s-node02    20.6M  19.9G      0        0       0        0   exists,up 

bash-4.4$ ceph df
--- RAW STORAGE ---
CLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED
hdd    60 GiB  60 GiB  62 MiB    62 MiB       0.10
TOTAL  60 GiB  60 GiB  62 MiB    62 MiB       0.10

--- POOLS ---
POOL  ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr   1    1  449 KiB        2  1.3 MiB      0     19 GiB
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-ceph-dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-ceph-dashboard&#34;&gt;#&lt;/a&gt; 6. Ceph dashboard&lt;/h4&gt;
&lt;h5 id=&#34;61-æš´éœ²æœåŠ¡&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#61-æš´éœ²æœåŠ¡&#34;&gt;#&lt;/a&gt; 6.1 æš´éœ²æœåŠ¡&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc -n rook-ceph
NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
rook-ceph-mgr             ClusterIP   10.96.54.15     &amp;lt;none&amp;gt;        9283/TCP            133m
rook-ceph-mgr-dashboard   ClusterIP   10.96.97.117    &amp;lt;none&amp;gt;        7000/TCP            133m        #æš´éœ²ingresssä¹Ÿå¯
rook-ceph-mon-a           ClusterIP   10.96.125.216   &amp;lt;none&amp;gt;        6789/TCP,3300/TCP   170m
rook-ceph-mon-b           ClusterIP   10.96.34.183    &amp;lt;none&amp;gt;        6789/TCP,3300/TCP   133m
rook-ceph-mon-c           ClusterIP   10.96.232.252   &amp;lt;none&amp;gt;        6789/TCP,3300/TCP   133m


[root@k8s-master01 examples]# kubectl create -f dashboard-external-http.yaml           #æš´éœ²nodeport
[root@k8s-master01 examples]# kubectl get svc -n rook-ceph rook-ceph-mgr-dashboard-external-http
NAME                                     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
rook-ceph-mgr-dashboard-external-https   NodePort   10.96.11.120   &amp;lt;none&amp;gt;        8443:32611/TCP   45s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;62-é…ç½®ingressè®¿é—®ceph&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#62-é…ç½®ingressè®¿é—®ceph&#34;&gt;#&lt;/a&gt; 6.2 é…ç½® ingress è®¿é—® ceph&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# cat dashboard-ingress-https.yaml 
#
# This example is for Kubernetes running an nginx-ingress
# and an ACME (e.g. Let&#39;s Encrypt) certificate service
#
# The nginx-ingress annotations support the dashboard
# running using HTTPS with a self-signed certificate
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rook-ceph-mgr-dashboard
  namespace: rook-ceph # namespace:cluster
#  annotations:
#    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
#    kubernetes.io/tls-acme: &amp;quot;true&amp;quot;
#    nginx.ingress.kubernetes.io/backend-protocol: &amp;quot;HTTPS&amp;quot;
#    nginx.ingress.kubernetes.io/server-snippet: |
#      proxy_ssl_verify off;

spec:
  ingressClassName: &amp;quot;nginx&amp;quot;
#  tls:
#    - hosts:
#        - rook-ceph.hmallleasing.com
#      secretName: rook-ceph.example.com
  rules:
    - host: rook-ceph.hmallleasing.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: rook-ceph-mgr-dashboard
                port:
                  name: http-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;63-ç™»å½•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#63-ç™»å½•&#34;&gt;#&lt;/a&gt; 6.3 ç™»å½•&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;http://192.168.40.100:32611
ç”¨æˆ·åï¼šadmin
å¯†ç ï¼škubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&amp;quot;&amp;#123;[&#39;data&#39;][&#39;password&#39;]&amp;#125;&amp;quot; | base64 --decode &amp;amp;&amp;amp; echo
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-ceph-å—å­˜å‚¨çš„ä½¿ç”¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-ceph-å—å­˜å‚¨çš„ä½¿ç”¨&#34;&gt;#&lt;/a&gt; 7. Ceph å—å­˜å‚¨çš„ä½¿ç”¨&lt;/h4&gt;
&lt;p&gt;å—å­˜å‚¨ä¸€èˆ¬ç”¨äºä¸€ä¸ª Pod æŒ‚è½½ä¸€å—å­˜å‚¨ä½¿ç”¨ï¼Œç›¸å½“äºä¸€ä¸ªæœåŠ¡å™¨æ–°æŒ‚äº†ä¸€ä¸ªç›˜ï¼Œåªç»™ä¸€ä¸ªåº”ç”¨ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;h5 id=&#34;71-åˆ›å»º-storageclass-å’Œ-ceph-çš„å­˜å‚¨æ± &#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#71-åˆ›å»º-storageclass-å’Œ-ceph-çš„å­˜å‚¨æ± &#34;&gt;#&lt;/a&gt; 7.1 åˆ›å»º StorageClass å’Œ ceph çš„å­˜å‚¨æ± &lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# kubectl get csidriver
NAME                            ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE
rook-ceph.cephfs.csi.ceph.com   true             false            false             &amp;lt;unset&amp;gt;         false               Persistent   15h       #æ–‡ä»¶å­˜å‚¨csi
rook-ceph.rbd.csi.ceph.com      true             false            false             &amp;lt;unset&amp;gt;         false               Persistent   15h       #å—å­˜å‚¨csi

[root@k8s-master01 ~]# cd /root/rook/deploy/examples/
[root@k8s-master01 examples]# vim csi/rbd/storageclass.yaml
...
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph # namespace:cluster
spec:
  failureDomain: host
  replicated:
    size: 3                #æ•°æ®ä¿å­˜å‡ ä»½ï¼Œæµ‹è¯•ç¯å¢ƒå¯ä»¥å°†å‰¯æœ¬æ•°è®¾ç½®æˆäº† 2ï¼ˆä¸èƒ½è®¾ç½®ä¸º 1ï¼‰ï¼Œç”Ÿäº§ç¯å¢ƒæœ€å°‘ä¸º 3ï¼Œä¸”è¦å°äºç­‰äº osd çš„æ•°é‡
...
allowVolumeExpansion: true     #æ˜¯å¦å¯ä»¥æ‰©å®¹
reclaimPolicy: Delete          #pvå›æ”¶ç­–ç•¥

[root@k8s-master01 examples]# kubectl create -f csi/rbd/storageclass.yaml -n rook-ceph

[root@k8s-master01 examples]# kubectl get cephblockpool -n rook-ceph
NAME          PHASE
replicapool   Ready
[root@k8s-master01 examples]# kubectl get sc
NAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage       nfzl.com/nfs                 Delete          Immediate           false                  16h
rook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   37s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;72-æŒ‚è½½æµ‹è¯•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#72-æŒ‚è½½æµ‹è¯•&#34;&gt;#&lt;/a&gt; 7.2 æŒ‚è½½æµ‹è¯•&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat ceph-block-pvc.yaml        #åˆ›å»ºPVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-block-pvc
spec:
  storageClassName: &amp;quot;rook-ceph-block&amp;quot;     # æ˜ç¡®æŒ‡å®šä½¿ç”¨å“ªä¸ªscçš„ä¾›åº”å•†æ¥åˆ›å»ºpv
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi                      # æ ¹æ®ä¸šåŠ¡å®é™…å¤§å°è¿›è¡Œèµ„æºç”³è¯·

[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc.yaml 

[root@k8s-master01 ~]# kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
ceph-block-pvc   Bound    pvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            rook-ceph-block   3s

[root@k8s-master01 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS      REASON   AGE
pvc-86c94d8d-c359-47b8-b5d3-31dcdaf86551   1Gi        RWO            Delete           Bound    default/ceph-block-pvc   rook-ceph-block
	  
[root@k8s-master01 ~]# cat ceph-block-pvc-pod.yaml    #æŒ‚è½½PVCæµ‹è¯• 
apiVersion: v1
kind: Pod
metadata:
  name: ceph-block-pvc-pod
spec:
  containers:
  - name: ceph-block-pvc-pod
    image: nginx
    volumeMounts:
    - name: nginx-page
      mountPath: /usr/share/nginx/html
  volumes:
  - name: nginx-page
    persistentVolumeClaim:      
      claimName: ceph-block-pv

[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-pod.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;73-statefulset-volumeclaimtemplates&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#73-statefulset-volumeclaimtemplates&#34;&gt;#&lt;/a&gt; 7.3 StatefulSet volumeClaimTemplates&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat ceph-block-pvc-sts.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:

  - port: 80
    name: web
      clusterIP: None
      selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # å¿…é¡»åŒ¹é… .spec.template.metadata.labels
  serviceName: &amp;quot;nginx&amp;quot;
  replicas: 3 # é»˜è®¤å€¼æ˜¯ 1
  template:
    metadata:
      labels:
        app: nginx # å¿…é¡»åŒ¹é… .spec.selector.matchLabels
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
    name: www
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: &amp;quot;rook-ceph-block&amp;quot;
      resources:
        requests:
          storage: 1Gi
    	  
[root@k8s-master01 ~]# kubectl apply -f ceph-block-pvc-sts.yaml 

[root@k8s-master01 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS       AGE
web-0                                     1/1     Running   0              4m19s
web-1                                     1/1     Running   0              4m10s
web-2                                     1/1     Running   0              2m21s

[root@k8s-master01 ~]# kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
www-web-0   Bound    pvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            rook-ceph-block   4m23s
www-web-1   Bound    pvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            rook-ceph-block   4m14s
www-web-2   Bound    pvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            rook-ceph-block   2m25s

[root@k8s-master01 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS      REASON   AGE
pvc-27cab5bf-f989-4050-aa84-1b2dac9fa745   1Gi        RWO            Delete           Bound    default/www-web-0   rook-ceph-block            4m25s
pvc-6b858cd9-288f-48bc-bc96-33e6eb519613   1Gi        RWO            Delete           Bound    default/www-web-2   rook-ceph-block            2m27s
pvc-76fb08f4-2195-4678-b6b8-286c2f722cc9   1Gi        RWO            Delete           Bound    default/www-web-1   rook-ceph-block            4m16s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-å…±äº«æ–‡ä»¶ç³»ç»Ÿçš„ä½¿ç”¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-å…±äº«æ–‡ä»¶ç³»ç»Ÿçš„ä½¿ç”¨&#34;&gt;#&lt;/a&gt; 8. å…±äº«æ–‡ä»¶ç³»ç»Ÿçš„ä½¿ç”¨&lt;/h4&gt;
&lt;p&gt;å…±äº«æ–‡ä»¶ç³»ç»Ÿä¸€èˆ¬ç”¨äºå¤šä¸ª Pod å…±äº«ä¸€ä¸ªå­˜å‚¨&lt;/p&gt;
&lt;h5 id=&#34;81-åˆ›å»ºå…±äº«ç±»å‹çš„æ–‡ä»¶ç³»ç»Ÿ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#81-åˆ›å»ºå…±äº«ç±»å‹çš„æ–‡ä»¶ç³»ç»Ÿ&#34;&gt;#&lt;/a&gt; 8.1 åˆ›å»ºå…±äº«ç±»å‹çš„æ–‡ä»¶ç³»ç»Ÿ&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd /root/rook/deploy/examples/
[root@k8s-master01 examples]# kubectl apply -f filesystem.yaml
[root@k8s-master01 examples]# kubectl get pod -l app=rook-ceph-mds -n rook-ceph
NAME                                    READY   STATUS    RESTARTS   AGE
rook-ceph-mds-myfs-a-7d76cb5988-9nz9p   2/2     Running   0          36s
rook-ceph-mds-myfs-b-76ff7c784c-vs8nm   2/2     Running   0          33s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;82-åˆ›å»ºå…±äº«ç±»å‹æ–‡ä»¶ç³»ç»Ÿçš„-storageclass&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#82-åˆ›å»ºå…±äº«ç±»å‹æ–‡ä»¶ç³»ç»Ÿçš„-storageclass&#34;&gt;#&lt;/a&gt; 8.2 åˆ›å»ºå…±äº«ç±»å‹æ–‡ä»¶ç³»ç»Ÿçš„ StorageClass&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# cd csi/cephfs
[root@k8s-master01 cephfs]# kubectl create -f storageclass.yaml
[root@k8s-master01 cephfs]# kubectl get sc
NAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  17h
rook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   82m
rook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   13s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;83-æŒ‚è½½æµ‹è¯•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#83-æŒ‚è½½æµ‹è¯•&#34;&gt;#&lt;/a&gt; 8.3 æŒ‚è½½æµ‹è¯•&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat cephfs-pvc-deploy.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  selector:
    app: nginx
  type: ClusterIP
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-share-pvc
spec:
  storageClassName: rook-cephfs 
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
---
apiVersion: apps/v1
kind: Deployment 
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      containers:
      - name: nginx
        image: nginx 
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
      volumes:
        - name: www
          persistentVolumeClaim:
            claimName: nginx-share-pvc
			
[root@k8s-master01 ~]# kubectl apply -f cephfs-pvc-deploy.yaml
[root@k8s-master01 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS        AGE
cluster-test-84dfc9c68b-5q4ng             1/1     Running   84 (4m2s ago)   16d
nfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (123m ago)    18h
web-6c59f8559-g5xzb                       1/1     Running   0               46s
web-6c59f8559-ns77q                       1/1     Running   0               46s
web-6c59f8559-qxb5f                       1/1     Running   0               46s

[root@k8s-master01 ~]# kubectl get pvc
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    52s

[root@k8s-master01 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             53s

[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash
root@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# echo &amp;quot;hello cephfs&amp;quot; &amp;gt;&amp;gt; index.html

[root@k8s-master01 ~]# kubectl get svc
NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
kubernetes           ClusterIP   10.96.0.1     &amp;lt;none&amp;gt;        443/TCP    16d
mysql-svc-external   ClusterIP   None          &amp;lt;none&amp;gt;        3306/TCP   9d
nginx                ClusterIP   10.96.58.17   &amp;lt;none&amp;gt;        80/TCP     4m34s
[root@k8s-master01 ~]# curl 10.96.58.17
hello cephfs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9pvc-æ‰©å®¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9pvc-æ‰©å®¹&#34;&gt;#&lt;/a&gt; 9.PVC æ‰©å®¹&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get sc
NAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage       nfzl.com/nfs                    Delete          Immediate           false                  18h
rook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   104m     #trueå…è®¸æ‰©å®¹
rook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   22m      #trueå…è®¸æ‰©å®¹

[root@k8s-master01 ~]# kubectl get pvc
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   2Gi        RWX            rook-cephfs    13m
[root@k8s-master01 ~]# kubectl edit pvc nginx-share-pvc
...
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi         #æ›´æ”¹pvcå¤§å°
  storageClassName: rook-cephfs
...

[root@k8s-master01 ~]# kubectl get pvc       #æŸ¥çœ‹PVCæ˜¯å¦æ‰©å®¹
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    15m

[root@k8s-master01 ~]# kubectl get pv         #æŸ¥çœ‹PVæ˜¯å¦æ‰©å®¹
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            Delete           Bound    default/nginx-share-pvc   rook-cephfs             15m

[root@k8s-master01 ~]# kubectl exec -it web-6c59f8559-g5xzb -- bash       #è¿›å…¥å®¹å™¨ï¼ŒæŸ¥çœ‹podæ˜¯å¦æ‰©å®¹  
root@web-6c59f8559-g5xzb:/# df -h 
Filesystem                                                                                                                                             Size  Used Avail Use% Mounted on
overlay                                                                                                                                                 17G   13G  4.1G  76% /
tmpfs                                                                                                                                                   64M     0   64M   0% /dev
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/sda3                                                                                                                                               17G   13G  4.1G  76% /etc/hosts
shm                                                                                                                                                     64M     0   64M   0% /dev/shm
10.96.121.140:6789,10.96.131.130:6789,10.96.62.64:6789:/volumes/csi/csi-vol-3b645a11-58f4-475a-9404-5d84964f5291/e4bdf743-eb18-42c8-b04f-41964f76de4f  5.0G     0  5.0G   0% /usr/share/nginx/html
tmpfs                                                                                                                                                  3.8G   12K  3.8G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/asound
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/acpi
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /proc/scsi
tmpfs                                                                                                                                                  2.0G     0  2.0G   0% /sys/firmware
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;10-pvc-å¿«ç…§&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-pvc-å¿«ç…§&#34;&gt;#&lt;/a&gt; 10. PVC å¿«ç…§&lt;/h4&gt;
&lt;h5 id=&#34;101-æ–‡ä»¶å…±äº«ç±»å‹å¿«ç…§&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#101-æ–‡ä»¶å…±äº«ç±»å‹å¿«ç…§&#34;&gt;#&lt;/a&gt; 10.1 æ–‡ä»¶å…±äº«ç±»å‹å¿«ç…§&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cd rook/deploy/examples
[root@k8s-master01 examples]# kubectl create -f csi/cephfs/snapshotclass.yaml 

[root@k8s-master01 examples]# kubectl get volumesnapshotclass
NAME                         DRIVER                          DELETIONPOLICY   AGE
csi-cephfsplugin-snapclass   rook-ceph.cephfs.csi.ceph.com   Delete           25s


#æ‹æ‘„å¿«ç…§	
[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash         #pvcæ–°å¢æ•°æ®
root@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# touch &amp;#123;1..10&amp;#125;
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls
1  10  2  3  4	5  6  7  8  9  index.html

[root@k8s-master01 examples]# kubectl get pvc       #æŸ¥çœ‹pvså¹¶å¯¹nginx-share-pvcæ‹æ‘„å¿«ç…§
NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-share-pvc   Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h23m

[root@k8s-master01 examples]# cat csi/cephfs/snapshot.yaml         #æ‹æ‘„å¿«ç…§
---
# 1.17 &amp;lt;= K8s &amp;lt;= v1.19
# apiVersion: snapshot.storage.k8s.io/v1beta1
# K8s &amp;gt;= v1.20
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: cephfs-pvc-snapshot
spec:
  volumeSnapshotClassName: csi-cephfsplugin-snapclass
  source:
    persistentVolumeClaimName: nginx-share-pvc         #åŸºäºé‚£ä¸ªPVCæ‹æ‘„å¿«ç…§
	
[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/snapshot.yaml
[root@k8s-master01 examples]# kubectl get volumesnapshot
NAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE
cephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   4m6s           4m8s

#åˆ é™¤pvcæ•°æ®
[root@k8s-master01 examples]# kubectl exec -it web-6c59f8559-g5xzb -- bash
root@web-6c59f8559-g5xzb:/# cd /usr/share/nginx/html/
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls
1  10  2  3  4	5  6  7  8  9  index.html
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# rm -rf &amp;#123;1..10&amp;#125;
root@web-6c59f8559-g5xzb:/usr/share/nginx/html# ls
index.html

#pvcå›æ»šæ•°æ®
[root@k8s-master01 examples]# kubectl get volumesnapshot
NAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE
cephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   7m39s          7m41s
	
[root@k8s-master01 examples]# cat csi/cephfs/pvc-restore.yaml 
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc-restore
spec:
  storageClassName: rook-cephfs       #åˆ›å»ºpvçš„storageclassåç§°ç›¸åŒ
  dataSource:
    name: cephfs-pvc-snapshot         #volumesnapshotæ•°æ®æº
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi      #å¤§å°ç­‰äºsnapshotå¤§å°

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-restore.yaml

[root@k8s-master01 examples]# kubectl get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    54s          
nginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    4h50m

#æŒ‚è½½PVCæµ‹è¯•æ•°æ®æ˜¯å¦æ¢å¤
[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: csicephfs-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: cephfs-pvc-restore        #æŒ‚è½½æ¢å¤pvc
        readOnly: false

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml
[root@k8s-master01 examples]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS        AGE
cluster-test-84dfc9c68b-5q4ng             1/1     Running   88 (57m ago)    16d
csicephfs-demo-pod                        1/1     Running   0               24s
nfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (6h57m ago)   23h
web-6c59f8559-g5xzb                       1/1     Running   0               4h54m
web-6c59f8559-ns77q                       1/1     Running   0               4h54m
web-6c59f8559-qxb5f                       1/1     Running   0               4h54m
[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash
root@csicephfs-demo-pod:/# ls /var/lib/www/html/               #såˆ é™¤æ•°æ®å·²ç»æ¢å¤
1  10  2  3  4	5  6  7  8  9  index.html
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;102-pvc-å…‹éš†&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#102-pvc-å…‹éš†&#34;&gt;#&lt;/a&gt; 10.2 PVC å…‹éš†&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 examples]# kubectl get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    11m
nginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h1m


[root@k8s-master01 examples]# cat csi/cephfs/pvc-clone.yaml 
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc-clone
spec:
  storageClassName: rook-cephfs      # pvc çš„ storageClass åç§°
  dataSource:
    name: nginx-share-pvc          #å…‹éš†çš„PVCåç§°
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi                 #å¤§å°ç­‰äºæ‰€å…‹éš†çš„PVCå¤§å°

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pvc-clone.yaml

[root@k8s-master01 examples]# kubectl get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
cephfs-pvc-clone     Bound    pvc-0a19b65e-cb5e-4379-a7f7-e0783fcf8ddf   5Gi        RWX            rook-cephfs    22s
cephfs-pvc-restore   Bound    pvc-9e845f2b-df1f-450d-8aa2-f9a46db6adb6   5Gi        RWX            rook-cephfs    15m
nginx-share-pvc      Bound    pvc-4de733fe-c2fb-437b-baff-aaeba0235d54   5Gi        RWX            rook-cephfs    5h4m

#æŒ‚è½½å…‹éš†PVCæµ‹è¯•
[root@k8s-master01 examples]# cat csi/cephfs/pod.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: csicephfs-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: cephfs-pvc-clone      #æŒ‚è½½å…‹éš†çš„pvc
        readOnly: false

[root@k8s-master01 examples]# kubectl apply -f csi/cephfs/pod.yaml          
[root@k8s-master01 examples]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS         AGE
cluster-test-84dfc9c68b-5q4ng             1/1     Running   89 (9m54s ago)   16d
csicephfs-demo-pod                        1/1     Running   0                17s
nfs-client-provisioner-5dbbd8d796-lhdgw   1/1     Running   5 (7h9m ago)     23h
web-6c59f8559-g5xzb                       1/1     Running   0                5h6m
web-6c59f8559-ns77q                       1/1     Running   0                5h6m
web-6c59f8559-qxb5f                       1/1     Running   0                5h6m

[root@k8s-master01 examples]# kubectl exec -it csicephfs-demo-pod -- bash
root@csicephfs-demo-pod:/# cat /var/lib/www/html/index.html 
hello cephfs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11-æµ‹è¯•æ•°æ®æ¸…ç†&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-æµ‹è¯•æ•°æ®æ¸…ç†&#34;&gt;#&lt;/a&gt; 11. æµ‹è¯•æ•°æ®æ¸…ç†&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;å‚è€ƒæ–‡æ¡£ï¼šhttps://rook.io/docs/rook/v1.11/Getting-Started/ceph-teardown/#delete-the-cephcluster-crd
[root@k8s-master01 ~]# kubectl delete deploy web

[root@k8s-master01 ~]# kubectl delete pods csicephfs-demo-pod

[root@k8s-master01 ~]# kubectl delete pvc --all
[root@k8s-master01 ~]# kubectl get pvc
No resources found in default namespace.
[root@k8s-master01 ~]# kubectl get pv
No resources found


[root@k8s-master01 ~]# kubectl get volumesnapshot
NAME                  READYTOUSE   SOURCEPVC         SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS                SNAPSHOTCONTENT                                    CREATIONTIME   AGE
cephfs-pvc-snapshot   true         nginx-share-pvc                           5Gi           csi-cephfsplugin-snapclass   snapcontent-bdaddb97-debe-4f42-9423-13bf1c5b5402   61m            61m
[root@k8s-master01 ~]# kubectl delete volumesnapshot cephfs-pvc-snapshot
volumesnapshot.snapshot.storage.k8s.io &amp;quot;cephfs-pvc-snapshot&amp;quot; deleted

kubectl delete -n rook-ceph cephblockpool replicapool
kubectl delete -n rook-ceph cephfilesystem myfs

kubectl delete storageclass rook-ceph-block
kubectl delete storageclass rook-cephfs
kubectl delete -f csi/cephfs/kube-registry.yaml
kubectl delete storageclass csi-cephfs

kubectl -n rook-ceph delete cephcluster rook-ceph

kubectl delete -f operator.yaml
kubectl delete -f common.yaml
kubectl delete -f crds.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-24T13:43:19.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3890389502.html</id>
        <title>K8SæŒä¹…åŒ–å­˜å‚¨NFS+StorageClass</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3890389502.html"/>
        <content type="html">&lt;h3 id=&#34;k8sæŒä¹…åŒ–å­˜å‚¨nfsstorageclass&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sæŒä¹…åŒ–å­˜å‚¨nfsstorageclass&#34;&gt;#&lt;/a&gt; K8S æŒä¹…åŒ–å­˜å‚¨ NFS+StorageClass&lt;/h3&gt;
&lt;h4 id=&#34;1-æ­å»ºnfsæœåŠ¡å™¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-æ­å»ºnfsæœåŠ¡å™¨&#34;&gt;#&lt;/a&gt; 1. æ­å»º NFS æœåŠ¡å™¨&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#æ‰€æœ‰K8SèŠ‚ç‚¹å®‰è£…nfs-utils
[root@k8s-node02 ~]# yum install nfs-utils -y    

#K8S-node02èŠ‚ç‚¹é…ç½®nfsæœåŠ¡
[root@k8s-node02 ~]# mkdir /data/nfs -p
[root@k8s-node02 ~]# cat /etc/exports
/data/nfs 192.168.1.0/24(rw,no_root_squash)
[root@k8s-node02 ~]# exportfs -arv   #NFSé…ç½®ç”Ÿæ•ˆ 
[root@k8s-node02 ~]# systemctl start nfs-server &amp;amp;&amp;amp; systemctl enable nfs-server &amp;amp;&amp;amp; systemctl status nfs-server
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-åˆ›å»ºrbac&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-åˆ›å»ºrbac&#34;&gt;#&lt;/a&gt; 2.  åˆ›å»º RBAC&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-node02 ~]# cat 01-rbac.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;nodes&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;persistentvolumes&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;delete&amp;quot;]
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;persistentvolumeclaims&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;update&amp;quot;]
  - apiGroups: [&amp;quot;storage.k8s.io&amp;quot;]
    resources: [&amp;quot;storageclasses&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;]
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;events&amp;quot;]
    verbs: [&amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;patch&amp;quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [&amp;quot;&amp;quot;]
    resources: [&amp;quot;endpoints&amp;quot;]
    verbs: [&amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;patch&amp;quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
  
  
[root@k8s-master01 ~]# kubectl apply -f 01-rbac.yaml 
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-åˆ›å»ºnfs-provisioner&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-åˆ›å»ºnfs-provisioner&#34;&gt;#&lt;/a&gt; 3. åˆ›å»º nfs-provisioner&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 02-nfs-provisioner.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.cn-hangzhou.aliyuncs.com/old_xu/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME	# nfs-provisionerçš„åç§°ï¼Œåç»­storageClassè¦ä¸è¯¥åç§°ä¸€è‡´
              value: nfzl.com/nfs
            - name: NFS_SERVER		# NFSæœåŠ¡çš„IPåœ°å€
              value: 192.168.1.75
            - name: NFS_PATH		# NFSæœåŠ¡å…±äº«çš„è·¯å¾„
              value: /data/nfs
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.1.75
            path: /data/nfs

[root@k8s-master01 ~]# kubectl apply -f 02-nfs-provisioner.yaml 
[root@k8s-master01 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-6bcc4587f8-zp8qc   1/1     Running   0          17s
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-åˆ›å»ºstorageclass&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-åˆ›å»ºstorageclass&#34;&gt;#&lt;/a&gt; 4. åˆ›å»º StorageClass&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 03-storageClass.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage 	# pvcç”³è¯·æ—¶éœ€æ˜ç¡®æŒ‡å®šçš„storageClassåç§°
provisioner: nfzl.com/nfs        # ä¾›åº”å•†åç§°ï¼Œå¿…é¡»å’Œä¸Šé¢åˆ›å»ºçš„&amp;quot;PROVISIONER_NAME&amp;quot;å˜é‡å€¼è‡´
parameters:
  archiveOnDelete: &amp;quot;false&amp;quot;     # å¦‚æœå€¼ä¸ºfalseï¼Œåˆ é™¤PVCåä¹Ÿä¼šåˆ é™¤ç›®å½•å†…å®¹, &amp;quot;true&amp;quot;åˆ™ä¼šå¯¹æ•°æ®è¿›è¡Œä¿ç•™
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-åˆ›å»ºpvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-åˆ›å»ºpvc&#34;&gt;#&lt;/a&gt; 5. åˆ›å»º PVC&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 04-nginx-pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sc-pvc-001
spec:
  storageClassName: &amp;quot;nfs-storage&amp;quot;     # æ˜ç¡®æŒ‡å®šä½¿ç”¨å“ªä¸ªscçš„ä¾›åº”å•†æ¥åˆ›å»ºpv
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi                      # æ ¹æ®ä¸šåŠ¡å®é™…å¤§å°è¿›è¡Œèµ„æºç”³è¯·
      
[root@k8s-master01 ~]# kubectl apply -f 04-nginx-pvc.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/fgpaP15.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;6-æŒ‚è½½pvcæµ‹è¯•&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-æŒ‚è½½pvcæµ‹è¯•&#34;&gt;#&lt;/a&gt; 6. æŒ‚è½½ PVC æµ‹è¯•&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat 05-nginx-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-sc-001
spec:
  containers:
  - name: nginx-sc-001
    image: nginx
    volumeMounts:
    - name: nginx-page
      mountPath: /usr/share/nginx/html
  volumes:
  - name: nginx-page
    persistentVolumeClaim:      
      claimName: sc-pvc-001

[root@k8s-master01 ~]# kubectl apply -f 05-nginx-pod.yaml
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                                      READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
nginx-sc-001                              1/1     Running   0          15s   172.16.85.244   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

[root@k8s-master01 ~]# curl 172.16.85.244
hello world
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-23T12:08:26.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/722512536.html</id>
        <title>K8sç»†ç²’åº¦æƒé™æ§åˆ¶RBAC</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/722512536.html"/>
        <content type="html">&lt;h3 id=&#34;k8sç»†ç²’åº¦æƒé™æ§åˆ¶rbac&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sç»†ç²’åº¦æƒé™æ§åˆ¶rbac&#34;&gt;#&lt;/a&gt; K8s ç»†ç²’åº¦æƒé™æ§åˆ¶ RBAC&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/KCZPPkv.jpeg&#34; alt=&#34;rbac.jpg&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;1-åˆ›å»ºä¸åŒæƒé™çš„clusterrole&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-åˆ›å»ºä¸åŒæƒé™çš„clusterrole&#34;&gt;#&lt;/a&gt; 1. åˆ›å»ºä¸åŒæƒé™çš„ clusterrole&lt;/h4&gt;
&lt;h5 id=&#34;11-å‘½ä»¤ç©ºé—´åªè¯»namespace-readonly&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-å‘½ä»¤ç©ºé—´åªè¯»namespace-readonly&#34;&gt;#&lt;/a&gt; 1.1 å‘½ä»¤ç©ºé—´åªè¯» namespace-readonly&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat namespace-readonly.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: namespace-readonly
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-èµ„æºæŸ¥çœ‹resource-readonly&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-èµ„æºæŸ¥çœ‹resource-readonly&#34;&gt;#&lt;/a&gt; 1.2 èµ„æºæŸ¥çœ‹ resource-readonly&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat resource-readonly.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: resource-readonly
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  - daemonsets
  - deployments
  - deployments/scale
  - replicasets
  - replicasets/scale
  - statefulsets
  - statefulsets/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - deployments/scale
  - ingresses
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-podæ—¥å¿—æŸ¥çœ‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-podæ—¥å¿—æŸ¥çœ‹&#34;&gt;#&lt;/a&gt; 1.3 pod æ—¥å¿—æŸ¥çœ‹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat pod-log.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-log
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods
  - pods/log
  verbs:
  - get
  - list
  - watch
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-podåˆ é™¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-podåˆ é™¤&#34;&gt;#&lt;/a&gt; 1.4 Pod åˆ é™¤&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat pod-delete.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-delete
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods
  verbs:
  - get
  - list
  - delete
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-podæ‰§è¡Œ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-podæ‰§è¡Œ&#34;&gt;#&lt;/a&gt; 1.5 Pod æ‰§è¡Œ&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat pod-exec.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-exec
rules:
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods
  verbs:
  - get
  - list
- apiGroups:
  - &amp;quot;&amp;quot;
  resources:
  - pods/exec
  verbs:
  - create
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;16-åˆ›å»ºä¸åŒæƒé™çš„clusterrole&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-åˆ›å»ºä¸åŒæƒé™çš„clusterrole&#34;&gt;#&lt;/a&gt; 1.6 åˆ›å»ºä¸åŒæƒé™çš„ clusterrole&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 rbac]# kubectl apply -f .
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-åˆ›å»ºserviceaccount&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-åˆ›å»ºserviceaccount&#34;&gt;#&lt;/a&gt; 2. åˆ›å»º serviceaccount&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create ns kube-users

# kubectl create sa test -n kube-users   
# kubectl create sa dev -n kube-users    
# kubectl create sa ops -n kube-users    

# kubectl create token test -n kube-users
# kubectl create token dev -n kube-users
# kubectl create token ops -n kube-users
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-åˆ›å»ºclusterrolebinding&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-åˆ›å»ºclusterrolebinding&#34;&gt;#&lt;/a&gt; 3. åˆ›å»º ClusterRoleBinding&lt;/h4&gt;
&lt;h5 id=&#34;31-ç»‘å®šå…¨å±€å‘½åç©ºé—´æŸ¥çœ‹æƒé™&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-ç»‘å®šå…¨å±€å‘½åç©ºé—´æŸ¥çœ‹æƒé™&#34;&gt;#&lt;/a&gt; 3.1 ç»‘å®šå…¨å±€å‘½åç©ºé—´æŸ¥çœ‹æƒé™&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat clusterrolebinding-namespace-readonly.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: clusterrolebinding-namespace-readonly 
subjects:
- kind: Group
  name: system:serviceaccounts:kube-users
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: namespace-readonly
  apiGroup: rbac.authorization.k8s.io
  
# kubectl apply -f clusterrolebinding-namespace-readonly.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;32-ç»‘å®šæ—¥å¿—æŸ¥çœ‹æƒé™&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-ç»‘å®šæ—¥å¿—æŸ¥çœ‹æƒé™&#34;&gt;#&lt;/a&gt; 3.2 ç»‘å®šæ—¥å¿—æŸ¥çœ‹æƒé™&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-pod-log --clusterrole=pod-log --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-ç»‘å®šèµ„æºæŸ¥çœ‹æƒé™&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-ç»‘å®šèµ„æºæŸ¥çœ‹æƒé™&#34;&gt;#&lt;/a&gt; 3.3 ç»‘å®šèµ„æºæŸ¥çœ‹æƒé™&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-resource-readonly --clusterrole=resource-readonly --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;34-ç»‘å®špodæ‰§è¡Œæƒé™&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-ç»‘å®špodæ‰§è¡Œæƒé™&#34;&gt;#&lt;/a&gt; 3.4 ç»‘å®š Pod æ‰§è¡Œæƒé™&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-pod-exec --clusterrole=pod-exec --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;35-ç»‘å®špodåˆ é™¤æƒé™&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#35-ç»‘å®špodåˆ é™¤æƒé™&#34;&gt;#&lt;/a&gt; 3.5 ç»‘å®š Pod åˆ é™¤æƒé™&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectA
# kubectl create rolebinding ops-pod-delete --clusterrole=pod-delete --serviceaccount=kube-users:ops --namespace=projectB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-23T12:04:03.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/176412055.html</id>
        <title>K8så‡†å…¥æ§åˆ¶ResourceQuotaã€LimitRangeã€QoSæœåŠ¡è´¨é‡</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/176412055.html"/>
        <content type="html">&lt;h3 id=&#34;k8så‡†å…¥æ§åˆ¶resourcequota-limitrange-qosæœåŠ¡è´¨é‡&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8så‡†å…¥æ§åˆ¶resourcequota-limitrange-qosæœåŠ¡è´¨é‡&#34;&gt;#&lt;/a&gt; K8s å‡†å…¥æ§åˆ¶ ResourceQuotaã€LimitRangeã€QoS æœåŠ¡è´¨é‡&lt;/h3&gt;
&lt;h4 id=&#34;1-resourcequotaé…ç½®è§£æ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-resourcequotaé…ç½®è§£æ&#34;&gt;#&lt;/a&gt; 1. ResourceQuota é…ç½®è§£æ&lt;/h4&gt;
&lt;p&gt;ResourceQuotas å®ç°èµ„æºé…é¢ï¼Œé¿å…è¿‡åº¦åˆ›å»ºèµ„æºï¼Œé’ˆå¯¹ namespace è¿›è¡Œé™åˆ¶ã€‚cpu å†…å­˜åˆ™æ˜¯æ ¹æ® pod é…ç½®çš„ resources æ€»é¢è¿›è¡Œé™åˆ¶ï¼Œå¦‚æœæ²¡æœ‰é…ç½® resources å‚æ•°åˆ™æ— æ³•é™åˆ¶ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ResourceQuota
metadata:
  name: resourcequota-test
  namespace: test
  labels:
    app: resourcequota
spec:
  hard:
    pods: 3
    requests.cpu: 3
    requests.memory: 512Mi
    limits.cpu: 8
    limits.memory: 16Gi
    configmaps: 201
    requests.storage: 40Gi
    persistentvolumeclaims: 20
    replicationcontrollers: 20
    secrets: 20
    services: 50
    services.loadbalancers: &amp;quot;2&amp;quot;
    services.nodeports: &amp;quot;10&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;podsï¼šé™åˆ¶æœ€å¤šå¯åŠ¨ Pod çš„ä¸ªæ•°&lt;/li&gt;
&lt;li&gt;requests.cpuï¼šé™åˆ¶æœ€é«˜ CPU è¯·æ±‚æ•°&lt;/li&gt;
&lt;li&gt;requests.memoryï¼šé™åˆ¶æœ€é«˜å†…å­˜çš„è¯·æ±‚æ•°&lt;/li&gt;
&lt;li&gt;limits.cpuï¼šé™åˆ¶æœ€é«˜ CPU çš„ limit ä¸Šé™&lt;/li&gt;
&lt;li&gt;limits.memoryï¼šé™åˆ¶æœ€é«˜å†…å­˜çš„ limit ä¸Šé™&lt;/li&gt;
&lt;li&gt;servicesï¼šé™åˆ¶ services æ•°é‡&lt;/li&gt;
&lt;li&gt;services.nodeportsï¼šé™åˆ¶ services ä¸­ nodeport ç±»å‹ service æ•°é‡&lt;/li&gt;
&lt;li&gt;services.loadbalancersï¼šé™åˆ¶ services ä¸­ loadbalancers ç±»å‹ service æ•°é‡&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-resourcequotaé…ç½®ç¤ºä¾‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-resourcequotaé…ç½®ç¤ºä¾‹&#34;&gt;#&lt;/a&gt; 1.1 ResourceQuota é…ç½®ç¤ºä¾‹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.é™åˆ¶testå‘½åç©ºé—´podsæ•°é‡é‡ä¸º3ã€configmapæ•°é‡ä¸º2
[root@k8s-master01 resourcequota]# cat rq-test.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resourcequota-test
  namespace: test
  labels:
    app: resourcequota
spec:
  hard:
    pods: 3
#    requests.cpu: 3
#    requests.memory: 512Mi
#    limits.cpu: 8
#    limits.memory: 16Gi
    configmaps: 2
#    requests.storage: 40Gi
#    persistentvolumeclaims: 20
#    replicationcontrollers: 20
#    secrets: 20
#    services: 50
#    services.loadbalancers: &amp;quot;2&amp;quot;
#    services.nodeports: &amp;quot;10&amp;quot;

#2.testå‘½åç©ºé—´å·²åˆ›å»ºconfigmapæ•°é‡ä¸º1,é™åˆ¶æ•°é‡ä¸º2
[root@k8s-master01 resourcequota]# kubectl get resourcequota -n test
NAME                 AGE   REQUEST                      LIMIT
resourcequota-test   61s   configmaps: 1/2, pods: 0/3  

#3.testå‘½åç©ºé—´åˆ›å»ºç¬¬2ä¸ªconfigmapæ—¶æ­£å¸¸ï¼Œåˆ›å»ºç¬¬3ä¸ªconfigmapæ—¶æŠ¥é”™
[root@k8s-master01 resourcequota]# kubectl create cm rq-cm1 -n test --from-literal=key1=value1
[root@k8s-master01 resourcequota]# kubectl create cm rq-cm2 -n test --from-literal=key2=value2
error: failed to create configmap: configmaps &amp;quot;rq-cm2&amp;quot; is forbidden: exceeded quota: resourcequota-test, requested: configmaps=1, used: configmaps=2, limited: configmaps=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-limitrangeé…ç½®è§£æ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-limitrangeé…ç½®è§£æ&#34;&gt;#&lt;/a&gt; 2. LimitRange é…ç½®è§£æ&lt;/h4&gt;
&lt;p&gt;è™½ç„¶ ResourceQuota å¯ä»¥å®ç°èµ„æºé…é¢ï¼Œå¯ä»¥é™åˆ¶æŸä¸ªå‘½åç©ºé—´å†…å­˜å’Œ CPUï¼Œä½†æ˜¯å¦‚æœåˆ›å»ºçš„ Pod éƒ½æ²¡æœ‰é…ç½® resources å‚æ•°åˆ™æ— æ³•é™åˆ¶ã€‚å¦‚æœé…ç½® LimitRangeï¼ŒPod æ²¡æœ‰é…ç½® resources æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„ Pod ä¼šæ ¹æ® LimitRange é…ç½®è‡ªåŠ¨æ·»åŠ  CPU å†…å­˜é…ç½®ï¼Œå¹¶ä¸”å¯ä»¥é™åˆ¶ resources å‚æ•°æœ€å¤§é…ç½®å’Œæœ€å°é…ç½®ï¼ŒLimitRange é’ˆå¯¹ Pod è¿›è¡Œé™åˆ¶ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #é™åˆ¶CPUå†…å­˜é»˜è®¤limitsé…ç½®
      cpu: 1
      memory: 512Mi
    defaultRequest:  #é™åˆ¶CPUå†…å­˜é»˜è®¤requesté…ç½®
      cpu: 0.5
      memory: 256Mi
    max:                #é™åˆ¶CPUå†…å­˜æœ€å¤§é…ç½® 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #é™åˆ¶CPUå†…å­˜æœ€å°é…ç½®
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #é™åˆ¶pvcå¤§å°
    max:
      storage: 2Gi
    min:
      storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;defaultï¼šé»˜è®¤ limits é…ç½®&lt;/li&gt;
&lt;li&gt;defaultRequestï¼šé»˜è®¤ requests é…ç½®&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;21-é…ç½®é»˜è®¤çš„requestså’Œlimits&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-é…ç½®é»˜è®¤çš„requestså’Œlimits&#34;&gt;#&lt;/a&gt; 2.1 é…ç½®é»˜è®¤çš„ requests å’Œ limits&lt;/h5&gt;
&lt;p&gt;Pod æ²¡æœ‰é…ç½® resources æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„ Pod ä¼šæ ¹æ® LimitRange é…ç½®è‡ªåŠ¨æ·»åŠ  CPU å†…å­˜é…ç½®ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.åˆ›å»ºLimitRange
[root@k8s-master01 resourcequota]# cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #é™åˆ¶CPUå†…å­˜é»˜è®¤limitsé…ç½®
      cpu: 1
      memory: 512Mi
    defaultRequest:  #é™åˆ¶CPUå†…å­˜é»˜è®¤requesté…ç½®
      cpu: 0.5
      memory: 256Mi
    max:                #é™åˆ¶CPUå†…å­˜æœ€å¤§é…ç½® 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #é™åˆ¶CPUå†…å­˜æœ€å°é…ç½®
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #é™åˆ¶pvcå¤§å°
    max:
      storage: 2Gi
    min:
      storage: 1Gi  
      
[root@k8s-master01 resourcequota]# kubectl apply -f limitrange.yaml
[root@k8s-master01 resourcequota]# kubectl get limitrange -n test
NAME                  CREATED AT
cpu-mem-limit-range   2025-04-23T07:55:03Z

#2.åˆ›å»ºdeployment, æŸ¥çœ‹æ˜¯å¦ä¼šæ ¹æ®LimitRangeè‡ªåŠ¨æ·»åŠ CPUå†…å­˜é…ç½®
[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-limirange
  labels:
    app: deploy-limirange
  namespace: test
spec:
  selector:
    matchLabels:
      app: deploy-limirange
  replicas: 1
  template:
    metadata:
      labels:
        app: deploy-limirange
    spec:
      restartPolicy: Always
      containers:
        - name: deploy-limirange
          image: nginx
          imagePullPolicy: IfNotPresent

[root@k8s-master01 resourcequota]# kubectl get pod -n test
NAME                                READY   STATUS    RESTARTS   AGE
deploy-limirange-854c9545ff-grpxr   1/1     Running   0          39s
[root@k8s-master01 resourcequota]# kubectl get pod -n test -oyaml
...
  spec:
    containers:
    - image: nginx
      imagePullPolicy: IfNotPresent
      name: deploy-limirange
      resources:
        limits:
          cpu: &amp;quot;1&amp;quot;
          memory: 512Mi
        requests:
          cpu: 500m
          memory: 256Mi
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-é™åˆ¶requestså’ŒlimitsèŒƒå›´&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-é™åˆ¶requestså’ŒlimitsèŒƒå›´&#34;&gt;#&lt;/a&gt; 2.2 é™åˆ¶ requests å’Œ limits èŒƒå›´&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.åˆ›å»ºLimitRange
[root@k8s-master01 resourcequota]# cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #é™åˆ¶CPUå†…å­˜é»˜è®¤limitsé…ç½®
      cpu: 1
      memory: 512Mi
    defaultRequest:  #é™åˆ¶CPUå†…å­˜é»˜è®¤requesté…ç½®
      cpu: 0.5
      memory: 256Mi
    max:                #é™åˆ¶CPUå†…å­˜æœ€å¤§é…ç½® 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #é™åˆ¶CPUå†…å­˜æœ€å°é…ç½®
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #é™åˆ¶pvcå¤§å°
    max:
      storage: 2Gi
    min:
      storage: 1Gi  

#2.åˆ›å»ºdeployment, CPUå†…å­˜limitså’Œrequestsé«˜äº/ä½äºLimitRangeCPUå†…å­˜maxã€miné…ç½®
[root@k8s-master01 resourcequota]# cat deploy-limitrange.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-limirange
  labels:
    app: deploy-limirange
  namespace: test
spec:
  selector:
    matchLabels:
      app: deploy-limirange
  replicas: 1
  template:
    metadata:
      labels:
        app: deploy-limirange
    spec:
      restartPolicy: Always
      containers:
        - name: deploy-limirange
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 8096Mi
              cpu: 5
            requests:
              memory: 64Mi
              cpu: 10m

#3.ç”±äºåˆ›å»ºdeployment, CPUå†…å­˜limitså’Œrequestsé«˜äº/ä½äºLimitRangeCPUå†…å­˜maxã€miné…ç½®ï¼Œpodæ²¡æœ‰åˆ›å»º
[root@k8s-master01 resourcequota]# kubectl create -f deploy-limitrange.yaml 

[root@k8s-master01 resourcequota]# kubectl get deploy deploy-limirange -n test
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
deploy-limirange   0/1     0            0           2m7s
[root@k8s-master01 resourcequota]# kubectl get pods -n test

[root@k8s-master01 resourcequota]# kubectl describe rs deploy-limirange-54c5d69b4b -n test
Name:           deploy-limirange-54c5d69b4b
Namespace:      test
Selector:       app=deploy-limirange,pod-template-hash=54c5d69b4b
Labels:         app=deploy-limirange
                pod-template-hash=54c5d69b4b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/deploy-limirange
Replicas:       0 current / 1 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=deploy-limirange
           pod-template-hash=54c5d69b4b
  Containers:
   deploy-limirange:
    Image:      nginx
    Port:       &amp;lt;none&amp;gt;
    Host Port:  &amp;lt;none&amp;gt;
    Limits:
      cpu:     5
      memory:  8096Mi
    Requests:
      cpu:         10m
      memory:      64Mi
    Environment:   &amp;lt;none&amp;gt;
    Mounts:        &amp;lt;none&amp;gt;
  Volumes:         &amp;lt;none&amp;gt;
  Node-Selectors:  &amp;lt;none&amp;gt;
  Tolerations:     &amp;lt;none&amp;gt;
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate
Events:
  Type     Reason        Age                 From                   Message
  ----     ------        ----                ----                   -------
  Warning  FailedCreate  3m8s                replicaset-controller  Error creating: pods &amp;quot;deploy-limirange-54c5d69b4b-zxhzk&amp;quot; is forbidden: [minimum cpu usage per Container is 100m, but request is 10m, minimum memory usage per Container is 100Mi, but request is 64Mi, maximum cpu usage per Container is 4, but limit is 5, maximum memory usage per Container is 4Gi, but limit is 8096Mi]
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-é™åˆ¶å­˜å‚¨ç©ºé—´å¤§å°&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-é™åˆ¶å­˜å‚¨ç©ºé—´å¤§å°&#34;&gt;#&lt;/a&gt; 2.3 é™åˆ¶å­˜å‚¨ç©ºé—´å¤§å°&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.åˆ›å»ºLimitRange
[root@k8s-master01 resourcequota]# cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-mem-limit-range
  namespace: test
spec:
  limits:
  - default:         #é™åˆ¶CPUå†…å­˜é»˜è®¤limitsé…ç½®
      cpu: 1
      memory: 512Mi
    defaultRequest:  #é™åˆ¶CPUå†…å­˜é»˜è®¤requesté…ç½®
      cpu: 0.5
      memory: 256Mi
    max:                #é™åˆ¶CPUå†…å­˜æœ€å¤§é…ç½® 
      cpu: &amp;quot;4000m&amp;quot;
      memory: 4Gi
    min:                #é™åˆ¶CPUå†…å­˜æœ€å°é…ç½®
      cpu: &amp;quot;100m&amp;quot;
      memory: 100Mi
    type: Container
  - type: PersistentVolumeClaim    #é™åˆ¶pvcå¤§å°
    max:
      storage: 2Gi
    min:
      storage: 1Gi  
  
#2.ç”±äºåˆ›å»ºçš„pvcå¤§äº2Gï¼Œæ‰€ä»¥æŠ¥é”™  
[root@k8s-master01 ~]# cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sc-pvc-001
spec:
  storageClassName: &amp;quot;nfs-storage&amp;quot;     # æ˜ç¡®æŒ‡å®šä½¿ç”¨å“ªä¸ªscçš„ä¾›åº”å•†æ¥åˆ›å»ºpv
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 3Gi                      # æ ¹æ®ä¸šåŠ¡å®é™…å¤§å°è¿›è¡Œèµ„æºç”³è¯·  
[root@k8s-master01 ~]# kubectl create -f pvc.yaml -n test
Error from server (Forbidden): error when creating &amp;quot;pvc.yaml&amp;quot;: persistentvolumeclaims &amp;quot;sc-pvc-001&amp;quot; is forbidden: maximum storage usage per PersistentVolumeClaim is 2Gi, but request is 3Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-æœåŠ¡è´¨é‡-qos&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-æœåŠ¡è´¨é‡-qos&#34;&gt;#&lt;/a&gt; 3. æœåŠ¡è´¨é‡ QoS&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteedï¼šæœ€é«˜æœåŠ¡è´¨é‡ï¼Œå½“å®¿ä¸»æœºå†…å­˜ä¸å¤Ÿæ—¶ï¼Œä¼šå…ˆ kill æ‰ QoS ä¸º BestEffort å’Œ Burstable çš„ Podï¼Œå¦‚æœå†…å­˜è¿˜æ˜¯ä¸å¤Ÿï¼Œæ‰ä¼š kill æ‰ QoS ä¸º Guaranteedï¼Œè¯¥çº§åˆ« Pod çš„èµ„æºå ç”¨é‡ä¸€èˆ¬æ¯”è¾ƒæ˜ç¡®ï¼Œå³ requests çš„ cpu å’Œ memory å’Œ limits çš„ cpu å’Œ memory é…ç½®çš„ä¸€è‡´ã€‚&lt;/li&gt;
&lt;li&gt;Burstableï¼š æœåŠ¡è´¨é‡ä½äº Guaranteedï¼Œå½“å®¿ä¸»æœºå†…å­˜ä¸å¤Ÿæ—¶ï¼Œä¼šå…ˆ kill æ‰ QoS ä¸º BestEffort çš„ Podï¼Œå¦‚æœå†…å­˜è¿˜æ˜¯ä¸å¤Ÿä¹‹åå°±ä¼š kill æ‰ QoS çº§åˆ«ä¸º Burstable çš„ Podï¼Œç”¨æ¥ä¿è¯ QoS è´¨é‡ä¸º Guaranteed çš„ Podï¼Œè¯¥çº§åˆ« Pod ä¸€èˆ¬çŸ¥é“æœ€å°èµ„æºä½¿ç”¨é‡ï¼Œä½†æ˜¯å½“æœºå™¨èµ„æºå……è¶³æ—¶ï¼Œè¿˜æ˜¯æƒ³å°½å¯èƒ½çš„ä½¿ç”¨æ›´å¤šçš„èµ„æºï¼Œå³ limits å­—æ®µçš„ cpu å’Œ memory å¤§äº requests çš„ cpu å’Œ memory çš„é…ç½®ã€‚&lt;/li&gt;
&lt;li&gt;BestEffortï¼šå°½åŠ›è€Œä¸ºï¼Œå½“å®¿ä¸»æœºå†…å­˜ä¸å¤Ÿæ—¶ï¼Œé¦–å…ˆ kill çš„å°±æ˜¯è¯¥ QoS çš„ Podï¼Œç”¨ä»¥ä¿è¯ Burstable å’Œ Guaranteed çº§åˆ«çš„ Pod æ­£å¸¸è¿è¡Œã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;31-å®ç°qosä¸ºguaranteedçš„pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-å®ç°qosä¸ºguaranteedçš„pod&#34;&gt;#&lt;/a&gt; 3.1 å®ç° QoS ä¸º Guaranteed çš„ Pod&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pod ä¸­çš„æ¯ä¸ªå®¹å™¨å¿…é¡»æŒ‡å®š limits.memory å’Œ requests.memoryï¼Œå¹¶ä¸”ä¸¤è€…éœ€è¦ç›¸ç­‰ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pod ä¸­çš„æ¯ä¸ªå®¹å™¨å¿…é¡»æŒ‡å®š limits.cpu å’Œ limits.memoryï¼Œå¹¶ä¸”ä¸¤è€…éœ€è¦ç›¸ç­‰ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 1024Mi
              cpu: 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;32-å®ç°qosä¸ºburstableçš„pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-å®ç°qosä¸ºburstableçš„pod&#34;&gt;#&lt;/a&gt; 3.2 å®ç° QoS ä¸º Burstable çš„ Pod&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pod ä¸ç¬¦åˆ Guaranteed çš„é…ç½®è¦æ±‚ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pod ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå®¹å™¨é…ç½®äº† requests.cpu æˆ– requests.memoryã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-å®ç°qosä¸ºbesteffortçš„pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-å®ç°qosä¸ºbesteffortçš„pod&#34;&gt;#&lt;/a&gt; 3.3 å®ç° QoS ä¸º BestEffort çš„ Pod&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ä¸è®¾ç½® resources å‚æ•°&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-23T11:55:19.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/312010518.html</id>
        <title>K8säº²å’ŒåŠ›Affinity</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/312010518.html"/>
        <content type="html">&lt;h3 id=&#34;k8säº²å’ŒåŠ›affinity&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8säº²å’ŒåŠ›affinity&#34;&gt;#&lt;/a&gt; K8s äº²å’ŒåŠ› Affinity&lt;/h3&gt;
&lt;p&gt;Pod å’ŒèŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŸäº› Pod ä¼˜å…ˆé€‰æ‹©æœ‰ ssd=true æ ‡ç­¾çš„èŠ‚ç‚¹ï¼Œå¦‚æœæ²¡æœ‰åœ¨è€ƒè™‘éƒ¨ç½²åˆ°å…¶å®ƒèŠ‚ç‚¹ï¼›&lt;/li&gt;
&lt;li&gt;æŸäº› Pod éœ€è¦éƒ¨ç½²åœ¨ ssd=true å’Œ type=physical çš„èŠ‚ç‚¹ä¸Šï¼Œä½†æ˜¯ä¼˜å…ˆéƒ¨ç½²åœ¨ ssd=true çš„èŠ‚ç‚¹ä¸Šã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pod å’Œ Pod ä¹‹é—´çš„å…³ç³»ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åŒä¸€ä¸ªåº”ç”¨çš„ Pod ä¸åŒçš„å‰¯æœ¬æˆ–è€…åŒä¸€ä¸ªé¡¹ç›®çš„åº”ç”¨å°½é‡æˆ–å¿…é¡»ä¸éƒ¨ç½²åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹æˆ–è€…ç¬¦åˆæŸä¸ªæ ‡ç­¾çš„ä¸€ç±»èŠ‚ç‚¹ä¸Šæˆ–è€…ä¸åŒçš„åŒºåŸŸï¼›&lt;/li&gt;
&lt;li&gt;ç›¸äº’ä¾èµ–çš„ä¸¤ä¸ª Pod å°½é‡æˆ–å¿…é¡»éƒ¨ç½²åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šæˆ–è€…åŒä¸€ä¸ªåŸŸå†…ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;1-affinityåˆ†ç±»&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-affinityåˆ†ç±»&#34;&gt;#&lt;/a&gt; 1. Affinity åˆ†ç±»&lt;/h4&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/hTd0wmD.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;2-èŠ‚ç‚¹äº²å’ŒåŠ›é…ç½®è¯¦è§£&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-èŠ‚ç‚¹äº²å’ŒåŠ›é…ç½®è¯¦è§£&#34;&gt;#&lt;/a&gt; 2. èŠ‚ç‚¹äº²å’ŒåŠ›é…ç½®è¯¦è§£&lt;/h4&gt;
&lt;h5 id=&#34;21-ç¡¬äº²å’ŒåŠ›required&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-ç¡¬äº²å’ŒåŠ›required&#34;&gt;#&lt;/a&gt; 2.1 ç¡¬äº²å’ŒåŠ› required&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                      - k8s-node01
                      - k8s-node02
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;requiredDuringSchedulingIgnoredDuringExecutionï¼šç¡¬äº²å’ŒåŠ›é…ç½®&lt;/li&gt;
&lt;li&gt;nodeSelectorTermsï¼šèŠ‚ç‚¹é€‰æ‹©å™¨é…ç½®ï¼Œå¯ä»¥é…ç½®å¤šä¸ª matchExpressionsï¼ˆæ»¡è¶³å…¶ä¸€å³å¯ï¼‰&lt;/li&gt;
&lt;li&gt;matchExpressionsï¼šmatchExpressions ä¸‹å¯ä»¥é…ç½®å¤šä¸ª keyã€valuesï¼ˆéƒ½éœ€è¦æ»¡è¶³ï¼‰ï¼Œå…¶ä¸­ values å¯ä»¥é…ç½®å¤šä¸ªï¼ˆæ»¡è¶³å…¶ä¸€å³å¯ï¼‰&lt;/li&gt;
&lt;li&gt;operatorï¼š
&lt;ul&gt;
&lt;li&gt;IN ç›¸å½“äº key = value çš„å½¢å¼ï¼Œ&lt;strong&gt;NotIn ç›¸å½“äº key!=value çš„å½¢å¼ (åäº²å’ŒåŠ›)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Exists: èŠ‚ç‚¹å­˜åœ¨ label çš„ key ä¸ºæŒ‡å®šçš„å€¼å³å¯ï¼Œä¸èƒ½é…ç½® values å­—æ®µ&lt;/li&gt;
&lt;li&gt;DoesNotExist: èŠ‚ç‚¹ä¸å­˜åœ¨ label çš„ key ä¸ºæŒ‡å®šçš„å€¼å³å¯ï¼Œä¸èƒ½é…ç½® values å­—æ®µ&lt;/li&gt;
&lt;li&gt;Gtï¼šå¤§äº value æŒ‡å®šçš„å€¼&lt;/li&gt;
&lt;li&gt;Ltï¼šå°äº value æŒ‡å®šçš„å€¼&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;22-è½¯äº²å’ŒåŠ›preferred&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-è½¯äº²å’ŒåŠ›preferred&#34;&gt;#&lt;/a&gt; 2.2 è½¯äº²å’ŒåŠ› preferred&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 6
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: ssd
                    operator: In
                    values:
                      - &#39;true&#39;
            - weight: 50
              preference:
                matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                      - k8s-master01
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;preferredDuringSchedulingIgnoredDuringExecutionï¼šè½¯äº²å’ŒåŠ›é…ç½®&lt;/li&gt;
&lt;li&gt;weightï¼šè½¯äº²å’ŒåŠ›çš„æƒé‡ï¼Œæƒé‡è¶Šé«˜ä¼˜å…ˆçº§è¶Šå¤§ï¼ŒèŒƒå›´ 1-100&lt;/li&gt;
&lt;li&gt;matchExpressionsï¼šmatchExpressions ä¸‹å¯ä»¥é…ç½®å¤šä¸ª keyã€valuesï¼ˆéƒ½éœ€è¦æ»¡è¶³ï¼‰ï¼Œå…¶ä¸­ values å¯ä»¥é…ç½®å¤šä¸ªï¼ˆæ»¡è¶³å…¶ä¸€å³å¯ï¼‰&lt;/li&gt;
&lt;li&gt;operatorï¼š
&lt;ul&gt;
&lt;li&gt;IN ç›¸å½“äº key = value çš„å½¢å¼ï¼Œ&lt;strong&gt;NotIn ç›¸å½“äº key!=value çš„å½¢å¼ (åäº²å’ŒåŠ›)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Exists: èŠ‚ç‚¹å­˜åœ¨ label çš„ key ä¸ºæŒ‡å®šçš„å€¼å³å¯ï¼Œä¸èƒ½é…ç½® values å­—æ®µ&lt;/li&gt;
&lt;li&gt;DoesNotExist: èŠ‚ç‚¹ä¸å­˜åœ¨ label çš„ key ä¸ºæŒ‡å®šçš„å€¼å³å¯ï¼Œä¸èƒ½é…ç½® values å­—æ®µ&lt;/li&gt;
&lt;li&gt;Gtï¼šå¤§äº value æŒ‡å®šçš„å€¼&lt;/li&gt;
&lt;li&gt;Ltï¼šå°äº value æŒ‡å®šçš„å€¼&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-podäº²å’ŒåŠ›è¯¦è§£&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-podäº²å’ŒåŠ›è¯¦è§£&#34;&gt;#&lt;/a&gt; 3. Pod äº²å’ŒåŠ›è¯¦è§£&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:              
        podAntiAffinity:   #podç¡¬åäº²å’ŒåŠ›
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx-deploy
            topologyKey: kubernetes.io/hostname
        podAntiAffinity:       #podè½¯åäº²å’ŒåŠ›
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nginx-deploy
              namespaces:     #å’Œå“ªä¸ªå‘½åç©ºé—´çš„Podè¿›è¡ŒåŒ¹é…ï¼Œä¸ºç©ºä¸ºå½“å‰å‘½åç©ºé—´
              - default
              topologyKey: kubernetes.io/hostname
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;labelSelectorï¼šPod é€‰æ‹©å™¨é…ç½®ï¼Œå¯ä»¥é…ç½®å¤šä¸ª&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;matchExpressionsï¼šmatchExpressions ä¸‹å¯ä»¥é…ç½®å¤šä¸ª keyã€valuesï¼ˆéƒ½éœ€è¦æ»¡è¶³ï¼‰ï¼Œå…¶ä¸­ values å¯ä»¥é…ç½®å¤šä¸ªï¼ˆæ»¡è¶³å…¶ä¸€å³å¯ï¼‰&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;topologyKeyï¼šåŒ¹é…çš„æ‹“æ‰‘åŸŸçš„ keyï¼Œä¹Ÿå°±æ˜¯èŠ‚ç‚¹ä¸Š label çš„ keyï¼Œkey å’Œ value ç›¸åŒçš„ä¸ºåŒä¸€ä¸ªåŸŸï¼Œå¯ä»¥ç”¨äºæ ‡æ³¨ä¸åŒçš„æœºæˆ¿å’Œåœ°åŒº&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Namespaces: å’Œå“ªä¸ªå‘½åç©ºé—´çš„ Pod è¿›è¡ŒåŒ¹é…ï¼Œä¸ºç©ºä¸ºå½“å‰å‘½åç©ºé—´&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;operatorï¼šé…ç½®å’ŒèŠ‚ç‚¹äº²å’ŒåŠ›ä¸€è‡´ï¼Œä½†æ˜¯æ²¡æœ‰ Gt å’Œ Lt&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IN ç›¸å½“äº key = value çš„å½¢å¼ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exists: èŠ‚ç‚¹å­˜åœ¨ label çš„ key ä¸ºæŒ‡å®šçš„å€¼å³å¯ï¼Œä¸èƒ½é…ç½® values å­—æ®µï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DoesNotExist: èŠ‚ç‚¹ä¸å­˜åœ¨ label çš„ key ä¸ºæŒ‡å®šçš„å€¼å³å¯ï¼Œä¸èƒ½é…ç½® values å­—æ®µ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-èŠ‚ç‚¹äº²å’ŒåŠ›é…ç½®ç¤ºä¾‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-èŠ‚ç‚¹äº²å’ŒåŠ›é…ç½®ç¤ºä¾‹&#34;&gt;#&lt;/a&gt; 4. èŠ‚ç‚¹äº²å’ŒåŠ›é…ç½®ç¤ºä¾‹&lt;/h4&gt;
&lt;p&gt;Pod å°½é‡éƒ¨ç½²åœ¨ ssd=true å’Œ type=physical çš„èŠ‚ç‚¹ä¸Šï¼Œä½†æ˜¯ä¼˜å…ˆéƒ¨ç½²åœ¨ ssd=true çš„èŠ‚ç‚¹ä¸Šï¼Œä¸èƒ½éƒ¨ç½² label ä¸º gpu=true çš„èŠ‚ç‚¹ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl label nodes k8s-node01 ssd=true
[root@k8s-master01 ~]# kubectl label nodes k8s-master01 ssd=true
[root@k8s-master01 ~]# kubectl label nodes k8s-master01 gpu=true
[root@k8s-master01 ~]# kubectl label nodes k8s-node02 type=physical

[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
      annotations:
        app: nginx-deploy
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: ssd
                    operator: In
                    values:
                      - &#39;true&#39;
                  - key: gpu
                    operator: NotIn
                    values:
                      - &#39;true&#39;
            - weight: 50
              preference:
                matchExpressions:
                  - key: type
                    operator: In
                    values:
                      - physical
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;


[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
nginx-deploy-7d65fbdf-2b4jr   1/1     Running   0          5s    172.16.85.236   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-jjzwr   1/1     Running   0          5s    172.16.58.251   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-kx5lm   1/1     Running   0          5s    172.16.85.237   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-lrmcg   1/1     Running   0          5s    172.16.85.238   k8s-node01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7d65fbdf-n6mlp   1/1     Running   0          5s    172.16.58.250   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-podäº²å’ŒåŠ›-åäº²å’ŒåŠ›é…ç½®ç¤ºä¾‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-podäº²å’ŒåŠ›-åäº²å’ŒåŠ›é…ç½®ç¤ºä¾‹&#34;&gt;#&lt;/a&gt; 5. Pod äº²å’ŒåŠ›ã€åäº²å’ŒåŠ›é…ç½®ç¤ºä¾‹&lt;/h4&gt;
&lt;h5 id=&#34;51-podåäº²å’ŒåŠ›required&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-podåäº²å’ŒåŠ›required&#34;&gt;#&lt;/a&gt; 5.1 Pod åäº²å’ŒåŠ› required&lt;/h5&gt;
&lt;p&gt;åŒä¸€ä¸ªåº”ç”¨éƒ¨ç½²åœ¨ä¸åŒçš„å®¿ä¸»æœº&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.èŠ‚ç‚¹å­˜åœ¨æ±¡ç‚¹podæ— æ³•è°ƒåº¦è‡³è¯¥èŠ‚ç‚¹
# kubectl describe nodes|grep -i taint
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;

#2.podåäº²å’ŒåŠ›required
# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - nginx-deploy
              topologyKey: kubernetes.io/hostname
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;

#3.éƒ¨ç½²deployment
[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-deploy-5787887b6f-4654b   1/1     Running   0          4s    172.16.85.234    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4s    172.16.122.152   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-fdkft   1/1     Running   0          4s    172.16.58.247    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4s    172.16.32.152    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4s    172.16.195.14    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

#4.å°†å‰¯æœ¬æ‰©æˆ6ä¸ªï¼Œç”±äºK8sé›†ç¾¤åªæœ‰5ä¸ªèŠ‚ç‚¹ï¼Œå³5ä¸ªtopologyKeyï¼ˆæ‹“æ‰‘åŸŸï¼‰ï¼Œæ¯ä¸ªåŸŸåªèƒ½æœ‰ä¸€ä¸ªå‰¯æœ¬ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªpodä¼špending
[root@k8s-master01 ~]# kubectl scale deploy nginx-deploy --replicas=6 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES
nginx-deploy-5787887b6f-4654b   1/1     Running   0          4m44s   172.16.85.234    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-8mq7s   1/1     Running   0          4m44s   172.16.122.152   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-fdkft   1/1     Running   0          4m44s   172.16.58.247    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-jzcmd   1/1     Running   0          4m44s   172.16.32.152    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-qdq9g   1/1     Running   0          4m44s   172.16.195.14    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-5787887b6f-sztm7   0/1     Pending   0          9s      &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;         &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

[root@k8s-master01 ~]# kubectl describe pods nginx-deploy-5787887b6f-sztm7
...
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  102s  default-scheduler  0/5 nodes are available: 5 node(s) didn&#39;t match pod anti-affinity rules. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;å°†å‰¯æœ¬æ‰©æˆ 6 ä¸ªï¼Œæœ‰ä¸€ä¸ªä¼š pending çŠ¶æ€ï¼ŒåŸå›  K8s é›†ç¾¤åªæœ‰ 5 ä¸ªèŠ‚ç‚¹ï¼Œå³ 5 ä¸ª topologyKeyï¼ˆæ‹“æ‰‘åŸŸï¼‰ï¼Œæ¯ä¸ªæ‹“æ‰‘åŸŸåªèƒ½æœ‰ä¸€ä¸ªå‰¯æœ¬ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ª pod ä¼š pendingã€‚&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;topologyKeyï¼šåŒ¹é…çš„æ‹“æ‰‘åŸŸçš„ keyï¼Œä¹Ÿå°±æ˜¯èŠ‚ç‚¹ä¸Š label çš„ keyï¼Œkey å’Œ value ç›¸åŒçš„ä¸ºåŒä¸€ä¸ªåŸŸï¼Œå¯ä»¥ç”¨äºæ ‡æ³¨ä¸åŒçš„æœºæˆ¿å’Œåœ°åŒº&lt;/strong&gt;ã€‚&lt;/p&gt;
&lt;h5 id=&#34;52-podåäº²å’ŒåŠ›preferred&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-podåäº²å’ŒåŠ›preferred&#34;&gt;#&lt;/a&gt; 5.2 Pod åäº²å’ŒåŠ› preferred&lt;/h5&gt;
&lt;p&gt;åŒä¸€ä¸ªåº”ç”¨å°½é‡éƒ¨ç½²åœ¨ä¸åŒçš„å®¿ä¸»æœº&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.èŠ‚ç‚¹å­˜åœ¨æ±¡ç‚¹podæ— æ³•è°ƒåº¦è‡³è¯¥èŠ‚ç‚¹
# kubectl describe nodes|grep -i taint
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;
Taints:             &amp;lt;none&amp;gt;

#2.podåäº²å’ŒåŠ›preferred
# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 6
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - nginx-deploy
                topologyKey: kubernetes.io/hostname
              weight: 100
      restartPolicy: Always
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
      volumes:
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;

#3.éƒ¨ç½²deployment
[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-deploy-7c47567b79-97qs5   1/1     Running   0          6s    172.16.122.153   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-g49h4   1/1     Running   0          6s    172.16.85.235    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-g5n2s   1/1     Running   0          6s    172.16.58.248    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-g5v5b   1/1     Running   0          6s    172.16.195.15    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-pjwws   1/1     Running   0          6s    172.16.58.249    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-7c47567b79-q2hn5   1/1     Running   0          6s    172.16.32.153    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;53-podäº²å’ŒåŠ›required&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#53-podäº²å’ŒåŠ›required&#34;&gt;#&lt;/a&gt; 5.3 Pod äº²å’ŒåŠ› required&lt;/h5&gt;
&lt;p&gt;åŒä¸€ä¸ªåº”ç”¨å¿…é¡»éƒ¨ç½²åœ¨åŒä¸€ä¸ªå®¿ä¸»æœº&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 8
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:              
        podAffinity:   #podç¡¬äº²å’ŒåŠ›
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx-deploy
            topologyKey: kubernetes.io/hostname
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
      volumes:
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File

[root@k8s-master01 ~]# kubectl apply -f nginx-deploy.yaml 
[root@k8s-master01 ~]# kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
nginx-deploy-dbcc4d65c-2sthn   1/1     Running   0          12s   172.16.58.255   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-78nxf   1/1     Running   0          12s   172.16.58.197   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-82ssq   1/1     Running   0          12s   172.16.58.194   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-986cb   1/1     Running   0          12s   172.16.58.254   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-9rnt7   1/1     Running   0          12s   172.16.58.252   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-knm8q   1/1     Running   0          12s   172.16.58.195   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-kx56f   1/1     Running   0          12s   172.16.58.253   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-deploy-dbcc4d65c-sqlhf   1/1     Running   0          12s   172.16.58.198   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;54-podäº²å’ŒåŠ›preferre&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#54-podäº²å’ŒåŠ›preferre&#34;&gt;#&lt;/a&gt; 5.4 Pod äº²å’ŒåŠ› preferre&lt;/h5&gt;
&lt;p&gt;åŒä¸€ä¸ªåº”ç”¨å°½é‡éƒ¨ç½²åœ¨åŒä¸€ä¸ªå®¿ä¸»æœº&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 20
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      affinity:              
        podAffinity:       #podè½¯äº²å’ŒåŠ›
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nginx-deploy
              namespaces:     #å’Œå“ªä¸ªå‘½åç©ºé—´çš„Podè¿›è¡ŒåŒ¹é…ï¼Œä¸ºç©ºä¸ºå½“å‰å‘½åç©ºé—´
              - default
              topologyKey: kubernetes.io/hostname
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
      volumes:
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-20T09:59:58.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3254599477.html</id>
        <title>K8så®¹å¿å’Œæ±¡ç‚¹</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3254599477.html"/>
        <content type="html">&lt;h3 id=&#34;k8så®¹å¿å’Œæ±¡ç‚¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8så®¹å¿å’Œæ±¡ç‚¹&#34;&gt;#&lt;/a&gt; K8s å®¹å¿å’Œæ±¡ç‚¹&lt;/h3&gt;
&lt;p&gt;Taint æŒ‡å®šæœåŠ¡å™¨ä¸Šæ‰“ä¸Šæ±¡ç‚¹ï¼Œè®©ä¸èƒ½å®¹å¿è¿™ä¸ªæ±¡ç‚¹çš„ Pod ä¸èƒ½éƒ¨ç½²åœ¨æ‰“äº†æ±¡ç‚¹çš„æœåŠ¡å™¨ä¸Šã€‚Toleration æ˜¯è®© Pod å®¹å¿èŠ‚ç‚¹ä¸Šé…ç½®çš„æ±¡ç‚¹ï¼Œå¯ä»¥è®©ä¸€äº›éœ€è¦ç‰¹æ®Šé…ç½®çš„ Pod èƒ½å¤Ÿè°ƒç”¨åˆ°å…·æœ‰æ±¡ç‚¹å’Œç‰¹æ®Šé…ç½®çš„èŠ‚ç‚¹ä¸Šã€‚&lt;/p&gt;
&lt;h4 id=&#34;1-tainté…ç½®è§£æ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-tainté…ç½®è§£æ&#34;&gt;#&lt;/a&gt; 1. Taint é…ç½®è§£æ&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#1.Taintè¯­æ³•
# kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT

#2.åˆ›å»ºTaintç¤ºä¾‹
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule

#3.æŸ¥çœ‹æ±¡ç‚¹
# kubectl describe node k8s-node01 | grep Taints -A 10

#4.åˆ é™¤æ±¡ç‚¹
# kubectl taint nodes k8s-node01 ssd-                   #åŸºäºKeyåˆ é™¤
# kubectl taint nodes k8s-node01 ssd:PreferNoSchedule-  #åŸºäºKey+Effectåˆ é™¤

#5.ä¿®æ”¹æ±¡ç‚¹ï¼ˆKeyå’ŒEffectç›¸åŒï¼‰
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule --overwrite
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;EFFECT æ’æ–¥ç­‰çº§ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NoScheduleï¼šç¦æ­¢è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ï¼Œå·²ç»åœ¨è¯¥èŠ‚ç‚¹ä¸Šçš„ Pod ä¸å—å½±å“&lt;/li&gt;
&lt;li&gt;NoExecuteï¼šç¦æ­¢è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ï¼Œå¦‚æœä¸ç¬¦åˆè¿™ä¸ªæ±¡ç‚¹ï¼Œä¼šç«‹é©¬è¢«é©±é€ï¼ˆæˆ–åœ¨ä¸€æ®µæ—¶é—´åï¼‰&lt;/li&gt;
&lt;li&gt;PreferNoScheduleï¼šå°½é‡é¿å…å°† Pod è°ƒåº¦åˆ°æŒ‡å®šçš„èŠ‚ç‚¹ä¸Šï¼Œå¦‚æœæ²¡æœ‰æ›´åˆé€‚çš„èŠ‚ç‚¹ï¼Œå¯ä»¥éƒ¨ç½²åˆ°è¯¥èŠ‚ç‚¹&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2tolerationé…ç½®è§£æ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2tolerationé…ç½®è§£æ&#34;&gt;#&lt;/a&gt; 2.Toleration é…ç½®è§£æ&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#1.å®Œå…¨åŒ¹é…
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;taintValue&amp;quot;
  effect: &amp;quot;NoSchedule
 
#2.ä¸å®Œå…¨åŒ¹é… 
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Exists&amp;quot;
  effect: &amp;quot;NoSchedule&amp;quot;
  
#3.å¤§èŒƒå›´åŒ¹é…ï¼ˆä¸æ¨èkeyä¸ºå†…ç½®Taintï¼Œä¼šå¯¼è‡´èŠ‚ç‚¹æ•…éšœpodæ— æ³•æ¼‚ç§»ï¼‰
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Exists
  
#4.å®¹å¿æ—¶é—´é…ç½®
tolerations:
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoExecute&amp;quot;
  tolerationSeconds: 3600
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-taint-tolerationé…ç½®ç¤ºä¾‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-taint-tolerationé…ç½®ç¤ºä¾‹&#34;&gt;#&lt;/a&gt; 3. Taintã€Toleration é…ç½®ç¤ºä¾‹&lt;/h4&gt;
&lt;p&gt;æœ‰ä¸€ä¸ª K8s èŠ‚ç‚¹æ˜¯çº¯ SSD ç¡¬ç›˜çš„èŠ‚ç‚¹ï¼Œç°éœ€è¦åªæœ‰ä¸€äº›éœ€è¦é«˜æ€§èƒ½å­˜å‚¨çš„ Pod æ‰èƒ½è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.ç»™èŠ‚ç‚¹æ‰“ä¸Šæ±¡ç‚¹å’Œæ ‡ç­¾
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule
# kubectl label node k8s-node01 ssd=true

#2.é…ç½®Tolerationï¼š
# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
      nodeSelector:
        ssd: &#39;true&#39;
      tolerations:
        - key: ssd
          operator: Exists
          effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-k8så†…ç½®æ±¡ç‚¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-k8så†…ç½®æ±¡ç‚¹&#34;&gt;#&lt;/a&gt; 4. K8s å†…ç½®æ±¡ç‚¹&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/not-ready%EF%BC%9A%E8%8A%82%E7%82%B9%E6%9C%AA%E5%87%86%E5%A4%87%E5%A5%BD%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81Ready%E7%9A%84%E5%80%BC%E4%B8%BAFalse%E3%80%82&#34;&gt;node.kubernetes.io/not-readyï¼šèŠ‚ç‚¹æœªå‡†å¤‡å¥½ï¼Œç›¸å½“äºèŠ‚ç‚¹çŠ¶æ€ Ready çš„å€¼ä¸º Falseã€‚&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/unreachable%EF%BC%9ANode&#34;&gt;node.kubernetes.io/unreachableï¼šNode&lt;/a&gt; Controller è®¿é—®ä¸åˆ°èŠ‚ç‚¹ï¼Œç›¸å½“äºèŠ‚ç‚¹çŠ¶æ€ Ready çš„å€¼ä¸º Unknownã€‚&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/out-of-disk%EF%BC%9A%E8%8A%82%E7%82%B9%E7%A3%81%E7%9B%98%E8%80%97%E5%B0%BD%E3%80%82&#34;&gt;node.kubernetes.io/out-of-diskï¼šèŠ‚ç‚¹ç£ç›˜è€—å°½ã€‚&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/memory-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E5%86%85%E5%AD%98%E5%8E%8B%E5%8A%9B%E3%80%82&#34;&gt;node.kubernetes.io/memory-pressureï¼šèŠ‚ç‚¹å­˜åœ¨å†…å­˜å‹åŠ›ã€‚&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/disk-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E7%A3%81%E7%9B%98%E5%8E%8B%E5%8A%9B%E3%80%82&#34;&gt;node.kubernetes.io/disk-pressureï¼šèŠ‚ç‚¹å­˜åœ¨ç£ç›˜å‹åŠ›ã€‚&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/network-unavailable%EF%BC%9A%E8%8A%82%E7%82%B9%E7%BD%91%E7%BB%9C%E4%B8%8D%E5%8F%AF%E8%BE%BE%E3%80%82&#34;&gt;node.kubernetes.io/network-unavailableï¼šèŠ‚ç‚¹ç½‘ç»œä¸å¯è¾¾ã€‚&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/unschedulable%EF%BC%9A%E8%8A%82%E7%82%B9%E4%B8%8D%E5%8F%AF%E8%B0%83%E5%BA%A6%E3%80%82&#34;&gt;node.kubernetes.io/unschedulableï¼šèŠ‚ç‚¹ä¸å¯è°ƒåº¦ã€‚&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.cloudprovider.kubernetes.io/uninitialized%EF%BC%9A%E5%A6%82%E6%9E%9CKubelet%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8C%87%E5%AE%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%A4%96%E9%83%A8%E7%9A%84cloudprovider%EF%BC%8C%E5%AE%83%E5%B0%86%E7%BB%99%E5%BD%93%E5%89%8D%E8%8A%82%E7%82%B9%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AATaint%E5%B0%86%E5%85%B6%E6%A0%87%E8%AE%B0%E4%B8%BA%E4%B8%8D%E5%8F%AF%E7%94%A8%E3%80%82%E5%9C%A8cloud-controller-manager%E7%9A%84%E4%B8%80%E4%B8%AAcontroller%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%E5%90%8E%EF%BC%8CKubelet%E5%B0%86%E5%88%A0%E9%99%A4%E8%BF%99%E4%B8%AATaint%E3%80%82&#34;&gt;node.cloudprovider.kubernetes.io/uninitializedï¼šå¦‚æœ Kubelet å¯åŠ¨æ—¶æŒ‡å®šäº†ä¸€ä¸ªå¤–éƒ¨çš„ cloudproviderï¼Œå®ƒå°†ç»™å½“å‰èŠ‚ç‚¹æ·»åŠ ä¸€ä¸ª Taint å°†å…¶æ ‡è®°ä¸ºä¸å¯ç”¨ã€‚åœ¨ cloud-controller-manager çš„ä¸€ä¸ª controller åˆå§‹åŒ–è¿™ä¸ªèŠ‚ç‚¹åï¼ŒKubelet å°†åˆ é™¤è¿™ä¸ª Taintã€‚&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/vO7kURL.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Deployment åˆ›å»ºå K8s é»˜è®¤ä¸º Pod æ·»åŠ å®¹å¿ï¼Œå½“ Pod æ‰€åœ¨çš„èŠ‚ç‚¹å®•æœºï¼Œ300 ç§’å pod ä¼šæ¼‚ç§»ï¼Œé»˜è®¤å®¹å¿æ—¶é—´ 300 ç§’ã€‚&lt;/p&gt;
&lt;h4 id=&#34;5èŠ‚ç‚¹å®•æœºå¿«é€Ÿæ¢å¤ä¸šåŠ¡åº”ç”¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5èŠ‚ç‚¹å®•æœºå¿«é€Ÿæ¢å¤ä¸šåŠ¡åº”ç”¨&#34;&gt;#&lt;/a&gt; 5. èŠ‚ç‚¹å®•æœºå¿«é€Ÿæ¢å¤ä¸šåŠ¡åº”ç”¨&lt;/h4&gt;
&lt;p&gt;èŠ‚ç‚¹ä¸å¥åº·ï¼Œ180 ç§’åå†é©±é€ï¼ˆé»˜è®¤æ˜¯ 300 ç§’ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
      tolerations:
        - key: node.kubernetes.io/unreachable
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 180
        - key: node.kubernetes.io/not-ready
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 180
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-20T07:51:58.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3142072607.html</id>
        <title>K8såˆå§‹åŒ–å®¹å™¨ã€ä¸´æ—¶å®¹å™¨</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3142072607.html"/>
        <content type="html">&lt;h3 id=&#34;k8såˆå§‹åŒ–å®¹å™¨-ä¸´æ—¶å®¹å™¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8såˆå§‹åŒ–å®¹å™¨-ä¸´æ—¶å®¹å™¨&#34;&gt;#&lt;/a&gt; K8s åˆå§‹åŒ–å®¹å™¨ã€ä¸´æ—¶å®¹å™¨&lt;/h3&gt;
&lt;h4 id=&#34;1-åˆå§‹åŒ–å®¹å™¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-åˆå§‹åŒ–å®¹å™¨&#34;&gt;#&lt;/a&gt; 1. åˆå§‹åŒ–å®¹å™¨&lt;/h4&gt;
&lt;h5 id=&#34;1-1-åˆå§‹åŒ–å®¹å™¨çš„ç”¨é€”&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-1-åˆå§‹åŒ–å®¹å™¨çš„ç”¨é€”&#34;&gt;#&lt;/a&gt; 1. 1 åˆå§‹åŒ–å®¹å™¨çš„ç”¨é€”&lt;/h5&gt;
&lt;p&gt;åˆå§‹åŒ–å®¹å™¨ä¸»è¦æ˜¯åœ¨ä¸»åº”ç”¨å¯åŠ¨ä¹‹å‰ï¼Œåšä¸€äº›åˆå§‹åŒ–çš„æ“ä½œï¼Œæ¯”å¦‚åˆ›å»ºæ–‡ä»¶ã€ä¿®æ”¹å†…æ ¸å‚æ•°ã€ç­‰å¾…ä¾èµ–ç¨‹åºå¯åŠ¨æˆ–å…¶ä»–éœ€è¦åœ¨ä¸»ç¨‹åºå¯åŠ¨ä¹‹å‰éœ€è¦åšçš„å·¥ä½œã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Init å®¹å™¨å¯ä»¥åŒ…å«ä¸€äº›å®‰è£…è¿‡ç¨‹ä¸­åº”ç”¨å®¹å™¨ä¸­ä¸å­˜åœ¨çš„å®ç”¨å·¥å…·æˆ–ä¸ªæ€§åŒ–ä»£ç ï¼›&lt;/li&gt;
&lt;li&gt;Init å®¹å™¨å¯ä»¥å®‰å…¨åœ°è¿è¡Œè¿™äº›å·¥å…·ï¼Œé¿å…è¿™äº›å·¥å…·å¯¼è‡´åº”ç”¨é•œåƒçš„å®‰å…¨æ€§é™ä½ï¼›&lt;/li&gt;
&lt;li&gt;Init å®¹å™¨å¯ä»¥ä»¥ root èº«ä»½è¿è¡Œï¼Œæ‰§è¡Œä¸€äº›é«˜æƒé™å‘½ä»¤ï¼›&lt;/li&gt;
&lt;li&gt;Init å®¹å™¨ç›¸å…³æ“ä½œæ‰§è¡Œå®Œæˆä»¥åå³é€€å‡ºï¼Œä¸ä¼šç»™ä¸šåŠ¡å®¹å™¨å¸¦æ¥å®‰å…¨éšæ‚£ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;12-åˆå§‹åŒ–å®¹å™¨å’ŒpoststartåŒºåˆ«&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-åˆå§‹åŒ–å®¹å™¨å’ŒpoststartåŒºåˆ«&#34;&gt;#&lt;/a&gt; 1.2 åˆå§‹åŒ–å®¹å™¨å’Œ PostStart åŒºåˆ«&lt;/h5&gt;
&lt;p&gt;PostStartï¼šä¾èµ–ä¸»åº”ç”¨çš„ç¯å¢ƒï¼Œè€Œä¸”å¹¶ä¸ä¸€å®šå…ˆäº Command è¿è¡Œã€‚&lt;/p&gt;
&lt;p&gt;InitContainerï¼šä¸ä¾èµ–ä¸»åº”ç”¨çš„ç¯å¢ƒï¼Œå¯ä»¥æœ‰æ›´é«˜çš„æƒé™å’Œæ›´å¤šçš„å·¥å…·ï¼Œä¸€å®šä¼šåœ¨ä¸»åº”ç”¨å¯åŠ¨ä¹‹å‰å®Œæˆ&lt;/p&gt;
&lt;h5 id=&#34;13-åˆå§‹åŒ–å®¹å™¨å’Œæ™®é€šå®¹å™¨çš„åŒºåˆ«&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-åˆå§‹åŒ–å®¹å™¨å’Œæ™®é€šå®¹å™¨çš„åŒºåˆ«&#34;&gt;#&lt;/a&gt; 1.3 åˆå§‹åŒ–å®¹å™¨å’Œæ™®é€šå®¹å™¨çš„åŒºåˆ«&lt;/h5&gt;
&lt;p&gt;Init å®¹å™¨ä¸æ™®é€šçš„å®¹å™¨éå¸¸åƒï¼Œé™¤äº†å¦‚ä¸‹å‡ ç‚¹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ç¬¬ä¸€ä¸ª Init å®¹å™¨è¿è¡ŒæˆåŠŸåæ‰ä¼šè¿è¡Œä¸‹ä¸€ä¸ª Init å®¹å™¨ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ‰€æœ‰çš„ Init å®¹å™¨è¿è¡ŒæˆåŠŸåæ‰ä¼šè¿è¡Œä¸»å®¹å™¨ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¦‚æœ Pod çš„ Init å®¹å™¨å¤±è´¥ï¼ŒKubernetes ä¼šä¸æ–­åœ°é‡å¯è¯¥ Podï¼Œç›´åˆ° Init å®¹å™¨æˆåŠŸä¸ºæ­¢ï¼Œä½†æ˜¯ Pod å¯¹åº”çš„ restartPolicy å€¼ä¸º Neverï¼ŒKubernetes ä¸ä¼šé‡æ–°å¯åŠ¨ Podã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Init å®¹å™¨ä¸æ”¯æŒ lifecycleã€livenessProbeã€readinessProbe å’Œ startupProbe&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;14-åˆå§‹åŒ–å®¹å™¨ç¤ºä¾‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-åˆå§‹åŒ–å®¹å™¨ç¤ºä¾‹&#34;&gt;#&lt;/a&gt; 1.4 åˆå§‹åŒ–å®¹å™¨ç¤ºä¾‹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat init.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      initContainers:           # åˆå§‹åŒ–å®¹å™¨è®¾å®š
      - name: fix-permissions
        image: busybox
        command: [&amp;quot;sh&amp;quot;,&amp;quot;-c&amp;quot;,&amp;quot;echo hello kubernetes&amp;gt;/usr/share/nginx/html/index.html&amp;quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: share-volume
          mountPath: /usr/share/nginx/html
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: share-volume
          mountPath: /usr/share/nginx/html
      volumes:
      - name: share-volume
        emptyDir: &amp;#123;&amp;#125;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File

[root@k8s-master01 ~]# kubectl create -f init.yaml

[root@k8s-master01 ~]# curl 172.16.32.145
hello kubernetes
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-ä¸´æ—¶å®¹å™¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-ä¸´æ—¶å®¹å™¨&#34;&gt;#&lt;/a&gt; 2. ä¸´æ—¶å®¹å™¨&lt;/h4&gt;
&lt;h5 id=&#34;21-æ³¨å…¥ä¸´æ—¶å®¹å™¨åˆ°pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-æ³¨å…¥ä¸´æ—¶å®¹å™¨åˆ°pod&#34;&gt;#&lt;/a&gt; 2.1 æ³¨å…¥ä¸´æ—¶å®¹å™¨åˆ° Pod&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods
NAME                            READY   STATUS    RESTARTS      AGE
nginx-deploy-7dd6cd4b44-ktw5k   1/1     Running   1             16h
nginx-deploy-7dd6cd4b44-mjcgq   1/1     Running   1 (28m ago)   16h
nginx-deploy-7dd6cd4b44-wdm6p   1/1     Running   1 (28m ago)   16h

#1.è¿›å…¥å®¹å™¨å‘ç°podæ²¡æœ‰pså’Œnetstatå‘½ä»¤
[root@k8s-master01 ~]# kubectl exec -it nginx-deploy-7dd6cd4b44-ktw5k  -- bash
root@nginx-deploy-7dd6cd4b44-ktw5k:/# ps aux
root@nginx-deploy-7dd6cd4b44-ktw5k:/# netstat -lntp

#2.æ³¨å…¥ä¸´æ—¶å®¹å™¨è‡³è¯¥Pod
[root@k8s-master01 ~]# kubectl debug nginx-deploy-7dd6cd4b44-wdm6p -ti --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;21-æ³¨å…¥ä¸´æ—¶å®¹å™¨åˆ°èŠ‚ç‚¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-æ³¨å…¥ä¸´æ—¶å®¹å™¨åˆ°èŠ‚ç‚¹&#34;&gt;#&lt;/a&gt; 2.1 æ³¨å…¥ä¸´æ—¶å®¹å™¨åˆ°èŠ‚ç‚¹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;kubectl debug node k8s-node01 -it --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-19T13:07:20.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3833778957.html</id>
        <title>K8sè®¡åˆ’ä»»åŠ¡Jobã€Cronjob</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3833778957.html"/>
        <content type="html">&lt;h3 id=&#34;k8sè®¡åˆ’ä»»åŠ¡job-cronjob&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sè®¡åˆ’ä»»åŠ¡job-cronjob&#34;&gt;#&lt;/a&gt; K8s è®¡åˆ’ä»»åŠ¡ Jobã€Cronjob&lt;/h3&gt;
&lt;h4 id=&#34;1-jobé…ç½®å‚æ•°è¯¦è§£&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-jobé…ç½®å‚æ•°è¯¦è§£&#34;&gt;#&lt;/a&gt; 1. Job é…ç½®å‚æ•°è¯¦è§£&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    job-name: echo
  name: echo
  namespace: default
spec:
  #suspend: true # 1.21+
  #ttlSecondsAfterFinished: 100
  backoffLimit: 4
  completions: 1
  parallelism: 1
  template:
    spec:
      containers:
      - name: echo
        image: busybox
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - echo &amp;quot;Hello Job&amp;quot;
      restartPolicy: Never
      
[root@k8s-master01 ~]# kubectl get jobs
NAME   STATUS     COMPLETIONS   DURATION   AGE
echo   Complete   1/1           70s        2m5s

[root@k8s-master01 ~]# kubectl get pods
NAME          READY   STATUS      RESTARTS      AGE
echo-564c8    0/1     Completed   0             2m10s

[root@k8s-master01 ~]# kubectl logs echo-564c8
Hello Job
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;backoffLimit:ï¼šå¦‚æœä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œå¤±è´¥å¤šå°‘æ¬¡åä¸å†æ‰§è¡Œ&lt;/li&gt;
&lt;li&gt;completionsï¼šæœ‰å¤šå°‘ä¸ª Pod æ‰§è¡ŒæˆåŠŸï¼Œè®¤ä¸ºä»»åŠ¡æ˜¯æˆåŠŸçš„ï¼Œé»˜è®¤ä¸ºç©ºå’Œ parallelism æ•°å€¼ä¸€æ ·&lt;/li&gt;
&lt;li&gt;parallelismï¼šå¹¶è¡Œæ‰§è¡Œä»»åŠ¡çš„æ•°é‡ï¼Œå¦‚æœ parallelism æ•°å€¼å¤§äº completions æ•°å€¼ï¼Œåªä¼šåˆ›å»º completions çš„æ•°é‡ï¼›å¦‚æœ completions æ˜¯ 4ï¼Œå¹¶å‘æ˜¯ 3ï¼Œç¬¬ä¸€æ¬¡ä¼šåˆ›å»º 3 ä¸ª Pod æ‰§è¡Œä»»åŠ¡ï¼Œç¬¬äºŒæ¬¡åªä¼šåˆ›å»ºä¸€ä¸ª Pod æ‰§è¡Œä»»åŠ¡&lt;/li&gt;
&lt;li&gt;ttlSecondsAfterFinishedï¼šJob åœ¨æ‰§è¡Œç»“æŸä¹‹åï¼ˆçŠ¶æ€ä¸º completed æˆ– Failedï¼‰è‡ªåŠ¨æ¸…ç†ã€‚è®¾ç½®ä¸º 0 è¡¨ç¤ºæ‰§è¡Œç»“æŸç«‹å³åˆ é™¤ï¼Œä¸è®¾ç½®åˆ™ä¸ä¼šæ¸…é™¤ï¼Œéœ€è¦å¼€å¯ TTLAfterFinished ç‰¹æ€§&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-cronjobé…ç½®å‚æ•°è¯¦è§£&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-cronjobé…ç½®å‚æ•°è¯¦è§£&#34;&gt;#&lt;/a&gt; 2. CronJob é…ç½®å‚æ•°è¯¦è§£&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat cronjob.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  concurrencyPolicy: Allow   #å…è®¸åŒæ—¶è¿è¡Œå¤šä¸ªä»»åŠ¡
  failedJobsHistoryLimit: 10  #ä¿ç•™å¤šå°‘å¤±è´¥çš„ä»»åŠ¡
  successfulJobsHistoryLimit: 10  #ä¿ç•™å¤šå°‘å·²å®Œæˆçš„ä»»åŠ¡
  #suspend: true             #å¦‚æœtrueåˆ™å–æ¶ˆå‘¨æœŸæ€§æ‰§è¡Œä»»åŠ¡
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command:
            - sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure 
          
[root@k8s-master01 ~]# kubectl get  cj
NAME    SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   &amp;lt;none&amp;gt;     False     0        6s              81s

[root@k8s-master01 ~]# kubectl get  jobs
NAME             STATUS     COMPLETIONS   DURATION   AGE
hello-29084454   Complete   1/1           4s         72s
hello-29084455   Complete   1/1           5s         12s

[root@k8s-master01 ~]# kubectl get  pods
NAME                   READY   STATUS      RESTARTS   AGE
hello-29084454-hwv7p   0/1     Completed   0          78s
hello-29084455-vf99w   0/1     Completed   0          18s

[root@k8s-master01 ~]# kubectl logs -f hello-29084455-vf99w
Sat Apr 19 12:55:02 UTC 2025
Hello from the Kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;apiVersion: batch/v1beta1   #1.21+ batch/v1&lt;/li&gt;
&lt;li&gt;scheduleï¼šè°ƒåº¦å‘¨æœŸï¼Œå’Œ Linux ä¸€è‡´ï¼Œåˆ†åˆ«æ˜¯åˆ†æ—¶æ—¥æœˆå‘¨ã€‚&lt;/li&gt;
&lt;li&gt;restartPolicyï¼šé‡å¯ç­–ç•¥ï¼Œå’Œ Pod ä¸€è‡´ã€‚&lt;/li&gt;
&lt;li&gt;concurrencyPolicyï¼šå¹¶å‘è°ƒåº¦ç­–ç•¥ã€‚å¯é€‰å‚æ•°å¦‚ä¸‹ï¼š
&lt;ul&gt;
&lt;li&gt;Allowï¼šå…è®¸åŒæ—¶è¿è¡Œå¤šä¸ªä»»åŠ¡ã€‚&lt;/li&gt;
&lt;li&gt;Forbidï¼šä¸å…è®¸å¹¶å‘è¿è¡Œï¼Œå¦‚æœä¹‹å‰çš„ä»»åŠ¡å°šæœªå®Œæˆï¼Œæ–°çš„ä»»åŠ¡ä¸ä¼šè¢«åˆ›å»ºã€‚&lt;/li&gt;
&lt;li&gt;Replaceï¼šå¦‚æœä¹‹å‰çš„ä»»åŠ¡å°šæœªå®Œæˆï¼Œæ–°çš„ä»»åŠ¡ä¼šæ›¿æ¢çš„ä¹‹å‰çš„ä»»åŠ¡ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;suspendï¼šå¦‚æœè®¾ç½®ä¸º trueï¼Œåˆ™æš‚åœåç»­çš„ä»»åŠ¡ï¼Œé»˜è®¤ä¸º falseã€‚&lt;/li&gt;
&lt;li&gt;successfulJobsHistoryLimitï¼šä¿ç•™å¤šå°‘å·²å®Œæˆçš„ä»»åŠ¡ï¼ŒæŒ‰éœ€é…ç½®ã€‚&lt;/li&gt;
&lt;li&gt;failedJobsHistoryLimitï¼šä¿ç•™å¤šå°‘å¤±è´¥çš„ä»»åŠ¡ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-19T13:00:21.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/169153047.html</id>
        <title>K8sæŒä¹…åŒ–å­˜å‚¨</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/169153047.html"/>
        <content type="html">&lt;h3 id=&#34;k8sæŒä¹…åŒ–å­˜å‚¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sæŒä¹…åŒ–å­˜å‚¨&#34;&gt;#&lt;/a&gt; K8s æŒä¹…åŒ–å­˜å‚¨&lt;/h3&gt;
&lt;h4 id=&#34;1-volume&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-volume&#34;&gt;#&lt;/a&gt; 1. Volume&lt;/h4&gt;
&lt;p&gt;Containerï¼ˆå®¹å™¨ï¼‰ä¸­çš„ç£ç›˜æ–‡ä»¶æ˜¯çŸ­æš‚çš„ï¼Œå½“å®¹å™¨å´©æºƒæ—¶ï¼Œkubelet ä¼šé‡æ–°å¯åŠ¨å®¹å™¨ï¼ŒContainer ä¼šä»¥æœ€å¹²å‡€çš„çŠ¶æ€å¯åŠ¨ï¼Œæœ€åˆçš„æ–‡ä»¶å°†ä¸¢å¤±ã€‚å¦å¤–ï¼Œå½“ä¸€ä¸ª Pod è¿è¡Œå¤šä¸ª Container æ—¶ï¼Œå„ä¸ªå®¹å™¨å¯èƒ½éœ€è¦å…±äº«ä¸€äº›æ–‡ä»¶ã€‚Kubernetes Volume å¯ä»¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä¸€äº›éœ€è¦æŒä¹…åŒ–æ•°æ®çš„ç¨‹åºæ‰ä¼šç”¨åˆ° Volumesï¼Œæˆ–è€…ä¸€äº›éœ€è¦å…±äº«æ•°æ®çš„å®¹å™¨éœ€è¦ volumesã€‚&lt;/li&gt;
&lt;li&gt;æ—¥å¿—æ”¶é›†çš„éœ€æ±‚éœ€è¦åœ¨åº”ç”¨ç¨‹åºçš„å®¹å™¨é‡Œé¢åŠ ä¸€ä¸ª sidecarï¼Œè¿™ä¸ªå®¹å™¨æ˜¯ä¸€ä¸ªæ”¶é›†æ—¥å¿—çš„å®¹å™¨ï¼Œæ¯”å¦‚ filebeatï¼Œå®ƒé€šè¿‡ volumes å…±äº«åº”ç”¨ç¨‹åºçš„æ—¥å¿—æ–‡ä»¶ç›®å½•ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-emptydirå®ç°æ•°æ®å…±äº«&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-emptydirå®ç°æ•°æ®å…±äº«&#34;&gt;#&lt;/a&gt; 1.1 EmptyDir å®ç°æ•°æ®å…±äº«&lt;/h5&gt;
&lt;p&gt;å’Œä¸Šè¿° volume ä¸åŒçš„æ˜¯ï¼Œå¦‚æœåˆ é™¤ Podï¼ŒemptyDir å·ä¸­çš„æ•°æ®ä¹Ÿå°†è¢«åˆ é™¤ï¼Œä¸€èˆ¬ emptyDir å·ç”¨äº Pod ä¸­çš„ä¸åŒ Container å…±äº«æ•°æ®ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
        - name: share-volume
          emptyDir: &amp;#123;&amp;#125;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
            - name: share-volume
              mountPath: /opt
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
            - name: share-volume
              mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-volumes-hostpathæŒ‚è½½å®¿ä¸»æœºè·¯å¾„&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-volumes-hostpathæŒ‚è½½å®¿ä¸»æœºè·¯å¾„&#34;&gt;#&lt;/a&gt; 1.2 Volumes HostPath æŒ‚è½½å®¿ä¸»æœºè·¯å¾„&lt;/h5&gt;
&lt;p&gt;hostPath å·å¯å°†èŠ‚ç‚¹ä¸Šçš„æ–‡ä»¶æˆ–ç›®å½•æŒ‚è½½åˆ° Pod ä¸Šï¼Œç”¨äº Pod è‡ªå®šä¹‰æ—¥å¿—è¾“å‡ºæˆ–è®¿é—® Docker å†…éƒ¨çš„å®¹å™¨ç­‰ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: share-volume
        emptyDir: &amp;#123;&amp;#125;
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: share-volume
            mountPath: /opt
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
          - name: share-volume
            mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPath å·å¸¸ç”¨çš„ typeï¼ˆç±»å‹ï¼‰å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;type ä¸ºç©ºå­—ç¬¦ä¸²ï¼šé»˜è®¤é€‰é¡¹ï¼Œæ„å‘³ç€æŒ‚è½½ hostPath å·ä¹‹å‰ä¸ä¼šæ‰§è¡Œä»»ä½•æ£€æŸ¥ã€‚&lt;/li&gt;
&lt;li&gt;DirectoryOrCreateï¼šå¦‚æœç»™å®šçš„ path ä¸å­˜åœ¨ä»»ä½•ä¸œè¥¿ï¼Œé‚£ä¹ˆå°†æ ¹æ®éœ€è¦åˆ›å»ºä¸€ä¸ªæƒé™ä¸º 0755 çš„ç©ºç›®å½•ï¼Œå’Œ Kubelet å…·æœ‰ç›¸åŒçš„ç»„å’Œæƒé™ã€‚&lt;/li&gt;
&lt;li&gt;Directoryï¼šç›®å½•å¿…é¡»å­˜åœ¨äºç»™å®šçš„è·¯å¾„ä¸‹ã€‚&lt;/li&gt;
&lt;li&gt;FileOrCreateï¼šå¦‚æœç»™å®šçš„è·¯å¾„ä¸å­˜å‚¨ä»»ä½•å†…å®¹ï¼Œåˆ™ä¼šæ ¹æ®éœ€è¦åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ï¼Œæƒé™è®¾ç½®ä¸º 0644ï¼Œå’Œ Kubelet å…·æœ‰ç›¸åŒçš„ç»„å’Œæ‰€æœ‰æƒã€‚&lt;/li&gt;
&lt;li&gt;Fileï¼šæ–‡ä»¶ï¼Œå¿…é¡»å­˜åœ¨äºç»™å®šè·¯å¾„ä¸­ã€‚&lt;/li&gt;
&lt;li&gt;Socketï¼šUNIX å¥—æ¥å­—ï¼Œå¿…é¡»å­˜åœ¨äºç»™å®šè·¯å¾„ä¸­ã€‚&lt;/li&gt;
&lt;li&gt;CharDeviceï¼šå­—ç¬¦è®¾å¤‡ï¼Œå¿…é¡»å­˜åœ¨äºç»™å®šè·¯å¾„ä¸­ã€‚&lt;/li&gt;
&lt;li&gt;BlockDeviceï¼šå—è®¾å¤‡ï¼Œå¿…é¡»å­˜åœ¨äºç»™å®šè·¯å¾„ä¸­ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;13-æŒ‚è½½nfsè‡³å®¹å™¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-æŒ‚è½½nfsè‡³å®¹å™¨&#34;&gt;#&lt;/a&gt; 1.3 æŒ‚è½½ NFS è‡³å®¹å™¨&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.å®‰è£…nfs
# yum install nfs-utils -y       
# mkdir /data/nfs -p
# vim /etc/exports 
/data 192.168.1.0/24(rw,no_root_squash)
# exportfs -arv   
# systemctl start nfs-server &amp;amp;&amp;amp; systemctl enable nfs-server &amp;amp;&amp;amp; systemctl status nfs-server 

#2.æµ‹è¯•å®¢æˆ·ç«¯æŒ‚è½½
# showmount -e 192.168.1.75
# mount -t nfs 192.168.1.75:/data/nfs /mnt

#3.DeployæŒ‚è½½NFS
[root@k8s-master01 ~]# cat nginx-deploy-nfs.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
      annotations:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: nfs-volume
        nfs:
          server: 192.168.1.75
          path: /data/nfs
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: nfs-volume
            mountPath: /usr/share/nginx/html
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-pv-pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-pv-pvc&#34;&gt;#&lt;/a&gt; 2. PVã€PVC&lt;/h4&gt;
&lt;p&gt;PersistentVolumeï¼šç®€ç§° PVï¼Œæ˜¯ç”± Kubernetes ç®¡ç†å‘˜è®¾ç½®çš„å­˜å‚¨ï¼Œå¯ä»¥é…ç½® Cephã€NFSã€GlusterFS ç­‰å¸¸ç”¨å­˜å‚¨é…ç½®ï¼Œç›¸å¯¹äº Volume é…ç½®ï¼Œæä¾›äº†æ›´å¤šçš„åŠŸèƒ½ï¼Œæ¯”å¦‚ç”Ÿå‘½å‘¨æœŸçš„ç®¡ç†ã€å¤§å°çš„é™åˆ¶ã€‚PV åˆ†ä¸ºé™æ€å’ŒåŠ¨æ€ã€‚&lt;/p&gt;
&lt;p&gt;PersistentVolumeClaimï¼šç®€ç§° PVCï¼Œæ˜¯å¯¹å­˜å‚¨ PV çš„è¯·æ±‚ï¼Œè¡¨ç¤ºéœ€è¦ä»€ä¹ˆç±»å‹çš„ PVï¼Œéœ€è¦å­˜å‚¨çš„æŠ€æœ¯äººå‘˜åªéœ€è¦é…ç½® PVC å³å¯ä½¿ç”¨å­˜å‚¨ï¼Œæˆ–è€… Volume é…ç½® PVC çš„åç§°å³å¯ã€‚&lt;/p&gt;
&lt;h5 id=&#34;21-pvå›æ”¶ç­–ç•¥&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-pvå›æ”¶ç­–ç•¥&#34;&gt;#&lt;/a&gt; 2.1 PV å›æ”¶ç­–ç•¥&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Retainï¼šä¿ç•™ï¼Œè¯¥ç­–ç•¥å…è®¸æ‰‹åŠ¨å›æ”¶èµ„æºï¼Œå½“åˆ é™¤ PVC æ—¶ï¼ŒPV ä»ç„¶å­˜åœ¨ï¼ŒPV è¢«è§†ä¸ºå·²é‡Šæ”¾ï¼Œç®¡ç†å‘˜å¯ä»¥æ‰‹åŠ¨å›æ”¶å·ã€‚&lt;/li&gt;
&lt;li&gt;Recycleï¼šå›æ”¶ï¼Œå¦‚æœ Volume æ’ä»¶æ”¯æŒï¼ŒRecycle ç­–ç•¥ä¼šå¯¹å·æ‰§è¡Œ rm -rf æ¸…ç†è¯¥ PVï¼Œå¹¶ä½¿å…¶å¯ç”¨äºä¸‹ä¸€ä¸ªæ–°çš„ PVCï¼Œä½†æ˜¯æœ¬ç­–ç•¥å°†æ¥ä¼šè¢«å¼ƒç”¨ï¼Œç›®å‰åªæœ‰ NFS å’Œ HostPath æ”¯æŒè¯¥ç­–ç•¥ã€‚&lt;/li&gt;
&lt;li&gt;Deleteï¼šåˆ é™¤ï¼Œå¦‚æœ Volume æ’ä»¶æ”¯æŒï¼Œåˆ é™¤ PVC æ—¶ä¼šåŒæ—¶åˆ é™¤ PVï¼ŒåŠ¨æ€å·é»˜è®¤ä¸º Deleteï¼Œç›®å‰æ”¯æŒ Delete çš„å­˜å‚¨åç«¯åŒ…æ‹¬ AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder ç­‰ã€‚&lt;/li&gt;
&lt;li&gt;å¯ä»¥é€šè¿‡ persistentVolumeReclaimPolicy: Recycle å­—æ®µé…ç½®&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;22-pvè®¿é—®ç­–ç•¥&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-pvè®¿é—®ç­–ç•¥&#34;&gt;#&lt;/a&gt; 2.2 PV è®¿é—®ç­–ç•¥&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnceï¼šå¯ä»¥è¢«å•èŠ‚ç‚¹ä»¥è¯»å†™æ¨¡å¼æŒ‚è½½ï¼Œå‘½ä»¤è¡Œä¸­å¯ä»¥è¢«ç¼©å†™ä¸º RWOã€‚&lt;/li&gt;
&lt;li&gt;ReadOnlyManyï¼šå¯ä»¥è¢«å¤šä¸ªèŠ‚ç‚¹ä»¥åªè¯»æ¨¡å¼æŒ‚è½½ï¼Œå‘½ä»¤è¡Œä¸­å¯ä»¥è¢«ç¼©å†™ä¸º ROXã€‚&lt;/li&gt;
&lt;li&gt;ReadWriteManyï¼šå¯ä»¥è¢«å¤šä¸ªèŠ‚ç‚¹ä»¥è¯»å†™æ¨¡å¼æŒ‚è½½ï¼Œå‘½ä»¤è¡Œä¸­å¯ä»¥è¢«ç¼©å†™ä¸º RWXã€‚&lt;/li&gt;
&lt;li&gt;ReadWriteOncePod ï¼šåªå…è®¸è¢«å•ä¸ª Pod è®¿é—®ï¼Œéœ€è¦ K8s 1.22 + ä»¥ä¸Šç‰ˆæœ¬ï¼Œå¹¶ä¸”æ˜¯ CSI åˆ›å»ºçš„ PV æ‰å¯ä½¿ç”¨ï¼Œç¼©å†™ä¸º RWOP&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOnce&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadOnlyMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOncePod&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HostPath&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;- (works when Pods are collocated)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;âœ“&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;23-å­˜å‚¨åˆ†ç±»&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-å­˜å‚¨åˆ†ç±»&#34;&gt;#&lt;/a&gt; 2.3 å­˜å‚¨åˆ†ç±»&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;æ–‡ä»¶å­˜å‚¨ï¼šä¸€äº›æ•°æ®å¯èƒ½éœ€è¦è¢«å¤šä¸ªèŠ‚ç‚¹ä½¿ç”¨ï¼Œæ¯”å¦‚ç”¨æˆ·çš„å¤´åƒã€ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ç­‰ï¼Œå®ç°æ–¹å¼ï¼šNFSã€NASã€FTPã€CephFS ç­‰ã€‚&lt;/li&gt;
&lt;li&gt;å—å­˜å‚¨ï¼šä¸€äº›æ•°æ®åªèƒ½è¢«ä¸€ä¸ªèŠ‚ç‚¹ä½¿ç”¨ï¼Œæˆ–è€…æ˜¯éœ€è¦å°†ä¸€å—è£¸ç›˜æ•´ä¸ªæŒ‚è½½ä½¿ç”¨ï¼Œæ¯”å¦‚æ•°æ®åº“ã€Redis ç­‰ï¼Œå®ç°æ–¹å¼ï¼šCephã€GlusterFSã€å…¬æœ‰äº‘ã€‚&lt;/li&gt;
&lt;li&gt;å¯¹è±¡å­˜å‚¨ï¼šç”±ç¨‹åºä»£ç ç›´æ¥å®ç°çš„ä¸€ç§å­˜å‚¨æ–¹å¼ï¼Œäº‘åŸç”Ÿåº”ç”¨æ— çŠ¶æ€åŒ–å¸¸ç”¨çš„å®ç°æ–¹å¼ï¼Œå®ç°æ–¹å¼ï¼šä¸€èˆ¬æ˜¯ç¬¦åˆ S3 åè®®çš„äº‘å­˜å‚¨ï¼Œæ¯”å¦‚ AWS çš„ S3 å­˜å‚¨ã€Minioã€ä¸ƒç‰›äº‘ç­‰ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;24-pvé…ç½®ç¤ºä¾‹nfs&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-pvé…ç½®ç¤ºä¾‹nfs&#34;&gt;#&lt;/a&gt; 2.4 PV é…ç½®ç¤ºä¾‹ NFS&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv1
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-slow
  nfs:
    path: /data/pv1
    server: 192.168.1.75
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;capacityï¼šå®¹é‡é…ç½®&lt;/p&gt;
&lt;p&gt;volumeModeï¼šå·çš„æ¨¡å¼ï¼Œç›®å‰æ”¯æŒ Filesystemï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰ å’Œ Blockï¼ˆå—ï¼‰ï¼Œå…¶ä¸­ Block ç±»å‹éœ€è¦åç«¯å­˜å‚¨æ”¯æŒï¼Œé»˜è®¤ä¸ºæ–‡ä»¶ç³»ç»Ÿ&lt;/p&gt;
&lt;p&gt;accessModesï¼šè¯¥ PV çš„è®¿é—®æ¨¡å¼&lt;/p&gt;
&lt;p&gt;storageClassNameï¼šPV çš„ç±»ï¼Œä¸€ä¸ªç‰¹å®šç±»å‹çš„ PV åªèƒ½ç»‘å®šåˆ°ç‰¹å®šç±»åˆ«çš„ PVCï¼›&lt;/p&gt;
&lt;p&gt;persistentVolumeReclaimPolicyï¼šå›æ”¶ç­–ç•¥&lt;/p&gt;
&lt;p&gt;mountOptionsï¼šéå¿…é¡»ï¼Œæ–°ç‰ˆæœ¬ä¸­å·²å¼ƒç”¨&lt;/p&gt;
&lt;p&gt;nfsï¼šNFS æœåŠ¡é…ç½®ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªé€‰é¡¹&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pathï¼šNFS ä¸Šçš„å…±äº«ç›®å½•&lt;/li&gt;
&lt;li&gt;serverï¼šNFS çš„ IP åœ°å€&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;25-pvé…ç½®ç¤ºä¾‹hostpath&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-pvé…ç½®ç¤ºä¾‹hostpath&#34;&gt;#&lt;/a&gt; 2.5 PV é…ç½®ç¤ºä¾‹ HostPath&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: hostpath
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: hostpath
  hostPath:
    path: &amp;quot;/mnt/data&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPathï¼šhostPath æœåŠ¡é…ç½®&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pathï¼šå®¿ä¸»æœºè·¯å¾„&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;26-pvçš„çŠ¶æ€&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#26-pvçš„çŠ¶æ€&#34;&gt;#&lt;/a&gt; 2.6 PV çš„çŠ¶æ€&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Availableï¼šå¯ç”¨ï¼Œæ²¡æœ‰è¢« PVC ç»‘å®šçš„ç©ºé—²èµ„æºã€‚&lt;/li&gt;
&lt;li&gt;Boundï¼šå·²ç»‘å®šï¼Œå·²ç»è¢« PVC ç»‘å®šã€‚&lt;/li&gt;
&lt;li&gt;Releasedï¼šå·²é‡Šæ”¾ï¼ŒPVC è¢«åˆ é™¤ï¼Œä½†æ˜¯èµ„æºè¿˜æœªè¢«é‡æ–°ä½¿ç”¨ã€‚&lt;/li&gt;
&lt;li&gt;Failedï¼šå¤±è´¥ï¼Œè‡ªåŠ¨å›æ”¶å¤±è´¥ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;27-pvcç»‘å®špv&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-pvcç»‘å®špv&#34;&gt;#&lt;/a&gt; 2.7 PVC ç»‘å®š PV&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: nfs-slow
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi      
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;PVC çš„ç©ºé—´ç”³è¯·å¤§å°â‰¤PV çš„å¤§å°&lt;/li&gt;
&lt;li&gt;PVC çš„ StorageClassName å’Œ PV çš„ä¸€è‡´&lt;/li&gt;
&lt;li&gt;PVC çš„ accessModes å’Œ PV çš„ä¸€è‡´&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;28-depoymentæŒ‚è½½pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#28-depoymentæŒ‚è½½pvc&#34;&gt;#&lt;/a&gt; 2.8 Depoyment æŒ‚è½½ PVC&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      volumes:
      - name: nfs-pvc-storage  #volumeåç§°
        persistentVolumeClaim:
          claimName: nfs-pvc   #PVCåç§°
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
         - name: nfs-pvc-storage
          mountPath: /usr/share/nginx/html
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŒ‚è½½ PVC çš„ Pod ä¸€ç›´å¤„äº Pendingï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PVC æ²¡æœ‰åˆ›å»ºæˆåŠŸæˆ– PVC ä¸å­˜åœ¨&lt;/li&gt;
&lt;li&gt;PVC å’Œ Pod ä¸åœ¨åŒä¸€ä¸ª Namespace&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-18T14:25:17.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3992668367.html</id>
        <title>K8sé…ç½®ç®¡ç†Configmap</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3992668367.html"/>
        <content type="html">&lt;h3 id=&#34;k8sé…ç½®ç®¡ç†configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sé…ç½®ç®¡ç†configmap&#34;&gt;#&lt;/a&gt; K8s é…ç½®ç®¡ç† Configmap&lt;/h3&gt;
&lt;h4 id=&#34;1-configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-configmap&#34;&gt;#&lt;/a&gt; 1. Configmap&lt;/h4&gt;
&lt;h5 id=&#34;1-1-åŸºäºfrom-env-fileåˆ›å»ºconfigmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-1-åŸºäºfrom-env-fileåˆ›å»ºconfigmap&#34;&gt;#&lt;/a&gt; 1. 1 åŸºäº from-env-file åˆ›å»º Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat cm_env.conf 
podname=nf-flms-system
podip=192.168.1.100
env=prod
nacosaddr=nacos.svc.cluster.local

#kubectl create cm cmenv --from-env-file=./cm_env.conf 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-åŸºäºfrom-literalåˆ›å»ºconfigmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-åŸºäºfrom-literalåˆ›å»ºconfigmap&#34;&gt;#&lt;/a&gt; 1.2 åŸºäº from-literal åˆ›å»º Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm cmliteral --from-literal=level=INFO --from-literal=passwd=Superman*2023
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-åŸºäºfrom-fileåˆ›å»ºconfigmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-åŸºäºfrom-fileåˆ›å»ºconfigmap&#34;&gt;#&lt;/a&gt; 1.3 åŸºäº from-file åˆ›å»º Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat s.hmallleasing.com.conf 
server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    client_max_body_size 1G; 
    location / &amp;#123;
        proxy_pass http://192.168.1.134;
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        proxy_connect_timeout 30;
        proxy_send_timeout 60;
        proxy_read_timeout 60;
        
        proxy_buffering on;
        proxy_buffer_size 32k;
        proxy_buffers 4 128k;
        proxy_temp_file_write_size 10240k;		
        proxy_max_temp_file_size 10240k;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;

# kubectl create cm nginxconfig --from-file=./s.hmallleasing.com.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-deploymentæŒ‚è½½configmapç¤ºä¾‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-deploymentæŒ‚è½½configmapç¤ºä¾‹&#34;&gt;#&lt;/a&gt; 1.4 Deployment æŒ‚è½½ configmap ç¤ºä¾‹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.æ‰¹é‡æŒ‚è½½ConfigMapç”Ÿæˆç¯å¢ƒå˜é‡
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.è‡ªå®šä¹‰ç¯å¢ƒå˜é‡
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.æŒ‚è½½å•ä¸ªConfigMapç”Ÿæˆç¯å¢ƒå˜é‡ï¼Œè¿™é‡Œå’ŒConfigMapä¸­çš„é”®åæ˜¯ä¸ä¸€æ ·çš„     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # è¿™ä¸ªå€¼æ¥è‡ªConfigMap
              key: level            # æ¥è‡ªConfigMapçš„key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # æä¾›ä½ æƒ³è¦æŒ‚è½½çš„ ConfigMap çš„åå­—
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-é‡å‘½åæŒ‚è½½çš„configmaq-keyçš„åç§°&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-é‡å‘½åæŒ‚è½½çš„configmaq-keyçš„åç§°&#34;&gt;#&lt;/a&gt; 1.5 é‡å‘½åæŒ‚è½½çš„ configmaq key çš„åç§°&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.æ‰¹é‡æŒ‚è½½ConfigMapç”Ÿæˆç¯å¢ƒå˜é‡
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.è‡ªå®šä¹‰ç¯å¢ƒå˜é‡
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.æŒ‚è½½å•ä¸ªConfigMapç”Ÿæˆç¯å¢ƒå˜é‡ï¼Œè¿™é‡Œå’ŒConfigMapä¸­çš„é”®åæ˜¯ä¸ä¸€æ ·çš„     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # è¿™ä¸ªå€¼æ¥è‡ªConfigMap
              key: level            # æ¥è‡ªConfigMapçš„key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # æä¾›ä½ æƒ³è¦æŒ‚è½½çš„ ConfigMap çš„åå­—
          items:                # é‡å‘½åæŒ‚è½½çš„configmaq keyçš„åç§°ä¸ºnginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
 
#æŸ¥çœ‹æŒ‚è½½çš„configmaq keyçš„åç§°é‡å‘½åä¸ºnginx.conf
[root@k8s-master01 cm]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-deploy-bc476bc56-flln4   1/1     Running   0          10h
nginx-deploy-bc476bc56-jhsh6   1/1     Running   0          10h
nginx-deploy-bc476bc56-splv9   1/1     Running   0          10h
[root@k8s-master01 cm]# kubectl exec -it nginx-deploy-bc476bc56-flln4 -- bash
root@nginx-deploy-bc476bc56-flln4:/# ls /etc/nginx/conf.d/
nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;16-ä¿®æ”¹æŒ‚è½½çš„configmaq-æƒé™&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-ä¿®æ”¹æŒ‚è½½çš„configmaq-æƒé™&#34;&gt;#&lt;/a&gt; 1.6 ä¿®æ”¹æŒ‚è½½çš„ configmaq æƒé™&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.æ‰¹é‡æŒ‚è½½ConfigMapç”Ÿæˆç¯å¢ƒå˜é‡
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.è‡ªå®šä¹‰ç¯å¢ƒå˜é‡
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.æŒ‚è½½å•ä¸ªConfigMapç”Ÿæˆç¯å¢ƒå˜é‡ï¼Œè¿™é‡Œå’ŒConfigMapä¸­çš„é”®åæ˜¯ä¸ä¸€æ ·çš„     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # è¿™ä¸ªå€¼æ¥è‡ªConfigMap
              key: level            # æ¥è‡ªConfigMapçš„key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # æä¾›ä½ æƒ³è¦æŒ‚è½½çš„ ConfigMap çš„åå­—
          items:                # é‡å‘½åæŒ‚è½½çš„configmaq keyçš„åç§°ä¸ºnginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
            mode: 0644        # é…ç½®æŒ‚è½½æƒé™ï¼Œé’ˆå¯¹å•ä¸ªkeyç”Ÿæ•ˆ
          defaultMode: 0666   # é…ç½®æŒ‚è½½æƒé™ï¼Œé’ˆå¯¹æ•´ä¸ªkeyç”Ÿæ•ˆ
    
#æŸ¥çœ‹æŒ‚è½½æƒé™
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/nginx.conf 
lrwxrwxrwx 1 root root 17 Apr 16 13:37 /etc/nginx/conf.d/nginx.conf -&amp;gt; ..data/nginx.conf
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/..data/nginx.conf 
-rw-rw-rw- 1 root root 722 Apr 16 13:37 /etc/nginx/conf.d/..data/nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;17-subpathè§£å†³æŒ‚è½½è¦†ç›–é—®é¢˜&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#17-subpathè§£å†³æŒ‚è½½è¦†ç›–é—®é¢˜&#34;&gt;#&lt;/a&gt; 1.7 subpath è§£å†³æŒ‚è½½è¦†ç›–é—®é¢˜&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.åˆ›å»ºconfigmap
[root@k8s-master01 cm]# cat nginx.conf 

user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events &amp;#123;
    worker_connections  512;
&amp;#125;


http &amp;#123;
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  &#39;$remote_addr - $remote_user [$time_local] &amp;quot;$request&amp;quot; &#39;
                      &#39;$status $body_bytes_sent &amp;quot;$http_referer&amp;quot; &#39;
                      &#39;&amp;quot;$http_user_agent&amp;quot; &amp;quot;$http_x_forwarded_for&amp;quot;&#39;;

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
&amp;#125;

[root@k8s-master01 cm]# kubectl create cm nginx-config --from-file=./nginx.conf

#subpathè§£å†³æŒ‚è½½è¦†ç›–é—®é¢˜
[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # â‘ æ‰¹é‡æŒ‚è½½ConfigMapç”Ÿæˆç¯å¢ƒå˜é‡
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # â‘¡è‡ªå®šä¹‰ç¯å¢ƒå˜é‡
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # â‘¢æŒ‚è½½å•ä¸ªConfigMapç”Ÿæˆç¯å¢ƒå˜é‡ï¼Œè¿™é‡Œå’ŒConfigMapä¸­çš„é”®åæ˜¯ä¸ä¸€æ ·çš„     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # è¿™ä¸ªå€¼æ¥è‡ªConfigMap
              key: level            # æ¥è‡ªConfigMapçš„key
        volumeMounts:
        - name: config
          mountPath: &amp;quot;/etc/nginx/nginx.conf&amp;quot;   #åªæŒ‚åœ¨nginx.confä¸€ä¸ªæ–‡ä»¶,ä¸è¦†ç›–ç›®å½•
          subPath: nginx.conf      
      volumes:
      - name: config
        configMap:
          name: nginx-config      # æä¾›ä½ æƒ³è¦æŒ‚è½½çš„ConfigMapçš„åå­—
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-secret&#34;&gt;#&lt;/a&gt; 2. Secret&lt;/h4&gt;
&lt;h5 id=&#34;21-secretæ‹‰å–ç§æœ‰ä»“åº“é•œåƒ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-secretæ‹‰å–ç§æœ‰ä»“åº“é•œåƒ&#34;&gt;#&lt;/a&gt; 2.1 Secret æ‹‰å–ç§æœ‰ä»“åº“é•œåƒ&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret docker-registry harboradmin \
--docker-server=s.hmallleasing.com \
--docker-username=admin \
--docker-password=Superman*2023 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-åˆ›å»ºssl-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-åˆ›å»ºssl-secret&#34;&gt;#&lt;/a&gt; 2.2 åˆ›å»º ssl Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret tls dev.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-åŸºäºå‘½ä»¤åˆ›å»ºgeneric-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-åŸºäºå‘½ä»¤åˆ›å»ºgeneric-secret&#34;&gt;#&lt;/a&gt; 2.3 åŸºäºå‘½ä»¤åˆ›å»º generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.é€šè¿‡from-env-fileåˆ›å»º
# cat db.conf 
username=xuyong
passwd=Superman*2023

# kubectl create secret generic dbconf --from-env-file=./db.conf

#2.é€šè¿‡from-literalåˆ›å»º
kubectl create secret generic db-user-pass \
    --from-literal=username=admin \
    --from-literal=password=&#39;S!B\*d$zDsb=&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-secretåŠ å¯†-è§£å¯†&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-secretåŠ å¯†-è§£å¯†&#34;&gt;#&lt;/a&gt; 2.4 Secret åŠ å¯†ã€è§£å¯†&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;1.åŠ å¯†
# echo -n &amp;quot;Superman*2023&amp;quot; | base64
U3VwZXJtYW4qMjAyMw==

2.è§£å¯†
# echo &amp;quot;U3VwZXJtYW4qMjAyMw==&amp;quot; | base64 --decode
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-åŸºäºæ–‡ä»¶åˆ›å»ºéåŠ å¯†generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-åŸºäºæ–‡ä»¶åˆ›å»ºéåŠ å¯†generic-secret&#34;&gt;#&lt;/a&gt; 2.5 åŸºäºæ–‡ä»¶åˆ›å»ºéåŠ å¯† generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get secret dbconf -oyaml
apiVersion: v1
data:
  passwd: U3VwZXJtYW4qMjAyMw==
  username: eHV5b25n
kind: Secret
metadata:
  name: dbconf
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-6-åŸºäºyamlåˆ›å»ºåŠ å¯†generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-6-åŸºäºyamlåˆ›å»ºåŠ å¯†generic-secret&#34;&gt;#&lt;/a&gt; 2. 6 åŸºäº yaml åˆ›å»ºåŠ å¯† generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat mysql-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
  namespace: dev
stringData:
  MYSQL_ROOT_PASSWORD: Superman*2023
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;27-deploymentæŒ‚è½½secretç¤ºä¾‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-deploymentæŒ‚è½½secretç¤ºä¾‹&#34;&gt;#&lt;/a&gt; 2.7 Deployment æŒ‚è½½ Secret ç¤ºä¾‹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - name: MYSQL_ROOT_PASSWORD  
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_ROOT_PASSWORD
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-configmapsecretçƒ­æ›´æ–°&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-configmapsecretçƒ­æ›´æ–°&#34;&gt;#&lt;/a&gt; 3. ConfigMap&amp;amp;Secret çƒ­æ›´æ–°&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm nginxconfig --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f -
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T13:47:47.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/858611107.html</id>
        <title>K8sæœåŠ¡å‘å¸ƒService</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/858611107.html"/>
        <content type="html">&lt;h3 id=&#34;k8sæœåŠ¡å‘å¸ƒservice&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sæœåŠ¡å‘å¸ƒservice&#34;&gt;#&lt;/a&gt; K8s æœåŠ¡å‘å¸ƒ Service&lt;/h3&gt;
&lt;h4 id=&#34;1-serviceç±»å‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-serviceç±»å‹&#34;&gt;#&lt;/a&gt; 1. Service ç±»å‹&lt;/h4&gt;
&lt;p&gt;Kubernetes Service Typeï¼ˆæœåŠ¡ç±»å‹ï¼‰ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ç§ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIPï¼šåœ¨é›†ç¾¤å†…éƒ¨ä½¿ç”¨ï¼Œé»˜è®¤å€¼ï¼Œåªèƒ½ä»é›†ç¾¤ä¸­è®¿é—®ã€‚&lt;/li&gt;
&lt;li&gt;NodePortï¼šåœ¨æ‰€æœ‰å®‰è£…äº† Kube-Proxy çš„èŠ‚ç‚¹ä¸Šæ‰“å¼€ä¸€ä¸ªç«¯å£ï¼Œæ­¤ç«¯å£å¯ä»¥ä»£ç†è‡³åç«¯ Podï¼Œå¯ä»¥é€šè¿‡ NodePort ä»é›†ç¾¤å¤–éƒ¨è®¿é—®é›†ç¾¤å†…çš„æœåŠ¡ï¼Œæ ¼å¼ä¸º NodeIP:NodePortã€‚&lt;/li&gt;
&lt;li&gt;LoadBalancerï¼šä½¿ç”¨äº‘æä¾›å•†çš„è´Ÿè½½å‡è¡¡å™¨å…¬å¼€æœåŠ¡ï¼Œæˆæœ¬è¾ƒé«˜ã€‚&lt;/li&gt;
&lt;li&gt;ExternalNameï¼šé€šè¿‡è¿”å›å®šä¹‰çš„ CNAME åˆ«åï¼Œæ²¡æœ‰è®¾ç½®ä»»ä½•ç±»å‹çš„ä»£ç†ï¼Œéœ€è¦ 1.7 æˆ–æ›´é«˜ç‰ˆæœ¬ kube-dns æ”¯æŒã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-nodeportç±»å‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-nodeportç±»å‹&#34;&gt;#&lt;/a&gt; 1.1 NodePort ç±»å‹&lt;/h5&gt;
&lt;p&gt;å¦‚æœå°† Service çš„ type å­—æ®µè®¾ç½®ä¸º NodePortï¼Œåˆ™ Kubernetes å°†ä» --service-node-port-range å‚æ•°æŒ‡å®šçš„èŒƒå›´ï¼ˆé»˜è®¤ä¸º 30000-32767ï¼‰ä¸­è‡ªåŠ¨åˆ†é…ç«¯å£ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®š NodePortï¼Œåˆ›å»ºè¯¥ Service åï¼Œé›†ç¾¤æ¯ä¸ªèŠ‚ç‚¹éƒ½å°†æš´éœ²ä¸€ä¸ªç«¯å£ï¼Œé€šè¿‡æŸä¸ªå®¿ä¸»æœºçš„ IP + ç«¯å£å³å¯è®¿é—®åˆ°åç«¯çš„åº”ç”¨ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-clusteripç±»å‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-clusteripç±»å‹&#34;&gt;#&lt;/a&gt; 1.2 ClusterIP ç±»å‹&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-ä½¿ç”¨serviceä»£ç†k8så¤–éƒ¨æœåŠ¡&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-ä½¿ç”¨serviceä»£ç†k8så¤–éƒ¨æœåŠ¡&#34;&gt;#&lt;/a&gt; 1.3 ä½¿ç”¨ Service ä»£ç† K8s å¤–éƒ¨æœåŠ¡&lt;/h5&gt;
&lt;p&gt;ä½¿ç”¨åœºæ™¯ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¸Œæœ›åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨æŸä¸ªå›ºå®šçš„åç§°è€Œé IP åœ°å€è®¿é—®å¤–éƒ¨çš„ä¸­é—´ä»¶æœåŠ¡ï¼›&lt;/li&gt;
&lt;li&gt;å¸Œæœ› Service æŒ‡å‘å¦ä¸€ä¸ª Namespace ä¸­æˆ–å…¶ä»–é›†ç¾¤ä¸­çš„æœåŠ¡ï¼›&lt;/li&gt;
&lt;li&gt;æ­£åœ¨å°†å·¥ä½œè´Ÿè½½è½¬ç§»åˆ° Kubernetes é›†ç¾¤ï¼Œä½†æ˜¯ä¸€éƒ¨åˆ†æœåŠ¡ä»è¿è¡Œåœ¨ Kubernetes é›†ç¾¤ä¹‹å¤–çš„ backendã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
spec:
  clusterIP: None
  ports:
  - name: mysql
    port: 3306 
    protocol: TCP
    targetPort: 3306
  type: ClusterIP
---
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
subsets:
- addresses:
  - ip: 192.168.40.150
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-externalname-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-externalname-service&#34;&gt;#&lt;/a&gt; 1.4 ExternalName Service&lt;/h5&gt;
&lt;p&gt;ExternalName Service æ˜¯ Service çš„ç‰¹ä¾‹ï¼Œå®ƒæ²¡æœ‰ Selectorï¼Œä¹Ÿæ²¡æœ‰å®šä¹‰ä»»ä½•ç«¯å£å’Œ Endpointï¼Œå®ƒé€šè¿‡è¿”å›è¯¥å¤–éƒ¨æœåŠ¡çš„åˆ«åæ¥æä¾›æœåŠ¡ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚å¯ä»¥å®šä¹‰ä¸€ä¸ª Serviceï¼Œåç«¯è®¾ç½®ä¸ºä¸€ä¸ªå¤–éƒ¨åŸŸåï¼Œè¿™æ ·é€šè¿‡ Service çš„åç§°å³å¯è®¿é—®åˆ°è¯¥åŸŸåã€‚ä½¿ç”¨ nslookup è§£æä»¥ä¸‹æ–‡ä»¶å®šä¹‰çš„ Serviceï¼Œé›†ç¾¤çš„ DNS &lt;a href=&#34;http://xn--my-uu2cmg2cx7mswf9rko5lsx1a5n3h.database.example.com&#34;&gt;æœåŠ¡å°†è¿”å›ä¸€ä¸ªå€¼ä¸º my.database.example.com&lt;/a&gt; çš„ CNAME è®°å½•ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-å¤šç«¯å£-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-å¤šç«¯å£-service&#34;&gt;#&lt;/a&gt; 1.5 å¤šç«¯å£ Service&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
    - port: 443
      targetPort: 443
      protocol: TCP
      name: https
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:25:51.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/108692210.html</id>
        <title>K8sèµ„æºè°ƒåº¦deploymentã€statefulsetã€daemonset</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/108692210.html"/>
        <content type="html">&lt;h3 id=&#34;k8sèµ„æºè°ƒåº¦deployment-statefulset-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sèµ„æºè°ƒåº¦deployment-statefulset-daemonset&#34;&gt;#&lt;/a&gt; K8s èµ„æºè°ƒåº¦ deploymentã€statefulsetã€daemonset&lt;/h3&gt;
&lt;h4 id=&#34;1-æ— çŠ¶æ€åº”ç”¨ç®¡ç†-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-æ— çŠ¶æ€åº”ç”¨ç®¡ç†-deployment&#34;&gt;#&lt;/a&gt; 1. æ— çŠ¶æ€åº”ç”¨ç®¡ç† Deployment&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.0
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç¤ºä¾‹è§£æï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;nginx-deployï¼šDeployment çš„åç§°ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;replicasï¼š åˆ›å»º Pod çš„å‰¯æœ¬æ•°ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;selectorï¼šå®šä¹‰ Deployment å¦‚ä½•æ‰¾åˆ°è¦ç®¡ç†çš„ Podï¼Œä¸ template çš„ labelï¼ˆæ ‡ç­¾ï¼‰å¯¹åº”ï¼ŒapiVersion ä¸º apps/v1 å¿…é¡»æŒ‡å®šè¯¥å­—æ®µï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;template å­—æ®µåŒ…å«ä»¥ä¸‹å­—æ®µï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;app: nginx-deploy ä½¿ç”¨ labelï¼ˆæ ‡ç­¾ï¼‰æ ‡è®° Podï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;specï¼šè¡¨ç¤º Pod è¿è¡Œä¸€ä¸ªåå­—ä¸º nginx çš„å®¹å™¨ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;imageï¼šè¿è¡Œæ­¤ Pod ä½¿ç”¨çš„é•œåƒï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Portï¼šå®¹å™¨ç”¨äºå‘é€å’Œæ¥æ”¶æµé‡çš„ç«¯å£ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;11-æ›´æ–°-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-æ›´æ–°-deployment&#34;&gt;#&lt;/a&gt; 1.1 æ›´æ–° Deployment&lt;/h5&gt;
&lt;p&gt;å‡å¦‚æ›´æ–° Nginx Pod çš„ image ä½¿ç”¨ nginx:latestï¼Œå¹¶ä½¿ç”¨ --record è®°å½•å½“å‰æ›´æ”¹çš„å‚æ•°ï¼ŒåæœŸå›æ»šæ—¶å¯ä»¥æŸ¥çœ‹åˆ°å¯¹åº”çš„ä¿¡æ¯ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ›´æ–°è¿‡ç¨‹ä¸ºæ–°æ—§äº¤æ›¿æ›´æ–°ï¼Œé¦–å…ˆæ–°å»ºä¸€ä¸ª Podï¼Œå½“ Pod çŠ¶æ€ä¸º Running æ—¶ï¼Œåˆ é™¤ä¸€ä¸ªæ—§çš„ Podï¼ŒåŒæ—¶å†åˆ›å»ºä¸€ä¸ªæ–°çš„ Podã€‚å½“è§¦å‘ä¸€ä¸ªæ›´æ–°åï¼Œä¼šæœ‰æ–°çš„ ReplicaSet äº§ç”Ÿï¼Œæ—§çš„ ReplicaSet ä¼šè¢«ä¿å­˜ï¼ŒæŸ¥çœ‹æ­¤æ—¶ ReplicaSetï¼Œå¯ä»¥ä» AGE æˆ– READY çœ‹å‡ºæ¥æ–°æ—§ ReplicaSetï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get rs
NAME                      DESIRED   CURRENT   READY   AGE
nginx-deploy-65bfb77869   0         0         0       50s
nginx-deploy-85b94dddb4   3         3         3       8s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;é€šè¿‡ describe æŸ¥çœ‹ Deployment çš„è¯¦ç»†ä¿¡æ¯ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl describe deploy nginx-deploy
Name:                   nginx-deploy
Namespace:              default
CreationTimestamp:      Mon, 14 Apr 2025 11:28:03 +0800
Labels:                 app=nginx-deploy
Annotations:            app: nginx-deploy
                        deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
Selector:               app=nginx-deploy
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx-deploy
  Containers:
   nginx-deploy:
    Image:         nginx:latest
    Port:          &amp;lt;none&amp;gt;
    Host Port:     &amp;lt;none&amp;gt;
    Environment:   &amp;lt;none&amp;gt;
    Mounts:        &amp;lt;none&amp;gt;
  Volumes:         &amp;lt;none&amp;gt;
  Node-Selectors:  &amp;lt;none&amp;gt;
  Tolerations:     &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  nginx-deploy-65bfb77869 (0/0 replicas created)
NewReplicaSet:   nginx-deploy-85b94dddb4 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  71s   deployment-controller  Scaled up replica set nginx-deploy-65bfb77869 from 0 to 3
  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 0 to 1
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 3 to 2
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 1 to 2
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 2 to 1
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 2 to 3
  Normal  ScalingReplicaSet  26s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 1 to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;åœ¨ describe ä¸­å¯ä»¥çœ‹å‡ºï¼Œç¬¬ä¸€æ¬¡åˆ›å»ºæ—¶ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªåä¸º nginx-deploy-65bfb77869 çš„ ReplicaSetï¼Œå¹¶ç›´æ¥å°†å…¶æ‰©å±•ä¸º 3 ä¸ªå‰¯æœ¬ã€‚æ›´æ–°éƒ¨ç½²æ—¶ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ ReplicaSetï¼Œå‘½åä¸º nginx-deploy-85b94dddb4ï¼Œå¹¶å°†å…¶å‰¯æœ¬æ•°æ‰©å±•ä¸º 1ï¼Œç„¶åå°†æ—§çš„ ReplicaSet ç¼©å°ä¸º 2ï¼Œè¿™æ ·è‡³å°‘å¯ä»¥æœ‰ 2 ä¸ª Pod å¯ç”¨ï¼Œæœ€å¤šåˆ›å»ºäº† 4 ä¸ª Podã€‚ä»¥æ­¤ç±»æ¨ï¼Œä½¿ç”¨ç›¸åŒçš„æ»šåŠ¨æ›´æ–°ç­–ç•¥å‘ä¸Šå’Œå‘ä¸‹æ‰©å±•æ–°æ—§ ReplicaSetï¼Œæœ€ç»ˆæ–°çš„ ReplicaSet å¯ä»¥æ‹¥æœ‰ 3 ä¸ªå‰¯æœ¬ï¼Œå¹¶å°†æ—§çš„ ReplicaSet ç¼©å°ä¸º 0ã€‚&lt;/p&gt;
&lt;h5 id=&#34;12-å›æ»š-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-å›æ»š-deployment&#34;&gt;#&lt;/a&gt; 1.2 å›æ»š Deployment&lt;/h5&gt;
&lt;p&gt;å½“æ›´æ–°äº†ç‰ˆæœ¬ä¸ç¨³å®šæˆ–é…ç½®ä¸åˆç†æ—¶ï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œå›æ»šæ“ä½œï¼Œå‡è®¾æˆ‘ä»¬åˆè¿›è¡Œäº†å‡ æ¬¡æ›´æ–°ï¼ˆæ­¤å¤„ä»¥æ›´æ–°é•œåƒç‰ˆæœ¬è§¦å‘æ›´æ–°ï¼Œæ›´æ”¹é…ç½®æ•ˆæœç±»ä¼¼ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record
# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä½¿ç”¨ kubectl rollout history æŸ¥çœ‹æ›´æ–°å†å²ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
3         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹ Deployment æŸæ¬¡æ›´æ–°çš„è¯¦ç»†ä¿¡æ¯ï¼Œä½¿ç”¨ --revision æŒ‡å®šæŸæ¬¡æ›´æ–°ç‰ˆæœ¬å·ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy --revision=4
deployment.apps/nginx-deploy with revision #4
Pod Template:
  Labels:	app=nginx-deploy
	pod-template-hash=65b576b795
  Annotations:	kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
  Containers:
   nginx-deploy:
    Image:	nginx:1.21.2
    Port:	&amp;lt;none&amp;gt;
    Host Port:	&amp;lt;none&amp;gt;
    Environment:	&amp;lt;none&amp;gt;
    Mounts:	&amp;lt;none&amp;gt;
  Volumes:	&amp;lt;none&amp;gt;
  Node-Selectors:	&amp;lt;none&amp;gt;
  Tolerations:	&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœåªéœ€è¦å›æ»šåˆ°ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬ï¼Œä½¿ç”¨ kubectl rollout undo å³å¯ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å†æ¬¡æŸ¥çœ‹æ›´æ–°å†å²ï¼Œå‘ç° REVISION3 å›åˆ°äº† nginx:1.21.1ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
5         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœè¦å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬ï¼Œä½¿ç”¨ --to-revision å‚æ•°ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy --to-revision=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-æ‰©å®¹-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-æ‰©å®¹-deployment&#34;&gt;#&lt;/a&gt; 1.3 æ‰©å®¹ Deployment&lt;/h5&gt;
&lt;p&gt;å½“å…¬å¸è®¿é—®é‡å˜å¤§ï¼Œæˆ–è€…æœ‰é¢„æœŸå†…çš„æ´»åŠ¨æ—¶ï¼Œä¸‰ä¸ª Pod å¯èƒ½å·²æ— æ³•æ”¯æ’‘ä¸šåŠ¡æ—¶ï¼Œå¯ä»¥æå‰å¯¹å…¶è¿›è¡Œæ‰©å±•ã€‚&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ kubectl scale åŠ¨æ€è°ƒæ•´ Pod çš„å‰¯æœ¬æ•°ï¼Œæ¯”å¦‚å¢åŠ  Pod ä¸º 5 ä¸ªï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale deployment nginx-deploy --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹ Podï¼Œæ­¤æ—¶ Pod å·²ç»å˜æˆäº† 5 ä¸ªï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
nginx-deploy-85b94dddb4-2qrh6   1/1     Running   0          2m9s
nginx-deploy-85b94dddb4-gvkqj   1/1     Running   0          2m10s
nginx-deploy-85b94dddb4-mdfjs   1/1     Running   0          22s
nginx-deploy-85b94dddb4-rhgpr   1/1     Running   0          2m8s
nginx-deploy-85b94dddb4-vwjhl   1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-æš‚åœå’Œæ¢å¤-deployment-æ›´æ–°&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-æš‚åœå’Œæ¢å¤-deployment-æ›´æ–°&#34;&gt;#&lt;/a&gt; 1.4 æš‚åœå’Œæ¢å¤ Deployment æ›´æ–°&lt;/h5&gt;
&lt;p&gt;ä¸Šè¿°æ¼”ç¤ºçš„å‡ä¸ºæ›´æ”¹æŸä¸€å¤„çš„é…ç½®ï¼Œæ›´æ”¹åç«‹å³è§¦å‘æ›´æ–°ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹å¯èƒ½éœ€è¦é’ˆå¯¹ä¸€ä¸ªèµ„æºæ–‡ä»¶æ›´æ”¹å¤šå¤„åœ°æ–¹ï¼Œè€Œå¹¶ä¸éœ€è¦å¤šæ¬¡è§¦å‘æ›´æ–°ï¼Œæ­¤æ—¶å¯ä»¥ä½¿ç”¨ Deployment æš‚åœåŠŸèƒ½ï¼Œä¸´æ—¶ç¦ç”¨æ›´æ–°æ“ä½œï¼Œå¯¹ Deployment è¿›è¡Œå¤šæ¬¡ä¿®æ”¹ååœ¨è¿›è¡Œæ›´æ–°ã€‚&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ kubectl rollout pause å‘½ä»¤å³å¯æš‚åœ Deployment æ›´æ–°ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout pause deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç„¶åå¯¹ Deployment è¿›è¡Œç›¸å…³æ›´æ–°æ“ä½œï¼Œæ¯”å¦‚å…ˆæ›´æ–°é•œåƒï¼Œç„¶åå¯¹å…¶èµ„æºè¿›è¡Œé™åˆ¶ï¼ˆå¦‚æœä½¿ç”¨çš„æ˜¯ kubectl edit å‘½ä»¤ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œå¤šæ¬¡ä¿®æ”¹ï¼Œæ— éœ€æš‚åœæ›´æ–°ï¼Œkubectlset å‘½ä»¤ä¸€èˆ¬ä¼šé›†æˆåœ¨ CICD æµæ°´çº¿ä¸­ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.3
# kubectl set resources deployment nginx-deploy -c=nginx-deploy --limits=cpu=200m,memory=512Mi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;é€šè¿‡ rollout history å¯ä»¥çœ‹åˆ°æ²¡æœ‰æ–°çš„æ›´æ–°ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#  kubectl rollout history deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿›è¡Œå®Œæœ€åä¸€å¤„é…ç½®æ›´æ”¹åï¼Œä½¿ç”¨ kubectl rollout resume æ¢å¤ Deployment æ›´æ–°ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout resume deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¯ä»¥æŸ¥çœ‹åˆ°æ¢å¤æ›´æ–°çš„ Deployment åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ RSï¼ˆReplicaSet ç¼©å†™ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get rs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¯ä»¥æŸ¥çœ‹ Deployment çš„ imageï¼ˆé•œåƒï¼‰å·²ç»å˜ä¸º nginx:1.21.3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-æ›´æ–°-deployment-çš„æ³¨æ„äº‹é¡¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-æ›´æ–°-deployment-çš„æ³¨æ„äº‹é¡¹&#34;&gt;#&lt;/a&gt; 1.5 æ›´æ–° Deployment çš„æ³¨æ„äº‹é¡¹&lt;/h5&gt;
&lt;p&gt;åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œrevision ä¿ç•™ 10 ä¸ªæ—§çš„ ReplicaSetï¼Œå…¶ä½™çš„å°†åœ¨åå°è¿›è¡Œåƒåœ¾å›æ”¶ï¼Œå¯ä»¥åœ¨.spec.revisionHistoryLimit è®¾ç½®ä¿ç•™ ReplicaSet çš„ä¸ªæ•°ã€‚å½“è®¾ç½®ä¸º 0 æ—¶ï¼Œä¸ä¿ç•™å†å²è®°å½•ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  namespace: default
  labels:
    app: nginx-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.3
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ›´æ–°ç­–ç•¥ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spec.strategy.type==Recreateï¼Œè¡¨ç¤ºé‡å»ºï¼Œå…ˆåˆ æ‰æ—§çš„ Pod å†åˆ›å»ºæ–°çš„ Podï¼›&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: Recreate
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.type==RollingUpdateï¼Œè¡¨ç¤ºæ»šåŠ¨æ›´æ–°ï¼Œå¯ä»¥æŒ‡å®š maxUnavailable å’Œ maxSurge æ¥æ§åˆ¶æ»šåŠ¨æ›´æ–°è¿‡ç¨‹ï¼›&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxUnavailableï¼ŒæŒ‡å®šåœ¨å›æ»šæ›´æ–°æ—¶æœ€å¤§ä¸å¯ç”¨çš„ Pod æ•°é‡ï¼Œå¯é€‰å­—æ®µï¼Œé»˜è®¤ä¸º 25%ï¼Œå¯ä»¥è®¾ç½®ä¸ºæ•°å­—æˆ–ç™¾åˆ†æ¯”ï¼Œå¦‚æœ maxSurge ä¸º 0ï¼Œåˆ™è¯¥å€¼ä¸èƒ½ä¸º 0ï¼›&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxSurge å¯ä»¥è¶…è¿‡æœŸæœ›å€¼çš„æœ€å¤§ Pod æ•°ï¼Œå¯é€‰å­—æ®µï¼Œé»˜è®¤ä¸º 25%ï¼Œå¯ä»¥è®¾ç½®æˆæ•°å­—æˆ–ç™¾åˆ†æ¯”ï¼Œå¦‚æœ maxUnavailable ä¸º 0ï¼Œåˆ™è¯¥å€¼ä¸èƒ½ä¸º 0ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-æœ‰çŠ¶æ€åº”ç”¨ç®¡ç†-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-æœ‰çŠ¶æ€åº”ç”¨ç®¡ç†-statefulset&#34;&gt;#&lt;/a&gt; 2. æœ‰çŠ¶æ€åº”ç”¨ç®¡ç† StatefulSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: &#39;1&#39;
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 128Mi
      restartPolicy: Always
  serviceName: web
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;kind: Service å®šä¹‰äº†ä¸€ä¸ªåå­—ä¸º web çš„ Headless Serviceï¼Œåˆ›å»ºçš„ Service æ ¼å¼ä¸º nginx-0.web.default.svc.cluster.localï¼Œå…¶ä»–çš„ç±»ä¼¼ï¼Œå› ä¸ºæ²¡æœ‰æŒ‡å®š Namespaceï¼ˆå‘½åç©ºé—´ï¼‰ï¼Œæ‰€ä»¥é»˜è®¤éƒ¨ç½²åœ¨ defaultï¼›&lt;/li&gt;
&lt;li&gt;kind: StatefulSet å®šä¹‰äº†ä¸€ä¸ªåå­—ä¸º nginx çš„ StatefulSetï¼Œreplicas è¡¨ç¤ºéƒ¨ç½² Pod çš„å‰¯æœ¬æ•°ï¼Œæœ¬å®ä¾‹ä¸º 3ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;21-åˆ›å»º-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-åˆ›å»º-statefulset&#34;&gt;#&lt;/a&gt; 2.1 åˆ›å»º StatefulSet&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
nginx-0   1/1     Running   0          8m51s
nginx-1   1/1     Running   0          8m50s
nginx-2   1/1     Running   0          8m48s
[root@k8s-master01 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   6d1h
web          ClusterIP   None         &amp;lt;none&amp;gt;        80/TCP    9m28s
[root@k8s-master01 ~]# kubectl get sts
NAME    READY   AGE
nginx   3/3     8m58s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-statefulsetåˆ›å»ºpodæµç¨‹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-statefulsetåˆ›å»ºpodæµç¨‹&#34;&gt;#&lt;/a&gt; 2.2 StatefulSet åˆ›å»º Pod æµç¨‹&lt;/h5&gt;
&lt;p&gt;StatefulSet ç®¡ç†çš„ Pod éƒ¨ç½²å’Œæ‰©å±•è§„åˆ™å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯¹äºå…·æœ‰ N ä¸ªå‰¯æœ¬çš„ StatefulSetï¼Œå°†æŒ‰é¡ºåºä» 0 åˆ° N-1 å¼€å§‹åˆ›å»º Podï¼›&lt;/li&gt;
&lt;li&gt;å½“åˆ é™¤ Pod æ—¶ï¼Œå°†æŒ‰ç…§ N-1 åˆ° 0 çš„åé¡ºåºç»ˆæ­¢ï¼›&lt;/li&gt;
&lt;li&gt;åœ¨ç¼©æ”¾ Pod ä¹‹å‰ï¼Œå¿…é¡»ä¿è¯å½“å‰çš„ Pod æ˜¯ Runningï¼ˆè¿è¡Œä¸­ï¼‰æˆ–è€… Readyï¼ˆå°±ç»ªï¼‰ï¼›&lt;/li&gt;
&lt;li&gt;åœ¨ç»ˆæ­¢ Pod ä¹‹å‰ï¼Œå®ƒæ‰€æœ‰çš„ç»§ä»»è€…å¿…é¡»æ˜¯å®Œå…¨å…³é—­çŠ¶æ€ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet çš„ pod.Spec.TerminationGracePeriodSecondsï¼ˆç»ˆæ­¢ Pod çš„ç­‰å¾…æ—¶é—´ï¼‰ä¸åº”è¯¥æŒ‡å®šä¸º 0ï¼Œè®¾ç½®ä¸º 0 å¯¹ StatefulSet çš„ Pod æ˜¯æå…¶ä¸å®‰å…¨çš„åšæ³•ï¼Œä¼˜é›…åœ°åˆ é™¤ StatefulSet çš„ Pod æ˜¯éå¸¸æœ‰å¿…è¦çš„ï¼Œè€Œä¸”æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥ç¡®ä¿åœ¨ Kubelet ä» APIServer åˆ é™¤ä¹‹å‰ï¼Œè®© Pod æ­£å¸¸å…³é—­ã€‚&lt;/p&gt;
&lt;p&gt;å½“åˆ›å»ºä¸Šé¢çš„ Nginx å®ä¾‹æ—¶ï¼ŒPod å°†æŒ‰ nginx-0ã€nginx-1ã€nginx-2 çš„é¡ºåºéƒ¨ç½² 3 ä¸ª Podã€‚åœ¨ nginx-0 å¤„äº Running æˆ–è€… Ready ä¹‹å‰ï¼Œnginx-1 ä¸ä¼šè¢«éƒ¨ç½²ï¼Œç›¸åŒçš„ï¼Œnginx-2 åœ¨ web-1 æœªå¤„äº Running å’Œ Ready ä¹‹å‰ä¹Ÿä¸ä¼šè¢«éƒ¨ç½²ã€‚å¦‚æœåœ¨ nginx-1 å¤„äº Running å’Œ Ready çŠ¶æ€æ—¶ï¼Œnginx-0 å˜æˆ Failed å¤±è´¥ï¼‰çŠ¶æ€ï¼Œé‚£ä¹ˆ nginx-2 å°†ä¸ä¼šè¢«å¯åŠ¨ï¼Œç›´åˆ° nginx-0 æ¢å¤ä¸º Running å’Œ Ready çŠ¶æ€ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœç”¨æˆ·å°† StatefulSet çš„ replicas è®¾ç½®ä¸º 1ï¼Œé‚£ä¹ˆ nginx-2 å°†é¦–å…ˆè¢«ç»ˆæ­¢ï¼Œåœ¨å®Œå…¨å…³é—­å¹¶åˆ é™¤ nginx-2 ä¹‹å‰ï¼Œä¸ä¼šåˆ é™¤ nginx-1ã€‚å¦‚æœ nginx-2 ç»ˆæ­¢å¹¶ä¸”å®Œå…¨å…³é—­åï¼Œnginx-0 çªç„¶å¤±è´¥ï¼Œé‚£ä¹ˆåœ¨ nginx-0 æœªæ¢å¤æˆ Running æˆ–è€… Ready æ—¶ï¼Œnginx-1 ä¸ä¼šè¢«åˆ é™¤ã€‚&lt;/p&gt;
&lt;h5 id=&#34;23-tatefulset-æ‰©å®¹å’Œç¼©å®¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-tatefulset-æ‰©å®¹å’Œç¼©å®¹&#34;&gt;#&lt;/a&gt; 2.3 tatefulSet æ‰©å®¹å’Œç¼©å®¹&lt;/h5&gt;
&lt;p&gt;å’Œ Deployment ç±»ä¼¼ï¼Œå¯ä»¥é€šè¿‡æ›´æ–° replicas å­—æ®µæ‰©å®¹ / ç¼©å®¹ StatefulSetï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ kubectlscaleã€kubectl edit å’Œ kubectl patch æ¥æ‰©å®¹ / ç¼©å®¹ä¸€ä¸ª StatefulSetã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale sts nginx --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-statefulset-æ›´æ–°ç­–ç•¥&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-statefulset-æ›´æ–°ç­–ç•¥&#34;&gt;#&lt;/a&gt; 2.4 StatefulSet æ›´æ–°ç­–ç•¥&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;On Delete ç­–ç•¥&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OnDelete æ›´æ–°ç­–ç•¥å®ç°äº†ä¼ ç»Ÿï¼ˆ1.7 ç‰ˆæœ¬ä¹‹å‰ï¼‰çš„è¡Œä¸ºï¼Œå®ƒä¹Ÿæ˜¯é»˜è®¤çš„æ›´æ–°ç­–ç•¥ã€‚å½“æˆ‘ä»¬é€‰æ‹©è¿™ä¸ªæ›´æ–°ç­–ç•¥å¹¶ä¿®æ”¹ StatefulSet çš„.spec.template å­—æ®µæ—¶ï¼ŒStatefulSet æ§åˆ¶å™¨ä¸ä¼šè‡ªåŠ¨æ›´æ–° Podï¼Œå¿…é¡»æ‰‹åŠ¨åˆ é™¤ Pod æ‰èƒ½ä½¿æ§åˆ¶å™¨åˆ›å»ºæ–°çš„ Podã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: OnDelete
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;RollingUpdate ç­–ç•¥&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RollingUpdateï¼ˆæ»šåŠ¨æ›´æ–°ï¼‰æ›´æ–°ç­–ç•¥ä¼šè‡ªåŠ¨æ›´æ–°ä¸€ä¸ª StatefulSet ä¸­æ‰€æœ‰çš„ Podï¼Œé‡‡ç”¨ä¸åºå·ç´¢å¼•ç›¸åçš„é¡ºåºè¿›è¡Œæ»šåŠ¨æ›´æ–°ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-åˆ†æ®µæ›´æ–°&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-åˆ†æ®µæ›´æ–°&#34;&gt;#&lt;/a&gt; 2.5 åˆ†æ®µæ›´æ–°&lt;/h5&gt;
&lt;p&gt;å°†åˆ†åŒºæ”¹ä¸º 2ï¼Œæ­¤æ—¶ä¼šè‡ªåŠ¨æ›´æ–° nginx-2ã€nginx-3ã€nginx-4ï¼ˆå› ä¸ºä¹‹å‰æ›´æ”¹äº†æ›´æ–°ç­–ç•¥ï¼‰ï¼Œä½†æ˜¯ä¸ä¼šæ›´æ–° nginx-0 å’Œ nginx-1ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å°† sts é•œåƒä¸º nginx:1.21.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image sts nginx nginx=nginx:1.21.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŒ‰ç…§ä¸Šè¿°æ–¹å¼ï¼Œå¯ä»¥å®ç°åˆ†é˜¶æ®µæ›´æ–°ï¼Œç±»ä¼¼äºç°åº¦ / é‡‘ä¸é›€å‘å¸ƒã€‚æŸ¥çœ‹æœ€ç»ˆçš„ç»“æœå¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;26-statefulset-æŒ‚è½½åŠ¨æ€å­˜å‚¨&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#26-statefulset-æŒ‚è½½åŠ¨æ€å­˜å‚¨&#34;&gt;#&lt;/a&gt; 2.6 StatefulSet æŒ‚è½½åŠ¨æ€å­˜å‚¨&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx 
  serviceName: &amp;quot;nginx&amp;quot;
  replicas: 3 1
  template:
    metadata:
      labels:
        app: nginx 
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: &amp;quot;rook-ceph-block&amp;quot;
      resources:
        requests:
          storage: 10Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3å®ˆæŠ¤è¿›ç¨‹é›†-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3å®ˆæŠ¤è¿›ç¨‹é›†-daemonset&#34;&gt;#&lt;/a&gt; 3. å®ˆæŠ¤è¿›ç¨‹é›† DaemonSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
        - name: nginx-ds
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ­¤æ—¶ä¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºä¸€ä¸ª Podï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-ds-47dxc   1/1     Running   0          56s   172.16.85.213    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-4m89f   1/1     Running   0          56s   172.16.32.143    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-mtpc2   1/1     Running   0          56s   172.16.195.12    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-t5rxc   1/1     Running   0          56s   172.16.122.142   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-x86kc   1/1     Running   0          56s   172.16.58.222    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŒ‡å®šèŠ‚ç‚¹éƒ¨ç½² Pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;      nodeSelector:
        ingress: &#39;true&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ›´æ–°å’Œå›æ»š DaemonSet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image ds nginx-ds nginx-ds=1.21.0 --record=true
# kubectl rollout undo daemonset &amp;lt;daemonset-name&amp;gt; --to-revision=&amp;lt;revision&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DaemonSet çš„æ›´æ–°å’Œå›æ»šä¸ Deployment ç±»ä¼¼ï¼Œæ­¤å¤„ä¸å†æ¼”ç¤ºã€‚&lt;/p&gt;
&lt;h4 id=&#34;4-hpa&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-hpa&#34;&gt;#&lt;/a&gt; 4. HPA&lt;/h4&gt;
&lt;p&gt;åˆ›å»º deploymentã€service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-hpa-svc
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx-hpa
  type: ClusterIP

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-hpa
  labels:
    app: nginx-hpa
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-hpa
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-hpa
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-hpa
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;åˆ›å»º HPA&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl autoscale deployment nginx-hpa --cpu-percent=10 --min=1 --max=10
# kubectl get hpa
NAME        REFERENCE              TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
nginx-hpa   Deployment/nginx-hpa   cpu: 0%/10%   1         10        1          16s

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æµ‹è¯•è‡ªåŠ¨æ‰©ç¼©å®¹&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;while true; do wget -q -O- http://10.96.18.221 &amp;gt; /dev/null; done
[root@k8s-master01 ~]# kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
nginx-hpa-d8bcbdf7d-4mkxp   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-974q5   1/1     Running   0          6m36s
nginx-hpa-d8bcbdf7d-g6p2h   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-lvvsq   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tgqmr   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tzfbs   1/1     Running   0          21s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:25:00.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/1771242682.html</id>
        <title>K8sé›¶å®•æœºæœåŠ¡å‘å¸ƒ-æ¢é’ˆ</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/1771242682.html"/>
        <content type="html">&lt;h3 id=&#34;k8sé›¶å®•æœºæœåŠ¡å‘å¸ƒ-æ¢é’ˆ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8sé›¶å®•æœºæœåŠ¡å‘å¸ƒ-æ¢é’ˆ&#34;&gt;#&lt;/a&gt; K8s é›¶å®•æœºæœåŠ¡å‘å¸ƒ - æ¢é’ˆ&lt;/h3&gt;
&lt;h4 id=&#34;1-podçŠ¶æ€åŠ-pod-æ•…éšœæ’æŸ¥å‘½ä»¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-podçŠ¶æ€åŠ-pod-æ•…éšœæ’æŸ¥å‘½ä»¤&#34;&gt;#&lt;/a&gt; 1. Pod çŠ¶æ€åŠ Pod æ•…éšœæ’æŸ¥å‘½ä»¤&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;çŠ¶æ€&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;è¯´æ˜&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pendingï¼ˆæŒ‚èµ·ï¼‰&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod å·²è¢« Kubernetes ç³»ç»Ÿæ¥æ”¶ï¼Œä½†ä»æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨æœªè¢«åˆ›å»ºï¼Œå¯ä»¥é€šè¿‡ kubectl describe æŸ¥çœ‹å¤„äº Pending çŠ¶æ€çš„åŸå› &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Runningï¼ˆè¿è¡Œä¸­ï¼‰&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod å·²ç»è¢«ç»‘å®šåˆ°ä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå¹¶ä¸”æ‰€æœ‰çš„å®¹å™¨éƒ½å·²ç»è¢«åˆ›å»ºï¼Œè€Œä¸”è‡³å°‘æœ‰ä¸€ä¸ªæ˜¯è¿è¡ŒçŠ¶æ€ï¼Œæˆ–è€…æ˜¯æ­£åœ¨å¯åŠ¨æˆ–è€…é‡å¯ï¼Œå¯ä»¥é€šè¿‡ kubectl logs æŸ¥çœ‹ Pod çš„æ—¥å¿—&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Succeededï¼ˆæˆåŠŸï¼‰&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;æ‰€æœ‰å®¹å™¨æ‰§è¡ŒæˆåŠŸå¹¶ç»ˆæ­¢ï¼Œå¹¶ä¸”ä¸ä¼šå†æ¬¡é‡å¯ï¼Œå¯ä»¥é€šè¿‡ kubectl logs æŸ¥çœ‹ Pod æ—¥å¿—&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Failedï¼ˆå¤±è´¥ï¼‰&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;æ‰€æœ‰å®¹å™¨éƒ½å·²ç»ˆæ­¢ï¼Œå¹¶ä¸”è‡³å°‘æœ‰ä¸€ä¸ªå®¹å™¨ä»¥å¤±è´¥çš„æ–¹å¼ç»ˆæ­¢ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªå®¹å™¨è¦ä¹ˆä»¥éé›¶çŠ¶æ€é€€å‡ºï¼Œè¦ä¹ˆè¢«ç³»ç»Ÿç»ˆæ­¢ï¼Œå¯ä»¥é€šè¿‡ logs å’Œ describe æŸ¥çœ‹ Pod æ—¥å¿—å’ŒçŠ¶æ€&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Unknownï¼ˆæœªçŸ¥ï¼‰&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;é€šå¸¸æ˜¯ç”±äºé€šä¿¡é—®é¢˜é€ æˆçš„æ— æ³•è·å¾— Pod çš„çŠ¶æ€&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ImagePullBackOff ErrImagePull&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;é•œåƒæ‹‰å–å¤±è´¥ï¼Œä¸€èˆ¬æ˜¯ç”±äºé•œåƒä¸å­˜åœ¨ã€ç½‘ç»œä¸é€šæˆ–è€…éœ€è¦ç™»å½•è®¤è¯å¼•èµ·çš„ï¼Œå¯ä»¥ä½¿ç”¨ describe å‘½ä»¤æŸ¥çœ‹å…·ä½“åŸå› &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CrashLoopBackOff&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;å®¹å™¨å¯åŠ¨å¤±è´¥ï¼Œå¯ä»¥é€šè¿‡ logs å‘½ä»¤æŸ¥çœ‹å…·ä½“åŸå› ï¼Œä¸€èˆ¬ä¸ºå¯åŠ¨å‘½ä»¤ä¸æ­£ç¡®ï¼Œå¥åº·æ£€æŸ¥ä¸é€šè¿‡ç­‰&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OOMKilled&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;å®¹å™¨å†…å­˜æº¢å‡ºï¼Œä¸€èˆ¬æ˜¯å®¹å™¨çš„å†…å­˜ Limit è®¾ç½®çš„è¿‡å°ï¼Œæˆ–è€…ç¨‹åºæœ¬èº«æœ‰å†…å­˜æº¢å‡ºï¼Œå¯ä»¥é€šè¿‡ logs æŸ¥çœ‹ç¨‹åºå¯åŠ¨æ—¥å¿—&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Terminating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod æ­£åœ¨è¢«åˆ é™¤ï¼Œå¯ä»¥é€šè¿‡ describe æŸ¥çœ‹çŠ¶æ€&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SysctlForbidden&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod è‡ªå®šä¹‰äº†å†…æ ¸é…ç½®ï¼Œä½† kubelet æ²¡æœ‰æ·»åŠ å†…æ ¸é…ç½®æˆ–é…ç½®çš„å†…æ ¸å‚æ•°ä¸æ”¯æŒï¼Œå¯ä»¥é€šè¿‡ describe æŸ¥çœ‹å…·ä½“åŸå› &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Completed&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;å®¹å™¨å†…éƒ¨ä¸»è¿›ç¨‹é€€å‡ºï¼Œä¸€èˆ¬è®¡åˆ’ä»»åŠ¡æ‰§è¡Œç»“æŸä¼šæ˜¾ç¤ºè¯¥çŠ¶æ€ï¼Œæ­¤æ—¶å¯ä»¥é€šè¿‡ logs æŸ¥çœ‹å®¹å™¨æ—¥å¿—&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ContainerCreating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod æ­£åœ¨åˆ›å»ºï¼Œä¸€èˆ¬ä¸ºæ­£åœ¨ä¸‹è½½é•œåƒï¼Œæˆ–è€…æœ‰é…ç½®ä¸å½“çš„åœ°æ–¹ï¼Œå¯ä»¥é€šè¿‡ describe æŸ¥çœ‹å…·ä½“åŸå› &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;2-podé•œåƒæ‹‰å–ç­–ç•¥&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-podé•œåƒæ‹‰å–ç­–ç•¥&#34;&gt;#&lt;/a&gt; 2. Pod é•œåƒæ‹‰å–ç­–ç•¥&lt;/h4&gt;
&lt;p&gt;é€šè¿‡ spec.containers [].imagePullPolicy å‚æ•°å¯ä»¥æŒ‡å®šé•œåƒçš„æ‹‰å–ç­–ç•¥ï¼Œç›®å‰æ”¯æŒçš„ç­–ç•¥å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ“ä½œæ–¹å¼&lt;/th&gt;
&lt;th&gt;è¯´æ˜&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;æ€»æ˜¯æ‹‰å–ï¼Œå½“é•œåƒ tag ä¸º latest æ—¶ï¼Œä¸” imagePullPolicy æœªé…ç½®ï¼Œé»˜è®¤ä¸º Always&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;ä¸ç®¡æ˜¯å¦å­˜åœ¨éƒ½ä¸ä¼šæ‹‰å–&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IfNotPresent&lt;/td&gt;
&lt;td&gt;é•œåƒä¸å­˜åœ¨æ—¶æ‹‰å–é•œåƒï¼Œå¦‚æœ tag ä¸ºé latestï¼Œä¸” imagePullPolicy æœªé…ç½®ï¼Œé»˜è®¤ä¸º IfNotPresent&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;æ›´æ”¹é•œåƒæ‹‰å–ç­–ç•¥ä¸º IfNotPresentï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-pod-é‡å¯ç­–ç•¥&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-pod-é‡å¯ç­–ç•¥&#34;&gt;#&lt;/a&gt; 3. &lt;strong&gt;Pod&lt;/strong&gt; é‡å¯ç­–ç•¥&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ“ä½œæ–¹å¼&lt;/th&gt;
&lt;th&gt;è¯´æ˜&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;é»˜è®¤ç­–ç•¥ã€‚å®¹å™¨å¤±æ•ˆæ—¶ï¼Œè‡ªåŠ¨é‡å¯è¯¥å®¹å™¨&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OnFailure&lt;/td&gt;
&lt;td&gt;å®¹å™¨ä»¥ä¸ä¸º 0 çš„çŠ¶æ€ç ç»ˆæ­¢ï¼Œè‡ªåŠ¨é‡å¯è¯¥å®¹å™¨&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;æ— è®ºä½•ç§çŠ¶æ€ï¼Œéƒ½ä¸ä¼šé‡å¯&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;æŒ‡å®šé‡å¯ç­–ç•¥ä¸º Always ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-podçš„ä¸‰ç§æ¢é’ˆ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-podçš„ä¸‰ç§æ¢é’ˆ&#34;&gt;#&lt;/a&gt; 4. Pod çš„ä¸‰ç§æ¢é’ˆ&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ç§ç±»&lt;/th&gt;
&lt;th&gt;è¯´æ˜&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;startupProbe&lt;/td&gt;
&lt;td&gt;Kubernetes1.16 æ–°åŠ çš„æ¢æµ‹æ–¹å¼ï¼Œç”¨äºåˆ¤æ–­å®¹å™¨å†…çš„åº”ç”¨ç¨‹åºæ˜¯å¦å·²ç»å¯åŠ¨ã€‚å¦‚æœé…ç½®äº† startupProbeï¼Œå°±ä¼šå…ˆç¦ç”¨å…¶ä»–æ¢æµ‹ï¼Œç›´åˆ°å®ƒæˆåŠŸä¸ºæ­¢ã€‚å¦‚æœæ¢æµ‹å¤±è´¥ï¼ŒKubelet ä¼šæ€æ­»å®¹å™¨ï¼Œä¹‹åæ ¹æ®é‡å¯ç­–ç•¥è¿›è¡Œå¤„ç†ï¼Œå¦‚æœæ¢æµ‹æˆåŠŸï¼Œæˆ–æ²¡æœ‰é…ç½® startupProbeï¼Œåˆ™çŠ¶æ€ä¸ºæˆåŠŸï¼Œä¹‹åå°±ä¸å†æ¢æµ‹ã€‚&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;livenessProbe&lt;/td&gt;
&lt;td&gt;ç”¨äºæ¢æµ‹å®¹å™¨æ˜¯å¦åœ¨è¿è¡Œï¼Œå¦‚æœæ¢æµ‹å¤±è´¥ï¼Œkubelet ä¼š â€œæ€æ­»â€ å®¹å™¨å¹¶æ ¹æ®é‡å¯ç­–ç•¥è¿›è¡Œç›¸åº”çš„å¤„ç†ã€‚å¦‚æœæœªæŒ‡å®šè¯¥æ¢é’ˆï¼Œå°†é»˜è®¤ä¸º Success&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;readinessProbe&lt;/td&gt;
&lt;td&gt;ä¸€èˆ¬ç”¨äºæ¢æµ‹å®¹å™¨å†…çš„ç¨‹åºæ˜¯å¦å¥åº·ï¼Œå³åˆ¤æ–­å®¹å™¨æ˜¯å¦ä¸ºå°±ç»ªï¼ˆReadyï¼‰çŠ¶æ€ã€‚å¦‚æœæ˜¯ï¼Œåˆ™å¯ä»¥å¤„ç†è¯·æ±‚ï¼Œåä¹‹ Endpoints Controller å°†ä»æ‰€æœ‰çš„ Service çš„ Endpoints ä¸­åˆ é™¤æ­¤å®¹å™¨æ‰€åœ¨ Pod çš„ IP åœ°å€ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†é»˜è®¤ä¸º Success&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;5-podæ¢é’ˆçš„å®ç°æ–¹å¼&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-podæ¢é’ˆçš„å®ç°æ–¹å¼&#34;&gt;#&lt;/a&gt; 5. Pod æ¢é’ˆçš„å®ç°æ–¹å¼&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;å®ç°æ–¹å¼&lt;/th&gt;
&lt;th&gt;è¯´æ˜&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ExecAction&lt;/td&gt;
&lt;td&gt;åœ¨å®¹å™¨å†…æ‰§è¡Œä¸€ä¸ªæŒ‡å®šçš„å‘½ä»¤ï¼Œå¦‚æœå‘½ä»¤è¿”å›å€¼ä¸º 0ï¼Œåˆ™è®¤ä¸ºå®¹å™¨å¥åº·&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCPSocketAction&lt;/td&gt;
&lt;td&gt;é€šè¿‡ TCP è¿æ¥æ£€æŸ¥å®¹å™¨æŒ‡å®šçš„ç«¯å£ï¼Œå¦‚æœç«¯å£å¼€æ”¾ï¼Œåˆ™è®¤ä¸ºå®¹å™¨å¥åº·&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HTTPGetAction&lt;/td&gt;
&lt;td&gt;å¯¹æŒ‡å®šçš„ URL è¿›è¡Œ Get è¯·æ±‚ï¼Œå¦‚æœçŠ¶æ€ç åœ¨ 200~400 ä¹‹é—´ï¼Œåˆ™è®¤ä¸ºå®¹å™¨å¥åº·&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;6-å¥åº·æ£€æŸ¥é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-å¥åº·æ£€æŸ¥é…ç½®&#34;&gt;#&lt;/a&gt; 6. å¥åº·æ£€æŸ¥é…ç½®&lt;/h4&gt;
&lt;p&gt;é…ç½®å¥åº·æ£€æŸ¥ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-prestopå’Œ-poststarté…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-prestopå’Œ-poststarté…ç½®&#34;&gt;#&lt;/a&gt; 7. PreStop å’Œ PostStart é…ç½®&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
          lifecycle:
            postStart:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - mkdir /data
            preStop:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - sleep 30
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:23:48.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/985149017.html</id>
        <title>äºŒè¿›åˆ¶é«˜å¯ç”¨å®‰è£…K8Sé›†ç¾¤</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/985149017.html"/>
        <content type="html">&lt;h2 id=&#34;äºŒè¿›åˆ¶é«˜å¯ç”¨å®‰è£…k8sé›†ç¾¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#äºŒè¿›åˆ¶é«˜å¯ç”¨å®‰è£…k8sé›†ç¾¤&#34;&gt;#&lt;/a&gt; äºŒè¿›åˆ¶é«˜å¯ç”¨å®‰è£… K8s é›†ç¾¤&lt;/h2&gt;
&lt;h4 id=&#34;1-åŸºæœ¬é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-åŸºæœ¬é…ç½®&#34;&gt;#&lt;/a&gt; 1. åŸºæœ¬é…ç½®&lt;/h4&gt;
&lt;h5 id=&#34;11-åŸºæœ¬ç¯å¢ƒé…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-åŸºæœ¬ç¯å¢ƒé…ç½®&#34;&gt;#&lt;/a&gt; 1.1 åŸºæœ¬ç¯å¢ƒé…ç½®&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ä¸»æœºå&lt;/th&gt;
&lt;th&gt;IP åœ°å€&lt;/th&gt;
&lt;th&gt;è¯´æ˜&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master èŠ‚ç‚¹ * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived è™šæ‹Ÿ IPï¼ˆä¸å ç”¨æœºå™¨ï¼‰&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker èŠ‚ç‚¹ * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;è¯·ç»Ÿä¸€æ›¿æ¢è¿™äº›ç½‘æ®µï¼ŒPod ç½‘æ®µå’Œ service å’Œå®¿ä¸»æœºç½‘æ®µä¸è¦é‡å¤ï¼ï¼ï¼&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* é…ç½®ä¿¡æ¯ *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;å¤‡æ³¨&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ç³»ç»Ÿç‰ˆæœ¬&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod ç½‘æ®µ&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service ç½‘æ®µ&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;æ›´æ”¹ä¸»æœºåï¼ˆå…¶å®ƒèŠ‚ç‚¹æŒ‰éœ€ä¿®æ”¹ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® hostsï¼Œä¿®æ”¹ /etc/hosts å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® yum æºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# é…ç½®åŸºç¡€æº
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å¿…å¤‡å·¥å…·å®‰è£…ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å…³é—­é˜²ç«å¢™ã€selinuxã€dnsmasqã€swapã€å¼€å¯ rsyslogã€‚æœåŠ¡å™¨é…ç½®å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å…³é—­ swap åˆ†åŒºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å®‰è£… ntpdateï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åŒæ­¥æ—¶é—´å¹¶é…ç½®ä¸Šæµ·æ—¶åŒºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# åŠ å…¥åˆ°crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® limitï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# æœ«å°¾æ·»åŠ å¦‚ä¸‹å†…å®¹
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å‡çº§ç³»ç»Ÿï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;å…å¯†é’¥ç™»å½•å…¶ä»–èŠ‚ç‚¹ï¼Œå®‰è£…è¿‡ç¨‹ä¸­ç”Ÿæˆé…ç½®æ–‡ä»¶å’Œè¯ä¹¦å‡åœ¨ Master01 ä¸Šæ“ä½œï¼Œé›†ç¾¤ç®¡ç†ä¹Ÿåœ¨ Master01 ä¸Šæ“ä½œï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼šå…¬æœ‰äº‘ç¯å¢ƒï¼Œå¯èƒ½éœ€è¦æŠŠ kubectl æ”¾åœ¨ä¸€ä¸ªé Master èŠ‚ç‚¹ä¸Š&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;ä¸‹è½½å®‰è£…æ‰€æœ‰çš„æºç æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-å†…æ ¸é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-å†…æ ¸é…ç½®&#34;&gt;#&lt;/a&gt; 1.2 å†…æ ¸é…ç½®&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å®‰è£… ipvsadmï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® ipvs æ¨¡å—ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º ipvs.confï¼Œå¹¶é…ç½®å¼€æœºè‡ªåŠ¨åŠ è½½ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# åŠ å…¥ä»¥ä¸‹å†…å®¹
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ç„¶åæ‰§è¡Œ systemctl enable --now systemd-modules-load.service å³å¯ï¼ˆæŠ¥é”™ä¸ç”¨ç®¡ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å†…æ ¸ä¼˜åŒ–é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åº”ç”¨é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½®å®Œå†…æ ¸åï¼Œé‡å¯æœºå™¨ï¼Œä¹‹åæŸ¥çœ‹å†…æ ¸æ¨¡å—æ˜¯å¦å·²è‡ªåŠ¨åŠ è½½ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-é«˜å¯ç”¨ç»„ä»¶å®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-é«˜å¯ç”¨ç»„ä»¶å®‰è£…&#34;&gt;#&lt;/a&gt; 2. é«˜å¯ç”¨ç»„ä»¶å®‰è£…&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼šå¦‚æœå®‰è£…çš„ä¸æ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼Œhaproxy å’Œ keepalived æ— éœ€å®‰è£…&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼šå…¬æœ‰äº‘è¦ç”¨å…¬æœ‰äº‘è‡ªå¸¦çš„è´Ÿè½½å‡è¡¡ï¼Œæ¯”å¦‚é˜¿é‡Œäº‘çš„ SLBã€NLBï¼Œè…¾è®¯äº‘çš„ ELBï¼Œç”¨æ¥æ›¿ä»£ haproxy å’Œ keepalivedï¼Œå› ä¸ºå…¬æœ‰äº‘å¤§éƒ¨åˆ†éƒ½æ˜¯ä¸æ”¯æŒ keepalived çš„ã€‚&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;é€šè¿‡ yum å®‰è£… HAProxy å’Œ KeepAlivedï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;é…ç½® HAProxyï¼Œéœ€è¦æ³¨æ„é»„è‰²éƒ¨åˆ†çš„ IPï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:8443       #HAProxyç›‘å¬ç«¯å£
  bind 127.0.0.1:8443     #HAProxyç›‘å¬ç«¯å£
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IPåœ°å€
  server k8s-master02	192.168.1.72:6443  check       #API Server IPåœ°å€
  server k8s-master03	192.168.1.73:6443  check       #API Server IPåœ°å€
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;é…ç½® KeepAlivedï¼Œéœ€è¦æ³¨æ„é»„è‰²éƒ¨åˆ†çš„é…ç½®ã€‚&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;çš„é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #ç½‘å¡åç§°
    mcast_src_ip 192.168.1.71      #K8s-master01 IPåœ°å€
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIPåœ°å€
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 èŠ‚ç‚¹&lt;/mark&gt;çš„é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #ç½‘å¡åç§°
    mcast_src_ip 192.168.1.72       #K8s-master02 IPåœ°å€
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIPåœ°å€
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 èŠ‚ç‚¹&lt;/mark&gt;çš„é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #ç½‘å¡åç§°
    mcast_src_ip 192.168.1.73        #K8s-master03 IPåœ°å€
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIPåœ°å€
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ master èŠ‚ç‚¹&lt;/mark&gt;é…ç½® KeepAlived å¥åº·æ£€æŸ¥æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ master èŠ‚ç‚¹&lt;/mark&gt;é…ç½®å¥åº·æ£€æŸ¥æ–‡ä»¶æ·»åŠ æ‰§è¡Œæƒé™ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ master èŠ‚ç‚¹&lt;/mark&gt;å¯åŠ¨ haproxy å’Œ keepalivedï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;é‡è¦ï¼šå¦‚æœå®‰è£…äº† keepalived å’Œ haproxyï¼Œéœ€è¦æµ‹è¯• keepalived æ˜¯å¦æ˜¯æ­£å¸¸çš„&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;æ‰€æœ‰èŠ‚ç‚¹æµ‹è¯•VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœ ping ä¸é€šä¸” telnet æ²¡æœ‰å‡ºç° ] ï¼Œåˆ™è®¤ä¸º VIP ä¸å¯ä»¥ï¼Œä¸å¯åœ¨ç»§ç»­å¾€ä¸‹æ‰§è¡Œï¼Œéœ€è¦æ’æŸ¥ keepalived çš„é—®é¢˜ï¼Œæ¯”å¦‚é˜²ç«å¢™å’Œ selinuxï¼Œhaproxy å’Œ keepalived çš„çŠ¶æ€ï¼Œç›‘å¬ç«¯å£ç­‰&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ‰€æœ‰èŠ‚ç‚¹æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€å¿…é¡»ä¸º disable å’Œ inactiveï¼šsystemctl status firewalld&lt;/li&gt;
&lt;li&gt;æ‰€æœ‰èŠ‚ç‚¹æŸ¥çœ‹ selinux çŠ¶æ€ï¼Œå¿…é¡»ä¸º disableï¼šgetenforce&lt;/li&gt;
&lt;li&gt;master èŠ‚ç‚¹æŸ¥çœ‹ haproxy å’Œ keepalived çŠ¶æ€ï¼šsystemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master èŠ‚ç‚¹æŸ¥çœ‹ç›‘å¬ç«¯å£ï¼šnetstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å¦‚æœä»¥ä¸Šéƒ½æ²¡æœ‰é—®é¢˜ï¼Œéœ€è¦ç¡®è®¤ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;æ˜¯å¦æ˜¯å…¬æœ‰äº‘æœºå™¨&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ˜¯å¦æ˜¯ç§æœ‰äº‘æœºå™¨ï¼ˆç±»ä¼¼ OpenStackï¼‰&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ä¸Šè¿°å…¬æœ‰äº‘ä¸€èˆ¬éƒ½æ˜¯ä¸æ”¯æŒ keepalivedï¼Œç§æœ‰äº‘å¯èƒ½ä¹Ÿæœ‰é™åˆ¶ï¼Œéœ€è¦å’Œè‡ªå·±çš„ç§æœ‰äº‘ç®¡ç†å‘˜å’¨è¯¢&lt;/p&gt;
&lt;h4 id=&#34;3-runtimeå®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtimeå®‰è£…&#34;&gt;#&lt;/a&gt; 3. Runtime å®‰è£…&lt;/h4&gt;
&lt;p&gt;å¦‚æœå®‰è£…çš„ç‰ˆæœ¬ä½äº 1.24ï¼Œé€‰æ‹© Docker å’Œ Containerd å‡å¯ï¼Œé«˜äº 1.24 å»ºè®®é€‰æ‹© Containerd ä½œä¸º Runtimeï¼Œä¸å†æ¨èä½¿ç”¨ Docker ä½œä¸º Runtimeã€‚&lt;/p&gt;
&lt;h5 id=&#34;31-å®‰è£…containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-å®‰è£…containerd&#34;&gt;#&lt;/a&gt; 3.1 å®‰è£… Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½®å®‰è£…æºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å®‰è£… docker-ceï¼ˆå¦‚æœåœ¨ä»¥å‰å·²ç»å®‰è£…è¿‡ï¼Œéœ€è¦é‡æ–°å®‰è£…æ›´æ–°ä¸€ä¸‹ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;å¯ä»¥æ— éœ€å¯åŠ¨ Dockerï¼Œåªéœ€è¦é…ç½®å’Œå¯åŠ¨ Containerd å³å¯ã€‚&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;é¦–å…ˆé…ç½® Containerd æ‰€éœ€çš„æ¨¡å—ï¼ˆ&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åŠ è½½æ¨¡å—ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ï¼Œé…ç½® Containerd æ‰€éœ€çš„å†…æ ¸ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åŠ è½½å†…æ ¸ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ç”Ÿæˆ Containerd çš„é…ç½®æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;æ›´æ”¹ Containerd çš„ Cgroup å’Œ Pause é•œåƒé…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å¯åŠ¨ Containerdï¼Œå¹¶é…ç½®å¼€æœºè‡ªå¯åŠ¨ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® crictl å®¢æˆ·ç«¯è¿æ¥çš„è¿è¡Œæ—¶ä½ç½®ï¼ˆå¯é€‰ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-k8såŠetcdå®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-k8såŠetcdå®‰è£…&#34;&gt;#&lt;/a&gt; 4 . K8S åŠ etcd å®‰è£…&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; ä¸‹è½½ kubernetes å®‰è£…åŒ…ï¼ˆ1.32.3 éœ€è¦æ›´æ”¹ä¸ºä½ çœ‹åˆ°çš„æœ€æ–°ç‰ˆæœ¬ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æœ€æ–°ç‰ˆè·å–åœ°å€ï¼š&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;ä»¥ä¸‹æ“ä½œéƒ½åœ¨ master01 æ‰§è¡Œ&lt;/mark&gt;&lt;/p&gt;
&lt;p&gt;ä¸‹è½½ etcd å®‰è£…åŒ…ï¼š&lt;a href=&#34;https://github.com/etcd-io/etcd/releases/&#34;&gt;https://github.com/etcd-io/etcd/releases/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è§£å‹ kubernetes å®‰è£…æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è§£å‹ etcd å®‰è£…æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&amp;#123;,ctl&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç‰ˆæœ¬æŸ¥çœ‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubelet --version
Kubernetes v1.32.3
[root@k8s-master01 ~]# etcdctl version
etcdctl version: 3.5.16
API version: 3.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å°†ç»„ä»¶å‘é€åˆ°å…¶ä»–èŠ‚ç‚¹&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;
WorkNodes=&#39;k8s-node01 k8s-node02&#39;
for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done
for NODE in $WorkNodes; do     scp /usr/local/bin/kube&amp;#123;let,-proxy&amp;#125; $NODE:/usr/local/bin/ ; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;åˆ‡æ¢åˆ° 1.32.x åˆ†æ”¯ï¼ˆå…¶ä»–ç‰ˆæœ¬å¯ä»¥åˆ‡æ¢åˆ°å…¶ä»–åˆ†æ”¯ï¼Œ.x å³å¯ï¼Œä¸éœ€è¦æ›´æ”¹ä¸ºå…·ä½“çš„å°ç‰ˆæœ¬ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-ç”Ÿæˆè¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-ç”Ÿæˆè¯ä¹¦&#34;&gt;#&lt;/a&gt; 5 . ç”Ÿæˆè¯ä¹¦&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;äºŒè¿›åˆ¶å®‰è£…æœ€å…³é”®æ­¥éª¤ï¼Œä¸€æ­¥é”™è¯¯å…¨ç›˜çš†è¾“ï¼Œä¸€å®šè¦æ³¨æ„æ¯ä¸ªæ­¥éª¤éƒ½è¦æ˜¯æ­£ç¡®çš„&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; ä¸‹è½½ç”Ÿæˆè¯ä¹¦å·¥å…·ï¼ˆä¸‹è½½ä¸æˆåŠŸå¯ä»¥å»ç™¾åº¦ç½‘ç›˜ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&amp;quot; -O /usr/local/bin/cfssl
wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&amp;quot; -O /usr/local/bin/cfssljson
chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-etcdè¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-etcdè¯ä¹¦&#34;&gt;#&lt;/a&gt; 5.1 Etcd è¯ä¹¦&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º etcd è¯ä¹¦ç›®å½•ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/etcd/ssl -p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º kubernetes ç›¸å…³ç›®å½•ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/pki
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;ç”Ÿæˆ etcd è¯ä¹¦&lt;/p&gt;
&lt;p&gt;ç”Ÿæˆè¯ä¹¦çš„ CSRï¼ˆè¯ä¹¦ç­¾åè¯·æ±‚æ–‡ä»¶ï¼Œé…ç½®äº†ä¸€äº›åŸŸåã€å…¬å¸ã€å•ä½ï¼‰æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

# ç”Ÿæˆetcd CAè¯ä¹¦å’ŒCAè¯ä¹¦çš„key
cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca


cfssl gencert \
   -ca=/etc/etcd/ssl/etcd-ca.pem \
   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \
   -config=ca-config.json \
   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \
   -profile=kubernetes \
   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd

æ‰§è¡Œç»“æœ
[INFO] generate received request
 	[INFO] received CSR
     [INFO] generating key: rsa-2048
     [INFO] encoded CSR
     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å°†è¯ä¹¦å¤åˆ¶åˆ°å…¶ä»– master èŠ‚ç‚¹&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;

for NODE in $MasterNodes; do
     ssh $NODE &amp;quot;mkdir -p /etc/etcd/ssl&amp;quot;
     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do
       scp /etc/etcd/ssl/$&amp;#123;FILE&amp;#125; $NODE:/etc/etcd/ssl/$&amp;#123;FILE&amp;#125;
     done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;52-k8sç»„ä»¶è¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-k8sç»„ä»¶è¯ä¹¦&#34;&gt;#&lt;/a&gt; 5.2 K8s ç»„ä»¶è¯ä¹¦&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; ç”Ÿæˆ kubernetes CA è¯ä¹¦ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;521-apiserverè¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#521-apiserverè¯ä¹¦&#34;&gt;#&lt;/a&gt; 5.2.1 APIServer è¯ä¹¦&lt;/h6&gt;
&lt;p&gt;æ³¨æ„ï¼š10.96.0. æ˜¯ k8s service çš„ç½‘æ®µï¼Œå¦‚æœè¯´éœ€è¦æ›´æ”¹ k8s service ç½‘æ®µï¼Œé‚£å°±éœ€è¦æ›´æ”¹ 10.96.0.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç”Ÿæˆ apiserver çš„èšåˆè¯ä¹¦ï¼šï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca 

cfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿”å›ç»“æœï¼ˆå¿½ç•¥è­¦å‘Šï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2020/12/11 18:15:28 [INFO] generate received request
2020/12/11 18:15:28 [INFO] received CSR
2020/12/11 18:15:28 [INFO] generating key: rsa-2048

2020/12/11 18:15:28 [INFO] encoded CSR
2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845
2020/12/11 18:15:28 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;522-controllermanager&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#522-controllermanager&#34;&gt;#&lt;/a&gt; 5.2.2 ControllerManager&lt;/h6&gt;
&lt;p&gt;ç”Ÿæˆ controller-manage çš„è¯ä¹¦ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager

æ³¨æ„ï¼šä¿®æ”¹é»„è‰²éƒ¨åˆ†çš„IPåœ°å€
# set-clusterï¼šè®¾ç½®ä¸€ä¸ªé›†ç¾¤é¡¹ï¼Œ

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# è®¾ç½®ä¸€ä¸ªç¯å¢ƒé¡¹ï¼Œä¸€ä¸ªä¸Šä¸‹æ–‡
kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# set-credentials è®¾ç½®ä¸€ä¸ªç”¨æˆ·é¡¹

kubectl config set-credentials system:kube-controller-manager \
     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \
     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig


# ä½¿ç”¨æŸä¸ªç¯å¢ƒå½“åšé»˜è®¤ç¯å¢ƒ

kubectl config use-context system:kube-controller-manager@kubernetes \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;523-schedulerè¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#523-schedulerè¯ä¹¦&#34;&gt;#&lt;/a&gt; 5.2.3 Scheduler è¯ä¹¦&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler

æ³¨æ„ï¼šä¿®æ”¹é»„è‰²éƒ¨åˆ†çš„IPåœ°å€

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig


kubectl config set-credentials system:kube-scheduler \
     --client-certificate=/etc/kubernetes/pki/scheduler.pem \
     --client-key=/etc/kubernetes/pki/scheduler-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config set-context system:kube-scheduler@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-scheduler \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config use-context system:kube-scheduler@kubernetes \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;524-ç”Ÿæˆç®¡ç†å‘˜è¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#524-ç”Ÿæˆç®¡ç†å‘˜è¯ä¹¦&#34;&gt;#&lt;/a&gt; 5.2.4 ç”Ÿæˆç®¡ç†å‘˜è¯ä¹¦&lt;/h6&gt;
&lt;p&gt;Kubectl /etc/Kubernetes/admin.conf ~/.kube/config&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin

æ³¨æ„ï¼šä¿®æ”¹é»„è‰²éƒ¨åˆ†çš„IP

kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig
kubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;525-åˆ›å»ºserviceaccountè¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#525-åˆ›å»ºserviceaccountè¯ä¹¦&#34;&gt;#&lt;/a&gt; 5.2.5 åˆ›å»º ServiceAccount è¯ä¹¦&lt;/h6&gt;
&lt;p&gt;åˆ›å»ºä¸€å¯¹å…¬é’¥ï¼Œç”¨æ¥ç­¾å‘ ServiceAccount çš„ Tokenï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl genrsa -out /etc/kubernetes/pki/sa.key 2048
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿”å›ç»“æœï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Generating RSA private key, 2048 bit long modulus (2 primes)
...................................................................................+++++
...............+++++
e is 65537 (0x010001)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å‘é€è¯ä¹¦è‡³å…¶ä»–èŠ‚ç‚¹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do 
  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do 
    scp /etc/kubernetes/pki/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/pki/$&amp;#123;FILE&amp;#125;;
  done; 
  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do 
    scp /etc/kubernetes/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;;
  done;
done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹è¯ä¹¦æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# ls /etc/kubernetes/pki/
admin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem
admin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem
admin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr
[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l
23
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-kubernetesç»„ä»¶é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-kubernetesç»„ä»¶é…ç½®&#34;&gt;#&lt;/a&gt; 6. Kubernetes ç»„ä»¶é…ç½®&lt;/h4&gt;
&lt;h5 id=&#34;61-ecdé…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#61-ecdé…ç½®&#34;&gt;#&lt;/a&gt; 6.1 Ecd é…ç½®&lt;/h5&gt;
&lt;p&gt;Etcd é…ç½®å¤§è‡´ç›¸åŒï¼Œæ³¨æ„ä¿®æ”¹æ¯ä¸ª Master èŠ‚ç‚¹çš„ etcd é…ç½®çš„ä¸»æœºåå’Œ IP åœ°å€&lt;/p&gt;
&lt;h6 id=&#34;611-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#611-master01&#34;&gt;#&lt;/a&gt; 6.1.1 Master01&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master01&#39;     # k8s-master01åç§°
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.71:2380&#39;            # k8s-master01 IP
listen-client-urls: &#39;https://192.168.1.71:2379,http://127.0.0.1:2379&#39;   # k8s-master01 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.71:2380&#39;  # k8s-master01 IP
advertise-client-urls: &#39;https://192.168.1.71:2379&#39;        # k8s-master01 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;     # k8s-master01ã€k8s-master02ã€k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;612-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#612-master02&#34;&gt;#&lt;/a&gt; 6.1.2 Master02&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml	
name: &#39;k8s-master02&#39;   # k8s-master02åç§°
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.72:2380&#39;      # k8s-master02 IP
listen-client-urls: &#39;https://192.168.1.72:2379,http://127.0.0.1:2379&#39;    # k8s-master02 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.72:2380&#39;    # k8s-master02 IP
advertise-client-urls: &#39;https://192.168.1.72:2379&#39;     # k8s-master02 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;             # k8s-master01ã€k8s-master02ã€k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;613-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#613-master03&#34;&gt;#&lt;/a&gt; 6.1.3 Master03&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master03&#39;           # k8s-master03åç§°
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.73:2380&#39;           # k8s-master03 IP
listen-client-urls: &#39;https://192.168.1.73:2379,http://127.0.0.1:2379&#39;       # k8s-master03 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.73:2380&#39;      # k8s-master03 IP
advertise-client-urls: &#39;https://192.168.1.73:2379&#39;            # k8s-master03 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;                # k8s-master01ã€k8s-master02ã€k8s-master03 IP
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;614-å¯åŠ¨etcd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#614-å¯åŠ¨etcd&#34;&gt;#&lt;/a&gt; 6.1.4 å¯åŠ¨ Etcd&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º etcd service å¹¶å¯åŠ¨&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Service
Documentation=https://coreos.com/etcd/docs/latest/
After=network.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml
Restart=on-failure
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd3.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º etcd çš„è¯ä¹¦ç›®å½•ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/kubernetes/pki/etcd
ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/
systemctl daemon-reload
systemctl enable --now etcd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹ etcd çŠ¶æ€ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export ETCDCTL_API=3
etcdctl --endpoints=&amp;quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&amp;quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;62-apiserveré…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#62-apiserveré…ç½®&#34;&gt;#&lt;/a&gt; 6.2 APIServer é…ç½®&lt;/h5&gt;
&lt;h6 id=&#34;621-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#621-master01&#34;&gt;#&lt;/a&gt; 6.2.1 Master01&lt;/h6&gt;
&lt;p&gt;æ³¨æ„ï¼šæœ¬æ–‡æ¡£ä½¿ç”¨çš„ k8s service ç½‘æ®µä¸º 10.96.0.0/16ï¼Œè¯¥ç½‘æ®µä¸èƒ½å’Œå®¿ä¸»æœºçš„ç½‘æ®µã€Pod ç½‘æ®µçš„é‡å¤ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.71 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;622-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#622-master02&#34;&gt;#&lt;/a&gt; 6.2.2 Master02&lt;/h6&gt;
&lt;p&gt;æ³¨æ„ï¼šæœ¬æ–‡æ¡£ä½¿ç”¨çš„ k8s service ç½‘æ®µä¸º 10.96.0.0/16ï¼Œè¯¥ç½‘æ®µä¸èƒ½å’Œå®¿ä¸»æœºçš„ç½‘æ®µã€Pod ç½‘æ®µçš„é‡å¤ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.72 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;623-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#623-master03&#34;&gt;#&lt;/a&gt; 6.2.3 Master03&lt;/h6&gt;
&lt;p&gt;æ³¨æ„ï¼šæœ¬æ–‡æ¡£ä½¿ç”¨çš„ k8s service ç½‘æ®µä¸º 10.96.0.0/16ï¼Œè¯¥ç½‘æ®µä¸èƒ½å’Œå®¿ä¸»æœºçš„ç½‘æ®µã€Pod ç½‘æ®µçš„é‡å¤ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.73 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;624-å¯åŠ¨apiserver&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#624-å¯åŠ¨apiserver&#34;&gt;#&lt;/a&gt; 6.2.4 å¯åŠ¨ apiserver&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;å¼€å¯ kube-apiserverï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload &amp;amp;&amp;amp; systemctl enable --now kube-apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ£€æµ‹ kube-server çŠ¶æ€ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl status kube-apiserver

â— kube-apiserver.service â€“ Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s agoÂ 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœç³»ç»Ÿæ—¥å¿—æœ‰è¿™äº›æç¤ºå¯ä»¥å¿½ç•¥:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to â€œpick_firstâ€
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;CONNECTING &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;READY &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = â€œtransport is closingâ€
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;63-controllermanage&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#63-controllermanage&#34;&gt;#&lt;/a&gt; 6.3 ControllerManage&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;é…ç½® kube-controller-manager serviceï¼ˆæ‰€æœ‰ master èŠ‚ç‚¹é…ç½®ä¸€æ ·ï¼‰&lt;/p&gt;
&lt;p&gt;æ³¨æ„ï¼šæœ¬æ–‡æ¡£ä½¿ç”¨çš„ k8s Pod ç½‘æ®µä¸º 172.16.0.0/16ï¼Œè¯¥ç½‘æ®µä¸èƒ½å’Œå®¿ä¸»æœºçš„ç½‘æ®µã€k8s Service ç½‘æ®µçš„é‡å¤ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
      --v=2 \
      --root-ca-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \
      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --leader-elect=true \
      --use-service-account-credentials=true \
      --node-monitor-grace-period=40s \
      --node-monitor-period=5s \
      --controllers=*,bootstrapsigner,tokencleaner \
      --allocate-node-cidrs=true \
      --cluster-cidr=172.16.0.0/16 \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
      --node-cidr-mask-size=24
      
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;å¯åŠ¨ kube-controller-manager&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager
Created symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service â†’ /usr/lib/systemd/system/kube-controller-manager.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹å¯åŠ¨çŠ¶æ€&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl  status kube-controller-manager
â— kube-controller-manager.service â€“ Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)
 Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago
     Docs: https://github.com/  ubernetes/  ubernetes
 Main PID: 7518 (kube-controller)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;64-scheduler&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#64-scheduler&#34;&gt;#&lt;/a&gt; 6.4 Scheduler&lt;/h5&gt;
&lt;p&gt;æ‰€æœ‰ Master èŠ‚ç‚¹é…ç½® kube-scheduler serviceï¼ˆæ‰€æœ‰ master èŠ‚ç‚¹é…ç½®ä¸€æ ·ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service 
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
      --v=2 \
      --leader-elect=true \
      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¯åŠ¨ schedulerï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-scheduler
Created symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service â†’ /usr/lib/systemd/system/kube-scheduler.service.
[root@k8s-master01 pki]# systemctl status kube-scheduler
â— kube-scheduler.service - Kubernetes Scheduler
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 5815 (kube-scheduler)
    Tasks: 9
   Memory: 19.8M
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-tls-bootstrappingé…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-tls-bootstrappingé…ç½®&#34;&gt;#&lt;/a&gt; 7. TLS Bootstrapping é…ç½®&lt;/h4&gt;
&lt;p&gt;åªéœ€è¦åœ¨&lt;mark&gt; Master01&lt;/mark&gt; åˆ›å»º bootstrap&lt;/p&gt;
&lt;p&gt;æ³¨æ„ï¼š ä¿®æ”¹é»„è‰²éƒ¨åˆ†çš„ IP åœ°å€&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/bootstrap
kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig

[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¯ä»¥æ­£å¸¸æŸ¥è¯¢é›†ç¾¤çŠ¶æ€ï¼Œæ‰å¯ä»¥ç»§ç»­å¾€ä¸‹ï¼Œå¦åˆ™ä¸è¡Œï¼Œéœ€è¦æ’æŸ¥ k8s ç»„ä»¶æ˜¯å¦æœ‰æ•…éšœï¼ˆåªè¦æœ‰ç»“æœå³å¯ï¼Œå¦‚æœè¿”å›ä¸ä¸€æ ·ä¸å½±å“ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok        
scheduler            Healthy   ok        
etcd-0               Healthy   ok
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;åˆ›å»º bootstrap ç›¸å…³èµ„æºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml 
secret/bootstrap-token-c8ad9c created
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-nodeèŠ‚ç‚¹é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-nodeèŠ‚ç‚¹é…ç½®&#34;&gt;#&lt;/a&gt; 8. Node èŠ‚ç‚¹é…ç½®&lt;/h4&gt;
&lt;h5 id=&#34;81-å¤åˆ¶è¯ä¹¦&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#81-å¤åˆ¶è¯ä¹¦&#34;&gt;#&lt;/a&gt; 8.1 å¤åˆ¶è¯ä¹¦&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;å¤åˆ¶è¯ä¹¦è‡³å…¶ä»–èŠ‚ç‚¹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/kubernetes/

for NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do
     ssh $NODE mkdir -p /etc/kubernetes/pki
     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do
       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;
 done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ‰§è¡Œç»“æœï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    
â€¦
bootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;82-kubeleté…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#82-kubeleté…ç½®&#34;&gt;#&lt;/a&gt; 8.2 Kubelet é…ç½®&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º Kubelet é…ç½®ç›®å½•&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® kubelet service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service

[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kubelet

Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® kubelet service çš„é…ç½®æ–‡ä»¶ï¼ˆä¹Ÿå¯ä»¥å†™åˆ° kubelet.serviceï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Runtimeä¸ºContainerd
# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf

[Service]
Environment=&amp;quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&amp;quot;
Environment=&amp;quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&amp;quot;
Environment=&amp;quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&amp;quot;
Environment=&amp;quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=&#39;&#39; &amp;quot;
ExecStart=
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º kubelet çš„é…ç½®æ–‡ä»¶&lt;/p&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼šå¦‚æœæ›´æ”¹äº† k8s çš„ service ç½‘æ®µï¼Œéœ€è¦æ›´æ”¹ kubelet-conf.yml çš„ clusterDNS: é…ç½®ï¼Œæ”¹æˆ k8s Service ç½‘æ®µçš„ç¬¬åä¸ªåœ°å€ï¼Œæ¯”å¦‚ 10.96.0.10&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¯åŠ¨&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt; kubelet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ­¤æ—¶ç³»ç»Ÿæ—¥å¿— /var/log/messages**** æ˜¾ç¤ºåªæœ‰å¦‚ä¸‹ä¸¤ç§ä¿¡æ¯ä¸ºæ­£å¸¸ ****ï¼Œå®‰è£… calico åå³å¯æ¢å¤&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;å¦‚æœæœ‰å¾ˆå¤šæŠ¥é”™æ—¥å¿—ï¼Œæˆ–è€…æœ‰å¤§é‡çœ‹ä¸æ‡‚çš„æŠ¥é”™ï¼Œè¯´æ˜ kubelet çš„é…ç½®æœ‰è¯¯ï¼Œéœ€è¦æ£€æŸ¥ kubelet é…ç½®&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Master01 æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ (Ready æˆ– NotReady éƒ½æ­£å¸¸)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl get node
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;83-kube-proxyé…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#83-kube-proxyé…ç½®&#34;&gt;#&lt;/a&gt; 8.3 kube-proxy é…ç½®&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼Œå¦‚æœä¸æ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼Œ192.168.1.70:8443 æ”¹ä¸º master01 çš„åœ°å€ï¼Œ8443 æ”¹ä¸º apiserver çš„ç«¯å£ï¼Œé»˜è®¤æ˜¯ 6443&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;ç”Ÿæˆ kube-proxy çš„è¯ä¹¦ï¼Œä»¥ä¸‹æ“ä½œåªåœ¨&lt;mark&gt; Master01&lt;/mark&gt; æ‰§è¡Œ&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/pki
cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config set-credentials system:kube-proxy \
     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \
     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

kubectl config set-context system:kube-proxy@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-proxy \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config use-context system:kube-proxy@kubernetes \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å°† kubeconfig å‘é€è‡³å…¶ä»–èŠ‚ç‚¹&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do
     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done

for NODE in k8s-node01 k8s-node02; do
     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;æ·»åŠ  kube-proxy çš„é…ç½®å’Œ service æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /usr/lib/systemd/system/kube-proxy.service

[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --v=2

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœæ›´æ”¹äº†é›†ç¾¤ Pod çš„ç½‘æ®µï¼Œéœ€è¦æ›´æ”¹ kube-proxy.yaml çš„ clusterCIDR ä¸ºè‡ªå·±çš„ Pod ç½‘æ®µï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/kubernetes/kube-proxy.yaml

apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  acceptContentTypes: &amp;quot;&amp;quot;
  burst: 10
  contentType: application/vnd.kubernetes.protobuf
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
  qps: 5
clusterCIDR: 172.16.0.0/16 
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
hostnameOverride: &amp;quot;&amp;quot;
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
ipvs:
  masqueradeAll: true
  minSyncPeriod: 5s
  scheduler: &amp;quot;rr&amp;quot;
  syncPeriod: 30s
kind: KubeProxyConfiguration
metricsBindAddress: 127.0.0.1:10249
mode: &amp;quot;ipvs&amp;quot;
nodePortAddresses: null
oomScoreAdj: -999
portRange: &amp;quot;&amp;quot;
udpIdleTimeout: 250ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å¯åŠ¨ kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload
[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service â†’ /usr/lib/systemd/system/kube-proxy.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ­¤æ—¶ç³»ç»Ÿæ—¥å¿— /var/log/messages**** æ˜¾ç¤ºåªæœ‰å¦‚ä¸‹ä¸¤ç§ä¿¡æ¯ä¸ºæ­£å¸¸ ****ï¼Œå®‰è£… calico åå³å¯æ¢å¤&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;9-calicoç»„ä»¶çš„å®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-calicoç»„ä»¶çš„å®‰è£…&#34;&gt;#&lt;/a&gt; 9. Calico ç»„ä»¶çš„å®‰è£…&lt;/h4&gt;
&lt;p&gt;ä»¥ä¸‹æ­¥éª¤åªåœ¨ master01 æ‰§è¡Œï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ›´æ”¹ calico çš„ç½‘æ®µï¼Œä¸»è¦éœ€è¦å°†çº¢è‰²éƒ¨åˆ†çš„ç½‘æ®µï¼Œæ”¹ä¸ºè‡ªå·±çš„ Pod ç½‘æ®µ&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &amp;quot;s#POD_CIDR#172.16.0.0/16#g&amp;quot; calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æ£€æŸ¥ç½‘æ®µæ˜¯è‡ªå·±çš„ Pod ç½‘æ®µï¼Œ grep &amp;quot;IPV4POOL_CIDR&amp;quot; calico.yaml  -A 1&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;æŸ¥çœ‹å®¹å™¨å’ŒèŠ‚ç‚¹çŠ¶æ€ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 calico]# kubectl get po -n kube-system
NAME                                       READY   STATUS    RESTARTS      AGE
calico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s
calico-node-8fxqp                          1/1     Running   0             85s
calico-node-8nkfl                          1/1     Running   0             86s
calico-node-pmpf4                          1/1     Running   0             86s
calico-node-vnlk7                          1/1     Running   0             86s
calico-node-xpchb                          1/1     Running   0             85s
calico-typha-67c6dc57d6-259t8              1/1     Running   0             86s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;å¦‚æœå®¹å™¨çŠ¶æ€å¼‚å¸¸å¯ä»¥ä½¿ç”¨ kubectl describe æˆ–è€… kubectl logs æŸ¥çœ‹å®¹å™¨çš„æ—¥å¿—&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -n kube-system&lt;/li&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;10-å®‰è£…coredns&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-å®‰è£…coredns&#34;&gt;#&lt;/a&gt; 10. å®‰è£… CoreDNS&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœæ›´æ”¹äº† k8s service çš„ç½‘æ®µéœ€è¦å°† coredns çš„ serviceIP æ”¹æˆ k8s service ç½‘æ®µçš„ç¬¬åä¸ª IP&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk &#39;&amp;#123;print $3&amp;#125;&#39;`0
sed -i &amp;quot;s#KUBEDNS_SERVICE_IP#$&amp;#123;COREDNS_SERVICE_IP&amp;#125;#g&amp;quot; CoreDNS/coredns.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å®‰è£… coredns&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11-metricséƒ¨ç½²&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-metricséƒ¨ç½²&#34;&gt;#&lt;/a&gt; 11. Metrics éƒ¨ç½²&lt;/h4&gt;
&lt;p&gt;åœ¨æ–°ç‰ˆçš„ Kubernetes ä¸­ç³»ç»Ÿèµ„æºçš„é‡‡é›†å‡ä½¿ç”¨ Metrics-serverï¼Œå¯ä»¥é€šè¿‡ Metrics é‡‡é›†èŠ‚ç‚¹å’Œ Pod çš„å†…å­˜ã€ç£ç›˜ã€CPU å’Œç½‘ç»œçš„ä½¿ç”¨ç‡ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ä¸‹æ“ä½œå‡åœ¨&lt;mark&gt; master01 èŠ‚ç‚¹&lt;/mark&gt;æ‰§è¡Œï¼Œå®‰è£… metrics server:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/metrics-server
kubectl  create -f . 

serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç­‰å¾… metrics server å¯åŠ¨ç„¶åæŸ¥çœ‹çŠ¶æ€ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl  top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master01   231m         5%     1620Mi          42%       
k8s-master02   274m         6%     1203Mi          31%       
k8s-master03   202m         5%     1251Mi          32%       
k8s-node01     69m          1%     667Mi           17%       
k8s-node02     73m          1%     650Mi           16%
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœæœ‰å¦‚ä¸‹æŠ¥é”™ï¼Œå¯ä»¥ç­‰å¾… 10 åˆ†é’Ÿåï¼Œå†æ¬¡æŸ¥çœ‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-dashboardéƒ¨ç½²&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-dashboardéƒ¨ç½²&#34;&gt;#&lt;/a&gt; 12. Dashboard éƒ¨ç½²&lt;/h4&gt;
&lt;h5 id=&#34;121-å®‰è£…dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#121-å®‰è£…dashboard&#34;&gt;#&lt;/a&gt; 12.1 å®‰è£… Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard ç”¨äºå±•ç¤ºé›†ç¾¤ä¸­çš„å„ç±»èµ„æºï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥é€šè¿‡ Dashboard å®æ—¶æŸ¥çœ‹ Pod çš„æ—¥å¿—å’Œåœ¨å®¹å™¨ä¸­æ‰§è¡Œä¸€äº›å‘½ä»¤ç­‰ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;122-ç™»å½•dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#122-ç™»å½•dashboard&#34;&gt;#&lt;/a&gt; 12.2 ç™»å½• dashboard&lt;/h5&gt;
&lt;p&gt;åœ¨è°·æ­Œæµè§ˆå™¨ï¼ˆChromeï¼‰å¯åŠ¨æ–‡ä»¶ä¸­åŠ å…¥å¯åŠ¨å‚æ•°ï¼Œç”¨äºè§£å†³æ— æ³•è®¿é—® Dashboard çš„é—®é¢˜ï¼Œå‚è€ƒä¸‹å›¾ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;æ›´æ”¹ dashboard çš„ svc ä¸º NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;å°† ClusterIP æ›´æ”¹ä¸º NodePortï¼ˆå¦‚æœå·²ç»ä¸º NodePort å¿½ç•¥æ­¤æ­¥éª¤ï¼‰&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;æŸ¥çœ‹ç«¯å£å·ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ ¹æ®è‡ªå·±çš„å®ä¾‹ç«¯å£å·ï¼Œé€šè¿‡ä»»æ„å®‰è£…äº† kube-proxy çš„å®¿ä¸»æœºçš„ IP + ç«¯å£å³å¯è®¿é—®åˆ° dashboardï¼š&lt;/p&gt;
&lt;p&gt;è®¿é—® Dashboardï¼š&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; ï¼ˆæŠŠ IP åœ°å€å’Œç«¯å£æ”¹æˆä½ è‡ªå·±çš„ï¼‰é€‰æ‹©ç™»å½•æ–¹å¼ä¸ºä»¤ç‰Œï¼ˆå³ token æ–¹å¼ï¼‰ï¼Œå‚è€ƒä¸‹å›¾ï¼š&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;åˆ›å»ºç™»å½• Tokenï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å°† token å€¼è¾“å…¥åˆ°ä»¤ç‰Œåï¼Œå•å‡»ç™»å½•å³å¯è®¿é—® Dashboardï¼Œå‚è€ƒä¸‹å›¾ï¼š&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;14-containerdé…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-containerdé…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;#&lt;/a&gt; 14. Containerd é…ç½®é•œåƒåŠ é€Ÿ&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#æ·»åŠ ä»¥ä¸‹é…ç½®é•œåƒåŠ é€ŸæœåŠ¡
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ‰€æœ‰èŠ‚ç‚¹é‡æ–°å¯åŠ¨ Containerdï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;15-dockeré…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-dockeré…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;#&lt;/a&gt; 15. Docker é…ç½®é•œåƒåŠ é€Ÿ&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ‰€æœ‰èŠ‚ç‚¹é‡æ–°å¯åŠ¨ Dockerï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-10T12:58:40.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/2771271649.html</id>
        <title>äº‘åŸç”ŸK8så®‰å…¨ä¸“å®¶CKSè®¤è¯è€ƒé¢˜è¯¦è§£</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/2771271649.html"/>
        <content type="html">&lt;div class=&#34;hbe hbe-container&#34; id=&#34;hexo-blog-encrypt&#34; data-wpm=&#34;æŠ±æ­‰, è¿™ä¸ªå¯†ç çœ‹ç€ä¸å¤ªå¯¹, è¯·å†è¯•è¯•ã€‚&#34; data-whm=&#34;æŠ±æ­‰, è¿™ä¸ªæ–‡ç« ä¸èƒ½è¢«æ ¡éªŒ, ä¸è¿‡æ‚¨è¿˜æ˜¯èƒ½çœ‹çœ‹è§£å¯†åçš„å†…å®¹ã€‚&#34;&gt;
  &lt;script id=&#34;hbeData&#34; type=&#34;hbeData&#34; data-hmacdigest=&#34;51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d&#34;&gt;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&lt;/script&gt;
  &lt;div class=&#34;hbe hbe-content&#34;&gt;
    &lt;div class=&#34;hbe hbe-input hbe-input-xray&#34;&gt;
      &lt;input class=&#34;hbe hbe-input-field hbe-input-field-xray&#34; type=&#34;password&#34; id=&#34;hbePass&#34;&gt;
      &lt;label class=&#34;hbe hbe-input-label hbe-input-label-xray&#34; for=&#34;hbePass&#34;&gt;
        &lt;span class=&#34;hbe hbe-input-label-content hbe-input-label-content-xray&#34;&gt;æ‚¨å¥½, è¿™é‡Œéœ€è¦è¾“å…¥å¯†ç ã€‚&lt;/span&gt;
      &lt;/label&gt;
      &lt;svg class=&#34;hbe hbe-graphic hbe-graphic-xray&#34; width=&#34;300%&#34; height=&#34;100%&#34; viewBox=&#34;0 0 1200 60&#34; preserveAspectRatio=&#34;none&#34;&gt;
        &lt;path d=&#34;M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
        &lt;path d=&#34;M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
      &lt;/svg&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;script data-pjax src=&#34;/lib/hbe.js&#34;&gt;&lt;/script&gt;&lt;link href=&#34;/css/hbe.style.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt;</content>
        <category term="Kubernetes" />
        <updated>2025-04-09T13:38:39.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3166738000.html</id>
        <title>Kubeadmé«˜å¯ç”¨å®‰è£…K8sé›†ç¾¤</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3166738000.html"/>
        <content type="html">&lt;h2 id=&#34;kubeadmé«˜å¯ç”¨å®‰è£…k8sé›†ç¾¤&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#kubeadmé«˜å¯ç”¨å®‰è£…k8sé›†ç¾¤&#34;&gt;#&lt;/a&gt; Kubeadm é«˜å¯ç”¨å®‰è£… K8s é›†ç¾¤&lt;/h2&gt;
&lt;h4 id=&#34;1-åŸºæœ¬é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-åŸºæœ¬é…ç½®&#34;&gt;#&lt;/a&gt; 1. åŸºæœ¬é…ç½®&lt;/h4&gt;
&lt;h5 id=&#34;11-åŸºæœ¬ç¯å¢ƒé…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-åŸºæœ¬ç¯å¢ƒé…ç½®&#34;&gt;#&lt;/a&gt; 1.1 åŸºæœ¬ç¯å¢ƒé…ç½®&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ä¸»æœºå&lt;/th&gt;
&lt;th&gt;IP åœ°å€&lt;/th&gt;
&lt;th&gt;è¯´æ˜&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master èŠ‚ç‚¹ * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived è™šæ‹Ÿ IPï¼ˆä¸å ç”¨æœºå™¨ï¼‰&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker èŠ‚ç‚¹ * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;è¯·ç»Ÿä¸€æ›¿æ¢è¿™äº›ç½‘æ®µï¼ŒPod ç½‘æ®µå’Œ service å’Œå®¿ä¸»æœºç½‘æ®µä¸è¦é‡å¤ï¼ï¼ï¼&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* é…ç½®ä¿¡æ¯ *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;å¤‡æ³¨&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ç³»ç»Ÿç‰ˆæœ¬&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod ç½‘æ®µ&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service ç½‘æ®µ&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;æ›´æ”¹ä¸»æœºåï¼ˆå…¶å®ƒèŠ‚ç‚¹æŒ‰éœ€ä¿®æ”¹ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® hostsï¼Œä¿®æ”¹ /etc/hosts å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® yum æºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# é…ç½®åŸºç¡€æº
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å¿…å¤‡å·¥å…·å®‰è£…ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å…³é—­é˜²ç«å¢™ã€selinuxã€dnsmasqã€swapã€å¼€å¯ rsyslogã€‚æœåŠ¡å™¨é…ç½®å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å…³é—­ swap åˆ†åŒºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å®‰è£… ntpdateï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åŒæ­¥æ—¶é—´å¹¶é…ç½®ä¸Šæµ·æ—¶åŒºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# åŠ å…¥åˆ°crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® limitï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# æœ«å°¾æ·»åŠ å¦‚ä¸‹å†…å®¹
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å‡çº§ç³»ç»Ÿï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;å…å¯†é’¥ç™»å½•å…¶ä»–èŠ‚ç‚¹ï¼Œå®‰è£…è¿‡ç¨‹ä¸­ç”Ÿæˆé…ç½®æ–‡ä»¶å’Œè¯ä¹¦å‡åœ¨ Master01 ä¸Šæ“ä½œï¼Œé›†ç¾¤ç®¡ç†ä¹Ÿåœ¨ Master01 ä¸Šæ“ä½œï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼šå…¬æœ‰äº‘ç¯å¢ƒï¼Œå¯èƒ½éœ€è¦æŠŠ kubectl æ”¾åœ¨ä¸€ä¸ªé Master èŠ‚ç‚¹ä¸Š&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;ä¸‹è½½å®‰è£…æ‰€æœ‰çš„æºç æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-å†…æ ¸é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-å†…æ ¸é…ç½®&#34;&gt;#&lt;/a&gt; 1.2 å†…æ ¸é…ç½®&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å®‰è£… ipvsadmï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® ipvs æ¨¡å—ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åˆ›å»º ipvs.confï¼Œå¹¶é…ç½®å¼€æœºè‡ªåŠ¨åŠ è½½ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# åŠ å…¥ä»¥ä¸‹å†…å®¹
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ç„¶åæ‰§è¡Œ systemctl enable --now systemd-modules-load.service å³å¯ï¼ˆæŠ¥é”™ä¸ç”¨ç®¡ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å†…æ ¸ä¼˜åŒ–é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åº”ç”¨é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½®å®Œå†…æ ¸åï¼Œé‡å¯æœºå™¨ï¼Œä¹‹åæŸ¥çœ‹å†…æ ¸æ¨¡å—æ˜¯å¦å·²è‡ªåŠ¨åŠ è½½ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-é«˜å¯ç”¨ç»„ä»¶å®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-é«˜å¯ç”¨ç»„ä»¶å®‰è£…&#34;&gt;#&lt;/a&gt; 2. é«˜å¯ç”¨ç»„ä»¶å®‰è£…&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼šå¦‚æœå®‰è£…çš„ä¸æ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼Œhaproxy å’Œ keepalived æ— éœ€å®‰è£…&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;æ³¨æ„ï¼šå…¬æœ‰äº‘è¦ç”¨å…¬æœ‰äº‘è‡ªå¸¦çš„è´Ÿè½½å‡è¡¡ï¼Œæ¯”å¦‚é˜¿é‡Œäº‘çš„ SLBã€NLBï¼Œè…¾è®¯äº‘çš„ ELBï¼Œç”¨æ¥æ›¿ä»£ haproxy å’Œ keepalivedï¼Œå› ä¸ºå…¬æœ‰äº‘å¤§éƒ¨åˆ†éƒ½æ˜¯ä¸æ”¯æŒ keepalived çš„ã€‚&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;é€šè¿‡ yum å®‰è£… HAProxy å’Œ KeepAlivedï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;é…ç½® HAProxyï¼Œéœ€è¦æ³¨æ„é»„è‰²éƒ¨åˆ†çš„ IPï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443       #HAProxyç›‘å¬ç«¯å£
  bind 127.0.0.1:16443     #HAProxyç›‘å¬ç«¯å£
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IPåœ°å€
  server k8s-master02	192.168.1.72:6443  check       #API Server IPåœ°å€
  server k8s-master03	192.168.1.73:6443  check       #API Server IPåœ°å€
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;é…ç½® KeepAlivedï¼Œéœ€è¦æ³¨æ„é»„è‰²éƒ¨åˆ†çš„é…ç½®ã€‚&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;çš„é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #ç½‘å¡åç§°
    mcast_src_ip 192.168.1.71      #K8s-master01 IPåœ°å€
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIPåœ°å€
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 èŠ‚ç‚¹&lt;/mark&gt;çš„é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #ç½‘å¡åç§°
    mcast_src_ip 192.168.1.72       #K8s-master02 IPåœ°å€
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIPåœ°å€
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 èŠ‚ç‚¹&lt;/mark&gt;çš„é…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #ç½‘å¡åç§°
    mcast_src_ip 192.168.1.73        #K8s-master03 IPåœ°å€
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIPåœ°å€
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ master èŠ‚ç‚¹&lt;/mark&gt;é…ç½® KeepAlived å¥åº·æ£€æŸ¥æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ master èŠ‚ç‚¹&lt;/mark&gt;é…ç½®å¥åº·æ£€æŸ¥æ–‡ä»¶æ·»åŠ æ‰§è¡Œæƒé™ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰ master èŠ‚ç‚¹&lt;/mark&gt;å¯åŠ¨ haproxy å’Œ keepalivedï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;é‡è¦ï¼šå¦‚æœå®‰è£…äº† keepalived å’Œ haproxyï¼Œéœ€è¦æµ‹è¯• keepalived æ˜¯å¦æ˜¯æ­£å¸¸çš„&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;æ‰€æœ‰èŠ‚ç‚¹æµ‹è¯•VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœ ping ä¸é€šä¸” telnet æ²¡æœ‰å‡ºç° ] ï¼Œåˆ™è®¤ä¸º VIP ä¸å¯ä»¥ï¼Œä¸å¯åœ¨ç»§ç»­å¾€ä¸‹æ‰§è¡Œï¼Œéœ€è¦æ’æŸ¥ keepalived çš„é—®é¢˜ï¼Œæ¯”å¦‚é˜²ç«å¢™å’Œ selinuxï¼Œhaproxy å’Œ keepalived çš„çŠ¶æ€ï¼Œç›‘å¬ç«¯å£ç­‰&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ‰€æœ‰èŠ‚ç‚¹æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€å¿…é¡»ä¸º disable å’Œ inactiveï¼šsystemctl status firewalld&lt;/li&gt;
&lt;li&gt;æ‰€æœ‰èŠ‚ç‚¹æŸ¥çœ‹ selinux çŠ¶æ€ï¼Œå¿…é¡»ä¸º disableï¼šgetenforce&lt;/li&gt;
&lt;li&gt;master èŠ‚ç‚¹æŸ¥çœ‹ haproxy å’Œ keepalived çŠ¶æ€ï¼šsystemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master èŠ‚ç‚¹æŸ¥çœ‹ç›‘å¬ç«¯å£ï¼šnetstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å¦‚æœä»¥ä¸Šéƒ½æ²¡æœ‰é—®é¢˜ï¼Œéœ€è¦ç¡®è®¤ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;æ˜¯å¦æ˜¯å…¬æœ‰äº‘æœºå™¨&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ˜¯å¦æ˜¯ç§æœ‰äº‘æœºå™¨ï¼ˆç±»ä¼¼ OpenStackï¼‰&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ä¸Šè¿°å…¬æœ‰äº‘ä¸€èˆ¬éƒ½æ˜¯ä¸æ”¯æŒ keepalivedï¼Œç§æœ‰äº‘å¯èƒ½ä¹Ÿæœ‰é™åˆ¶ï¼Œéœ€è¦å’Œè‡ªå·±çš„ç§æœ‰äº‘ç®¡ç†å‘˜å’¨è¯¢&lt;/p&gt;
&lt;h4 id=&#34;3-runtimeå®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtimeå®‰è£…&#34;&gt;#&lt;/a&gt; 3. Runtime å®‰è£…&lt;/h4&gt;
&lt;p&gt;å¦‚æœå®‰è£…çš„ç‰ˆæœ¬ä½äº 1.24ï¼Œé€‰æ‹© Docker å’Œ Containerd å‡å¯ï¼Œé«˜äº 1.24 å»ºè®®é€‰æ‹© Containerd ä½œä¸º Runtimeï¼Œä¸å†æ¨èä½¿ç”¨ Docker ä½œä¸º Runtimeã€‚&lt;/p&gt;
&lt;h5 id=&#34;31-å®‰è£…containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-å®‰è£…containerd&#34;&gt;#&lt;/a&gt; 3.1 å®‰è£… Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½®å®‰è£…æºï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å®‰è£… docker-ceï¼ˆå¦‚æœåœ¨ä»¥å‰å·²ç»å®‰è£…è¿‡ï¼Œéœ€è¦é‡æ–°å®‰è£…æ›´æ–°ä¸€ä¸‹ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;å¯ä»¥æ— éœ€å¯åŠ¨ Dockerï¼Œåªéœ€è¦é…ç½®å’Œå¯åŠ¨ Containerd å³å¯ã€‚&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;é¦–å…ˆé…ç½® Containerd æ‰€éœ€çš„æ¨¡å—ï¼ˆ&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åŠ è½½æ¨¡å—ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ï¼Œé…ç½® Containerd æ‰€éœ€çš„å†…æ ¸ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;åŠ è½½å†…æ ¸ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ç”Ÿæˆ Containerd çš„é…ç½®æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;æ›´æ”¹ Containerd çš„ Cgroup å’Œ Pause é•œåƒé…ç½®ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å¯åŠ¨ Containerdï¼Œå¹¶é…ç½®å¼€æœºè‡ªå¯åŠ¨ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½® crictl å®¢æˆ·ç«¯è¿æ¥çš„è¿è¡Œæ—¶ä½ç½®ï¼ˆå¯é€‰ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-å®‰è£…kubernetesç»„ä»¶&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-å®‰è£…kubernetesç»„ä»¶&#34;&gt;#&lt;/a&gt; 4 . å®‰è£… Kubernetes ç»„ä»¶&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;é…ç½®æºï¼ˆæ³¨æ„æ›´æ”¹ç‰ˆæœ¬å·ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;é¦–å…ˆåœ¨&lt;mark&gt; Master01 èŠ‚ç‚¹&lt;/mark&gt;æŸ¥çœ‹æœ€æ–°çš„ Kubernetes ç‰ˆæœ¬æ˜¯å¤šå°‘ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum list kubeadm.x86_64 --showduplicates | sort -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;å®‰è£… 1.32 æœ€æ–°ç‰ˆæœ¬ kubeadmã€kubelet å’Œ kubectlï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;è®¾ç½® Kubelet å¼€æœºè‡ªå¯åŠ¨ï¼ˆç”±äºè¿˜æœªåˆå§‹åŒ–ï¼Œæ²¡æœ‰ kubelet çš„é…ç½®æ–‡ä»¶ï¼Œæ­¤æ—¶ kubelet æ— æ³•å¯åŠ¨ï¼Œæ— éœ€å…³å¿ƒï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æ­¤æ—¶ kubelet æ˜¯èµ·ä¸æ¥çš„ï¼Œæ—¥å¿—ä¼šæœ‰æŠ¥é”™ä¸å½±å“ï¼&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-é›†ç¾¤åˆå§‹åŒ–&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-é›†ç¾¤åˆå§‹åŒ–&#34;&gt;#&lt;/a&gt; 5 . é›†ç¾¤åˆå§‹åŒ–&lt;/h4&gt;
&lt;p&gt;ä»¥ä¸‹æ“ä½œåœ¨&lt;mark&gt; master01&lt;/mark&gt;ï¼ˆæ³¨æ„é»„è‰²éƒ¨åˆ†ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.71
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  name: k8s-master01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  certSANs:
  - 192.168.1.70               # å¦‚æœæ­å»ºçš„ä¸æ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼ŒæŠŠæ­¤å¤„æ”¹ä¸ºmasterçš„IP
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.1.70:16443 # å¦‚æœæ­å»ºçš„ä¸æ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼ŒæŠŠæ­¤å¤„IPæ”¹ä¸ºmasterçš„IPï¼Œç«¯å£æ”¹æˆ6443
controllerManager: &amp;#123;&amp;#125;
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.32.3    # æ›´æ”¹æ­¤å¤„çš„ç‰ˆæœ¬å·å’Œkubeadm versionä¸€è‡´
networking:
  dnsDomain: cluster.local
  podSubnet: 172.16.0.0/16    # æ³¨æ„æ­¤å¤„çš„ç½‘æ®µï¼Œä¸è¦ä¸serviceå’ŒèŠ‚ç‚¹ç½‘æ®µå†²çª
  serviceSubnet: 10.96.0.0/16 # æ³¨æ„æ­¤å¤„çš„ç½‘æ®µï¼Œä¸è¦ä¸podå’ŒèŠ‚ç‚¹ç½‘æ®µå†²çª
scheduler: &amp;#123;&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;master01 èŠ‚ç‚¹&lt;/mark&gt;æ›´æ–° kubeadm æ–‡ä»¶ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å°† new.yaml æ–‡ä»¶å¤åˆ¶åˆ°&lt;mark&gt;å…¶ä»– master èŠ‚ç‚¹&lt;/mark&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä¹‹å&lt;mark&gt;æ‰€æœ‰ Master èŠ‚ç‚¹&lt;/mark&gt;æå‰ä¸‹è½½é•œåƒï¼Œå¯ä»¥èŠ‚çœåˆå§‹åŒ–æ—¶é—´ï¼ˆå…¶ä»–èŠ‚ç‚¹ä¸éœ€è¦æ›´æ”¹ä»»ä½•é…ç½®ï¼ŒåŒ…æ‹¬ IP åœ°å€ä¹Ÿä¸éœ€è¦æ›´æ”¹ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config images pull --config /root/new.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ­£ç¡®çš„åé¦ˆä¿¡æ¯å¦‚ä¸‹ï¼ˆ&lt;em&gt;&lt;strong&gt;* ç‰ˆæœ¬å¯èƒ½ä¸ä¸€æ · *&lt;/strong&gt;&lt;/em&gt;ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml 
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;åˆå§‹åŒ–ï¼Œåˆå§‹åŒ–ä»¥åä¼šåœ¨ /etc/kubernetes ç›®å½•ä¸‹ç”Ÿæˆå¯¹åº”çš„è¯ä¹¦å’Œé…ç½®æ–‡ä»¶ï¼Œä¹‹åå…¶ä»– Master èŠ‚ç‚¹åŠ å…¥ Master01 å³å¯ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init --config /root/new.yaml  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;åˆå§‹åŒ–æˆåŠŸä»¥åï¼Œä¼šäº§ç”Ÿ Token å€¼ï¼Œç”¨äºå…¶ä»–èŠ‚ç‚¹åŠ å…¥æ—¶ä½¿ç”¨ï¼Œå› æ­¤è¦è®°å½•ä¸‹åˆå§‹åŒ–æˆåŠŸç”Ÿæˆçš„ token å€¼ï¼ˆä»¤ç‰Œå€¼ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

# ä¸è¦å¤åˆ¶æ–‡æ¡£å½“ä¸­çš„ï¼Œè¦å»ä½¿ç”¨èŠ‚ç‚¹ç”Ÿæˆçš„
  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&amp;quot;kubeadm init phase upload-certs --upload-certs&amp;quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;é…ç½®ç¯å¢ƒå˜é‡ï¼Œç”¨äºè®¿é—® Kubernetes é›†ç¾¤ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF
source /root/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 èŠ‚ç‚¹&lt;/mark&gt;æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€ï¼šï¼ˆæ˜¾ç¤º NotReady ä¸å½±å“ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE   VERSION
k8s-master01   NotReady   control-plane   24s   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;é‡‡ç”¨åˆå§‹åŒ–å®‰è£…æ–¹å¼ï¼Œæ‰€æœ‰çš„ç³»ç»Ÿç»„ä»¶å‡ä»¥å®¹å™¨çš„æ–¹å¼è¿è¡Œå¹¶ä¸”åœ¨ kube-system å‘½åç©ºé—´å†…ï¼Œæ­¤æ—¶å¯ä»¥æŸ¥çœ‹ Pod çŠ¶æ€ï¼ˆæ˜¾ç¤º pending ä¸å½±å“ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;# kubectl get pods -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-åˆå§‹åŒ–å¤±è´¥æ’æŸ¥&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-åˆå§‹åŒ–å¤±è´¥æ’æŸ¥&#34;&gt;#&lt;/a&gt; 5.1 åˆå§‹åŒ–å¤±è´¥æ’æŸ¥&lt;/h5&gt;
&lt;p&gt;å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œé‡ç½®åå†æ¬¡åˆå§‹åŒ–ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼ˆæ²¡æœ‰å¤±è´¥ä¸è¦æ‰§è¡Œï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœå¤šæ¬¡å°è¯•éƒ½æ˜¯åˆå§‹åŒ–å¤±è´¥ï¼Œéœ€è¦çœ‹ç³»ç»Ÿæ—¥å¿—ï¼ŒCentOS/RockyLinux æ—¥å¿—è·¯å¾„:/var/log/messagesï¼ŒUbuntu ç³»åˆ—æ—¥å¿—è·¯å¾„:/var/log/syslogï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/messages | grep -v &amp;quot;not found&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç»å¸¸å‡ºé”™çš„åŸå› ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Containerd çš„é…ç½®æ–‡ä»¶ä¿®æ”¹çš„ä¸å¯¹ï¼Œè‡ªè¡Œå‚è€ƒã€Šå®‰è£… containerdã€‹å°èŠ‚æ ¸å¯¹&lt;/li&gt;
&lt;li&gt;new.yaml é…ç½®é—®é¢˜ï¼Œæ¯”å¦‚éé«˜å¯ç”¨é›†ç¾¤å¿˜è®°ä¿®æ”¹ 16443 ç«¯å£ä¸º 6443&lt;/li&gt;
&lt;li&gt;new.yaml é…ç½®é—®é¢˜ï¼Œä¸‰ä¸ªç½‘æ®µæœ‰äº¤å‰ï¼Œå‡ºç° IP åœ°å€å†²çª&lt;/li&gt;
&lt;li&gt;VIP ä¸é€šå¯¼è‡´æ— æ³•åˆå§‹åŒ–æˆåŠŸï¼Œæ­¤æ—¶ messages æ—¥å¿—ä¼šæœ‰ VIP è¶…æ—¶çš„æŠ¥é”™&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;52-é«˜å¯ç”¨master&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-é«˜å¯ç”¨master&#34;&gt;#&lt;/a&gt; 5.2 é«˜å¯ç”¨ Master&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;å…¶ä»– master&lt;/strong&gt; åŠ å…¥é›†ç¾¤ï¼Œmaster02 å’Œ master03 åˆ†åˆ«æ‰§è¡Œ (åƒä¸‡ä¸è¦åœ¨ master01 å†æ¬¡æ‰§è¡Œï¼Œä¸èƒ½ç›´æ¥å¤åˆ¶æ–‡æ¡£å½“ä¸­çš„å‘½ä»¤ï¼Œè€Œæ˜¯ä½ è‡ªå·±åˆšæ‰ master01 åˆå§‹åŒ–ä¹‹åäº§ç”Ÿçš„å‘½ä»¤)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹å½“å‰çŠ¶æ€ï¼šï¼ˆå¦‚æœæ˜¾ç¤º NotReady ä¸å½±å“ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;53-tokenè¿‡æœŸå¤„ç†&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#53-tokenè¿‡æœŸå¤„ç†&#34;&gt;#&lt;/a&gt; 5.3 Token è¿‡æœŸå¤„ç†&lt;/h5&gt;
&lt;p&gt;æ³¨æ„ï¼šä»¥ä¸‹æ­¥éª¤æ˜¯ä¸Šè¿° init å‘½ä»¤äº§ç”Ÿçš„ Token è¿‡æœŸäº†æ‰éœ€è¦æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼Œå¦‚æœæ²¡æœ‰è¿‡æœŸä¸éœ€è¦æ‰§è¡Œï¼Œç›´æ¥ join å³å¯ã€‚&lt;/p&gt;
&lt;p&gt;Token è¿‡æœŸåç”Ÿæˆæ–°çš„ tokenï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm token create --print-join-command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Master éœ€è¦ç”Ÿæˆ --certificate-keyï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase upload-certs  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-nodeèŠ‚ç‚¹çš„é…ç½®&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-nodeèŠ‚ç‚¹çš„é…ç½®&#34;&gt;#&lt;/a&gt; 6. Node èŠ‚ç‚¹çš„é…ç½®&lt;/h4&gt;
&lt;p&gt;Node èŠ‚ç‚¹ä¸Šä¸»è¦éƒ¨ç½²å…¬å¸çš„ä¸€äº›ä¸šåŠ¡åº”ç”¨ï¼Œç”Ÿäº§ç¯å¢ƒä¸­ä¸å»ºè®® Master èŠ‚ç‚¹éƒ¨ç½²ç³»ç»Ÿç»„ä»¶ä¹‹å¤–çš„å…¶ä»– Podï¼Œæµ‹è¯•ç¯å¢ƒå¯ä»¥å…è®¸ Master èŠ‚ç‚¹éƒ¨ç½² Pod ä»¥èŠ‚çœç³»ç»Ÿèµ„æºã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ‰€æœ‰èŠ‚ç‚¹åˆå§‹åŒ–å®Œæˆåï¼ŒæŸ¥çœ‹é›†ç¾¤çŠ¶æ€ï¼ˆNotReady ä¸å½±å“ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
k8s-node01     NotReady   &amp;lt;none&amp;gt;          13s     v1.32.3
k8s-node02     NotReady   &amp;lt;none&amp;gt;          10s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-calicoç»„ä»¶çš„å®‰è£…&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-calicoç»„ä»¶çš„å®‰è£…&#34;&gt;#&lt;/a&gt; 7. Calico ç»„ä»¶çš„å®‰è£…&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;æ‰€æœ‰èŠ‚ç‚¹&lt;/mark&gt;ç¦æ­¢ NetworkManager ç®¡ç† Calico çš„ç½‘ç»œæ¥å£ï¼Œé˜²æ­¢æœ‰å†²çªæˆ–å¹²æ‰°ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt;/etc/NetworkManager/conf.d/calico.conf&amp;lt;&amp;lt;EOF
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
EOF
systemctl daemon-reload
systemctl restart NetworkManager
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä»¥ä¸‹æ­¥éª¤åªåœ¨&lt;mark&gt; master01&lt;/mark&gt; æ‰§è¡Œï¼ˆ.x ä¸éœ€è¦æ›´æ”¹ï¼‰ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x &amp;amp;&amp;amp; cd calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä¿®æ”¹ Pod ç½‘æ®µï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= &#39;&amp;#123;print $NF&amp;#125;&#39;`

sed -i &amp;quot;s#POD_CIDR#$&amp;#123;POD_SUBNET&amp;#125;#g&amp;quot; calico.yaml
kubectl apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹å®¹å™¨å’ŒèŠ‚ç‚¹çŠ¶æ€ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h
calico-node-7mzmb                          1/1     Running   0          24h
calico-node-ljqnl                          1/1     Running   0          24h
calico-node-njqlb                          1/1     Running   0          24h
calico-node-ph4m4                          1/1     Running   0          24h
calico-node-rx8rl                          1/1     Running   0          24h
coredns-76fccbbb6b-76559                   1/1     Running   0          24h
coredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h
etcd-k8s-master01                          1/1     Running   0          24h
etcd-k8s-master02                          1/1     Running   0          24h
etcd-k8s-master03                          1/1     Running   0          24h
kube-apiserver-k8s-master01                1/1     Running   0          24h
kube-apiserver-k8s-master02                1/1     Running   0          24h
kube-apiserver-k8s-master03                1/1     Running   0          24h
kube-controller-manager-k8s-master01       1/1     Running   0          24h
kube-controller-manager-k8s-master02       1/1     Running   0          24h
kube-controller-manager-k8s-master03       1/1     Running   0          24h
kube-proxy-9dtz4                           1/1     Running   0          24h
kube-proxy-jh7rl                           1/1     Running   0          24h
kube-proxy-jvvwt                           1/1     Running   0          24h
kube-proxy-sh89l                           1/1     Running   0          24h
kube-proxy-t2j49                           1/1     Running   0          24h
kube-scheduler-k8s-master01                1/1     Running   0          24h
kube-scheduler-k8s-master02                1/1     Running   0          24h
kube-scheduler-k8s-master03                1/1     Running   0          24h
metrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ­¤æ—¶èŠ‚ç‚¹å…¨éƒ¨å˜ä¸º Ready çŠ¶æ€ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   24h   v1.32.3
k8s-master02   Ready    control-plane   24h   v1.32.3
k8s-master03   Ready    control-plane   24h   v1.32.3
k8s-node01     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
k8s-node02     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-metricséƒ¨ç½²&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-metricséƒ¨ç½²&#34;&gt;#&lt;/a&gt; 8. Metrics éƒ¨ç½²&lt;/h4&gt;
&lt;p&gt;åœ¨æ–°ç‰ˆçš„ Kubernetes ä¸­ç³»ç»Ÿèµ„æºçš„é‡‡é›†å‡ä½¿ç”¨ Metrics-serverï¼Œå¯ä»¥é€šè¿‡ Metrics é‡‡é›†èŠ‚ç‚¹å’Œ Pod çš„å†…å­˜ã€ç£ç›˜ã€CPU å’Œç½‘ç»œçš„ä½¿ç”¨ç‡ã€‚&lt;/p&gt;
&lt;p&gt;å°†&lt;mark&gt; Master01 èŠ‚ç‚¹&lt;/mark&gt;çš„ front-proxy-ca.crt å¤åˆ¶åˆ°æ‰€æœ‰ Node èŠ‚ç‚¹&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt

scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(å…¶ä»–èŠ‚ç‚¹è‡ªè¡Œæ‹·è´):/etc/kubernetes/pki/front-proxy-ca.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä»¥ä¸‹æ“ä½œå‡åœ¨&lt;mark&gt; master01 èŠ‚ç‚¹&lt;/mark&gt;æ‰§è¡Œ:&lt;/p&gt;
&lt;p&gt;å®‰è£… metrics server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/kubeadm-metrics-server

# kubectl  create -f comp.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æŸ¥çœ‹çŠ¶æ€ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ç­‰ Pod å˜æˆ 1/1   Running åï¼ŒæŸ¥çœ‹èŠ‚ç‚¹å’Œ Pod èµ„æºä½¿ç”¨ç‡ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl top node
NAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
k8s-master01   132m         3%       932Mi           5%          
k8s-master02   131m         3%       845Mi           5%          
k8s-master03   148m         3%       912Mi           5%          
k8s-node01     54m          1%       600Mi           3%          
k8s-node02     49m          1%       602Mi           3%          
[root@k8s-master01 ~]#  kubectl top po -A
NAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   
ingress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            
ingress-nginx          ingress-nginx-controller-r978m               1m           104Mi           
krm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            
krm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             
krm                    nginx-574cf48959-vcfjs                       0m           2Mi             
kube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            
kube-system            calico-node-7mzmb                            16m          176Mi           
kube-system            calico-node-ljqnl                            15m          182Mi           
kube-system            calico-node-njqlb                            19m          180Mi           
kube-system            calico-node-ph4m4                            15m          178Mi           
kube-system            calico-node-rx8rl                            17m          180Mi           
kube-system            coredns-76fccbbb6b-76559                     2m           16Mi            
kube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            
kube-system            etcd-k8s-master01                            22m          86Mi            
kube-system            etcd-k8s-master02                            27m          84Mi            
kube-system            etcd-k8s-master03                            22m          84Mi            
kube-system            kube-apiserver-k8s-master01                  22m          267Mi           
kube-system            kube-apiserver-k8s-master02                  20m          242Mi           
kube-system            kube-apiserver-k8s-master03                  18m          241Mi           
kube-system            kube-controller-manager-k8s-master01         6m           69Mi            
kube-system            kube-controller-manager-k8s-master02         2m           21Mi            
kube-system            kube-controller-manager-k8s-master03         1m           19Mi            
kube-system            kube-proxy-9dtz4                             11m          30Mi            
kube-system            kube-proxy-jh7rl                             1m           27Mi            
kube-system            kube-proxy-jvvwt                             17m          29Mi            
kube-system            kube-proxy-sh89l                             1m           29Mi            
kube-system            kube-proxy-t2j49                             16m          29Mi            
kube-system            kube-scheduler-k8s-master01                  6m           25Mi            
kube-system            kube-scheduler-k8s-master02                  6m           25Mi            
kube-system            kube-scheduler-k8s-master03                  6m           25Mi            
kube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            
kubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            
kubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9-dashboardéƒ¨ç½²&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-dashboardéƒ¨ç½²&#34;&gt;#&lt;/a&gt; 9. Dashboard éƒ¨ç½²&lt;/h4&gt;
&lt;h5 id=&#34;91-å®‰è£…dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#91-å®‰è£…dashboard&#34;&gt;#&lt;/a&gt; 9.1 å®‰è£… Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard ç”¨äºå±•ç¤ºé›†ç¾¤ä¸­çš„å„ç±»èµ„æºï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥é€šè¿‡ Dashboard å®æ—¶æŸ¥çœ‹ Pod çš„æ—¥å¿—å’Œåœ¨å®¹å™¨ä¸­æ‰§è¡Œä¸€äº›å‘½ä»¤ç­‰ã€‚&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;92-ç™»å½•dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#92-ç™»å½•dashboard&#34;&gt;#&lt;/a&gt; 9.2 ç™»å½• dashboard&lt;/h5&gt;
&lt;p&gt;åœ¨è°·æ­Œæµè§ˆå™¨ï¼ˆChromeï¼‰å¯åŠ¨æ–‡ä»¶ä¸­åŠ å…¥å¯åŠ¨å‚æ•°ï¼Œç”¨äºè§£å†³æ— æ³•è®¿é—® Dashboard çš„é—®é¢˜ï¼Œå‚è€ƒä¸‹å›¾ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;æ›´æ”¹ dashboard çš„ svc ä¸º NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;å°† ClusterIP æ›´æ”¹ä¸º NodePortï¼ˆå¦‚æœå·²ç»ä¸º NodePort å¿½ç•¥æ­¤æ­¥éª¤ï¼‰&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;æŸ¥çœ‹ç«¯å£å·ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ ¹æ®è‡ªå·±çš„å®ä¾‹ç«¯å£å·ï¼Œé€šè¿‡ä»»æ„å®‰è£…äº† kube-proxy çš„å®¿ä¸»æœºçš„ IP + ç«¯å£å³å¯è®¿é—®åˆ° dashboardï¼š&lt;/p&gt;
&lt;p&gt;è®¿é—® Dashboardï¼š&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; ï¼ˆæŠŠ IP åœ°å€å’Œç«¯å£æ”¹æˆä½ è‡ªå·±çš„ï¼‰é€‰æ‹©ç™»å½•æ–¹å¼ä¸ºä»¤ç‰Œï¼ˆå³ token æ–¹å¼ï¼‰ï¼Œå‚è€ƒä¸‹å›¾ï¼š&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;åˆ›å»ºç™»å½• Tokenï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å°† token å€¼è¾“å…¥åˆ°ä»¤ç‰Œåï¼Œå•å‡»ç™»å½•å³å¯è®¿é—® Dashboardï¼Œå‚è€ƒä¸‹å›¾ï¼š&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;10å¿…çœ‹ä¸€äº›å¿…é¡»çš„é…ç½®æ›´æ”¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10å¿…çœ‹ä¸€äº›å¿…é¡»çš„é…ç½®æ›´æ”¹&#34;&gt;#&lt;/a&gt; 10.ã€å¿…çœ‹ã€‘ä¸€äº›å¿…é¡»çš„é…ç½®æ›´æ”¹&lt;/h4&gt;
&lt;p&gt;å°† Kube-proxy æ”¹ä¸º ipvs æ¨¡å¼ï¼Œå› ä¸ºåœ¨åˆå§‹åŒ–é›†ç¾¤çš„æ—¶å€™æ³¨é‡Šäº† ipvs é…ç½®ï¼Œæ‰€ä»¥éœ€è¦è‡ªè¡Œä¿®æ”¹ä¸€ä¸‹ï¼š&lt;/p&gt;
&lt;p&gt;åœ¨ master01 èŠ‚ç‚¹æ‰§è¡Œï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
mode: ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ›´æ–° Kube-Proxy çš„ Podï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;éªŒè¯ Kube-Proxy æ¨¡å¼:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode
ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11å¿…çœ‹æ³¨æ„äº‹é¡¹&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11å¿…çœ‹æ³¨æ„äº‹é¡¹&#34;&gt;#&lt;/a&gt; 11.ã€å¿…çœ‹ã€‘æ³¨æ„äº‹é¡¹&lt;/h4&gt;
&lt;p&gt;æ³¨æ„ï¼škubeadm å®‰è£…çš„é›†ç¾¤ï¼Œè¯ä¹¦æœ‰æ•ˆæœŸé»˜è®¤æ˜¯ä¸€å¹´ã€‚master èŠ‚ç‚¹çš„ kube-apiserverã€kube-schedulerã€kube-controller-managerã€etcd éƒ½æ˜¯ä»¥å®¹å™¨è¿è¡Œçš„ã€‚å¯ä»¥é€šè¿‡ kubectl get po -n kube-system æŸ¥çœ‹ã€‚&lt;/p&gt;
&lt;p&gt;å¯åŠ¨å’ŒäºŒè¿›åˆ¶ä¸åŒçš„æ˜¯ï¼Œkubelet çš„é…ç½®æ–‡ä»¶åœ¨ /etc/sysconfig/kubelet å’Œ /var/lib/kubelet/config.yamlï¼Œä¿®æ”¹åéœ€è¦é‡å¯ kubelet è¿›ç¨‹ã€‚&lt;/p&gt;
&lt;p&gt;å…¶ä»–ç»„ä»¶çš„é…ç½®æ–‡ä»¶åœ¨ /etc/kubernetes/manifests ç›®å½•ä¸‹ï¼Œæ¯”å¦‚ kube-apiserver.yamlï¼Œè¯¥ yaml æ–‡ä»¶æ›´æ”¹åï¼Œkubelet ä¼šè‡ªåŠ¨åˆ·æ–°é…ç½®ï¼Œä¹Ÿå°±æ˜¯ä¼šé‡å¯ podã€‚ä¸èƒ½å†æ¬¡åˆ›å»ºè¯¥æ–‡ä»¶ã€‚&lt;/p&gt;
&lt;p&gt;kube-proxy çš„é…ç½®åœ¨ kube-system å‘½åç©ºé—´ä¸‹çš„ configmap ä¸­ï¼Œå¯ä»¥é€šè¿‡&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;è¿›è¡Œæ›´æ”¹ï¼Œæ›´æ”¹å®Œæˆåï¼Œå¯ä»¥é€šè¿‡ patch é‡å¯ kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Kubeadm å®‰è£…åï¼Œmaster èŠ‚ç‚¹é»˜è®¤ä¸å…è®¸éƒ¨ç½² podï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆ é™¤ Taintï¼Œå³å¯éƒ¨ç½² Podï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-containerdé…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-containerdé…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;#&lt;/a&gt; 12. Containerd é…ç½®é•œåƒåŠ é€Ÿ&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#æ·»åŠ ä»¥ä¸‹é…ç½®é•œåƒåŠ é€ŸæœåŠ¡
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ‰€æœ‰èŠ‚ç‚¹é‡æ–°å¯åŠ¨ Containerdï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;13-dockeré…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-dockeré…ç½®é•œåƒåŠ é€Ÿ&#34;&gt;#&lt;/a&gt; 13. Docker é…ç½®é•œåƒåŠ é€Ÿ&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;æ‰€æœ‰èŠ‚ç‚¹é‡æ–°å¯åŠ¨ Dockerï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;æœ¬æ–‡å‡ºè‡ªäºï¼š&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-09T10:28:34.000Z</updated>
    </entry>
</feed>
