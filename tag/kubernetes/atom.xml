<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://ixuyong.cn</id>
    <title>LinuxSre云原生 • Posts by &#34;kubernetes&#34; tag</title>
    <link href="http://ixuyong.cn" />
    <updated>2025-04-20T09:59:58.000Z</updated>
    <category term="rsync" />
    <category term="Kubernetes" />
    <category term="Windows" />
    <category term="MySQL" />
    <category term="Harbor" />
    <category term="Redis" />
    <entry>
        <id>http://ixuyong.cn/posts/312010518.html</id>
        <title>K8s亲和力Affinity</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/312010518.html"/>
        <content type="html">&lt;h3 id=&#34;k8s亲和力affinity&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s亲和力affinity&#34;&gt;#&lt;/a&gt; K8s 亲和力 Affinity&lt;/h3&gt;
&lt;h4 id=&#34;1-affinity分类&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-affinity分类&#34;&gt;#&lt;/a&gt; 1. Affinity 分类&lt;/h4&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/hTd0wmD.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-20T09:59:58.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3254599477.html</id>
        <title>K8s容忍和污点</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3254599477.html"/>
        <content type="html">&lt;h3 id=&#34;k8s容忍和污点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s容忍和污点&#34;&gt;#&lt;/a&gt; K8s 容忍和污点&lt;/h3&gt;
&lt;p&gt;Taint 指定服务器上打上污点，让不能容忍这个污点的 Pod 不能部署在打了污点的服务器上。Toleration 是让 Pod 容忍节点上配置的污点，可以让一些需要特殊配置的 Pod 能够调用到具有污点和特殊配置的节点上。&lt;/p&gt;
&lt;h4 id=&#34;1-taint配置解析&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-taint配置解析&#34;&gt;#&lt;/a&gt; 1. Taint 配置解析&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#1.Taint语法
# kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT

#2.创建Taint示例
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule

#3.查看污点
# kubectl describe node k8s-node01 | grep Taints -A 10

#4.删除污点
# kubectl taint nodes k8s-node01 ssd-                   #基于Key删除
# kubectl taint nodes k8s-node01 ssd:PreferNoSchedule-  #基于Key+Effect删除

#5.修改污点（Key和Effect相同）
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule --overwrite
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;EFFECT 排斥等级：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NoSchedule：禁止调度到该节点，已经在该节点上的 Pod 不受影响&lt;/li&gt;
&lt;li&gt;NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后）&lt;/li&gt;
&lt;li&gt;PreferNoSchedule：尽量避免将 Pod 调度到指定的节点上，如果没有更合适的节点，可以部署到该节点&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2toleration配置解析&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2toleration配置解析&#34;&gt;#&lt;/a&gt; 2.Toleration 配置解析&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#1.完全匹配
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;taintValue&amp;quot;
  effect: &amp;quot;NoSchedule
 
#2.不完全匹配 
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Exists&amp;quot;
  effect: &amp;quot;NoSchedule&amp;quot;
  
#3.大范围匹配（不推荐key为内置Taint，会导致节点故障pod无法漂移）
tolerations:
- key: &amp;quot;taintKey&amp;quot;
  operator: &amp;quot;Exists
  
#4.容忍时间配置
tolerations:
- key: &amp;quot;key1&amp;quot;
  operator: &amp;quot;Equal&amp;quot;
  value: &amp;quot;value1&amp;quot;
  effect: &amp;quot;NoExecute&amp;quot;
  tolerationSeconds: 3600
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-taint-toleration配置示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-taint-toleration配置示例&#34;&gt;#&lt;/a&gt; 3. Taint、Toleration 配置示例&lt;/h4&gt;
&lt;p&gt;有一个 K8s 节点是纯 SSD 硬盘的节点，现需要只有一些需要高性能存储的 Pod 才能调度到该节点上。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.给节点打上污点和标签
# kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule
# kubectl label node k8s-node01 ssd=true

#2.配置Toleration：
# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
      nodeSelector:
        ssd: &#39;true&#39;
      tolerations:
        - key: ssd
          operator: Exists
          effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-k8s内置污点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-k8s内置污点&#34;&gt;#&lt;/a&gt; 4. K8s 内置污点&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/not-ready%EF%BC%9A%E8%8A%82%E7%82%B9%E6%9C%AA%E5%87%86%E5%A4%87%E5%A5%BD%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81Ready%E7%9A%84%E5%80%BC%E4%B8%BAFalse%E3%80%82&#34;&gt;node.kubernetes.io/not-ready：节点未准备好，相当于节点状态 Ready 的值为 False。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/unreachable%EF%BC%9ANode&#34;&gt;node.kubernetes.io/unreachable：Node&lt;/a&gt; Controller 访问不到节点，相当于节点状态 Ready 的值为 Unknown。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/out-of-disk%EF%BC%9A%E8%8A%82%E7%82%B9%E7%A3%81%E7%9B%98%E8%80%97%E5%B0%BD%E3%80%82&#34;&gt;node.kubernetes.io/out-of-disk：节点磁盘耗尽。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/memory-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E5%86%85%E5%AD%98%E5%8E%8B%E5%8A%9B%E3%80%82&#34;&gt;node.kubernetes.io/memory-pressure：节点存在内存压力。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/disk-pressure%EF%BC%9A%E8%8A%82%E7%82%B9%E5%AD%98%E5%9C%A8%E7%A3%81%E7%9B%98%E5%8E%8B%E5%8A%9B%E3%80%82&#34;&gt;node.kubernetes.io/disk-pressure：节点存在磁盘压力。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/network-unavailable%EF%BC%9A%E8%8A%82%E7%82%B9%E7%BD%91%E7%BB%9C%E4%B8%8D%E5%8F%AF%E8%BE%BE%E3%80%82&#34;&gt;node.kubernetes.io/network-unavailable：节点网络不可达。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.kubernetes.io/unschedulable%EF%BC%9A%E8%8A%82%E7%82%B9%E4%B8%8D%E5%8F%AF%E8%B0%83%E5%BA%A6%E3%80%82&#34;&gt;node.kubernetes.io/unschedulable：节点不可调度。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://node.cloudprovider.kubernetes.io/uninitialized%EF%BC%9A%E5%A6%82%E6%9E%9CKubelet%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8C%87%E5%AE%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%A4%96%E9%83%A8%E7%9A%84cloudprovider%EF%BC%8C%E5%AE%83%E5%B0%86%E7%BB%99%E5%BD%93%E5%89%8D%E8%8A%82%E7%82%B9%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AATaint%E5%B0%86%E5%85%B6%E6%A0%87%E8%AE%B0%E4%B8%BA%E4%B8%8D%E5%8F%AF%E7%94%A8%E3%80%82%E5%9C%A8cloud-controller-manager%E7%9A%84%E4%B8%80%E4%B8%AAcontroller%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%99%E4%B8%AA%E8%8A%82%E7%82%B9%E5%90%8E%EF%BC%8CKubelet%E5%B0%86%E5%88%A0%E9%99%A4%E8%BF%99%E4%B8%AATaint%E3%80%82&#34;&gt;node.cloudprovider.kubernetes.io/uninitialized：如果 Kubelet 启动时指定了一个外部的 cloudprovider，它将给当前节点添加一个 Taint 将其标记为不可用。在 cloud-controller-manager 的一个 controller 初始化这个节点后，Kubelet 将删除这个 Taint。&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/vO7kURL.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Deployment 创建后 K8s 默认为 Pod 添加容忍，当 Pod 所在的节点宕机，300 秒后 pod 会漂移，默认容忍时间 300 秒。&lt;/p&gt;
&lt;h4 id=&#34;5节点宕机快速恢复业务应用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5节点宕机快速恢复业务应用&#34;&gt;#&lt;/a&gt; 5. 节点宕机快速恢复业务应用&lt;/h4&gt;
&lt;p&gt;节点不健康，180 秒后再驱逐（默认是 300 秒）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
      tolerations:
        - key: node.kubernetes.io/unreachable
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 180
        - key: node.kubernetes.io/not-ready
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 180
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-20T07:51:58.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3142072607.html</id>
        <title>K8s初始化容器、临时容器</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3142072607.html"/>
        <content type="html">&lt;h3 id=&#34;k8s初始化容器-临时容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s初始化容器-临时容器&#34;&gt;#&lt;/a&gt; K8s 初始化容器、临时容器&lt;/h3&gt;
&lt;h4 id=&#34;1-初始化容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-初始化容器&#34;&gt;#&lt;/a&gt; 1. 初始化容器&lt;/h4&gt;
&lt;h5 id=&#34;1-1-初始化容器的用途&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-1-初始化容器的用途&#34;&gt;#&lt;/a&gt; 1. 1 初始化容器的用途&lt;/h5&gt;
&lt;p&gt;初始化容器主要是在主应用启动之前，做一些初始化的操作，比如创建文件、修改内核参数、等待依赖程序启动或其他需要在主程序启动之前需要做的工作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码；&lt;/li&gt;
&lt;li&gt;Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低；&lt;/li&gt;
&lt;li&gt;Init 容器可以以 root 身份运行，执行一些高权限命令；&lt;/li&gt;
&lt;li&gt;Init 容器相关操作执行完成以后即退出，不会给业务容器带来安全隐患。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;12-初始化容器和poststart区别&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-初始化容器和poststart区别&#34;&gt;#&lt;/a&gt; 1.2 初始化容器和 PostStart 区别&lt;/h5&gt;
&lt;p&gt;PostStart：依赖主应用的环境，而且并不一定先于 Command 运行。&lt;/p&gt;
&lt;p&gt;InitContainer：不依赖主应用的环境，可以有更高的权限和更多的工具，一定会在主应用启动之前完成&lt;/p&gt;
&lt;h5 id=&#34;13-初始化容器和普通容器的区别&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-初始化容器和普通容器的区别&#34;&gt;#&lt;/a&gt; 1.3 初始化容器和普通容器的区别&lt;/h5&gt;
&lt;p&gt;Init 容器与普通的容器非常像，除了如下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一个 Init 容器运行成功后才会运行下一个 Init 容器；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所有的 Init 容器运行成功后才会运行主容器；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止，但是 Pod 对应的 restartPolicy 值为 Never，Kubernetes 不会重新启动 Pod。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Init 容器不支持 lifecycle、livenessProbe、readinessProbe 和 startupProbe&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;14-初始化容器示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-初始化容器示例&#34;&gt;#&lt;/a&gt; 1.4 初始化容器示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat init.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      initContainers:           # 初始化容器设定
      - name: fix-permissions
        image: busybox
        command: [&amp;quot;sh&amp;quot;,&amp;quot;-c&amp;quot;,&amp;quot;echo hello kubernetes&amp;gt;/usr/share/nginx/html/index.html&amp;quot;]
        securityContext:
          privileged: true
        volumeMounts:
        - name: share-volume
          mountPath: /usr/share/nginx/html
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: timezone
          mountPath: /etc/timezone
        - name: tz-config
          mountPath: /usr/share/zoneinfo/Asia/Shanghai
        - name: tz-config
          mountPath: /etc/localtime
        - name: share-volume
          mountPath: /usr/share/nginx/html
      volumes:
      - name: share-volume
        emptyDir: &amp;#123;&amp;#125;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: File

[root@k8s-master01 ~]# kubectl create -f init.yaml

[root@k8s-master01 ~]# curl 172.16.32.145
hello kubernetes
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-临时容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-临时容器&#34;&gt;#&lt;/a&gt; 2. 临时容器&lt;/h4&gt;
&lt;h5 id=&#34;21-注入临时容器到pod&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-注入临时容器到pod&#34;&gt;#&lt;/a&gt; 2.1 注入临时容器到 Pod&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods
NAME                            READY   STATUS    RESTARTS      AGE
nginx-deploy-7dd6cd4b44-ktw5k   1/1     Running   1             16h
nginx-deploy-7dd6cd4b44-mjcgq   1/1     Running   1 (28m ago)   16h
nginx-deploy-7dd6cd4b44-wdm6p   1/1     Running   1 (28m ago)   16h

#1.进入容器发现pod没有ps和netstat命令
[root@k8s-master01 ~]# kubectl exec -it nginx-deploy-7dd6cd4b44-ktw5k  -- bash
root@nginx-deploy-7dd6cd4b44-ktw5k:/# ps aux
root@nginx-deploy-7dd6cd4b44-ktw5k:/# netstat -lntp

#2.注入临时容器至该Pod
[root@k8s-master01 ~]# kubectl debug nginx-deploy-7dd6cd4b44-wdm6p -ti --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;21-注入临时容器到节点&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-注入临时容器到节点&#34;&gt;#&lt;/a&gt; 2.1 注入临时容器到节点&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;kubectl debug node k8s-node01 -it --image=registry.cn-hangzhou.aliyuncs.com/old_xu/debug-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-19T13:07:20.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3833778957.html</id>
        <title>K8s计划任务Job、Cronjob</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3833778957.html"/>
        <content type="html">&lt;h3 id=&#34;k8s计划任务job-cronjob&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s计划任务job-cronjob&#34;&gt;#&lt;/a&gt; K8s 计划任务 Job、Cronjob&lt;/h3&gt;
&lt;h4 id=&#34;1-job配置参数详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-job配置参数详解&#34;&gt;#&lt;/a&gt; 1. Job 配置参数详解&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    job-name: echo
  name: echo
  namespace: default
spec:
  #suspend: true # 1.21+
  #ttlSecondsAfterFinished: 100
  backoffLimit: 4
  completions: 1
  parallelism: 1
  template:
    spec:
      containers:
      - name: echo
        image: busybox
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - echo &amp;quot;Hello Job&amp;quot;
      restartPolicy: Never
      
[root@k8s-master01 ~]# kubectl get jobs
NAME   STATUS     COMPLETIONS   DURATION   AGE
echo   Complete   1/1           70s        2m5s

[root@k8s-master01 ~]# kubectl get pods
NAME          READY   STATUS      RESTARTS      AGE
echo-564c8    0/1     Completed   0             2m10s

[root@k8s-master01 ~]# kubectl logs echo-564c8
Hello Job
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;backoffLimit:：如果任务执行失败，失败多少次后不再执行&lt;/li&gt;
&lt;li&gt;completions：有多少个 Pod 执行成功，认为任务是成功的，默认为空和 parallelism 数值一样&lt;/li&gt;
&lt;li&gt;parallelism：并行执行任务的数量，如果 parallelism 数值大于 completions 数值，只会创建 completions 的数量；如果 completions 是 4，并发是 3，第一次会创建 3 个 Pod 执行任务，第二次只会创建一个 Pod 执行任务&lt;/li&gt;
&lt;li&gt;ttlSecondsAfterFinished：Job 在执行结束之后（状态为 completed 或 Failed）自动清理。设置为 0 表示执行结束立即删除，不设置则不会清除，需要开启 TTLAfterFinished 特性&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-cronjob配置参数详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-cronjob配置参数详解&#34;&gt;#&lt;/a&gt; 2. CronJob 配置参数详解&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat cronjob.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  concurrencyPolicy: Allow   #允许同时运行多个任务
  failedJobsHistoryLimit: 10  #保留多少失败的任务
  successfulJobsHistoryLimit: 10  #保留多少已完成的任务
  #suspend: true             #如果true则取消周期性执行任务
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command:
            - sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure 
          
[root@k8s-master01 ~]# kubectl get  cj
NAME    SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   &amp;lt;none&amp;gt;     False     0        6s              81s

[root@k8s-master01 ~]# kubectl get  jobs
NAME             STATUS     COMPLETIONS   DURATION   AGE
hello-29084454   Complete   1/1           4s         72s
hello-29084455   Complete   1/1           5s         12s

[root@k8s-master01 ~]# kubectl get  pods
NAME                   READY   STATUS      RESTARTS   AGE
hello-29084454-hwv7p   0/1     Completed   0          78s
hello-29084455-vf99w   0/1     Completed   0          18s

[root@k8s-master01 ~]# kubectl logs -f hello-29084455-vf99w
Sat Apr 19 12:55:02 UTC 2025
Hello from the Kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;apiVersion: batch/v1beta1   #1.21+ batch/v1&lt;/li&gt;
&lt;li&gt;schedule：调度周期，和 Linux 一致，分别是分时日月周。&lt;/li&gt;
&lt;li&gt;restartPolicy：重启策略，和 Pod 一致。&lt;/li&gt;
&lt;li&gt;concurrencyPolicy：并发调度策略。可选参数如下：
&lt;ul&gt;
&lt;li&gt;Allow：允许同时运行多个任务。&lt;/li&gt;
&lt;li&gt;Forbid：不允许并发运行，如果之前的任务尚未完成，新的任务不会被创建。&lt;/li&gt;
&lt;li&gt;Replace：如果之前的任务尚未完成，新的任务会替换的之前的任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;suspend：如果设置为 true，则暂停后续的任务，默认为 false。&lt;/li&gt;
&lt;li&gt;successfulJobsHistoryLimit：保留多少已完成的任务，按需配置。&lt;/li&gt;
&lt;li&gt;failedJobsHistoryLimit：保留多少失败的任务。&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-19T13:00:21.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/169153047.html</id>
        <title>K8s持久化存储</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/169153047.html"/>
        <content type="html">&lt;h3 id=&#34;k8s持久化存储&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s持久化存储&#34;&gt;#&lt;/a&gt; K8s 持久化存储&lt;/h3&gt;
&lt;h4 id=&#34;1-volume&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-volume&#34;&gt;#&lt;/a&gt; 1. Volume&lt;/h4&gt;
&lt;p&gt;Container（容器）中的磁盘文件是短暂的，当容器崩溃时，kubelet 会重新启动容器，Container 会以最干净的状态启动，最初的文件将丢失。另外，当一个 Pod 运行多个 Container 时，各个容器可能需要共享一些文件。Kubernetes Volume 可以解决这两个问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一些需要持久化数据的程序才会用到 Volumes，或者一些需要共享数据的容器需要 volumes。&lt;/li&gt;
&lt;li&gt;日志收集的需求需要在应用程序的容器里面加一个 sidecar，这个容器是一个收集日志的容器，比如 filebeat，它通过 volumes 共享应用程序的日志文件目录。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-emptydir实现数据共享&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-emptydir实现数据共享&#34;&gt;#&lt;/a&gt; 1.1 EmptyDir 实现数据共享&lt;/h5&gt;
&lt;p&gt;和上述 volume 不同的是，如果删除 Pod，emptyDir 卷中的数据也将被删除，一般 emptyDir 卷用于 Pod 中的不同 Container 共享数据。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
        - name: share-volume
          emptyDir: &amp;#123;&amp;#125;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
            - name: share-volume
              mountPath: /opt
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
            - name: share-volume
              mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-volumes-hostpath挂载宿主机路径&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-volumes-hostpath挂载宿主机路径&#34;&gt;#&lt;/a&gt; 1.2 Volumes HostPath 挂载宿主机路径&lt;/h5&gt;
&lt;p&gt;hostPath 卷可将节点上的文件或目录挂载到 Pod 上，用于 Pod 自定义日志输出或访问 Docker 内部的容器等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: share-volume
        emptyDir: &amp;#123;&amp;#125;
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: share-volume
            mountPath: /opt
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
          - name: share-volume
            mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPath 卷常用的 type（类型）如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;type 为空字符串：默认选项，意味着挂载 hostPath 卷之前不会执行任何检查。&lt;/li&gt;
&lt;li&gt;DirectoryOrCreate：如果给定的 path 不存在任何东西，那么将根据需要创建一个权限为 0755 的空目录，和 Kubelet 具有相同的组和权限。&lt;/li&gt;
&lt;li&gt;Directory：目录必须存在于给定的路径下。&lt;/li&gt;
&lt;li&gt;FileOrCreate：如果给定的路径不存储任何内容，则会根据需要创建一个空文件，权限设置为 0644，和 Kubelet 具有相同的组和所有权。&lt;/li&gt;
&lt;li&gt;File：文件，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;Socket：UNIX 套接字，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;CharDevice：字符设备，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;BlockDevice：块设备，必须存在于给定路径中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;13-挂载nfs至容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-挂载nfs至容器&#34;&gt;#&lt;/a&gt; 1.3 挂载 NFS 至容器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.安装nfs
# yum install nfs-utils -y       
# mkdir /data/nfs -p
# vim /etc/exports 
/data 192.168.1.0/24(rw,no_root_squash)
# exportfs -arv   
# systemctl start nfs-server &amp;amp;&amp;amp; systemctl enable nfs-server &amp;amp;&amp;amp; systemctl status nfs-server 

#2.测试客户端挂载
# showmount -e 192.168.1.75
# mount -t nfs 192.168.1.75:/data/nfs /mnt

#3.Deploy挂载NFS
[root@k8s-master01 ~]# cat nginx-deploy-nfs.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
      annotations:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: nfs-volume
        nfs:
          server: 192.168.1.75
          path: /data/nfs
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: nfs-volume
            mountPath: /usr/share/nginx/html
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-pv-pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-pv-pvc&#34;&gt;#&lt;/a&gt; 2. PV、PVC&lt;/h4&gt;
&lt;p&gt;PersistentVolume：简称 PV，是由 Kubernetes 管理员设置的存储，可以配置 Ceph、NFS、GlusterFS 等常用存储配置，相对于 Volume 配置，提供了更多的功能，比如生命周期的管理、大小的限制。PV 分为静态和动态。&lt;/p&gt;
&lt;p&gt;PersistentVolumeClaim：简称 PVC，是对存储 PV 的请求，表示需要什么类型的 PV，需要存储的技术人员只需要配置 PVC 即可使用存储，或者 Volume 配置 PVC 的名称即可。&lt;/p&gt;
&lt;h5 id=&#34;21-pv回收策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-pv回收策略&#34;&gt;#&lt;/a&gt; 2.1 PV 回收策略&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Retain：保留，该策略允许手动回收资源，当删除 PVC 时，PV 仍然存在，PV 被视为已释放，管理员可以手动回收卷。&lt;/li&gt;
&lt;li&gt;Recycle：回收，如果 Volume 插件支持，Recycle 策略会对卷执行 rm -rf 清理该 PV，并使其可用于下一个新的 PVC，但是本策略将来会被弃用，目前只有 NFS 和 HostPath 支持该策略。&lt;/li&gt;
&lt;li&gt;Delete：删除，如果 Volume 插件支持，删除 PVC 时会同时删除 PV，动态卷默认为 Delete，目前支持 Delete 的存储后端包括 AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder 等。&lt;/li&gt;
&lt;li&gt;可以通过 persistentVolumeReclaimPolicy: Recycle 字段配置&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;22-pv访问策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-pv访问策略&#34;&gt;#&lt;/a&gt; 2.2 PV 访问策略&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce：可以被单节点以读写模式挂载，命令行中可以被缩写为 RWO。&lt;/li&gt;
&lt;li&gt;ReadOnlyMany：可以被多个节点以只读模式挂载，命令行中可以被缩写为 ROX。&lt;/li&gt;
&lt;li&gt;ReadWriteMany：可以被多个节点以读写模式挂载，命令行中可以被缩写为 RWX。&lt;/li&gt;
&lt;li&gt;ReadWriteOncePod ：只允许被单个 Pod 访问，需要 K8s 1.22 + 以上版本，并且是 CSI 创建的 PV 才可使用，缩写为 RWOP&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOnce&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadOnlyMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOncePod&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HostPath&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;- (works when Pods are collocated)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;23-存储分类&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-存储分类&#34;&gt;#&lt;/a&gt; 2.3 存储分类&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;文件存储：一些数据可能需要被多个节点使用，比如用户的头像、用户上传的文件等，实现方式：NFS、NAS、FTP、CephFS 等。&lt;/li&gt;
&lt;li&gt;块存储：一些数据只能被一个节点使用，或者是需要将一块裸盘整个挂载使用，比如数据库、Redis 等，实现方式：Ceph、GlusterFS、公有云。&lt;/li&gt;
&lt;li&gt;对象存储：由程序代码直接实现的一种存储方式，云原生应用无状态化常用的实现方式，实现方式：一般是符合 S3 协议的云存储，比如 AWS 的 S3 存储、Minio、七牛云等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;24-pv配置示例nfs&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-pv配置示例nfs&#34;&gt;#&lt;/a&gt; 2.4 PV 配置示例 NFS&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv1
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-slow
  nfs:
    path: /data/pv1
    server: 192.168.1.75
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;capacity：容量配置&lt;/p&gt;
&lt;p&gt;volumeMode：卷的模式，目前支持 Filesystem（文件系统） 和 Block（块），其中 Block 类型需要后端存储支持，默认为文件系统&lt;/p&gt;
&lt;p&gt;accessModes：该 PV 的访问模式&lt;/p&gt;
&lt;p&gt;storageClassName：PV 的类，一个特定类型的 PV 只能绑定到特定类别的 PVC；&lt;/p&gt;
&lt;p&gt;persistentVolumeReclaimPolicy：回收策略&lt;/p&gt;
&lt;p&gt;mountOptions：非必须，新版本中已弃用&lt;/p&gt;
&lt;p&gt;nfs：NFS 服务配置，包括以下两个选项&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;path：NFS 上的共享目录&lt;/li&gt;
&lt;li&gt;server：NFS 的 IP 地址&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;25-pv配置示例hostpath&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-pv配置示例hostpath&#34;&gt;#&lt;/a&gt; 2.5 PV 配置示例 HostPath&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: hostpath
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: hostpath
  hostPath:
    path: &amp;quot;/mnt/data&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPath：hostPath 服务配置&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;path：宿主机路径&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;26-pv的状态&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#26-pv的状态&#34;&gt;#&lt;/a&gt; 2.6 PV 的状态&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Available：可用，没有被 PVC 绑定的空闲资源。&lt;/li&gt;
&lt;li&gt;Bound：已绑定，已经被 PVC 绑定。&lt;/li&gt;
&lt;li&gt;Released：已释放，PVC 被删除，但是资源还未被重新使用。&lt;/li&gt;
&lt;li&gt;Failed：失败，自动回收失败。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;27-pvc绑定pv&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-pvc绑定pv&#34;&gt;#&lt;/a&gt; 2.7 PVC 绑定 PV&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: nfs-slow
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi      
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;PVC 的空间申请大小≤PV 的大小&lt;/li&gt;
&lt;li&gt;PVC 的 StorageClassName 和 PV 的一致&lt;/li&gt;
&lt;li&gt;PVC 的 accessModes 和 PV 的一致&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;28-depoyment挂载pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#28-depoyment挂载pvc&#34;&gt;#&lt;/a&gt; 2.8 Depoyment 挂载 PVC&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      volumes:
      - name: nfs-pvc-storage  #volume名称
        persistentVolumeClaim:
          claimName: nfs-pvc   #PVC名称
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
         - name: nfs-pvc-storage
          mountPath: /usr/share/nginx/html
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;挂载 PVC 的 Pod 一直处于 Pending：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PVC 没有创建成功或 PVC 不存在&lt;/li&gt;
&lt;li&gt;PVC 和 Pod 不在同一个 Namespace&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-18T14:25:17.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3992668367.html</id>
        <title>K8s配置管理Configmap</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3992668367.html"/>
        <content type="html">&lt;h3 id=&#34;k8s配置管理configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s配置管理configmap&#34;&gt;#&lt;/a&gt; K8s 配置管理 Configmap&lt;/h3&gt;
&lt;h4 id=&#34;1-configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-configmap&#34;&gt;#&lt;/a&gt; 1. Configmap&lt;/h4&gt;
&lt;h5 id=&#34;1-1-基于from-env-file创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-1-基于from-env-file创建configmap&#34;&gt;#&lt;/a&gt; 1. 1 基于 from-env-file 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat cm_env.conf 
podname=nf-flms-system
podip=192.168.1.100
env=prod
nacosaddr=nacos.svc.cluster.local

#kubectl create cm cmenv --from-env-file=./cm_env.conf 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-基于from-literal创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-基于from-literal创建configmap&#34;&gt;#&lt;/a&gt; 1.2 基于 from-literal 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm cmliteral --from-literal=level=INFO --from-literal=passwd=Superman*2023
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-基于from-file创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-基于from-file创建configmap&#34;&gt;#&lt;/a&gt; 1.3 基于 from-file 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat s.hmallleasing.com.conf 
server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    client_max_body_size 1G; 
    location / &amp;#123;
        proxy_pass http://192.168.1.134;
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        proxy_connect_timeout 30;
        proxy_send_timeout 60;
        proxy_read_timeout 60;
        
        proxy_buffering on;
        proxy_buffer_size 32k;
        proxy_buffers 4 128k;
        proxy_temp_file_write_size 10240k;		
        proxy_max_temp_file_size 10240k;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;

# kubectl create cm nginxconfig --from-file=./s.hmallleasing.com.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-deployment挂载configmap示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-deployment挂载configmap示例&#34;&gt;#&lt;/a&gt; 1.4 Deployment 挂载 configmap 示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-重命名挂载的configmaq-key的名称&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-重命名挂载的configmaq-key的名称&#34;&gt;#&lt;/a&gt; 1.5 重命名挂载的 configmaq key 的名称&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
          items:                # 重命名挂载的configmaq key的名称为nginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
 
#查看挂载的configmaq key的名称重命名为nginx.conf
[root@k8s-master01 cm]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-deploy-bc476bc56-flln4   1/1     Running   0          10h
nginx-deploy-bc476bc56-jhsh6   1/1     Running   0          10h
nginx-deploy-bc476bc56-splv9   1/1     Running   0          10h
[root@k8s-master01 cm]# kubectl exec -it nginx-deploy-bc476bc56-flln4 -- bash
root@nginx-deploy-bc476bc56-flln4:/# ls /etc/nginx/conf.d/
nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;16-修改挂载的configmaq-权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-修改挂载的configmaq-权限&#34;&gt;#&lt;/a&gt; 1.6 修改挂载的 configmaq 权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
          items:                # 重命名挂载的configmaq key的名称为nginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
            mode: 0644        # 配置挂载权限，针对单个key生效
          defaultMode: 0666   # 配置挂载权限，针对整个key生效
    
#查看挂载权限
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/nginx.conf 
lrwxrwxrwx 1 root root 17 Apr 16 13:37 /etc/nginx/conf.d/nginx.conf -&amp;gt; ..data/nginx.conf
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/..data/nginx.conf 
-rw-rw-rw- 1 root root 722 Apr 16 13:37 /etc/nginx/conf.d/..data/nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;17-subpath解决挂载覆盖问题&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#17-subpath解决挂载覆盖问题&#34;&gt;#&lt;/a&gt; 1.7 subpath 解决挂载覆盖问题&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.创建configmap
[root@k8s-master01 cm]# cat nginx.conf 

user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events &amp;#123;
    worker_connections  512;
&amp;#125;


http &amp;#123;
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  &#39;$remote_addr - $remote_user [$time_local] &amp;quot;$request&amp;quot; &#39;
                      &#39;$status $body_bytes_sent &amp;quot;$http_referer&amp;quot; &#39;
                      &#39;&amp;quot;$http_user_agent&amp;quot; &amp;quot;$http_x_forwarded_for&amp;quot;&#39;;

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
&amp;#125;

[root@k8s-master01 cm]# kubectl create cm nginx-config --from-file=./nginx.conf

#subpath解决挂载覆盖问题
[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # ①批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # ②自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # ③挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:
        - name: config
          mountPath: &amp;quot;/etc/nginx/nginx.conf&amp;quot;   #只挂在nginx.conf一个文件,不覆盖目录
          subPath: nginx.conf      
      volumes:
      - name: config
        configMap:
          name: nginx-config      # 提供你想要挂载的ConfigMap的名字
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-secret&#34;&gt;#&lt;/a&gt; 2. Secret&lt;/h4&gt;
&lt;h5 id=&#34;21-secret拉取私有仓库镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-secret拉取私有仓库镜像&#34;&gt;#&lt;/a&gt; 2.1 Secret 拉取私有仓库镜像&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret docker-registry harboradmin \
--docker-server=s.hmallleasing.com \
--docker-username=admin \
--docker-password=Superman*2023 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-创建ssl-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-创建ssl-secret&#34;&gt;#&lt;/a&gt; 2.2 创建 ssl Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret tls dev.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-基于命令创建generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-基于命令创建generic-secret&#34;&gt;#&lt;/a&gt; 2.3 基于命令创建 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.通过from-env-file创建
# cat db.conf 
username=xuyong
passwd=Superman*2023

# kubectl create secret generic dbconf --from-env-file=./db.conf

#2.通过from-literal创建
kubectl create secret generic db-user-pass \
    --from-literal=username=admin \
    --from-literal=password=&#39;S!B\*d$zDsb=&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-secret加密-解密&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-secret加密-解密&#34;&gt;#&lt;/a&gt; 2.4 Secret 加密、解密&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;1.加密
# echo -n &amp;quot;Superman*2023&amp;quot; | base64
U3VwZXJtYW4qMjAyMw==

2.解密
# echo &amp;quot;U3VwZXJtYW4qMjAyMw==&amp;quot; | base64 --decode
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-基于文件创建非加密generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-基于文件创建非加密generic-secret&#34;&gt;#&lt;/a&gt; 2.5 基于文件创建非加密 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get secret dbconf -oyaml
apiVersion: v1
data:
  passwd: U3VwZXJtYW4qMjAyMw==
  username: eHV5b25n
kind: Secret
metadata:
  name: dbconf
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-6-基于yaml创建加密generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-6-基于yaml创建加密generic-secret&#34;&gt;#&lt;/a&gt; 2. 6 基于 yaml 创建加密 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat mysql-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
  namespace: dev
stringData:
  MYSQL_ROOT_PASSWORD: Superman*2023
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;27-deployment挂载secret示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-deployment挂载secret示例&#34;&gt;#&lt;/a&gt; 2.7 Deployment 挂载 Secret 示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - name: MYSQL_ROOT_PASSWORD  
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_ROOT_PASSWORD
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-configmapsecret热更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-configmapsecret热更新&#34;&gt;#&lt;/a&gt; 3. ConfigMap&amp;amp;Secret 热更新&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm nginxconfig --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f -
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T13:47:47.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/858611107.html</id>
        <title>K8s服务发布Service</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/858611107.html"/>
        <content type="html">&lt;h3 id=&#34;k8s服务发布service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s服务发布service&#34;&gt;#&lt;/a&gt; K8s 服务发布 Service&lt;/h3&gt;
&lt;h4 id=&#34;1-service类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-service类型&#34;&gt;#&lt;/a&gt; 1. Service 类型&lt;/h4&gt;
&lt;p&gt;Kubernetes Service Type（服务类型）主要包括以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP：在集群内部使用，默认值，只能从集群中访问。&lt;/li&gt;
&lt;li&gt;NodePort：在所有安装了 Kube-Proxy 的节点上打开一个端口，此端口可以代理至后端 Pod，可以通过 NodePort 从集群外部访问集群内的服务，格式为 NodeIP:NodePort。&lt;/li&gt;
&lt;li&gt;LoadBalancer：使用云提供商的负载均衡器公开服务，成本较高。&lt;/li&gt;
&lt;li&gt;ExternalName：通过返回定义的 CNAME 别名，没有设置任何类型的代理，需要 1.7 或更高版本 kube-dns 支持。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-nodeport类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-nodeport类型&#34;&gt;#&lt;/a&gt; 1.1 NodePort 类型&lt;/h5&gt;
&lt;p&gt;如果将 Service 的 type 字段设置为 NodePort，则 Kubernetes 将从 --service-node-port-range 参数指定的范围（默认为 30000-32767）中自动分配端口，也可以手动指定 NodePort，创建该 Service 后，集群每个节点都将暴露一个端口，通过某个宿主机的 IP + 端口即可访问到后端的应用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-clusterip类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-clusterip类型&#34;&gt;#&lt;/a&gt; 1.2 ClusterIP 类型&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-使用service代理k8s外部服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-使用service代理k8s外部服务&#34;&gt;#&lt;/a&gt; 1.3 使用 Service 代理 K8s 外部服务&lt;/h5&gt;
&lt;p&gt;使用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;希望在生产环境中使用某个固定的名称而非 IP 地址访问外部的中间件服务；&lt;/li&gt;
&lt;li&gt;希望 Service 指向另一个 Namespace 中或其他集群中的服务；&lt;/li&gt;
&lt;li&gt;正在将工作负载转移到 Kubernetes 集群，但是一部分服务仍运行在 Kubernetes 集群之外的 backend。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
spec:
  clusterIP: None
  ports:
  - name: mysql
    port: 3306 
    protocol: TCP
    targetPort: 3306
  type: ClusterIP
---
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
subsets:
- addresses:
  - ip: 192.168.40.150
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-externalname-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-externalname-service&#34;&gt;#&lt;/a&gt; 1.4 ExternalName Service&lt;/h5&gt;
&lt;p&gt;ExternalName Service 是 Service 的特例，它没有 Selector，也没有定义任何端口和 Endpoint，它通过返回该外部服务的别名来提供服务。&lt;/p&gt;
&lt;p&gt;比如可以定义一个 Service，后端设置为一个外部域名，这样通过 Service 的名称即可访问到该域名。使用 nslookup 解析以下文件定义的 Service，集群的 DNS &lt;a href=&#34;http://xn--my-uu2cmg2cx7mswf9rko5lsx1a5n3h.database.example.com&#34;&gt;服务将返回一个值为 my.database.example.com&lt;/a&gt; 的 CNAME 记录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-多端口-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-多端口-service&#34;&gt;#&lt;/a&gt; 1.5 多端口 Service&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
    - port: 443
      targetPort: 443
      protocol: TCP
      name: https
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:25:51.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/108692210.html</id>
        <title>K8s资源调度deployment、statefulset、daemonset</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/108692210.html"/>
        <content type="html">&lt;h3 id=&#34;k8s资源调度deployment-statefulset-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s资源调度deployment-statefulset-daemonset&#34;&gt;#&lt;/a&gt; K8s 资源调度 deployment、statefulset、daemonset&lt;/h3&gt;
&lt;h4 id=&#34;1-无状态应用管理-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-无状态应用管理-deployment&#34;&gt;#&lt;/a&gt; 1. 无状态应用管理 Deployment&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.0
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例解析：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;nginx-deploy：Deployment 的名称；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;replicas： 创建 Pod 的副本数；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;selector：定义 Deployment 如何找到要管理的 Pod，与 template 的 label（标签）对应，apiVersion 为 apps/v1 必须指定该字段；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;template 字段包含以下字段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;app: nginx-deploy 使用 label（标签）标记 Pod；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;spec：表示 Pod 运行一个名字为 nginx 的容器；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image：运行此 Pod 使用的镜像；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Port：容器用于发送和接收流量的端口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;11-更新-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-更新-deployment&#34;&gt;#&lt;/a&gt; 1.1 更新 Deployment&lt;/h5&gt;
&lt;p&gt;假如更新 Nginx Pod 的 image 使用 nginx:latest，并使用 --record 记录当前更改的参数，后期回滚时可以查看到对应的信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新过程为新旧交替更新，首先新建一个 Pod，当 Pod 状态为 Running 时，删除一个旧的 Pod，同时再创建一个新的 Pod。当触发一个更新后，会有新的 ReplicaSet 产生，旧的 ReplicaSet 会被保存，查看此时 ReplicaSet，可以从 AGE 或 READY 看出来新旧 ReplicaSet：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get rs
NAME                      DESIRED   CURRENT   READY   AGE
nginx-deploy-65bfb77869   0         0         0       50s
nginx-deploy-85b94dddb4   3         3         3       8s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过 describe 查看 Deployment 的详细信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl describe deploy nginx-deploy
Name:                   nginx-deploy
Namespace:              default
CreationTimestamp:      Mon, 14 Apr 2025 11:28:03 +0800
Labels:                 app=nginx-deploy
Annotations:            app: nginx-deploy
                        deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
Selector:               app=nginx-deploy
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx-deploy
  Containers:
   nginx-deploy:
    Image:         nginx:latest
    Port:          &amp;lt;none&amp;gt;
    Host Port:     &amp;lt;none&amp;gt;
    Environment:   &amp;lt;none&amp;gt;
    Mounts:        &amp;lt;none&amp;gt;
  Volumes:         &amp;lt;none&amp;gt;
  Node-Selectors:  &amp;lt;none&amp;gt;
  Tolerations:     &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  nginx-deploy-65bfb77869 (0/0 replicas created)
NewReplicaSet:   nginx-deploy-85b94dddb4 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  71s   deployment-controller  Scaled up replica set nginx-deploy-65bfb77869 from 0 to 3
  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 0 to 1
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 3 to 2
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 1 to 2
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 2 to 1
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 2 to 3
  Normal  ScalingReplicaSet  26s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 1 to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 describe 中可以看出，第一次创建时，它创建了一个名为 nginx-deploy-65bfb77869 的 ReplicaSet，并直接将其扩展为 3 个副本。更新部署时，它创建了一个新的 ReplicaSet，命名为 nginx-deploy-85b94dddb4，并将其副本数扩展为 1，然后将旧的 ReplicaSet 缩小为 2，这样至少可以有 2 个 Pod 可用，最多创建了 4 个 Pod。以此类推，使用相同的滚动更新策略向上和向下扩展新旧 ReplicaSet，最终新的 ReplicaSet 可以拥有 3 个副本，并将旧的 ReplicaSet 缩小为 0。&lt;/p&gt;
&lt;h5 id=&#34;12-回滚-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-回滚-deployment&#34;&gt;#&lt;/a&gt; 1.2 回滚 Deployment&lt;/h5&gt;
&lt;p&gt;当更新了版本不稳定或配置不合理时，可以对其进行回滚操作，假设我们又进行了几次更新（此处以更新镜像版本触发更新，更改配置效果类似）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record
# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用 kubectl rollout history 查看更新历史：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
3         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 Deployment 某次更新的详细信息，使用 --revision 指定某次更新版本号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy --revision=4
deployment.apps/nginx-deploy with revision #4
Pod Template:
  Labels:	app=nginx-deploy
	pod-template-hash=65b576b795
  Annotations:	kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
  Containers:
   nginx-deploy:
    Image:	nginx:1.21.2
    Port:	&amp;lt;none&amp;gt;
    Host Port:	&amp;lt;none&amp;gt;
    Environment:	&amp;lt;none&amp;gt;
    Mounts:	&amp;lt;none&amp;gt;
  Volumes:	&amp;lt;none&amp;gt;
  Node-Selectors:	&amp;lt;none&amp;gt;
  Tolerations:	&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果只需要回滚到上一个稳定版本，使用 kubectl rollout undo 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再次查看更新历史，发现 REVISION3 回到了 nginx:1.21.1：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
5         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果要回滚到指定版本，使用 --to-revision 参数：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy --to-revision=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-扩容-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-扩容-deployment&#34;&gt;#&lt;/a&gt; 1.3 扩容 Deployment&lt;/h5&gt;
&lt;p&gt;当公司访问量变大，或者有预期内的活动时，三个 Pod 可能已无法支撑业务时，可以提前对其进行扩展。&lt;/p&gt;
&lt;p&gt;使用 kubectl scale 动态调整 Pod 的副本数，比如增加 Pod 为 5 个：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale deployment nginx-deploy --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 Pod，此时 Pod 已经变成了 5 个：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
nginx-deploy-85b94dddb4-2qrh6   1/1     Running   0          2m9s
nginx-deploy-85b94dddb4-gvkqj   1/1     Running   0          2m10s
nginx-deploy-85b94dddb4-mdfjs   1/1     Running   0          22s
nginx-deploy-85b94dddb4-rhgpr   1/1     Running   0          2m8s
nginx-deploy-85b94dddb4-vwjhl   1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-暂停和恢复-deployment-更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-暂停和恢复-deployment-更新&#34;&gt;#&lt;/a&gt; 1.4 暂停和恢复 Deployment 更新&lt;/h5&gt;
&lt;p&gt;上述演示的均为更改某一处的配置，更改后立即触发更新，大多数情况下可能需要针对一个资源文件更改多处地方，而并不需要多次触发更新，此时可以使用 Deployment 暂停功能，临时禁用更新操作，对 Deployment 进行多次修改后在进行更新。&lt;/p&gt;
&lt;p&gt;使用 kubectl rollout pause 命令即可暂停 Deployment 更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout pause deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后对 Deployment 进行相关更新操作，比如先更新镜像，然后对其资源进行限制（如果使用的是 kubectl edit 命令，可以直接进行多次修改，无需暂停更新，kubectlset 命令一般会集成在 CICD 流水线中）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.3
# kubectl set resources deployment nginx-deploy -c=nginx-deploy --limits=cpu=200m,memory=512Mi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过 rollout history 可以看到没有新的更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#  kubectl rollout history deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行完最后一处配置更改后，使用 kubectl rollout resume 恢复 Deployment 更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout resume deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以查看到恢复更新的 Deployment 创建了一个新的 RS（ReplicaSet 缩写）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get rs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以查看 Deployment 的 image（镜像）已经变为 nginx:1.21.3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-更新-deployment-的注意事项&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-更新-deployment-的注意事项&#34;&gt;#&lt;/a&gt; 1.5 更新 Deployment 的注意事项&lt;/h5&gt;
&lt;p&gt;在默认情况下，revision 保留 10 个旧的 ReplicaSet，其余的将在后台进行垃圾回收，可以在.spec.revisionHistoryLimit 设置保留 ReplicaSet 的个数。当设置为 0 时，不保留历史记录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  namespace: default
  labels:
    app: nginx-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.3
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spec.strategy.type==Recreate，表示重建，先删掉旧的 Pod 再创建新的 Pod；&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: Recreate
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.type==RollingUpdate，表示滚动更新，可以指定 maxUnavailable 和 maxSurge 来控制滚动更新过程；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxUnavailable，指定在回滚更新时最大不可用的 Pod 数量，可选字段，默认为 25%，可以设置为数字或百分比，如果 maxSurge 为 0，则该值不能为 0；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxSurge 可以超过期望值的最大 Pod 数，可选字段，默认为 25%，可以设置成数字或百分比，如果 maxUnavailable 为 0，则该值不能为 0。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-有状态应用管理-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-有状态应用管理-statefulset&#34;&gt;#&lt;/a&gt; 2. 有状态应用管理 StatefulSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: &#39;1&#39;
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 128Mi
      restartPolicy: Always
  serviceName: web
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;kind: Service 定义了一个名字为 web 的 Headless Service，创建的 Service 格式为 nginx-0.web.default.svc.cluster.local，其他的类似，因为没有指定 Namespace（命名空间），所以默认部署在 default；&lt;/li&gt;
&lt;li&gt;kind: StatefulSet 定义了一个名字为 nginx 的 StatefulSet，replicas 表示部署 Pod 的副本数，本实例为 3。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;21-创建-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-创建-statefulset&#34;&gt;#&lt;/a&gt; 2.1 创建 StatefulSet&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
nginx-0   1/1     Running   0          8m51s
nginx-1   1/1     Running   0          8m50s
nginx-2   1/1     Running   0          8m48s
[root@k8s-master01 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   6d1h
web          ClusterIP   None         &amp;lt;none&amp;gt;        80/TCP    9m28s
[root@k8s-master01 ~]# kubectl get sts
NAME    READY   AGE
nginx   3/3     8m58s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-statefulset创建pod流程&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-statefulset创建pod流程&#34;&gt;#&lt;/a&gt; 2.2 StatefulSet 创建 Pod 流程&lt;/h5&gt;
&lt;p&gt;StatefulSet 管理的 Pod 部署和扩展规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于具有 N 个副本的 StatefulSet，将按顺序从 0 到 N-1 开始创建 Pod；&lt;/li&gt;
&lt;li&gt;当删除 Pod 时，将按照 N-1 到 0 的反顺序终止；&lt;/li&gt;
&lt;li&gt;在缩放 Pod 之前，必须保证当前的 Pod 是 Running（运行中）或者 Ready（就绪）；&lt;/li&gt;
&lt;li&gt;在终止 Pod 之前，它所有的继任者必须是完全关闭状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet 的 pod.Spec.TerminationGracePeriodSeconds（终止 Pod 的等待时间）不应该指定为 0，设置为 0 对 StatefulSet 的 Pod 是极其不安全的做法，优雅地删除 StatefulSet 的 Pod 是非常有必要的，而且是安全的，因为它可以确保在 Kubelet 从 APIServer 删除之前，让 Pod 正常关闭。&lt;/p&gt;
&lt;p&gt;当创建上面的 Nginx 实例时，Pod 将按 nginx-0、nginx-1、nginx-2 的顺序部署 3 个 Pod。在 nginx-0 处于 Running 或者 Ready 之前，nginx-1 不会被部署，相同的，nginx-2 在 web-1 未处于 Running 和 Ready 之前也不会被部署。如果在 nginx-1 处于 Running 和 Ready 状态时，nginx-0 变成 Failed 失败）状态，那么 nginx-2 将不会被启动，直到 nginx-0 恢复为 Running 和 Ready 状态。&lt;/p&gt;
&lt;p&gt;如果用户将 StatefulSet 的 replicas 设置为 1，那么 nginx-2 将首先被终止，在完全关闭并删除 nginx-2 之前，不会删除 nginx-1。如果 nginx-2 终止并且完全关闭后，nginx-0 突然失败，那么在 nginx-0 未恢复成 Running 或者 Ready 时，nginx-1 不会被删除。&lt;/p&gt;
&lt;h5 id=&#34;23-tatefulset-扩容和缩容&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-tatefulset-扩容和缩容&#34;&gt;#&lt;/a&gt; 2.3 tatefulSet 扩容和缩容&lt;/h5&gt;
&lt;p&gt;和 Deployment 类似，可以通过更新 replicas 字段扩容 / 缩容 StatefulSet，也可以使用 kubectlscale、kubectl edit 和 kubectl patch 来扩容 / 缩容一个 StatefulSet。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale sts nginx --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-statefulset-更新策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-statefulset-更新策略&#34;&gt;#&lt;/a&gt; 2.4 StatefulSet 更新策略&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;On Delete 策略&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OnDelete 更新策略实现了传统（1.7 版本之前）的行为，它也是默认的更新策略。当我们选择这个更新策略并修改 StatefulSet 的.spec.template 字段时，StatefulSet 控制器不会自动更新 Pod，必须手动删除 Pod 才能使控制器创建新的 Pod。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: OnDelete
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;RollingUpdate 策略&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RollingUpdate（滚动更新）更新策略会自动更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序进行滚动更新。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-分段更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-分段更新&#34;&gt;#&lt;/a&gt; 2.5 分段更新&lt;/h5&gt;
&lt;p&gt;将分区改为 2，此时会自动更新 nginx-2、nginx-3、nginx-4（因为之前更改了更新策略），但是不会更新 nginx-0 和 nginx-1：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 sts 镜像为 nginx:1.21.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image sts nginx nginx=nginx:1.21.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;按照上述方式，可以实现分阶段更新，类似于灰度 / 金丝雀发布。查看最终的结果如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3守护进程集-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3守护进程集-daemonset&#34;&gt;#&lt;/a&gt; 3. 守护进程集 DaemonSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
        - name: nginx-ds
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时会在每个节点创建一个 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-ds-47dxc   1/1     Running   0          56s   172.16.85.213    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-4m89f   1/1     Running   0          56s   172.16.32.143    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-mtpc2   1/1     Running   0          56s   172.16.195.12    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-t5rxc   1/1     Running   0          56s   172.16.122.142   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-x86kc   1/1     Running   0          56s   172.16.58.222    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;指定节点部署 Pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;      nodeSelector:
        ingress: &#39;true&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新和回滚 DaemonSet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image ds nginx-ds nginx-ds=1.21.0 --record=true
# kubectl rollout undo daemonset &amp;lt;daemonset-name&amp;gt; --to-revision=&amp;lt;revision&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DaemonSet 的更新和回滚与 Deployment 类似，此处不再演示。&lt;/p&gt;
&lt;h4 id=&#34;4-hpa&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-hpa&#34;&gt;#&lt;/a&gt; 4. HPA&lt;/h4&gt;
&lt;p&gt;创建 deployment、service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-hpa-svc
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx-hpa
  type: ClusterIP

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-hpa
  labels:
    app: nginx-hpa
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-hpa
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-hpa
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-hpa
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建 HPA&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl autoscale deployment nginx-hpa --cpu-percent=10 --min=1 --max=10
# kubectl get hpa
NAME        REFERENCE              TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
nginx-hpa   Deployment/nginx-hpa   cpu: 0%/10%   1         10        1          16s

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试自动扩缩容&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;while true; do wget -q -O- http://10.96.18.221 &amp;gt; /dev/null; done
[root@k8s-master01 ~]# kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
nginx-hpa-d8bcbdf7d-4mkxp   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-974q5   1/1     Running   0          6m36s
nginx-hpa-d8bcbdf7d-g6p2h   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-lvvsq   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tgqmr   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tzfbs   1/1     Running   0          21s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:25:00.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/1771242682.html</id>
        <title>K8s零宕机服务发布-探针</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/1771242682.html"/>
        <content type="html">&lt;h3 id=&#34;k8s零宕机服务发布-探针&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s零宕机服务发布-探针&#34;&gt;#&lt;/a&gt; K8s 零宕机服务发布 - 探针&lt;/h3&gt;
&lt;h4 id=&#34;1-pod状态及-pod-故障排查命令&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-pod状态及-pod-故障排查命令&#34;&gt;#&lt;/a&gt; 1. Pod 状态及 Pod 故障排查命令&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;状态&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pending（挂起）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已被 Kubernetes 系统接收，但仍有一个或多个容器未被创建，可以通过 kubectl describe 查看处于 Pending 状态的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Running（运行中）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经被绑定到一个节点上，并且所有的容器都已经被创建，而且至少有一个是运行状态，或者是正在启动或者重启，可以通过 kubectl logs 查看 Pod 的日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Succeeded（成功）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;所有容器执行成功并终止，并且不会再次重启，可以通过 kubectl logs 查看 Pod 日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Failed（失败）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;所有容器都已终止，并且至少有一个容器以失败的方式终止，也就是说这个容器要么以非零状态退出，要么被系统终止，可以通过 logs 和 describe 查看 Pod 日志和状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Unknown（未知）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;通常是由于通信问题造成的无法获得 Pod 的状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ImagePullBackOff ErrImagePull&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;镜像拉取失败，一般是由于镜像不存在、网络不通或者需要登录认证引起的，可以使用 describe 命令查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CrashLoopBackOff&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器启动失败，可以通过 logs 命令查看具体原因，一般为启动命令不正确，健康检查不通过等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OOMKilled&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器内存溢出，一般是容器的内存 Limit 设置的过小，或者程序本身有内存溢出，可以通过 logs 查看程序启动日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Terminating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 正在被删除，可以通过 describe 查看状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SysctlForbidden&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 自定义了内核配置，但 kubelet 没有添加内核配置或配置的内核参数不支持，可以通过 describe 查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Completed&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器内部主进程退出，一般计划任务执行结束会显示该状态，此时可以通过 logs 查看容器日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ContainerCreating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 正在创建，一般为正在下载镜像，或者有配置不当的地方，可以通过 describe 查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;2-pod镜像拉取策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-pod镜像拉取策略&#34;&gt;#&lt;/a&gt; 2. Pod 镜像拉取策略&lt;/h4&gt;
&lt;p&gt;通过 spec.containers [].imagePullPolicy 参数可以指定镜像的拉取策略，目前支持的策略如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;总是拉取，当镜像 tag 为 latest 时，且 imagePullPolicy 未配置，默认为 Always&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;不管是否存在都不会拉取&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IfNotPresent&lt;/td&gt;
&lt;td&gt;镜像不存在时拉取镜像，如果 tag 为非 latest，且 imagePullPolicy 未配置，默认为 IfNotPresent&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;更改镜像拉取策略为 IfNotPresent：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-pod-重启策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-pod-重启策略&#34;&gt;#&lt;/a&gt; 3. &lt;strong&gt;Pod&lt;/strong&gt; 重启策略&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;默认策略。容器失效时，自动重启该容器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OnFailure&lt;/td&gt;
&lt;td&gt;容器以不为 0 的状态码终止，自动重启该容器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;无论何种状态，都不会重启&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;指定重启策略为 Always ：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-pod的三种探针&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-pod的三种探针&#34;&gt;#&lt;/a&gt; 4. Pod 的三种探针&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;种类&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;startupProbe&lt;/td&gt;
&lt;td&gt;Kubernetes1.16 新加的探测方式，用于判断容器内的应用程序是否已经启动。如果配置了 startupProbe，就会先禁用其他探测，直到它成功为止。如果探测失败，Kubelet 会杀死容器，之后根据重启策略进行处理，如果探测成功，或没有配置 startupProbe，则状态为成功，之后就不再探测。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;livenessProbe&lt;/td&gt;
&lt;td&gt;用于探测容器是否在运行，如果探测失败，kubelet 会 “杀死” 容器并根据重启策略进行相应的处理。如果未指定该探针，将默认为 Success&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;readinessProbe&lt;/td&gt;
&lt;td&gt;一般用于探测容器内的程序是否健康，即判断容器是否为就绪（Ready）状态。如果是，则可以处理请求，反之 Endpoints Controller 将从所有的 Service 的 Endpoints 中删除此容器所在 Pod 的 IP 地址。如果未指定，将默认为 Success&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;5-pod探针的实现方式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-pod探针的实现方式&#34;&gt;#&lt;/a&gt; 5. Pod 探针的实现方式&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;实现方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ExecAction&lt;/td&gt;
&lt;td&gt;在容器内执行一个指定的命令，如果命令返回值为 0，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCPSocketAction&lt;/td&gt;
&lt;td&gt;通过 TCP 连接检查容器指定的端口，如果端口开放，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HTTPGetAction&lt;/td&gt;
&lt;td&gt;对指定的 URL 进行 Get 请求，如果状态码在 200~400 之间，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;6-健康检查配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-健康检查配置&#34;&gt;#&lt;/a&gt; 6. 健康检查配置&lt;/h4&gt;
&lt;p&gt;配置健康检查：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-prestop和-poststart配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-prestop和-poststart配置&#34;&gt;#&lt;/a&gt; 7. PreStop 和 PostStart 配置&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
          lifecycle:
            postStart:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - mkdir /data
            preStop:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - sleep 30
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:23:48.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/985149017.html</id>
        <title>二进制高可用安装K8S集群</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/985149017.html"/>
        <content type="html">&lt;h2 id=&#34;二进制高可用安装k8s集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#二进制高可用安装k8s集群&#34;&gt;#&lt;/a&gt; 二进制高可用安装 K8s 集群&lt;/h2&gt;
&lt;h4 id=&#34;1-基本配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-基本配置&#34;&gt;#&lt;/a&gt; 1. 基本配置&lt;/h4&gt;
&lt;h5 id=&#34;11-基本环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-基本环境配置&#34;&gt;#&lt;/a&gt; 1.1 基本环境配置&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP 地址&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master 节点 * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived 虚拟 IP（不占用机器）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker 节点 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* 配置信息 *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;系统版本&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 网段&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service 网段&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改主机名（其它节点按需修改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 hosts，修改 /etc/hosts 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 yum 源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 配置基础源
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;必备工具安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭 swap 分区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ntpdate：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;同步时间并配置上海时区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 limit：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;升级系统：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;下载安装所有的源码文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-内核配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-内核配置&#34;&gt;#&lt;/a&gt; 1.2 内核配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ipvsadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 ipvs 模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 ipvs.conf，并配置开机自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# 加入以下内容
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;内核优化配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;应用配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置完内核后，重启机器，之后查看内核模块是否已自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-高可用组件安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-高可用组件安装&#34;&gt;#&lt;/a&gt; 2. 高可用组件安装&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;通过 yum 安装 HAProxy 和 KeepAlived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 HAProxy，需要注意黄色部分的 IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:8443       #HAProxy监听端口
  bind 127.0.0.1:8443     #HAProxy监听端口
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IP地址
  server k8s-master02	192.168.1.72:6443  check       #API Server IP地址
  server k8s-master03	192.168.1.73:6443  check       #API Server IP地址
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 KeepAlived，需要注意黄色部分的配置。&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #网卡名称
    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #网卡名称
    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #网卡名称
    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置 KeepAlived 健康检查文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置健康检查文件添加执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;启动 haproxy 和 keepalived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;所有节点测试VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld&lt;/li&gt;
&lt;li&gt;所有节点查看 selinux 状态，必须为 disable：getenforce&lt;/li&gt;
&lt;li&gt;master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master 节点查看监听端口：netstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果以上都没有问题，需要确认：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;是否是公有云机器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否是私有云机器（类似 OpenStack）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询&lt;/p&gt;
&lt;h4 id=&#34;3-runtime安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtime安装&#34;&gt;#&lt;/a&gt; 3. Runtime 安装&lt;/h4&gt;
&lt;p&gt;如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。&lt;/p&gt;
&lt;h5 id=&#34;31-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-安装containerd&#34;&gt;#&lt;/a&gt; 3.1 安装 Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置安装源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先配置 Containerd 所需的模块（&lt;mark&gt;所有节点&lt;/mark&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;，配置 Containerd 所需的内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;生成 Containerd 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改 Containerd 的 Cgroup 和 Pause 镜像配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 Containerd，并配置开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 crictl 客户端连接的运行时位置（可选）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-k8s及etcd安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-k8s及etcd安装&#34;&gt;#&lt;/a&gt; 4 . K8S 及 etcd 安装&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 下载 kubernetes 安装包（1.32.3 需要更改为你看到的最新版本）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最新版获取地址：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;以下操作都在 master01 执行&lt;/mark&gt;&lt;/p&gt;
&lt;p&gt;下载 etcd 安装包：&lt;a href=&#34;https://github.com/etcd-io/etcd/releases/&#34;&gt;https://github.com/etcd-io/etcd/releases/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解压 kubernetes 安装文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解压 etcd 安装文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&amp;#123;,ctl&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;版本查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubelet --version
Kubernetes v1.32.3
[root@k8s-master01 ~]# etcdctl version
etcdctl version: 3.5.16
API version: 3.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将组件发送到其他节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;
WorkNodes=&#39;k8s-node01 k8s-node02&#39;
for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done
for NODE in $WorkNodes; do     scp /usr/local/bin/kube&amp;#123;let,-proxy&amp;#125; $NODE:/usr/local/bin/ ; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;切换到 1.32.x 分支（其他版本可以切换到其他分支，.x 即可，不需要更改为具体的小版本）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-生成证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-生成证书&#34;&gt;#&lt;/a&gt; 5 . 生成证书&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;二进制安装最关键步骤，一步错误全盘皆输，一定要注意每个步骤都要是正确的&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 下载生成证书工具（下载不成功可以去百度网盘）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&amp;quot; -O /usr/local/bin/cfssl
wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&amp;quot; -O /usr/local/bin/cfssljson
chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-etcd证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-etcd证书&#34;&gt;#&lt;/a&gt; 5.1 Etcd 证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd 证书目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/etcd/ssl -p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 kubernetes 相关目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/pki
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;生成 etcd 证书&lt;/p&gt;
&lt;p&gt;生成证书的 CSR（证书签名请求文件，配置了一些域名、公司、单位）文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

# 生成etcd CA证书和CA证书的key
cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca


cfssl gencert \
   -ca=/etc/etcd/ssl/etcd-ca.pem \
   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \
   -config=ca-config.json \
   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \
   -profile=kubernetes \
   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd

执行结果
[INFO] generate received request
 	[INFO] received CSR
     [INFO] generating key: rsa-2048
     [INFO] encoded CSR
     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将证书复制到其他 master 节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;

for NODE in $MasterNodes; do
     ssh $NODE &amp;quot;mkdir -p /etc/etcd/ssl&amp;quot;
     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do
       scp /etc/etcd/ssl/$&amp;#123;FILE&amp;#125; $NODE:/etc/etcd/ssl/$&amp;#123;FILE&amp;#125;
     done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;52-k8s组件证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-k8s组件证书&#34;&gt;#&lt;/a&gt; 5.2 K8s 组件证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 生成 kubernetes CA 证书：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;521-apiserver证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#521-apiserver证书&#34;&gt;#&lt;/a&gt; 5.2.1 APIServer 证书&lt;/h6&gt;
&lt;p&gt;注意：10.96.0. 是 k8s service 的网段，如果说需要更改 k8s service 网段，那就需要更改 10.96.0.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;生成 apiserver 的聚合证书：：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca 

cfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;返回结果（忽略警告）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2020/12/11 18:15:28 [INFO] generate received request
2020/12/11 18:15:28 [INFO] received CSR
2020/12/11 18:15:28 [INFO] generating key: rsa-2048

2020/12/11 18:15:28 [INFO] encoded CSR
2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845
2020/12/11 18:15:28 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;522-controllermanager&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#522-controllermanager&#34;&gt;#&lt;/a&gt; 5.2.2 ControllerManager&lt;/h6&gt;
&lt;p&gt;生成 controller-manage 的证书：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager

注意：修改黄色部分的IP地址
# set-cluster：设置一个集群项，

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# 设置一个环境项，一个上下文
kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# set-credentials 设置一个用户项

kubectl config set-credentials system:kube-controller-manager \
     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \
     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig


# 使用某个环境当做默认环境

kubectl config use-context system:kube-controller-manager@kubernetes \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;523-scheduler证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#523-scheduler证书&#34;&gt;#&lt;/a&gt; 5.2.3 Scheduler 证书&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler

注意：修改黄色部分的IP地址

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig


kubectl config set-credentials system:kube-scheduler \
     --client-certificate=/etc/kubernetes/pki/scheduler.pem \
     --client-key=/etc/kubernetes/pki/scheduler-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config set-context system:kube-scheduler@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-scheduler \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config use-context system:kube-scheduler@kubernetes \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;524-生成管理员证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#524-生成管理员证书&#34;&gt;#&lt;/a&gt; 5.2.4 生成管理员证书&lt;/h6&gt;
&lt;p&gt;Kubectl /etc/Kubernetes/admin.conf ~/.kube/config&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin

注意：修改黄色部分的IP

kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig
kubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;525-创建serviceaccount证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#525-创建serviceaccount证书&#34;&gt;#&lt;/a&gt; 5.2.5 创建 ServiceAccount 证书&lt;/h6&gt;
&lt;p&gt;创建一对公钥，用来签发 ServiceAccount 的 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl genrsa -out /etc/kubernetes/pki/sa.key 2048
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;返回结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Generating RSA private key, 2048 bit long modulus (2 primes)
...................................................................................+++++
...............+++++
e is 65537 (0x010001)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;发送证书至其他节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do 
  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do 
    scp /etc/kubernetes/pki/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/pki/$&amp;#123;FILE&amp;#125;;
  done; 
  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do 
    scp /etc/kubernetes/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;;
  done;
done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看证书文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# ls /etc/kubernetes/pki/
admin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem
admin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem
admin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr
[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l
23
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-kubernetes组件配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-kubernetes组件配置&#34;&gt;#&lt;/a&gt; 6. Kubernetes 组件配置&lt;/h4&gt;
&lt;h5 id=&#34;61-ecd配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#61-ecd配置&#34;&gt;#&lt;/a&gt; 6.1 Ecd 配置&lt;/h5&gt;
&lt;p&gt;Etcd 配置大致相同，注意修改每个 Master 节点的 etcd 配置的主机名和 IP 地址&lt;/p&gt;
&lt;h6 id=&#34;611-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#611-master01&#34;&gt;#&lt;/a&gt; 6.1.1 Master01&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master01&#39;     # k8s-master01名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.71:2380&#39;            # k8s-master01 IP
listen-client-urls: &#39;https://192.168.1.71:2379,http://127.0.0.1:2379&#39;   # k8s-master01 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.71:2380&#39;  # k8s-master01 IP
advertise-client-urls: &#39;https://192.168.1.71:2379&#39;        # k8s-master01 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;     # k8s-master01、k8s-master02、k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;612-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#612-master02&#34;&gt;#&lt;/a&gt; 6.1.2 Master02&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml	
name: &#39;k8s-master02&#39;   # k8s-master02名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.72:2380&#39;      # k8s-master02 IP
listen-client-urls: &#39;https://192.168.1.72:2379,http://127.0.0.1:2379&#39;    # k8s-master02 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.72:2380&#39;    # k8s-master02 IP
advertise-client-urls: &#39;https://192.168.1.72:2379&#39;     # k8s-master02 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;             # k8s-master01、k8s-master02、k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;613-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#613-master03&#34;&gt;#&lt;/a&gt; 6.1.3 Master03&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master03&#39;           # k8s-master03名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.73:2380&#39;           # k8s-master03 IP
listen-client-urls: &#39;https://192.168.1.73:2379,http://127.0.0.1:2379&#39;       # k8s-master03 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.73:2380&#39;      # k8s-master03 IP
advertise-client-urls: &#39;https://192.168.1.73:2379&#39;            # k8s-master03 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;                # k8s-master01、k8s-master02、k8s-master03 IP
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;614-启动etcd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#614-启动etcd&#34;&gt;#&lt;/a&gt; 6.1.4 启动 Etcd&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd service 并启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Service
Documentation=https://coreos.com/etcd/docs/latest/
After=network.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml
Restart=on-failure
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd3.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd 的证书目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/kubernetes/pki/etcd
ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/
systemctl daemon-reload
systemctl enable --now etcd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 etcd 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export ETCDCTL_API=3
etcdctl --endpoints=&amp;quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&amp;quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;62-apiserver配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#62-apiserver配置&#34;&gt;#&lt;/a&gt; 6.2 APIServer 配置&lt;/h5&gt;
&lt;h6 id=&#34;621-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#621-master01&#34;&gt;#&lt;/a&gt; 6.2.1 Master01&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.71 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;622-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#622-master02&#34;&gt;#&lt;/a&gt; 6.2.2 Master02&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.72 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;623-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#623-master03&#34;&gt;#&lt;/a&gt; 6.2.3 Master03&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.73 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;624-启动apiserver&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#624-启动apiserver&#34;&gt;#&lt;/a&gt; 6.2.4 启动 apiserver&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;开启 kube-apiserver：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload &amp;amp;&amp;amp; systemctl enable --now kube-apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;检测 kube-server 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl status kube-apiserver

● kube-apiserver.service – Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s ago 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果系统日志有这些提示可以忽略:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to “pick_first”
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;CONNECTING &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;READY &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = “transport is closing”
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;63-controllermanage&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#63-controllermanage&#34;&gt;#&lt;/a&gt; 6.3 ControllerManage&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 kube-controller-manager service（所有 master 节点配置一样）&lt;/p&gt;
&lt;p&gt;注意：本文档使用的 k8s Pod 网段为 172.16.0.0/16，该网段不能和宿主机的网段、k8s Service 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
      --v=2 \
      --root-ca-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \
      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --leader-elect=true \
      --use-service-account-credentials=true \
      --node-monitor-grace-period=40s \
      --node-monitor-period=5s \
      --controllers=*,bootstrapsigner,tokencleaner \
      --allocate-node-cidrs=true \
      --cluster-cidr=172.16.0.0/16 \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
      --node-cidr-mask-size=24
      
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;启动 kube-controller-manager&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager
Created symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service → /usr/lib/systemd/system/kube-controller-manager.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看启动状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl  status kube-controller-manager
● kube-controller-manager.service – Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)
 Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago
     Docs: https://github.com/  ubernetes/  ubernetes
 Main PID: 7518 (kube-controller)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;64-scheduler&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#64-scheduler&#34;&gt;#&lt;/a&gt; 6.4 Scheduler&lt;/h5&gt;
&lt;p&gt;所有 Master 节点配置 kube-scheduler service（所有 master 节点配置一样）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service 
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
      --v=2 \
      --leader-elect=true \
      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动 scheduler：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-scheduler
Created symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service → /usr/lib/systemd/system/kube-scheduler.service.
[root@k8s-master01 pki]# systemctl status kube-scheduler
● kube-scheduler.service - Kubernetes Scheduler
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 5815 (kube-scheduler)
    Tasks: 9
   Memory: 19.8M
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-tls-bootstrapping配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-tls-bootstrapping配置&#34;&gt;#&lt;/a&gt; 7. TLS Bootstrapping 配置&lt;/h4&gt;
&lt;p&gt;只需要在&lt;mark&gt; Master01&lt;/mark&gt; 创建 bootstrap&lt;/p&gt;
&lt;p&gt;注意： 修改黄色部分的 IP 地址&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/bootstrap
kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig

[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以正常查询集群状态，才可以继续往下，否则不行，需要排查 k8s 组件是否有故障（只要有结果即可，如果返回不一样不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok        
scheduler            Healthy   ok        
etcd-0               Healthy   ok
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建 bootstrap 相关资源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml 
secret/bootstrap-token-c8ad9c created
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-node节点配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-node节点配置&#34;&gt;#&lt;/a&gt; 8. Node 节点配置&lt;/h4&gt;
&lt;h5 id=&#34;81-复制证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#81-复制证书&#34;&gt;#&lt;/a&gt; 8.1 复制证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;复制证书至其他节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/kubernetes/

for NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do
     ssh $NODE mkdir -p /etc/kubernetes/pki
     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do
       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;
 done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    
…
bootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;82-kubelet配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#82-kubelet配置&#34;&gt;#&lt;/a&gt; 8.2 Kubelet 配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 Kubelet 配置目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 kubelet service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service

[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kubelet

Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 kubelet service 的配置文件（也可以写到 kubelet.service）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Runtime为Containerd
# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf

[Service]
Environment=&amp;quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&amp;quot;
Environment=&amp;quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&amp;quot;
Environment=&amp;quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&amp;quot;
Environment=&amp;quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=&#39;&#39; &amp;quot;
ExecStart=
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 kubelet 的配置文件&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：如果更改了 k8s 的 service 网段，需要更改 kubelet-conf.yml 的 clusterDNS: 配置，改成 k8s Service 网段的第十个地址，比如 10.96.0.10&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动&lt;mark&gt;所有节点&lt;/mark&gt; kubelet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;如果有很多报错日志，或者有大量看不懂的报错，说明 kubelet 的配置有误，需要检查 kubelet 配置&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Master01 查看集群状态 (Ready 或 NotReady 都正常)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl get node
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;83-kube-proxy配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#83-kube-proxy配置&#34;&gt;#&lt;/a&gt; 8.3 kube-proxy 配置&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;注意，如果不是高可用集群，192.168.1.70:8443 改为 master01 的地址，8443 改为 apiserver 的端口，默认是 6443&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;生成 kube-proxy 的证书，以下操作只在&lt;mark&gt; Master01&lt;/mark&gt; 执行&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/pki
cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config set-credentials system:kube-proxy \
     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \
     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

kubectl config set-context system:kube-proxy@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-proxy \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config use-context system:kube-proxy@kubernetes \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 kubeconfig 发送至其他节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do
     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done

for NODE in k8s-node01 k8s-node02; do
     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;添加 kube-proxy 的配置和 service 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /usr/lib/systemd/system/kube-proxy.service

[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --v=2

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果更改了集群 Pod 的网段，需要更改 kube-proxy.yaml 的 clusterCIDR 为自己的 Pod 网段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/kubernetes/kube-proxy.yaml

apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  acceptContentTypes: &amp;quot;&amp;quot;
  burst: 10
  contentType: application/vnd.kubernetes.protobuf
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
  qps: 5
clusterCIDR: 172.16.0.0/16 
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
hostnameOverride: &amp;quot;&amp;quot;
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
ipvs:
  masqueradeAll: true
  minSyncPeriod: 5s
  scheduler: &amp;quot;rr&amp;quot;
  syncPeriod: 30s
kind: KubeProxyConfiguration
metricsBindAddress: 127.0.0.1:10249
mode: &amp;quot;ipvs&amp;quot;
nodePortAddresses: null
oomScoreAdj: -999
portRange: &amp;quot;&amp;quot;
udpIdleTimeout: 250ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload
[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /usr/lib/systemd/system/kube-proxy.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;9-calico组件的安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-calico组件的安装&#34;&gt;#&lt;/a&gt; 9. Calico 组件的安装&lt;/h4&gt;
&lt;p&gt;以下步骤只在 master01 执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更改 calico 的网段，主要需要将红色部分的网段，改为自己的 Pod 网段&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &amp;quot;s#POD_CIDR#172.16.0.0/16#g&amp;quot; calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;检查网段是自己的 Pod 网段， grep &amp;quot;IPV4POOL_CIDR&amp;quot; calico.yaml  -A 1&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看容器和节点状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 calico]# kubectl get po -n kube-system
NAME                                       READY   STATUS    RESTARTS      AGE
calico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s
calico-node-8fxqp                          1/1     Running   0             85s
calico-node-8nkfl                          1/1     Running   0             86s
calico-node-pmpf4                          1/1     Running   0             86s
calico-node-vnlk7                          1/1     Running   0             86s
calico-node-xpchb                          1/1     Running   0             85s
calico-typha-67c6dc57d6-259t8              1/1     Running   0             86s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;如果容器状态异常可以使用 kubectl describe 或者 kubectl logs 查看容器的日志&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -n kube-system&lt;/li&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;10-安装coredns&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-安装coredns&#34;&gt;#&lt;/a&gt; 10. 安装 CoreDNS&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果更改了 k8s service 的网段需要将 coredns 的 serviceIP 改成 k8s service 网段的第十个 IP&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk &#39;&amp;#123;print $3&amp;#125;&#39;`0
sed -i &amp;quot;s#KUBEDNS_SERVICE_IP#$&amp;#123;COREDNS_SERVICE_IP&amp;#125;#g&amp;quot; CoreDNS/coredns.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装 coredns&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11-metrics部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-metrics部署&#34;&gt;#&lt;/a&gt; 11. Metrics 部署&lt;/h4&gt;
&lt;p&gt;在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。&lt;/p&gt;
&lt;p&gt;以下操作均在&lt;mark&gt; master01 节点&lt;/mark&gt;执行，安装 metrics server:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/metrics-server
kubectl  create -f . 

serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;等待 metrics server 启动然后查看状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl  top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master01   231m         5%     1620Mi          42%       
k8s-master02   274m         6%     1203Mi          31%       
k8s-master03   202m         5%     1251Mi          32%       
k8s-node01     69m          1%     667Mi           17%       
k8s-node02     73m          1%     650Mi           16%
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果有如下报错，可以等待 10 分钟后，再次查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-dashboard部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-dashboard部署&#34;&gt;#&lt;/a&gt; 12. Dashboard 部署&lt;/h4&gt;
&lt;h5 id=&#34;121-安装dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#121-安装dashboard&#34;&gt;#&lt;/a&gt; 12.1 安装 Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;122-登录dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#122-登录dashboard&#34;&gt;#&lt;/a&gt; 12.2 登录 dashboard&lt;/h5&gt;
&lt;p&gt;在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更改 dashboard 的 svc 为 NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看端口号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：&lt;/p&gt;
&lt;p&gt;访问 Dashboard：&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建登录 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;14-containerd配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-containerd配置镜像加速&#34;&gt;#&lt;/a&gt; 14. Containerd 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Containerd：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;15-docker配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-docker配置镜像加速&#34;&gt;#&lt;/a&gt; 15. Docker 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-10T12:58:40.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/2771271649.html</id>
        <title>云原生K8s安全专家CKS认证考题详解</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/2771271649.html"/>
        <content type="html">&lt;div class=&#34;hbe hbe-container&#34; id=&#34;hexo-blog-encrypt&#34; data-wpm=&#34;抱歉, 这个密码看着不太对, 请再试试。&#34; data-whm=&#34;抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。&#34;&gt;
  &lt;script id=&#34;hbeData&#34; type=&#34;hbeData&#34; data-hmacdigest=&#34;51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d&#34;&gt;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&lt;/script&gt;
  &lt;div class=&#34;hbe hbe-content&#34;&gt;
    &lt;div class=&#34;hbe hbe-input hbe-input-xray&#34;&gt;
      &lt;input class=&#34;hbe hbe-input-field hbe-input-field-xray&#34; type=&#34;password&#34; id=&#34;hbePass&#34;&gt;
      &lt;label class=&#34;hbe hbe-input-label hbe-input-label-xray&#34; for=&#34;hbePass&#34;&gt;
        &lt;span class=&#34;hbe hbe-input-label-content hbe-input-label-content-xray&#34;&gt;您好, 这里需要输入密码。&lt;/span&gt;
      &lt;/label&gt;
      &lt;svg class=&#34;hbe hbe-graphic hbe-graphic-xray&#34; width=&#34;300%&#34; height=&#34;100%&#34; viewBox=&#34;0 0 1200 60&#34; preserveAspectRatio=&#34;none&#34;&gt;
        &lt;path d=&#34;M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
        &lt;path d=&#34;M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
      &lt;/svg&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;script data-pjax src=&#34;/lib/hbe.js&#34;&gt;&lt;/script&gt;&lt;link href=&#34;/css/hbe.style.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt;</content>
        <category term="Kubernetes" />
        <updated>2025-04-09T13:38:39.000Z</updated>
    </entry>
    <entry>
        <id>http://ixuyong.cn/posts/3166738000.html</id>
        <title>Kubeadm高可用安装K8s集群</title>
        <link rel="alternate" href="http://ixuyong.cn/posts/3166738000.html"/>
        <content type="html">&lt;h2 id=&#34;kubeadm高可用安装k8s集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#kubeadm高可用安装k8s集群&#34;&gt;#&lt;/a&gt; Kubeadm 高可用安装 K8s 集群&lt;/h2&gt;
&lt;h4 id=&#34;1-基本配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-基本配置&#34;&gt;#&lt;/a&gt; 1. 基本配置&lt;/h4&gt;
&lt;h5 id=&#34;11-基本环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-基本环境配置&#34;&gt;#&lt;/a&gt; 1.1 基本环境配置&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP 地址&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master 节点 * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived 虚拟 IP（不占用机器）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker 节点 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* 配置信息 *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;系统版本&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 网段&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service 网段&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改主机名（其它节点按需修改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 hosts，修改 /etc/hosts 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 yum 源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 配置基础源
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;必备工具安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭 swap 分区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ntpdate：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;同步时间并配置上海时区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 limit：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;升级系统：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;下载安装所有的源码文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-内核配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-内核配置&#34;&gt;#&lt;/a&gt; 1.2 内核配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ipvsadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 ipvs 模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 ipvs.conf，并配置开机自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# 加入以下内容
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;内核优化配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;应用配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置完内核后，重启机器，之后查看内核模块是否已自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-高可用组件安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-高可用组件安装&#34;&gt;#&lt;/a&gt; 2. 高可用组件安装&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;通过 yum 安装 HAProxy 和 KeepAlived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 HAProxy，需要注意黄色部分的 IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443       #HAProxy监听端口
  bind 127.0.0.1:16443     #HAProxy监听端口
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IP地址
  server k8s-master02	192.168.1.72:6443  check       #API Server IP地址
  server k8s-master03	192.168.1.73:6443  check       #API Server IP地址
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 KeepAlived，需要注意黄色部分的配置。&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #网卡名称
    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #网卡名称
    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #网卡名称
    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置 KeepAlived 健康检查文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置健康检查文件添加执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;启动 haproxy 和 keepalived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;所有节点测试VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld&lt;/li&gt;
&lt;li&gt;所有节点查看 selinux 状态，必须为 disable：getenforce&lt;/li&gt;
&lt;li&gt;master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master 节点查看监听端口：netstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果以上都没有问题，需要确认：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;是否是公有云机器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否是私有云机器（类似 OpenStack）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询&lt;/p&gt;
&lt;h4 id=&#34;3-runtime安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtime安装&#34;&gt;#&lt;/a&gt; 3. Runtime 安装&lt;/h4&gt;
&lt;p&gt;如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。&lt;/p&gt;
&lt;h5 id=&#34;31-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-安装containerd&#34;&gt;#&lt;/a&gt; 3.1 安装 Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置安装源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先配置 Containerd 所需的模块（&lt;mark&gt;所有节点&lt;/mark&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;，配置 Containerd 所需的内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;生成 Containerd 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改 Containerd 的 Cgroup 和 Pause 镜像配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 Containerd，并配置开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 crictl 客户端连接的运行时位置（可选）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-安装kubernetes组件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-安装kubernetes组件&#34;&gt;#&lt;/a&gt; 4 . 安装 Kubernetes 组件&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置源（注意更改版本号）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先在&lt;mark&gt; Master01 节点&lt;/mark&gt;查看最新的 Kubernetes 版本是多少：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum list kubeadm.x86_64 --showduplicates | sort -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;此时 kubelet 是起不来的，日志会有报错不影响！&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-集群初始化&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-集群初始化&#34;&gt;#&lt;/a&gt; 5 . 集群初始化&lt;/h4&gt;
&lt;p&gt;以下操作在&lt;mark&gt; master01&lt;/mark&gt;（注意黄色部分）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.71
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  name: k8s-master01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  certSANs:
  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443
controllerManager: &amp;#123;&amp;#125;
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致
networking:
  dnsDomain: cluster.local
  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突
  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突
scheduler: &amp;#123;&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;master01 节点&lt;/mark&gt;更新 kubeadm 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 new.yaml 文件复制到&lt;mark&gt;其他 master 节点&lt;/mark&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后&lt;mark&gt;所有 Master 节点&lt;/mark&gt;提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config images pull --config /root/new.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正确的反馈信息如下（&lt;em&gt;&lt;strong&gt;* 版本可能不一样 *&lt;/strong&gt;&lt;/em&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml 
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init --config /root/new.yaml  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

# 不要复制文档当中的，要去使用节点生成的
  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&amp;quot;kubeadm init phase upload-certs --upload-certs&amp;quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;配置环境变量，用于访问 Kubernetes 集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF
source /root/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;查看节点状态：（显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE   VERSION
k8s-master01   NotReady   control-plane   24s   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;# kubectl get pods -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-初始化失败排查&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-初始化失败排查&#34;&gt;#&lt;/a&gt; 5.1 初始化失败排查&lt;/h5&gt;
&lt;p&gt;如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/messages | grep -v &amp;quot;not found&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经常出错的原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突&lt;/li&gt;
&lt;li&gt;VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;52-高可用master&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-高可用master&#34;&gt;#&lt;/a&gt; 5.2 高可用 Master&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;其他 master&lt;/strong&gt; 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看当前状态：（如果显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;53-token过期处理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#53-token过期处理&#34;&gt;#&lt;/a&gt; 5.3 Token 过期处理&lt;/h5&gt;
&lt;p&gt;注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。&lt;/p&gt;
&lt;p&gt;Token 过期后生成新的 token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm token create --print-join-command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Master 需要生成 --certificate-key：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase upload-certs  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-node节点的配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-node节点的配置&#34;&gt;#&lt;/a&gt; 6. Node 节点的配置&lt;/h4&gt;
&lt;p&gt;Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点初始化完成后，查看集群状态（NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
k8s-node01     NotReady   &amp;lt;none&amp;gt;          13s     v1.32.3
k8s-node02     NotReady   &amp;lt;none&amp;gt;          10s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-calico组件的安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-calico组件的安装&#34;&gt;#&lt;/a&gt; 7. Calico 组件的安装&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt;/etc/NetworkManager/conf.d/calico.conf&amp;lt;&amp;lt;EOF
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
EOF
systemctl daemon-reload
systemctl restart NetworkManager
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下步骤只在&lt;mark&gt; master01&lt;/mark&gt; 执行（.x 不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x &amp;amp;&amp;amp; cd calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改 Pod 网段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= &#39;&amp;#123;print $NF&amp;#125;&#39;`

sed -i &amp;quot;s#POD_CIDR#$&amp;#123;POD_SUBNET&amp;#125;#g&amp;quot; calico.yaml
kubectl apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看容器和节点状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h
calico-node-7mzmb                          1/1     Running   0          24h
calico-node-ljqnl                          1/1     Running   0          24h
calico-node-njqlb                          1/1     Running   0          24h
calico-node-ph4m4                          1/1     Running   0          24h
calico-node-rx8rl                          1/1     Running   0          24h
coredns-76fccbbb6b-76559                   1/1     Running   0          24h
coredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h
etcd-k8s-master01                          1/1     Running   0          24h
etcd-k8s-master02                          1/1     Running   0          24h
etcd-k8s-master03                          1/1     Running   0          24h
kube-apiserver-k8s-master01                1/1     Running   0          24h
kube-apiserver-k8s-master02                1/1     Running   0          24h
kube-apiserver-k8s-master03                1/1     Running   0          24h
kube-controller-manager-k8s-master01       1/1     Running   0          24h
kube-controller-manager-k8s-master02       1/1     Running   0          24h
kube-controller-manager-k8s-master03       1/1     Running   0          24h
kube-proxy-9dtz4                           1/1     Running   0          24h
kube-proxy-jh7rl                           1/1     Running   0          24h
kube-proxy-jvvwt                           1/1     Running   0          24h
kube-proxy-sh89l                           1/1     Running   0          24h
kube-proxy-t2j49                           1/1     Running   0          24h
kube-scheduler-k8s-master01                1/1     Running   0          24h
kube-scheduler-k8s-master02                1/1     Running   0          24h
kube-scheduler-k8s-master03                1/1     Running   0          24h
metrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时节点全部变为 Ready 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   24h   v1.32.3
k8s-master02   Ready    control-plane   24h   v1.32.3
k8s-master03   Ready    control-plane   24h   v1.32.3
k8s-node01     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
k8s-node02     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-metrics部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-metrics部署&#34;&gt;#&lt;/a&gt; 8. Metrics 部署&lt;/h4&gt;
&lt;p&gt;在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。&lt;/p&gt;
&lt;p&gt;将&lt;mark&gt; Master01 节点&lt;/mark&gt;的 front-proxy-ca.crt 复制到所有 Node 节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt

scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下操作均在&lt;mark&gt; master01 节点&lt;/mark&gt;执行:&lt;/p&gt;
&lt;p&gt;安装 metrics server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/kubeadm-metrics-server

# kubectl  create -f comp.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl top node
NAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
k8s-master01   132m         3%       932Mi           5%          
k8s-master02   131m         3%       845Mi           5%          
k8s-master03   148m         3%       912Mi           5%          
k8s-node01     54m          1%       600Mi           3%          
k8s-node02     49m          1%       602Mi           3%          
[root@k8s-master01 ~]#  kubectl top po -A
NAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   
ingress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            
ingress-nginx          ingress-nginx-controller-r978m               1m           104Mi           
krm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            
krm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             
krm                    nginx-574cf48959-vcfjs                       0m           2Mi             
kube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            
kube-system            calico-node-7mzmb                            16m          176Mi           
kube-system            calico-node-ljqnl                            15m          182Mi           
kube-system            calico-node-njqlb                            19m          180Mi           
kube-system            calico-node-ph4m4                            15m          178Mi           
kube-system            calico-node-rx8rl                            17m          180Mi           
kube-system            coredns-76fccbbb6b-76559                     2m           16Mi            
kube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            
kube-system            etcd-k8s-master01                            22m          86Mi            
kube-system            etcd-k8s-master02                            27m          84Mi            
kube-system            etcd-k8s-master03                            22m          84Mi            
kube-system            kube-apiserver-k8s-master01                  22m          267Mi           
kube-system            kube-apiserver-k8s-master02                  20m          242Mi           
kube-system            kube-apiserver-k8s-master03                  18m          241Mi           
kube-system            kube-controller-manager-k8s-master01         6m           69Mi            
kube-system            kube-controller-manager-k8s-master02         2m           21Mi            
kube-system            kube-controller-manager-k8s-master03         1m           19Mi            
kube-system            kube-proxy-9dtz4                             11m          30Mi            
kube-system            kube-proxy-jh7rl                             1m           27Mi            
kube-system            kube-proxy-jvvwt                             17m          29Mi            
kube-system            kube-proxy-sh89l                             1m           29Mi            
kube-system            kube-proxy-t2j49                             16m          29Mi            
kube-system            kube-scheduler-k8s-master01                  6m           25Mi            
kube-system            kube-scheduler-k8s-master02                  6m           25Mi            
kube-system            kube-scheduler-k8s-master03                  6m           25Mi            
kube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            
kubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            
kubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9-dashboard部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-dashboard部署&#34;&gt;#&lt;/a&gt; 9. Dashboard 部署&lt;/h4&gt;
&lt;h5 id=&#34;91-安装dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#91-安装dashboard&#34;&gt;#&lt;/a&gt; 9.1 安装 Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;92-登录dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#92-登录dashboard&#34;&gt;#&lt;/a&gt; 9.2 登录 dashboard&lt;/h5&gt;
&lt;p&gt;在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更改 dashboard 的 svc 为 NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看端口号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：&lt;/p&gt;
&lt;p&gt;访问 Dashboard：&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建登录 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;10必看一些必须的配置更改&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10必看一些必须的配置更改&#34;&gt;#&lt;/a&gt; 10.【必看】一些必须的配置更改&lt;/h4&gt;
&lt;p&gt;将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：&lt;/p&gt;
&lt;p&gt;在 master01 节点执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
mode: ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新 Kube-Proxy 的 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;验证 Kube-Proxy 模式:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode
ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11必看注意事项&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11必看注意事项&#34;&gt;#&lt;/a&gt; 11.【必看】注意事项&lt;/h4&gt;
&lt;p&gt;注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。&lt;/p&gt;
&lt;p&gt;启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。&lt;/p&gt;
&lt;p&gt;其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。&lt;/p&gt;
&lt;p&gt;kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行更改，更改完成后，可以通过 patch 重启 kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-containerd配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-containerd配置镜像加速&#34;&gt;#&lt;/a&gt; 12. Containerd 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Containerd：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;13-docker配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-docker配置镜像加速&#34;&gt;#&lt;/a&gt; 13. Docker 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-09T10:28:34.000Z</updated>
    </entry>
</feed>
