<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://imxuyong.cn</id>
    <title>LinuxSre云原生</title>
    <link href="http://imxuyong.cn" />
    <updated>2025-04-19T13:00:21.000Z</updated>
    <category term="Harbor" />
    <category term="Kubernetes" />
    <category term="rsync" />
    <category term="Redis" />
    <category term="Windows" />
    <category term="MySQL" />
    <entry>
        <id>http://imxuyong.cn/posts/3833778957.html</id>
        <title>K8s计划任务Job、Cronjob</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/3833778957.html"/>
        <content type="html">&lt;h3 id=&#34;k8s计划任务job-cronjob&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s计划任务job-cronjob&#34;&gt;#&lt;/a&gt; K8s 计划任务 Job、Cronjob&lt;/h3&gt;
&lt;h4 id=&#34;1-job配置参数详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-job配置参数详解&#34;&gt;#&lt;/a&gt; 1. Job 配置参数详解&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    job-name: echo
  name: echo
  namespace: default
spec:
  #suspend: true # 1.21+
  #ttlSecondsAfterFinished: 100
  backoffLimit: 4
  completions: 1
  parallelism: 1
  template:
    spec:
      containers:
      - name: echo
        image: busybox
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - echo &amp;quot;Hello Job&amp;quot;
      restartPolicy: Never
      
[root@k8s-master01 ~]# kubectl get jobs
NAME   STATUS     COMPLETIONS   DURATION   AGE
echo   Complete   1/1           70s        2m5s

[root@k8s-master01 ~]# kubectl get pods
NAME          READY   STATUS      RESTARTS      AGE
echo-564c8    0/1     Completed   0             2m10s

[root@k8s-master01 ~]# kubectl logs echo-564c8
Hello Job
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;backoffLimit:：如果任务执行失败，失败多少次后不再执行&lt;/li&gt;
&lt;li&gt;completions：有多少个 Pod 执行成功，认为任务是成功的，默认为空和 parallelism 数值一样&lt;/li&gt;
&lt;li&gt;parallelism：并行执行任务的数量，如果 parallelism 数值大于 completions 数值，只会创建 completions 的数量；如果 completions 是 4，并发是 3，第一次会创建 3 个 Pod 执行任务，第二次只会创建一个 Pod 执行任务&lt;/li&gt;
&lt;li&gt;ttlSecondsAfterFinished：Job 在执行结束之后（状态为 completed 或 Failed）自动清理。设置为 0 表示执行结束立即删除，不设置则不会清除，需要开启 TTLAfterFinished 特性&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-cronjob配置参数详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-cronjob配置参数详解&#34;&gt;#&lt;/a&gt; 2. CronJob 配置参数详解&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# cat cronjob.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &amp;quot;*/1 * * * *&amp;quot;
  concurrencyPolicy: Allow   #允许同时运行多个任务
  failedJobsHistoryLimit: 10  #保留多少失败的任务
  successfulJobsHistoryLimit: 10  #保留多少已完成的任务
  #suspend: true             #如果true则取消周期性执行任务
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command:
            - sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure 
          
[root@k8s-master01 ~]# kubectl get  cj
NAME    SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   &amp;lt;none&amp;gt;     False     0        6s              81s

[root@k8s-master01 ~]# kubectl get  jobs
NAME             STATUS     COMPLETIONS   DURATION   AGE
hello-29084454   Complete   1/1           4s         72s
hello-29084455   Complete   1/1           5s         12s

[root@k8s-master01 ~]# kubectl get  pods
NAME                   READY   STATUS      RESTARTS   AGE
hello-29084454-hwv7p   0/1     Completed   0          78s
hello-29084455-vf99w   0/1     Completed   0          18s

[root@k8s-master01 ~]# kubectl logs -f hello-29084455-vf99w
Sat Apr 19 12:55:02 UTC 2025
Hello from the Kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;apiVersion: batch/v1beta1   #1.21+ batch/v1&lt;/li&gt;
&lt;li&gt;schedule：调度周期，和 Linux 一致，分别是分时日月周。&lt;/li&gt;
&lt;li&gt;restartPolicy：重启策略，和 Pod 一致。&lt;/li&gt;
&lt;li&gt;concurrencyPolicy：并发调度策略。可选参数如下：
&lt;ul&gt;
&lt;li&gt;Allow：允许同时运行多个任务。&lt;/li&gt;
&lt;li&gt;Forbid：不允许并发运行，如果之前的任务尚未完成，新的任务不会被创建。&lt;/li&gt;
&lt;li&gt;Replace：如果之前的任务尚未完成，新的任务会替换的之前的任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;suspend：如果设置为 true，则暂停后续的任务，默认为 false。&lt;/li&gt;
&lt;li&gt;successfulJobsHistoryLimit：保留多少已完成的任务，按需配置。&lt;/li&gt;
&lt;li&gt;failedJobsHistoryLimit：保留多少失败的任务。&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-19T13:00:21.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/169153047.html</id>
        <title>K8s持久化存储</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/169153047.html"/>
        <content type="html">&lt;h3 id=&#34;k8s持久化存储&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s持久化存储&#34;&gt;#&lt;/a&gt; K8s 持久化存储&lt;/h3&gt;
&lt;h4 id=&#34;1-volume&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-volume&#34;&gt;#&lt;/a&gt; 1. Volume&lt;/h4&gt;
&lt;p&gt;Container（容器）中的磁盘文件是短暂的，当容器崩溃时，kubelet 会重新启动容器，Container 会以最干净的状态启动，最初的文件将丢失。另外，当一个 Pod 运行多个 Container 时，各个容器可能需要共享一些文件。Kubernetes Volume 可以解决这两个问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一些需要持久化数据的程序才会用到 Volumes，或者一些需要共享数据的容器需要 volumes。&lt;/li&gt;
&lt;li&gt;日志收集的需求需要在应用程序的容器里面加一个 sidecar，这个容器是一个收集日志的容器，比如 filebeat，它通过 volumes 共享应用程序的日志文件目录。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-emptydir实现数据共享&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-emptydir实现数据共享&#34;&gt;#&lt;/a&gt; 1.1 EmptyDir 实现数据共享&lt;/h5&gt;
&lt;p&gt;和上述 volume 不同的是，如果删除 Pod，emptyDir 卷中的数据也将被删除，一般 emptyDir 卷用于 Pod 中的不同 Container 共享数据。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
        - name: share-volume
          emptyDir: &amp;#123;&amp;#125;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
            - name: share-volume
              mountPath: /opt
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
            - name: share-volume
              mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-volumes-hostpath挂载宿主机路径&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-volumes-hostpath挂载宿主机路径&#34;&gt;#&lt;/a&gt; 1.2 Volumes HostPath 挂载宿主机路径&lt;/h5&gt;
&lt;p&gt;hostPath 卷可将节点上的文件或目录挂载到 Pod 上，用于 Pod 自定义日志输出或访问 Docker 内部的容器等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: share-volume
        emptyDir: &amp;#123;&amp;#125;
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: share-volume
            mountPath: /opt
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
        - name: nginx2
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          command:
            - sh
            - &#39;-c&#39;
            - sleep 3600
          volumeMounts:
          - name: share-volume
            mountPath: /mnt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPath 卷常用的 type（类型）如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;type 为空字符串：默认选项，意味着挂载 hostPath 卷之前不会执行任何检查。&lt;/li&gt;
&lt;li&gt;DirectoryOrCreate：如果给定的 path 不存在任何东西，那么将根据需要创建一个权限为 0755 的空目录，和 Kubelet 具有相同的组和权限。&lt;/li&gt;
&lt;li&gt;Directory：目录必须存在于给定的路径下。&lt;/li&gt;
&lt;li&gt;FileOrCreate：如果给定的路径不存储任何内容，则会根据需要创建一个空文件，权限设置为 0644，和 Kubelet 具有相同的组和所有权。&lt;/li&gt;
&lt;li&gt;File：文件，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;Socket：UNIX 套接字，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;CharDevice：字符设备，必须存在于给定路径中。&lt;/li&gt;
&lt;li&gt;BlockDevice：块设备，必须存在于给定路径中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;13-挂载nfs至容器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-挂载nfs至容器&#34;&gt;#&lt;/a&gt; 1.3 挂载 NFS 至容器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.安装nfs
# yum install nfs-utils -y       
# mkdir /data/nfs -p
# vim /etc/exports 
/data 192.168.1.0/24(rw,no_root_squash)
# exportfs -arv   
# systemctl start nfs-server &amp;amp;&amp;amp; systemctl enable nfs-server &amp;amp;&amp;amp; systemctl status nfs-server 

#2.测试客户端挂载
# showmount -e 192.168.1.75
# mount -t nfs 192.168.1.75:/data/nfs /mnt

#3.Deploy挂载NFS
[root@k8s-master01 ~]# cat nginx-deploy-nfs.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-deploy
      annotations:
        app: nginx-deploy
    spec:
      restartPolicy: Always
      volumes:
      - name: nfs-volume
        nfs:
          server: 192.168.1.75
          path: /data/nfs
      - name: tz-config
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
          type: &amp;quot;&amp;quot;
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: &amp;quot;&amp;quot;
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
          volumeMounts:
          - name: nfs-volume
            mountPath: /usr/share/nginx/html
          - name: tz-config
            mountPath: /usr/share/zoneinfo/Asia/Shanghai
          - name: tz-config
            mountPath: /etc/localtime
          - name: timezone
            mountPath: /etc/timezone
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-pv-pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-pv-pvc&#34;&gt;#&lt;/a&gt; 2. PV、PVC&lt;/h4&gt;
&lt;p&gt;PersistentVolume：简称 PV，是由 Kubernetes 管理员设置的存储，可以配置 Ceph、NFS、GlusterFS 等常用存储配置，相对于 Volume 配置，提供了更多的功能，比如生命周期的管理、大小的限制。PV 分为静态和动态。&lt;/p&gt;
&lt;p&gt;PersistentVolumeClaim：简称 PVC，是对存储 PV 的请求，表示需要什么类型的 PV，需要存储的技术人员只需要配置 PVC 即可使用存储，或者 Volume 配置 PVC 的名称即可。&lt;/p&gt;
&lt;h5 id=&#34;21-pv回收策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-pv回收策略&#34;&gt;#&lt;/a&gt; 2.1 PV 回收策略&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Retain：保留，该策略允许手动回收资源，当删除 PVC 时，PV 仍然存在，PV 被视为已释放，管理员可以手动回收卷。&lt;/li&gt;
&lt;li&gt;Recycle：回收，如果 Volume 插件支持，Recycle 策略会对卷执行 rm -rf 清理该 PV，并使其可用于下一个新的 PVC，但是本策略将来会被弃用，目前只有 NFS 和 HostPath 支持该策略。&lt;/li&gt;
&lt;li&gt;Delete：删除，如果 Volume 插件支持，删除 PVC 时会同时删除 PV，动态卷默认为 Delete，目前支持 Delete 的存储后端包括 AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder 等。&lt;/li&gt;
&lt;li&gt;可以通过 persistentVolumeReclaimPolicy: Recycle 字段配置&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;22-pv访问策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-pv访问策略&#34;&gt;#&lt;/a&gt; 2.2 PV 访问策略&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce：可以被单节点以读写模式挂载，命令行中可以被缩写为 RWO。&lt;/li&gt;
&lt;li&gt;ReadOnlyMany：可以被多个节点以只读模式挂载，命令行中可以被缩写为 ROX。&lt;/li&gt;
&lt;li&gt;ReadWriteMany：可以被多个节点以读写模式挂载，命令行中可以被缩写为 RWX。&lt;/li&gt;
&lt;li&gt;ReadWriteOncePod ：只允许被单个 Pod 访问，需要 K8s 1.22 + 以上版本，并且是 CSI 创建的 PV 才可使用，缩写为 RWOP&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOnce&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadOnlyMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOncePod&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;depends on the driver&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HostPath&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;- (works when Pods are collocated)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;23-存储分类&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-存储分类&#34;&gt;#&lt;/a&gt; 2.3 存储分类&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;文件存储：一些数据可能需要被多个节点使用，比如用户的头像、用户上传的文件等，实现方式：NFS、NAS、FTP、CephFS 等。&lt;/li&gt;
&lt;li&gt;块存储：一些数据只能被一个节点使用，或者是需要将一块裸盘整个挂载使用，比如数据库、Redis 等，实现方式：Ceph、GlusterFS、公有云。&lt;/li&gt;
&lt;li&gt;对象存储：由程序代码直接实现的一种存储方式，云原生应用无状态化常用的实现方式，实现方式：一般是符合 S3 协议的云存储，比如 AWS 的 S3 存储、Minio、七牛云等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;24-pv配置示例nfs&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-pv配置示例nfs&#34;&gt;#&lt;/a&gt; 2.4 PV 配置示例 NFS&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv1
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-slow
  nfs:
    path: /data/pv1
    server: 192.168.1.75
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;capacity：容量配置&lt;/p&gt;
&lt;p&gt;volumeMode：卷的模式，目前支持 Filesystem（文件系统） 和 Block（块），其中 Block 类型需要后端存储支持，默认为文件系统&lt;/p&gt;
&lt;p&gt;accessModes：该 PV 的访问模式&lt;/p&gt;
&lt;p&gt;storageClassName：PV 的类，一个特定类型的 PV 只能绑定到特定类别的 PVC；&lt;/p&gt;
&lt;p&gt;persistentVolumeReclaimPolicy：回收策略&lt;/p&gt;
&lt;p&gt;mountOptions：非必须，新版本中已弃用&lt;/p&gt;
&lt;p&gt;nfs：NFS 服务配置，包括以下两个选项&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;path：NFS 上的共享目录&lt;/li&gt;
&lt;li&gt;server：NFS 的 IP 地址&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;25-pv配置示例hostpath&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-pv配置示例hostpath&#34;&gt;#&lt;/a&gt; 2.5 PV 配置示例 HostPath&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: hostpath
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: hostpath
  hostPath:
    path: &amp;quot;/mnt/data&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hostPath：hostPath 服务配置&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;path：宿主机路径&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;26-pv的状态&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#26-pv的状态&#34;&gt;#&lt;/a&gt; 2.6 PV 的状态&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Available：可用，没有被 PVC 绑定的空闲资源。&lt;/li&gt;
&lt;li&gt;Bound：已绑定，已经被 PVC 绑定。&lt;/li&gt;
&lt;li&gt;Released：已释放，PVC 被删除，但是资源还未被重新使用。&lt;/li&gt;
&lt;li&gt;Failed：失败，自动回收失败。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;27-pvc绑定pv&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-pvc绑定pv&#34;&gt;#&lt;/a&gt; 2.7 PVC 绑定 PV&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: nfs-slow
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi      
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;PVC 的空间申请大小≤PV 的大小&lt;/li&gt;
&lt;li&gt;PVC 的 StorageClassName 和 PV 的一致&lt;/li&gt;
&lt;li&gt;PVC 的 accessModes 和 PV 的一致&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;28-depoyment挂载pvc&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#28-depoyment挂载pvc&#34;&gt;#&lt;/a&gt; 2.8 Depoyment 挂载 PVC&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      volumes:
      - name: nfs-pvc-storage  #volume名称
        persistentVolumeClaim:
          claimName: nfs-pvc   #PVC名称
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
         - name: nfs-pvc-storage
          mountPath: /usr/share/nginx/html
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;挂载 PVC 的 Pod 一直处于 Pending：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PVC 没有创建成功或 PVC 不存在&lt;/li&gt;
&lt;li&gt;PVC 和 Pod 不在同一个 Namespace&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-18T14:25:17.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/3992668367.html</id>
        <title>K8s配置管理Configmap</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/3992668367.html"/>
        <content type="html">&lt;h3 id=&#34;k8s配置管理configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s配置管理configmap&#34;&gt;#&lt;/a&gt; K8s 配置管理 Configmap&lt;/h3&gt;
&lt;h4 id=&#34;1-configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-configmap&#34;&gt;#&lt;/a&gt; 1. Configmap&lt;/h4&gt;
&lt;h5 id=&#34;1-1-基于from-env-file创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-1-基于from-env-file创建configmap&#34;&gt;#&lt;/a&gt; 1. 1 基于 from-env-file 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat cm_env.conf 
podname=nf-flms-system
podip=192.168.1.100
env=prod
nacosaddr=nacos.svc.cluster.local

#kubectl create cm cmenv --from-env-file=./cm_env.conf 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-基于from-literal创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-基于from-literal创建configmap&#34;&gt;#&lt;/a&gt; 1.2 基于 from-literal 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm cmliteral --from-literal=level=INFO --from-literal=passwd=Superman*2023
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-基于from-file创建configmap&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-基于from-file创建configmap&#34;&gt;#&lt;/a&gt; 1.3 基于 from-file 创建 Configmap&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat s.hmallleasing.com.conf 
server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    client_max_body_size 1G; 
    location / &amp;#123;
        proxy_pass http://192.168.1.134;
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        proxy_connect_timeout 30;
        proxy_send_timeout 60;
        proxy_read_timeout 60;
        
        proxy_buffering on;
        proxy_buffer_size 32k;
        proxy_buffers 4 128k;
        proxy_temp_file_write_size 10240k;		
        proxy_max_temp_file_size 10240k;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;

# kubectl create cm nginxconfig --from-file=./s.hmallleasing.com.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-deployment挂载configmap示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-deployment挂载configmap示例&#34;&gt;#&lt;/a&gt; 1.4 Deployment 挂载 configmap 示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-重命名挂载的configmaq-key的名称&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-重命名挂载的configmaq-key的名称&#34;&gt;#&lt;/a&gt; 1.5 重命名挂载的 configmaq key 的名称&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
          items:                # 重命名挂载的configmaq key的名称为nginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
 
#查看挂载的configmaq key的名称重命名为nginx.conf
[root@k8s-master01 cm]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-deploy-bc476bc56-flln4   1/1     Running   0          10h
nginx-deploy-bc476bc56-jhsh6   1/1     Running   0          10h
nginx-deploy-bc476bc56-splv9   1/1     Running   0          10h
[root@k8s-master01 cm]# kubectl exec -it nginx-deploy-bc476bc56-flln4 -- bash
root@nginx-deploy-bc476bc56-flln4:/# ls /etc/nginx/conf.d/
nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;16-修改挂载的configmaq-权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#16-修改挂载的configmaq-权限&#34;&gt;#&lt;/a&gt; 1.6 修改挂载的 configmaq 权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 cm]# cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # 1.批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # 2.自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # 3.挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:              
        - name: nginx-config
          mountPath: &amp;quot;/etc/nginx/conf.d&amp;quot;
          readOnly: true
      volumes:
      - name: nginx-config
        configMap:
          name: nginxconfig      # 提供你想要挂载的 ConfigMap 的名字
          items:                # 重命名挂载的configmaq key的名称为nginx.conf
          - key: s.hmallleasing.com.conf  
            path: nginx.conf
            mode: 0644        # 配置挂载权限，针对单个key生效
          defaultMode: 0666   # 配置挂载权限，针对整个key生效
    
#查看挂载权限
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/nginx.conf 
lrwxrwxrwx 1 root root 17 Apr 16 13:37 /etc/nginx/conf.d/nginx.conf -&amp;gt; ..data/nginx.conf
root@nginx-deploy-7657fbffc7-k75l5:/# ls -l /etc/nginx/conf.d/..data/nginx.conf 
-rw-rw-rw- 1 root root 722 Apr 16 13:37 /etc/nginx/conf.d/..data/nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;17-subpath解决挂载覆盖问题&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#17-subpath解决挂载覆盖问题&#34;&gt;#&lt;/a&gt; 1.7 subpath 解决挂载覆盖问题&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.创建configmap
[root@k8s-master01 cm]# cat nginx.conf 

user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events &amp;#123;
    worker_connections  512;
&amp;#125;


http &amp;#123;
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  &#39;$remote_addr - $remote_user [$time_local] &amp;quot;$request&amp;quot; &#39;
                      &#39;$status $body_bytes_sent &amp;quot;$http_referer&amp;quot; &#39;
                      &#39;&amp;quot;$http_user_agent&amp;quot; &amp;quot;$http_x_forwarded_for&amp;quot;&#39;;

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
&amp;#125;

[root@k8s-master01 cm]# kubectl create cm nginx-config --from-file=./nginx.conf

#subpath解决挂载覆盖问题
[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        envFrom:         # ①批量挂载ConfigMap生成环境变量
        - configMapRef:
            name: cmenv
        env:
        - name: MYSQL_ADDR     # ②自定义环境变量
          value: &amp;quot;192.168.40.150&amp;quot;
        - name: MYSQL_PASSWD
          value: Superman*2022
        - name: LOG_LEVEL           # ③挂载单个ConfigMap生成环境变量，这里和ConfigMap中的键名是不一样的     
          valueFrom:
            configMapKeyRef:
              name: cmliteral       # 这个值来自ConfigMap
              key: level            # 来自ConfigMap的key
        volumeMounts:
        - name: config
          mountPath: &amp;quot;/etc/nginx/nginx.conf&amp;quot;   #只挂在nginx.conf一个文件,不覆盖目录
          subPath: nginx.conf      
      volumes:
      - name: config
        configMap:
          name: nginx-config      # 提供你想要挂载的ConfigMap的名字
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-secret&#34;&gt;#&lt;/a&gt; 2. Secret&lt;/h4&gt;
&lt;h5 id=&#34;21-secret拉取私有仓库镜像&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-secret拉取私有仓库镜像&#34;&gt;#&lt;/a&gt; 2.1 Secret 拉取私有仓库镜像&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret docker-registry harboradmin \
--docker-server=s.hmallleasing.com \
--docker-username=admin \
--docker-password=Superman*2023 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-创建ssl-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-创建ssl-secret&#34;&gt;#&lt;/a&gt; 2.2 创建 ssl Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create secret tls dev.hmallleasig.com --key *.hmallleasing.com_key.key --cert *.hmallleasing.com_chain.crt -n dev
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-基于命令创建generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-基于命令创建generic-secret&#34;&gt;#&lt;/a&gt; 2.3 基于命令创建 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;#1.通过from-env-file创建
# cat db.conf 
username=xuyong
passwd=Superman*2023

# kubectl create secret generic dbconf --from-env-file=./db.conf

#2.通过from-literal创建
kubectl create secret generic db-user-pass \
    --from-literal=username=admin \
    --from-literal=password=&#39;S!B\*d$zDsb=&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-secret加密-解密&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-secret加密-解密&#34;&gt;#&lt;/a&gt; 2.4 Secret 加密、解密&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;1.加密
# echo -n &amp;quot;Superman*2023&amp;quot; | base64
U3VwZXJtYW4qMjAyMw==

2.解密
# echo &amp;quot;U3VwZXJtYW4qMjAyMw==&amp;quot; | base64 --decode
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-基于文件创建非加密generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-基于文件创建非加密generic-secret&#34;&gt;#&lt;/a&gt; 2.5 基于文件创建非加密 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get secret dbconf -oyaml
apiVersion: v1
data:
  passwd: U3VwZXJtYW4qMjAyMw==
  username: eHV5b25n
kind: Secret
metadata:
  name: dbconf
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-6-基于yaml创建加密generic-secret&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-6-基于yaml创建加密generic-secret&#34;&gt;#&lt;/a&gt; 2. 6 基于 yaml 创建加密 generic Secret&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;# cat mysql-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
  namespace: dev
stringData:
  MYSQL_ROOT_PASSWORD: Superman*2023
type: Opaque
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;27-deployment挂载secret示例&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#27-deployment挂载secret示例&#34;&gt;#&lt;/a&gt; 2.7 Deployment 挂载 Secret 示例&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 study]# cat cm-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      imagePullSecrets:        
      - name: harboradmin
      containers:
      - image: nginx
        name: nginx
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - name: MYSQL_ROOT_PASSWORD  
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_ROOT_PASSWORD
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-configmapsecret热更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-configmapsecret热更新&#34;&gt;#&lt;/a&gt; 3. ConfigMap&amp;amp;Secret 热更新&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# kubectl create cm nginxconfig --from-file=nginx.conf --dry-run=client -oyaml | kubectl replace -f -
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T13:47:47.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/858611107.html</id>
        <title>K8s服务发布Service</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/858611107.html"/>
        <content type="html">&lt;h3 id=&#34;k8s服务发布service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s服务发布service&#34;&gt;#&lt;/a&gt; K8s 服务发布 Service&lt;/h3&gt;
&lt;h4 id=&#34;1-service类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-service类型&#34;&gt;#&lt;/a&gt; 1. Service 类型&lt;/h4&gt;
&lt;p&gt;Kubernetes Service Type（服务类型）主要包括以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP：在集群内部使用，默认值，只能从集群中访问。&lt;/li&gt;
&lt;li&gt;NodePort：在所有安装了 Kube-Proxy 的节点上打开一个端口，此端口可以代理至后端 Pod，可以通过 NodePort 从集群外部访问集群内的服务，格式为 NodeIP:NodePort。&lt;/li&gt;
&lt;li&gt;LoadBalancer：使用云提供商的负载均衡器公开服务，成本较高。&lt;/li&gt;
&lt;li&gt;ExternalName：通过返回定义的 CNAME 别名，没有设置任何类型的代理，需要 1.7 或更高版本 kube-dns 支持。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-nodeport类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-nodeport类型&#34;&gt;#&lt;/a&gt; 1.1 NodePort 类型&lt;/h5&gt;
&lt;p&gt;如果将 Service 的 type 字段设置为 NodePort，则 Kubernetes 将从 --service-node-port-range 参数指定的范围（默认为 30000-32767）中自动分配端口，也可以手动指定 NodePort，创建该 Service 后，集群每个节点都将暴露一个端口，通过某个宿主机的 IP + 端口即可访问到后端的应用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-clusterip类型&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-clusterip类型&#34;&gt;#&lt;/a&gt; 1.2 ClusterIP 类型&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-使用service代理k8s外部服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-使用service代理k8s外部服务&#34;&gt;#&lt;/a&gt; 1.3 使用 Service 代理 K8s 外部服务&lt;/h5&gt;
&lt;p&gt;使用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;希望在生产环境中使用某个固定的名称而非 IP 地址访问外部的中间件服务；&lt;/li&gt;
&lt;li&gt;希望 Service 指向另一个 Namespace 中或其他集群中的服务；&lt;/li&gt;
&lt;li&gt;正在将工作负载转移到 Kubernetes 集群，但是一部分服务仍运行在 Kubernetes 集群之外的 backend。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
spec:
  clusterIP: None
  ports:
  - name: mysql
    port: 3306 
    protocol: TCP
    targetPort: 3306
  type: ClusterIP
---
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    app: mysql-svc-external
  name: mysql-svc-external
subsets:
- addresses:
  - ip: 192.168.40.150
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-externalname-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-externalname-service&#34;&gt;#&lt;/a&gt; 1.4 ExternalName Service&lt;/h5&gt;
&lt;p&gt;ExternalName Service 是 Service 的特例，它没有 Selector，也没有定义任何端口和 Endpoint，它通过返回该外部服务的别名来提供服务。&lt;/p&gt;
&lt;p&gt;比如可以定义一个 Service，后端设置为一个外部域名，这样通过 Service 的名称即可访问到该域名。使用 nslookup 解析以下文件定义的 Service，集群的 DNS &lt;a href=&#34;http://xn--my-uu2cmg2cx7mswf9rko5lsx1a5n3h.database.example.com&#34;&gt;服务将返回一个值为 my.database.example.com&lt;/a&gt; 的 CNAME 记录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-多端口-service&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-多端口-service&#34;&gt;#&lt;/a&gt; 1.5 多端口 Service&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
    - port: 443
      targetPort: 443
      protocol: TCP
      name: https
  selector:
    app: nginx
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:25:51.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/108692210.html</id>
        <title>K8s资源调度deployment、statefulset、daemonset</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/108692210.html"/>
        <content type="html">&lt;h3 id=&#34;k8s资源调度deployment-statefulset-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s资源调度deployment-statefulset-daemonset&#34;&gt;#&lt;/a&gt; K8s 资源调度 deployment、statefulset、daemonset&lt;/h3&gt;
&lt;h4 id=&#34;1-无状态应用管理-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-无状态应用管理-deployment&#34;&gt;#&lt;/a&gt; 1. 无状态应用管理 Deployment&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.0
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;示例解析：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;nginx-deploy：Deployment 的名称；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;replicas： 创建 Pod 的副本数；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;selector：定义 Deployment 如何找到要管理的 Pod，与 template 的 label（标签）对应，apiVersion 为 apps/v1 必须指定该字段；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;template 字段包含以下字段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;app: nginx-deploy 使用 label（标签）标记 Pod；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;spec：表示 Pod 运行一个名字为 nginx 的容器；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image：运行此 Pod 使用的镜像；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Port：容器用于发送和接收流量的端口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;11-更新-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-更新-deployment&#34;&gt;#&lt;/a&gt; 1.1 更新 Deployment&lt;/h5&gt;
&lt;p&gt;假如更新 Nginx Pod 的 image 使用 nginx:latest，并使用 --record 记录当前更改的参数，后期回滚时可以查看到对应的信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新过程为新旧交替更新，首先新建一个 Pod，当 Pod 状态为 Running 时，删除一个旧的 Pod，同时再创建一个新的 Pod。当触发一个更新后，会有新的 ReplicaSet 产生，旧的 ReplicaSet 会被保存，查看此时 ReplicaSet，可以从 AGE 或 READY 看出来新旧 ReplicaSet：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get rs
NAME                      DESIRED   CURRENT   READY   AGE
nginx-deploy-65bfb77869   0         0         0       50s
nginx-deploy-85b94dddb4   3         3         3       8s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过 describe 查看 Deployment 的详细信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl describe deploy nginx-deploy
Name:                   nginx-deploy
Namespace:              default
CreationTimestamp:      Mon, 14 Apr 2025 11:28:03 +0800
Labels:                 app=nginx-deploy
Annotations:            app: nginx-deploy
                        deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
Selector:               app=nginx-deploy
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx-deploy
  Containers:
   nginx-deploy:
    Image:         nginx:latest
    Port:          &amp;lt;none&amp;gt;
    Host Port:     &amp;lt;none&amp;gt;
    Environment:   &amp;lt;none&amp;gt;
    Mounts:        &amp;lt;none&amp;gt;
  Volumes:         &amp;lt;none&amp;gt;
  Node-Selectors:  &amp;lt;none&amp;gt;
  Tolerations:     &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  nginx-deploy-65bfb77869 (0/0 replicas created)
NewReplicaSet:   nginx-deploy-85b94dddb4 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  71s   deployment-controller  Scaled up replica set nginx-deploy-65bfb77869 from 0 to 3
  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 0 to 1
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 3 to 2
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 1 to 2
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 2 to 1
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled up replica set nginx-deploy-85b94dddb4 from 2 to 3
  Normal  ScalingReplicaSet  26s   deployment-controller  Scaled down replica set nginx-deploy-65bfb77869 from 1 to 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在 describe 中可以看出，第一次创建时，它创建了一个名为 nginx-deploy-65bfb77869 的 ReplicaSet，并直接将其扩展为 3 个副本。更新部署时，它创建了一个新的 ReplicaSet，命名为 nginx-deploy-85b94dddb4，并将其副本数扩展为 1，然后将旧的 ReplicaSet 缩小为 2，这样至少可以有 2 个 Pod 可用，最多创建了 4 个 Pod。以此类推，使用相同的滚动更新策略向上和向下扩展新旧 ReplicaSet，最终新的 ReplicaSet 可以拥有 3 个副本，并将旧的 ReplicaSet 缩小为 0。&lt;/p&gt;
&lt;h5 id=&#34;12-回滚-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-回滚-deployment&#34;&gt;#&lt;/a&gt; 1.2 回滚 Deployment&lt;/h5&gt;
&lt;p&gt;当更新了版本不稳定或配置不合理时，可以对其进行回滚操作，假设我们又进行了几次更新（此处以更新镜像版本触发更新，更改配置效果类似）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record
# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用 kubectl rollout history 查看更新历史：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
3         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 Deployment 某次更新的详细信息，使用 --revision 指定某次更新版本号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy --revision=4
deployment.apps/nginx-deploy with revision #4
Pod Template:
  Labels:	app=nginx-deploy
	pod-template-hash=65b576b795
  Annotations:	kubernetes.io/change-cause: kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
  Containers:
   nginx-deploy:
    Image:	nginx:1.21.2
    Port:	&amp;lt;none&amp;gt;
    Host Port:	&amp;lt;none&amp;gt;
    Environment:	&amp;lt;none&amp;gt;
    Mounts:	&amp;lt;none&amp;gt;
  Volumes:	&amp;lt;none&amp;gt;
  Node-Selectors:	&amp;lt;none&amp;gt;
  Tolerations:	&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果只需要回滚到上一个稳定版本，使用 kubectl rollout undo 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再次查看更新历史，发现 REVISION3 回到了 nginx:1.21.1：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         &amp;lt;none&amp;gt;
2         kubectl set image deployment nginx-deploy nginx-deploy=nginx:latest --record=true
4         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.2 --record=true
5         kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.1 --record=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果要回滚到指定版本，使用 --to-revision 参数：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout undo deployment nginx-deploy --to-revision=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-扩容-deployment&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-扩容-deployment&#34;&gt;#&lt;/a&gt; 1.3 扩容 Deployment&lt;/h5&gt;
&lt;p&gt;当公司访问量变大，或者有预期内的活动时，三个 Pod 可能已无法支撑业务时，可以提前对其进行扩展。&lt;/p&gt;
&lt;p&gt;使用 kubectl scale 动态调整 Pod 的副本数，比如增加 Pod 为 5 个：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale deployment nginx-deploy --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 Pod，此时 Pod 已经变成了 5 个：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
nginx-deploy-85b94dddb4-2qrh6   1/1     Running   0          2m9s
nginx-deploy-85b94dddb4-gvkqj   1/1     Running   0          2m10s
nginx-deploy-85b94dddb4-mdfjs   1/1     Running   0          22s
nginx-deploy-85b94dddb4-rhgpr   1/1     Running   0          2m8s
nginx-deploy-85b94dddb4-vwjhl   1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-暂停和恢复-deployment-更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-暂停和恢复-deployment-更新&#34;&gt;#&lt;/a&gt; 1.4 暂停和恢复 Deployment 更新&lt;/h5&gt;
&lt;p&gt;上述演示的均为更改某一处的配置，更改后立即触发更新，大多数情况下可能需要针对一个资源文件更改多处地方，而并不需要多次触发更新，此时可以使用 Deployment 暂停功能，临时禁用更新操作，对 Deployment 进行多次修改后在进行更新。&lt;/p&gt;
&lt;p&gt;使用 kubectl rollout pause 命令即可暂停 Deployment 更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout pause deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后对 Deployment 进行相关更新操作，比如先更新镜像，然后对其资源进行限制（如果使用的是 kubectl edit 命令，可以直接进行多次修改，无需暂停更新，kubectlset 命令一般会集成在 CICD 流水线中）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.21.3
# kubectl set resources deployment nginx-deploy -c=nginx-deploy --limits=cpu=200m,memory=512Mi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过 rollout history 可以看到没有新的更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#  kubectl rollout history deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行完最后一处配置更改后，使用 kubectl rollout resume 恢复 Deployment 更新：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl rollout resume deployment nginx-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以查看到恢复更新的 Deployment 创建了一个新的 RS（ReplicaSet 缩写）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get rs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以查看 Deployment 的 image（镜像）已经变为 nginx:1.21.3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    - image: nginx:1.21.3
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.3
      imageID: docker.io/library/nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;15-更新-deployment-的注意事项&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-更新-deployment-的注意事项&#34;&gt;#&lt;/a&gt; 1.5 更新 Deployment 的注意事项&lt;/h5&gt;
&lt;p&gt;在默认情况下，revision 保留 10 个旧的 ReplicaSet，其余的将在后台进行垃圾回收，可以在.spec.revisionHistoryLimit 设置保留 ReplicaSet 的个数。当设置为 0 时，不保留历史记录。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  namespace: default
  labels:
    app: nginx-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:1.21.3
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spec.strategy.type==Recreate，表示重建，先删掉旧的 Pod 再创建新的 Pod；&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: Recreate
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.type==RollingUpdate，表示滚动更新，可以指定 maxUnavailable 和 maxSurge 来控制滚动更新过程；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxUnavailable，指定在回滚更新时最大不可用的 Pod 数量，可选字段，默认为 25%，可以设置为数字或百分比，如果 maxSurge 为 0，则该值不能为 0；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;spec.strategy.rollingUpdate.maxSurge 可以超过期望值的最大 Pod 数，可选字段，默认为 25%，可以设置成数字或百分比，如果 maxUnavailable 为 0，则该值不能为 0。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-有状态应用管理-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-有状态应用管理-statefulset&#34;&gt;#&lt;/a&gt; 2. 有状态应用管理 StatefulSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: web
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx
  type: ClusterIP
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: &#39;1&#39;
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 128Mi
      restartPolicy: Always
  serviceName: web
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;kind: Service 定义了一个名字为 web 的 Headless Service，创建的 Service 格式为 nginx-0.web.default.svc.cluster.local，其他的类似，因为没有指定 Namespace（命名空间），所以默认部署在 default；&lt;/li&gt;
&lt;li&gt;kind: StatefulSet 定义了一个名字为 nginx 的 StatefulSet，replicas 表示部署 Pod 的副本数，本实例为 3。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;21-创建-statefulset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-创建-statefulset&#34;&gt;#&lt;/a&gt; 2.1 创建 StatefulSet&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
nginx-0   1/1     Running   0          8m51s
nginx-1   1/1     Running   0          8m50s
nginx-2   1/1     Running   0          8m48s
[root@k8s-master01 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   6d1h
web          ClusterIP   None         &amp;lt;none&amp;gt;        80/TCP    9m28s
[root@k8s-master01 ~]# kubectl get sts
NAME    READY   AGE
nginx   3/3     8m58s
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-statefulset创建pod流程&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-statefulset创建pod流程&#34;&gt;#&lt;/a&gt; 2.2 StatefulSet 创建 Pod 流程&lt;/h5&gt;
&lt;p&gt;StatefulSet 管理的 Pod 部署和扩展规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于具有 N 个副本的 StatefulSet，将按顺序从 0 到 N-1 开始创建 Pod；&lt;/li&gt;
&lt;li&gt;当删除 Pod 时，将按照 N-1 到 0 的反顺序终止；&lt;/li&gt;
&lt;li&gt;在缩放 Pod 之前，必须保证当前的 Pod 是 Running（运行中）或者 Ready（就绪）；&lt;/li&gt;
&lt;li&gt;在终止 Pod 之前，它所有的继任者必须是完全关闭状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet 的 pod.Spec.TerminationGracePeriodSeconds（终止 Pod 的等待时间）不应该指定为 0，设置为 0 对 StatefulSet 的 Pod 是极其不安全的做法，优雅地删除 StatefulSet 的 Pod 是非常有必要的，而且是安全的，因为它可以确保在 Kubelet 从 APIServer 删除之前，让 Pod 正常关闭。&lt;/p&gt;
&lt;p&gt;当创建上面的 Nginx 实例时，Pod 将按 nginx-0、nginx-1、nginx-2 的顺序部署 3 个 Pod。在 nginx-0 处于 Running 或者 Ready 之前，nginx-1 不会被部署，相同的，nginx-2 在 web-1 未处于 Running 和 Ready 之前也不会被部署。如果在 nginx-1 处于 Running 和 Ready 状态时，nginx-0 变成 Failed 失败）状态，那么 nginx-2 将不会被启动，直到 nginx-0 恢复为 Running 和 Ready 状态。&lt;/p&gt;
&lt;p&gt;如果用户将 StatefulSet 的 replicas 设置为 1，那么 nginx-2 将首先被终止，在完全关闭并删除 nginx-2 之前，不会删除 nginx-1。如果 nginx-2 终止并且完全关闭后，nginx-0 突然失败，那么在 nginx-0 未恢复成 Running 或者 Ready 时，nginx-1 不会被删除。&lt;/p&gt;
&lt;h5 id=&#34;23-tatefulset-扩容和缩容&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-tatefulset-扩容和缩容&#34;&gt;#&lt;/a&gt; 2.3 tatefulSet 扩容和缩容&lt;/h5&gt;
&lt;p&gt;和 Deployment 类似，可以通过更新 replicas 字段扩容 / 缩容 StatefulSet，也可以使用 kubectlscale、kubectl edit 和 kubectl patch 来扩容 / 缩容一个 StatefulSet。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl scale sts nginx --replicas=5
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-statefulset-更新策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-statefulset-更新策略&#34;&gt;#&lt;/a&gt; 2.4 StatefulSet 更新策略&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;On Delete 策略&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OnDelete 更新策略实现了传统（1.7 版本之前）的行为，它也是默认的更新策略。当我们选择这个更新策略并修改 StatefulSet 的.spec.template 字段时，StatefulSet 控制器不会自动更新 Pod，必须手动删除 Pod 才能使控制器创建新的 Pod。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: OnDelete
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;RollingUpdate 策略&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RollingUpdate（滚动更新）更新策略会自动更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序进行滚动更新。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-分段更新&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-分段更新&#34;&gt;#&lt;/a&gt; 2.5 分段更新&lt;/h5&gt;
&lt;p&gt;将分区改为 2，此时会自动更新 nginx-2、nginx-3、nginx-4（因为之前更改了更新策略），但是不会更新 nginx-0 和 nginx-1：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 sts 镜像为 nginx:1.21.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image sts nginx nginx=nginx:1.21.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;按照上述方式，可以实现分阶段更新，类似于灰度 / 金丝雀发布。查看最终的结果如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -oyaml|grep image
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:latest
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:fad8e1cd52e24bce7b72cd7cb674a2efad671647b917055f5bd8a1f7ac9b1af8
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
    - image: nginx:1.21.1
      imagePullPolicy: IfNotPresent
      image: docker.io/library/nginx:1.21.1
      imageID: docker.io/library/nginx@sha256:a05b0cdd4fc1be3b224ba9662ebdf98fe44c09c0c9215b45f84344c12867002e
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3守护进程集-daemonset&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3守护进程集-daemonset&#34;&gt;#&lt;/a&gt; 3. 守护进程集 DaemonSet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
        - name: nginx-ds
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时会在每个节点创建一个 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
nginx-ds-47dxc   1/1     Running   0          56s   172.16.85.213    k8s-node01     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-4m89f   1/1     Running   0          56s   172.16.32.143    k8s-master01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-mtpc2   1/1     Running   0          56s   172.16.195.12    k8s-master03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-t5rxc   1/1     Running   0          56s   172.16.122.142   k8s-master02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
nginx-ds-x86kc   1/1     Running   0          56s   172.16.58.222    k8s-node02     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;指定节点部署 Pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;      nodeSelector:
        ingress: &#39;true&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新和回滚 DaemonSet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl set image ds nginx-ds nginx-ds=1.21.0 --record=true
# kubectl rollout undo daemonset &amp;lt;daemonset-name&amp;gt; --to-revision=&amp;lt;revision&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DaemonSet 的更新和回滚与 Deployment 类似，此处不再演示。&lt;/p&gt;
&lt;h4 id=&#34;4-hpa&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-hpa&#34;&gt;#&lt;/a&gt; 4. HPA&lt;/h4&gt;
&lt;p&gt;创建 deployment、service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-hpa-svc
  namespace: default
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app: nginx-hpa
  type: ClusterIP

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-hpa
  labels:
    app: nginx-hpa
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-hpa
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-hpa
    spec:
      restartPolicy: Always
      containers:
        - name: nginx-hpa
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 1024Mi
              cpu: 1
            requests:
              memory: 128Mi
              cpu: 100m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建 HPA&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl autoscale deployment nginx-hpa --cpu-percent=10 --min=1 --max=10
# kubectl get hpa
NAME        REFERENCE              TARGETS       MINPODS   MAXPODS   REPLICAS   AGE
nginx-hpa   Deployment/nginx-hpa   cpu: 0%/10%   1         10        1          16s

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;测试自动扩缩容&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;while true; do wget -q -O- http://10.96.18.221 &amp;gt; /dev/null; done
[root@k8s-master01 ~]# kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
nginx-hpa-d8bcbdf7d-4mkxp   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-974q5   1/1     Running   0          6m36s
nginx-hpa-d8bcbdf7d-g6p2h   1/1     Running   0          66s
nginx-hpa-d8bcbdf7d-lvvsq   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tgqmr   1/1     Running   0          111s
nginx-hpa-d8bcbdf7d-tzfbs   1/1     Running   0          21s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:25:00.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/1771242682.html</id>
        <title>K8s零宕机服务发布-探针</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/1771242682.html"/>
        <content type="html">&lt;h3 id=&#34;k8s零宕机服务发布-探针&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#k8s零宕机服务发布-探针&#34;&gt;#&lt;/a&gt; K8s 零宕机服务发布 - 探针&lt;/h3&gt;
&lt;h4 id=&#34;1-pod状态及-pod-故障排查命令&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-pod状态及-pod-故障排查命令&#34;&gt;#&lt;/a&gt; 1. Pod 状态及 Pod 故障排查命令&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;状态&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pending（挂起）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已被 Kubernetes 系统接收，但仍有一个或多个容器未被创建，可以通过 kubectl describe 查看处于 Pending 状态的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Running（运行中）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经被绑定到一个节点上，并且所有的容器都已经被创建，而且至少有一个是运行状态，或者是正在启动或者重启，可以通过 kubectl logs 查看 Pod 的日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Succeeded（成功）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;所有容器执行成功并终止，并且不会再次重启，可以通过 kubectl logs 查看 Pod 日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Failed（失败）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;所有容器都已终止，并且至少有一个容器以失败的方式终止，也就是说这个容器要么以非零状态退出，要么被系统终止，可以通过 logs 和 describe 查看 Pod 日志和状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Unknown（未知）&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;通常是由于通信问题造成的无法获得 Pod 的状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ImagePullBackOff ErrImagePull&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;镜像拉取失败，一般是由于镜像不存在、网络不通或者需要登录认证引起的，可以使用 describe 命令查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CrashLoopBackOff&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器启动失败，可以通过 logs 命令查看具体原因，一般为启动命令不正确，健康检查不通过等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;OOMKilled&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器内存溢出，一般是容器的内存 Limit 设置的过小，或者程序本身有内存溢出，可以通过 logs 查看程序启动日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Terminating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 正在被删除，可以通过 describe 查看状态&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SysctlForbidden&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 自定义了内核配置，但 kubelet 没有添加内核配置或配置的内核参数不支持，可以通过 describe 查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Completed&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;容器内部主进程退出，一般计划任务执行结束会显示该状态，此时可以通过 logs 查看容器日志&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ContainerCreating&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 正在创建，一般为正在下载镜像，或者有配置不当的地方，可以通过 describe 查看具体原因&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;2-pod镜像拉取策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-pod镜像拉取策略&#34;&gt;#&lt;/a&gt; 2. Pod 镜像拉取策略&lt;/h4&gt;
&lt;p&gt;通过 spec.containers [].imagePullPolicy 参数可以指定镜像的拉取策略，目前支持的策略如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;总是拉取，当镜像 tag 为 latest 时，且 imagePullPolicy 未配置，默认为 Always&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;不管是否存在都不会拉取&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IfNotPresent&lt;/td&gt;
&lt;td&gt;镜像不存在时拉取镜像，如果 tag 为非 latest，且 imagePullPolicy 未配置，默认为 IfNotPresent&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;更改镜像拉取策略为 IfNotPresent：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-pod-重启策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-pod-重启策略&#34;&gt;#&lt;/a&gt; 3. &lt;strong&gt;Pod&lt;/strong&gt; 重启策略&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Always&lt;/td&gt;
&lt;td&gt;默认策略。容器失效时，自动重启该容器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OnFailure&lt;/td&gt;
&lt;td&gt;容器以不为 0 的状态码终止，自动重启该容器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Never&lt;/td&gt;
&lt;td&gt;无论何种状态，都不会重启&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;指定重启策略为 Always ：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-pod的三种探针&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-pod的三种探针&#34;&gt;#&lt;/a&gt; 4. Pod 的三种探针&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;种类&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;startupProbe&lt;/td&gt;
&lt;td&gt;Kubernetes1.16 新加的探测方式，用于判断容器内的应用程序是否已经启动。如果配置了 startupProbe，就会先禁用其他探测，直到它成功为止。如果探测失败，Kubelet 会杀死容器，之后根据重启策略进行处理，如果探测成功，或没有配置 startupProbe，则状态为成功，之后就不再探测。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;livenessProbe&lt;/td&gt;
&lt;td&gt;用于探测容器是否在运行，如果探测失败，kubelet 会 “杀死” 容器并根据重启策略进行相应的处理。如果未指定该探针，将默认为 Success&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;readinessProbe&lt;/td&gt;
&lt;td&gt;一般用于探测容器内的程序是否健康，即判断容器是否为就绪（Ready）状态。如果是，则可以处理请求，反之 Endpoints Controller 将从所有的 Service 的 Endpoints 中删除此容器所在 Pod 的 IP 地址。如果未指定，将默认为 Success&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;5-pod探针的实现方式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-pod探针的实现方式&#34;&gt;#&lt;/a&gt; 5. Pod 探针的实现方式&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;实现方式&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ExecAction&lt;/td&gt;
&lt;td&gt;在容器内执行一个指定的命令，如果命令返回值为 0，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCPSocketAction&lt;/td&gt;
&lt;td&gt;通过 TCP 连接检查容器指定的端口，如果端口开放，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HTTPGetAction&lt;/td&gt;
&lt;td&gt;对指定的 URL 进行 Get 请求，如果状态码在 200~400 之间，则认为容器健康&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;6-健康检查配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-健康检查配置&#34;&gt;#&lt;/a&gt; 6. 健康检查配置&lt;/h4&gt;
&lt;p&gt;配置健康检查：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-prestop和-poststart配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-prestop和-poststart配置&#34;&gt;#&lt;/a&gt; 7. PreStop 和 PostStart 配置&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat nginx-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-deploy
  annotations:
    app: nginx-deploy
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx-deploy
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
        - name: nginx-deploy
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          startupProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          livenessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            tcpSocket:
              port: 80
          readinessProbe:
            initialDelaySeconds: 30
            timeoutSeconds: 2
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 2
            httpGet:
              path: /index.html
              port: 80
              scheme: HTTP
          lifecycle:
            postStart:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - mkdir /data
            preStop:
              exec:
                command:
                  - sh
                  - &#39;-c&#39;
                  - sleep 30
      restartPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-14T11:23:48.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/3071070979.html</id>
        <title>一键永久激活Window、office教程</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/3071070979.html"/>
        <content type="html">&lt;h3 id=&#34;一键永久激活window-office教程&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#一键永久激活window-office教程&#34;&gt;#&lt;/a&gt; 一键永久激活 Window、office 教程&lt;/h3&gt;
&lt;p&gt;1、按下 Win 键 + R，调出运行对话框，输入 powershell 并回车，启动命令提示符窗口。接着输入以下指令执行激活：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;irm https://get.activated.win | iex
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/ilMT403.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;该脚本包含四个功能：首个命令用于 Windows 系统永久激活，第二个用于 Office 永久激活，第三个将系统有效期延长至 2038 年，第四个则实现每 180 天自动循环激活。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/taJbKQr.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;2. 我们再次使用 Windows 徽标 + R 快捷键打开运行框，输入 slmgr.vbs/xpr 就可以看到系统已经永久激活了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;slmgr.vbs /xpr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/JMWlUpc.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;以上，既然看到这里了，如果觉得不错，随手点个赞、打赏一下吧，⭐～谢谢你看我的文章，我们下次再见。&lt;/p&gt;
</content>
        <category term="Windows" />
        <updated>2025-04-10T13:32:09.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/985149017.html</id>
        <title>二进制高可用安装K8S集群</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/985149017.html"/>
        <content type="html">&lt;h2 id=&#34;二进制高可用安装k8s集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#二进制高可用安装k8s集群&#34;&gt;#&lt;/a&gt; 二进制高可用安装 K8s 集群&lt;/h2&gt;
&lt;h4 id=&#34;1-基本配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-基本配置&#34;&gt;#&lt;/a&gt; 1. 基本配置&lt;/h4&gt;
&lt;h5 id=&#34;11-基本环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-基本环境配置&#34;&gt;#&lt;/a&gt; 1.1 基本环境配置&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP 地址&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master 节点 * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived 虚拟 IP（不占用机器）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker 节点 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* 配置信息 *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;系统版本&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 网段&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service 网段&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改主机名（其它节点按需修改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 hosts，修改 /etc/hosts 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 yum 源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 配置基础源
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;必备工具安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭 swap 分区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ntpdate：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;同步时间并配置上海时区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 limit：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;升级系统：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;下载安装所有的源码文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-内核配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-内核配置&#34;&gt;#&lt;/a&gt; 1.2 内核配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ipvsadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 ipvs 模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 ipvs.conf，并配置开机自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# 加入以下内容
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;内核优化配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;应用配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置完内核后，重启机器，之后查看内核模块是否已自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-高可用组件安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-高可用组件安装&#34;&gt;#&lt;/a&gt; 2. 高可用组件安装&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;通过 yum 安装 HAProxy 和 KeepAlived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 HAProxy，需要注意黄色部分的 IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:8443       #HAProxy监听端口
  bind 127.0.0.1:8443     #HAProxy监听端口
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IP地址
  server k8s-master02	192.168.1.72:6443  check       #API Server IP地址
  server k8s-master03	192.168.1.73:6443  check       #API Server IP地址
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 KeepAlived，需要注意黄色部分的配置。&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #网卡名称
    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #网卡名称
    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #网卡名称
    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置 KeepAlived 健康检查文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置健康检查文件添加执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;启动 haproxy 和 keepalived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;所有节点测试VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld&lt;/li&gt;
&lt;li&gt;所有节点查看 selinux 状态，必须为 disable：getenforce&lt;/li&gt;
&lt;li&gt;master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master 节点查看监听端口：netstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果以上都没有问题，需要确认：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;是否是公有云机器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否是私有云机器（类似 OpenStack）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询&lt;/p&gt;
&lt;h4 id=&#34;3-runtime安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtime安装&#34;&gt;#&lt;/a&gt; 3. Runtime 安装&lt;/h4&gt;
&lt;p&gt;如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。&lt;/p&gt;
&lt;h5 id=&#34;31-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-安装containerd&#34;&gt;#&lt;/a&gt; 3.1 安装 Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置安装源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先配置 Containerd 所需的模块（&lt;mark&gt;所有节点&lt;/mark&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;，配置 Containerd 所需的内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;生成 Containerd 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改 Containerd 的 Cgroup 和 Pause 镜像配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 Containerd，并配置开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 crictl 客户端连接的运行时位置（可选）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-k8s及etcd安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-k8s及etcd安装&#34;&gt;#&lt;/a&gt; 4 . K8S 及 etcd 安装&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 下载 kubernetes 安装包（1.32.3 需要更改为你看到的最新版本）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://dl.k8s.io/v1.32.0/kubernetes-server-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最新版获取地址：&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;以下操作都在 master01 执行&lt;/mark&gt;&lt;/p&gt;
&lt;p&gt;下载 etcd 安装包：&lt;a href=&#34;https://github.com/etcd-io/etcd/releases/&#34;&gt;https://github.com/etcd-io/etcd/releases/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.5.16/etcd-v3.5.16-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解压 kubernetes 安装文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解压 etcd 安装文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  tar -zxvf etcd-v3.5.16-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.16-linux-amd64/etcd&amp;#123;,ctl&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;版本查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubelet --version
Kubernetes v1.32.3
[root@k8s-master01 ~]# etcdctl version
etcdctl version: 3.5.16
API version: 3.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将组件发送到其他节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;
WorkNodes=&#39;k8s-node01 k8s-node02&#39;
for NODE in $MasterNodes; do echo $NODE; scp /usr/local/bin/kube&amp;#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&amp;#125; $NODE:/usr/local/bin/; scp /usr/local/bin/etcd* $NODE:/usr/local/bin/; done
for NODE in $WorkNodes; do     scp /usr/local/bin/kube&amp;#123;let,-proxy&amp;#125; $NODE:/usr/local/bin/ ; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;切换到 1.32.x 分支（其他版本可以切换到其他分支，.x 即可，不需要更改为具体的小版本）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-生成证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-生成证书&#34;&gt;#&lt;/a&gt; 5 . 生成证书&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;二进制安装最关键步骤，一步错误全盘皆输，一定要注意每个步骤都要是正确的&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 下载生成证书工具（下载不成功可以去百度网盘）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssl_linux-amd64&amp;quot; -O /usr/local/bin/cfssl
wget &amp;quot;https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64&amp;quot; -O /usr/local/bin/cfssljson
chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-etcd证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-etcd证书&#34;&gt;#&lt;/a&gt; 5.1 Etcd 证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd 证书目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/etcd/ssl -p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 kubernetes 相关目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/pki
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;生成 etcd 证书&lt;/p&gt;
&lt;p&gt;生成证书的 CSR（证书签名请求文件，配置了一些域名、公司、单位）文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

# 生成etcd CA证书和CA证书的key
cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca


cfssl gencert \
   -ca=/etc/etcd/ssl/etcd-ca.pem \
   -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \
   -config=ca-config.json \
   -hostname=127.0.0.1,k8s-master01,k8s-master02,k8s-master03,192.168.1.71,192.168.1.72,192.168.1.73 \
   -profile=kubernetes \
   etcd-csr.json | cfssljson -bare /etc/etcd/ssl/etcd

执行结果
[INFO] generate received request
 	[INFO] received CSR
     [INFO] generating key: rsa-2048
     [INFO] encoded CSR
     [INFO] signed certificate with serial number     250230878926052708909595617022917808304837732033
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将证书复制到其他 master 节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MasterNodes=&#39;k8s-master02 k8s-master03&#39;

for NODE in $MasterNodes; do
     ssh $NODE &amp;quot;mkdir -p /etc/etcd/ssl&amp;quot;
     for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do
       scp /etc/etcd/ssl/$&amp;#123;FILE&amp;#125; $NODE:/etc/etcd/ssl/$&amp;#123;FILE&amp;#125;
     done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;52-k8s组件证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-k8s组件证书&#34;&gt;#&lt;/a&gt; 5.2 K8s 组件证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01&lt;/mark&gt; 生成 kubernetes CA 证书：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# cd /root/k8s-ha-install/pki

cfssl gencert -initca ca-csr.json | cfssljson -bare /etc/kubernetes/pki/ca
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;521-apiserver证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#521-apiserver证书&#34;&gt;#&lt;/a&gt; 5.2.1 APIServer 证书&lt;/h6&gt;
&lt;p&gt;注意：10.96.0. 是 k8s service 的网段，如果说需要更改 k8s service 网段，那就需要更改 10.96.0.1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -ca=/etc/kubernetes/pki/ca.pem   -ca-key=/etc/kubernetes/pki/ca-key.pem   -config=ca-config.json   -hostname=10.96.0.1,192.168.1.70,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.1.71,192.168.1.72,192.168.1.73   -profile=kubernetes   apiserver-csr.json | cfssljson -bare /etc/kubernetes/pki/apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;生成 apiserver 的聚合证书：：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-ca 

cfssl gencert   -ca=/etc/kubernetes/pki/front-proxy-ca.pem   -ca-key=/etc/kubernetes/pki/front-proxy-ca-key.pem   -config=ca-config.json   -profile=kubernetes   front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/pki/front-proxy-client
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;返回结果（忽略警告）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2020/12/11 18:15:28 [INFO] generate received request
2020/12/11 18:15:28 [INFO] received CSR
2020/12/11 18:15:28 [INFO] generating key: rsa-2048

2020/12/11 18:15:28 [INFO] encoded CSR
2020/12/11 18:15:28 [INFO] signed certificate with serial number 597484897564859295955894546063479154194995827845
2020/12/11 18:15:28 [WARNING] This certificate lacks a &amp;quot;hosts&amp;quot; field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 (&amp;quot;Information Requirements&amp;quot;).
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;522-controllermanager&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#522-controllermanager&#34;&gt;#&lt;/a&gt; 5.2.2 ControllerManager&lt;/h6&gt;
&lt;p&gt;生成 controller-manage 的证书：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   manager-csr.json | cfssljson -bare /etc/kubernetes/pki/controller-manager

注意：修改黄色部分的IP地址
# set-cluster：设置一个集群项，

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# 设置一个环境项，一个上下文
kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# set-credentials 设置一个用户项

kubectl config set-credentials system:kube-controller-manager \
     --client-certificate=/etc/kubernetes/pki/controller-manager.pem \
     --client-key=/etc/kubernetes/pki/controller-manager-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig


# 使用某个环境当做默认环境

kubectl config use-context system:kube-controller-manager@kubernetes \
     --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;523-scheduler证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#523-scheduler证书&#34;&gt;#&lt;/a&gt; 5.2.3 Scheduler 证书&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   scheduler-csr.json | cfssljson -bare /etc/kubernetes/pki/scheduler

注意：修改黄色部分的IP地址

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig


kubectl config set-credentials system:kube-scheduler \
     --client-certificate=/etc/kubernetes/pki/scheduler.pem \
     --client-key=/etc/kubernetes/pki/scheduler-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config set-context system:kube-scheduler@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-scheduler \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

kubectl config use-context system:kube-scheduler@kubernetes \
     --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;524-生成管理员证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#524-生成管理员证书&#34;&gt;#&lt;/a&gt; 5.2.4 生成管理员证书&lt;/h6&gt;
&lt;p&gt;Kubectl /etc/Kubernetes/admin.conf ~/.kube/config&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   admin-csr.json | cfssljson -bare /etc/kubernetes/pki/admin

注意：修改黄色部分的IP

kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/admin.kubeconfig
kubectl config set-credentials kubernetes-admin     --client-certificate=/etc/kubernetes/pki/admin.pem     --client-key=/etc/kubernetes/pki/admin-key.pem     --embed-certs=true     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config set-context kubernetes-admin@kubernetes     --cluster=kubernetes     --user=kubernetes-admin     --kubeconfig=/etc/kubernetes/admin.kubeconfig

kubectl config use-context kubernetes-admin@kubernetes     --kubeconfig=/etc/kubernetes/admin.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;525-创建serviceaccount证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#525-创建serviceaccount证书&#34;&gt;#&lt;/a&gt; 5.2.5 创建 ServiceAccount 证书&lt;/h6&gt;
&lt;p&gt;创建一对公钥，用来签发 ServiceAccount 的 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl genrsa -out /etc/kubernetes/pki/sa.key 2048
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;返回结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Generating RSA private key, 2048 bit long modulus (2 primes)
...................................................................................+++++
...............+++++
e is 65537 (0x010001)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.pub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;发送证书至其他节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do 
  for FILE in $(ls /etc/kubernetes/pki | grep -v etcd); do 
    scp /etc/kubernetes/pki/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/pki/$&amp;#123;FILE&amp;#125;;
  done; 
  for FILE in admin.kubeconfig controller-manager.kubeconfig scheduler.kubeconfig; do 
    scp /etc/kubernetes/$&amp;#123;FILE&amp;#125; $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;;
  done;
done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看证书文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# ls /etc/kubernetes/pki/
admin.csr      apiserver.csr      ca.csr      controller-manager.csr      front-proxy-ca.csr      front-proxy-client.csr      sa.key         scheduler-key.pem
admin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  sa.pub         scheduler.pem
admin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      scheduler.csr
[root@k8s-master01 pki]# ls /etc/kubernetes/pki/ |wc -l
23
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-kubernetes组件配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-kubernetes组件配置&#34;&gt;#&lt;/a&gt; 6. Kubernetes 组件配置&lt;/h4&gt;
&lt;h5 id=&#34;61-ecd配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#61-ecd配置&#34;&gt;#&lt;/a&gt; 6.1 Ecd 配置&lt;/h5&gt;
&lt;p&gt;Etcd 配置大致相同，注意修改每个 Master 节点的 etcd 配置的主机名和 IP 地址&lt;/p&gt;
&lt;h6 id=&#34;611-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#611-master01&#34;&gt;#&lt;/a&gt; 6.1.1 Master01&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master01&#39;     # k8s-master01名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.71:2380&#39;            # k8s-master01 IP
listen-client-urls: &#39;https://192.168.1.71:2379,http://127.0.0.1:2379&#39;   # k8s-master01 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.71:2380&#39;  # k8s-master01 IP
advertise-client-urls: &#39;https://192.168.1.71:2379&#39;        # k8s-master01 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;     # k8s-master01、k8s-master02、k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;612-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#612-master02&#34;&gt;#&lt;/a&gt; 6.1.2 Master02&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml	
name: &#39;k8s-master02&#39;   # k8s-master02名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.72:2380&#39;      # k8s-master02 IP
listen-client-urls: &#39;https://192.168.1.72:2379,http://127.0.0.1:2379&#39;    # k8s-master02 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.72:2380&#39;    # k8s-master02 IP
advertise-client-urls: &#39;https://192.168.1.72:2379&#39;     # k8s-master02 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;             # k8s-master01、k8s-master02、k8s-master03 IP 
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;613-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#613-master03&#34;&gt;#&lt;/a&gt; 6.1.3 Master03&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/etcd/etcd.config.yml
name: &#39;k8s-master03&#39;           # k8s-master03名称
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: &#39;https://192.168.1.73:2380&#39;           # k8s-master03 IP
listen-client-urls: &#39;https://192.168.1.73:2379,http://127.0.0.1:2379&#39;       # k8s-master03 IP
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: &#39;https://192.168.1.73:2380&#39;      # k8s-master03 IP
advertise-client-urls: &#39;https://192.168.1.73:2379&#39;            # k8s-master03 IP
discovery:
discovery-fallback: &#39;proxy&#39;
discovery-proxy:
discovery-srv:
initial-cluster: &#39;k8s-master01=https://192.168.1.71:2380,k8s-master02=https://192.168.1.72:2380,k8s-master03=https://192.168.1.73:2380&#39;                # k8s-master01、k8s-master02、k8s-master03 IP
initial-cluster-token: &#39;etcd-k8s-cluster&#39;
initial-cluster-state: &#39;new&#39;
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: &#39;off&#39;
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
peer-transport-security:
  cert-file: &#39;/etc/kubernetes/pki/etcd/etcd.pem&#39;
  key-file: &#39;/etc/kubernetes/pki/etcd/etcd-key.pem&#39;
  peer-client-cert-auth: true
  trusted-ca-file: &#39;/etc/kubernetes/pki/etcd/etcd-ca.pem&#39;
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;614-启动etcd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#614-启动etcd&#34;&gt;#&lt;/a&gt; 6.1.4 启动 Etcd&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd service 并启动&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Service
Documentation=https://coreos.com/etcd/docs/latest/
After=network.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml
Restart=on-failure
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd3.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;创建 etcd 的证书目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir /etc/kubernetes/pki/etcd
ln -s /etc/etcd/ssl/* /etc/kubernetes/pki/etcd/
systemctl daemon-reload
systemctl enable --now etcd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看 etcd 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export ETCDCTL_API=3
etcdctl --endpoints=&amp;quot;192.168.1.73:2379,192.168.1.72:2379,192.168.1.71:2379&amp;quot; --cacert=/etc/kubernetes/pki/etcd/etcd-ca.pem --cert=/etc/kubernetes/pki/etcd/etcd.pem --key=/etc/kubernetes/pki/etcd/etcd-key.pem  endpoint status --write-out=table
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;62-apiserver配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#62-apiserver配置&#34;&gt;#&lt;/a&gt; 6.2 APIServer 配置&lt;/h5&gt;
&lt;h6 id=&#34;621-master01&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#621-master01&#34;&gt;#&lt;/a&gt; 6.2.1 Master01&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.71 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;622-master02&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#622-master02&#34;&gt;#&lt;/a&gt; 6.2.2 Master02&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.72 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;623-master03&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#623-master03&#34;&gt;#&lt;/a&gt; 6.2.3 Master03&lt;/h6&gt;
&lt;p&gt;注意：本文档使用的 k8s service 网段为 10.96.0.0/16，该网段不能和宿主机的网段、Pod 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim  /usr/lib/systemd/system/kube-apiserver.service 

[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
      --allow-privileged=true  \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.1.73 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.1.71:2379,https://192.168.1.72:2379,https://192.168.1.73:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/pki/ca.pem  \
      --tls-cert-file=/etc/kubernetes/pki/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/pki/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User
      # --token-auth-file=/etc/kubernetes/token.csv

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;624-启动apiserver&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#624-启动apiserver&#34;&gt;#&lt;/a&gt; 6.2.4 启动 apiserver&lt;/h6&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;开启 kube-apiserver：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload &amp;amp;&amp;amp; systemctl enable --now kube-apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;检测 kube-server 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl status kube-apiserver

● kube-apiserver.service – Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2020-08-22 21:26:49 CST; 26s ago 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果系统日志有这些提示可以忽略:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004739    7450 clientconn.go:948] ClientConn switching balancer to “pick_first”
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.004843    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;CONNECTING &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.010725    7450 balancer_conn_wrappers.go:78] pickfirstBalancer: HandleSubConnStateChange: 0xc011bd4c80, &amp;#123;READY &amp;lt;nil&amp;gt;&amp;#125;
Dec 11 20:51:15 k8s-master01 kube-apiserver: I1211 20:51:15.011370    7450 controlbuf.go:508] transport: loopyWriter.run returning. Connection error: desc = “transport is closing”
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;63-controllermanage&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#63-controllermanage&#34;&gt;#&lt;/a&gt; 6.3 ControllerManage&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 kube-controller-manager service（所有 master 节点配置一样）&lt;/p&gt;
&lt;p&gt;注意：本文档使用的 k8s Pod 网段为 172.16.0.0/16，该网段不能和宿主机的网段、k8s Service 网段的重复，请按需修改：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
      --v=2 \
      --root-ca-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
      --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key \
      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --leader-elect=true \
      --use-service-account-credentials=true \
      --node-monitor-grace-period=40s \
      --node-monitor-period=5s \
      --controllers=*,bootstrapsigner,tokencleaner \
      --allocate-node-cidrs=true \
      --cluster-cidr=172.16.0.0/16 \
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
      --node-cidr-mask-size=24
      
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;启动 kube-controller-manager&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-controller-manager
Created symlink /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service → /usr/lib/systemd/system/kube-controller-manager.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看启动状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl  status kube-controller-manager
● kube-controller-manager.service – Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/ ubern/system/kube-controller-manager.service; enabled; vendor preset: disabled)
 Active: active (running) since Fri 2020-12-11 20:53:05 CST; 8s ago
     Docs: https://github.com/  ubernetes/  ubernetes
 Main PID: 7518 (kube-controller)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;64-scheduler&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#64-scheduler&#34;&gt;#&lt;/a&gt; 6.4 Scheduler&lt;/h5&gt;
&lt;p&gt;所有 Master 节点配置 kube-scheduler service（所有 master 节点配置一样）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# vim /usr/lib/systemd/system/kube-scheduler.service 
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
      --v=2 \
      --leader-elect=true \
      --authentication-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --authorization-kubeconfig=/etc/kubernetes/scheduler.kubeconfig \
      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动 scheduler：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 pki]# systemctl daemon-reload

[root@k8s-master01 pki]# systemctl enable --now kube-scheduler
Created symlink /etc/systemd/system/multi-user.target.wants/kube-scheduler.service → /usr/lib/systemd/system/kube-scheduler.service.
[root@k8s-master01 pki]# systemctl status kube-scheduler
● kube-scheduler.service - Kubernetes Scheduler
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2022-05-04 17:31:13 CST; 6s ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 5815 (kube-scheduler)
    Tasks: 9
   Memory: 19.8M
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-tls-bootstrapping配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-tls-bootstrapping配置&#34;&gt;#&lt;/a&gt; 7. TLS Bootstrapping 配置&lt;/h4&gt;
&lt;p&gt;只需要在&lt;mark&gt; Master01&lt;/mark&gt; 创建 bootstrap&lt;/p&gt;
&lt;p&gt;注意： 修改黄色部分的 IP 地址&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/bootstrap
kubectl config set-cluster kubernetes     --certificate-authority=/etc/kubernetes/pki/ca.pem     --embed-certs=true     --server=https://192.168.1.70:8443     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-credentials tls-bootstrap-token-user     --token=c8ad9c.2e4d610cf3e7426e --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster=kubernetes     --user=tls-bootstrap-token-user     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
kubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig

[root@k8s-master01 bootstrap]# mkdir -p /root/.kube ; cp /etc/kubernetes/admin.kubeconfig /root/.kube/config
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以正常查询集群状态，才可以继续往下，否则不行，需要排查 k8s 组件是否有故障（只要有结果即可，如果返回不一样不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok        
scheduler            Healthy   ok        
etcd-0               Healthy   ok
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;创建 bootstrap 相关资源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl create -f bootstrap.secret.yaml 
secret/bootstrap-token-c8ad9c created
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-node节点配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-node节点配置&#34;&gt;#&lt;/a&gt; 8. Node 节点配置&lt;/h4&gt;
&lt;h5 id=&#34;81-复制证书&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#81-复制证书&#34;&gt;#&lt;/a&gt; 8.1 复制证书&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;复制证书至其他节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/kubernetes/

for NODE in k8s-master02 k8s-master03 k8s-node01 k8s-node02; do
     ssh $NODE mkdir -p /etc/kubernetes/pki
     for FILE in pki/ca.pem pki/ca-key.pem pki/front-proxy-ca.pem bootstrap-kubelet.kubeconfig; do
       scp /etc/kubernetes/$FILE $NODE:/etc/kubernetes/$&amp;#123;FILE&amp;#125;
 done
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ca.pem                                                                                                                                                                         100% 1407   459.5KB/s   00:00    
…
bootstrap-kubelet.kubeconfig                                                                                                                                                   100% 2291   685.4KB/s   00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;82-kubelet配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#82-kubelet配置&#34;&gt;#&lt;/a&gt; 8.2 Kubelet 配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 Kubelet 配置目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 kubelet service&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim  /usr/lib/systemd/system/kubelet.service

[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kubelet

Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 kubelet service 的配置文件（也可以写到 kubelet.service）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Runtime为Containerd
# vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf

[Service]
Environment=&amp;quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig&amp;quot;
Environment=&amp;quot;KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock&amp;quot;
Environment=&amp;quot;KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml&amp;quot;
Environment=&amp;quot;KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node=&#39;&#39; &amp;quot;
ExecStart=
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 kubelet 的配置文件&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：如果更改了 k8s 的 service 网段，需要更改 kubelet-conf.yml 的 clusterDNS: 配置，改成 k8s Service 网段的第十个地址，比如 10.96.0.10&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# vim /etc/kubernetes/kubelet-conf.yml

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动&lt;mark&gt;所有节点&lt;/mark&gt; kubelet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;如果有很多报错日志，或者有大量看不懂的报错，说明 kubelet 的配置有误，需要检查 kubelet 配置&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Master01 查看集群状态 (Ready 或 NotReady 都正常)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 bootstrap]# kubectl get node
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;83-kube-proxy配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#83-kube-proxy配置&#34;&gt;#&lt;/a&gt; 8.3 kube-proxy 配置&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;注意，如果不是高可用集群，192.168.1.70:8443 改为 master01 的地址，8443 改为 apiserver 的端口，默认是 6443&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;生成 kube-proxy 的证书，以下操作只在&lt;mark&gt; Master01&lt;/mark&gt; 执行&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/pki
cfssl gencert \
   -ca=/etc/kubernetes/pki/ca.pem \
   -ca-key=/etc/kubernetes/pki/ca-key.pem \
   -config=ca-config.json \
   -profile=kubernetes \
   kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/pki/kube-proxy

kubectl config set-cluster kubernetes \
     --certificate-authority=/etc/kubernetes/pki/ca.pem \
     --embed-certs=true \
     --server=https://192.168.1.70:8443 \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config set-credentials system:kube-proxy \
     --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \
     --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \
     --embed-certs=true \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

kubectl config set-context system:kube-proxy@kubernetes \
     --cluster=kubernetes \
     --user=system:kube-proxy \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig


kubectl config use-context system:kube-proxy@kubernetes \
     --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 kubeconfig 发送至其他节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for NODE in k8s-master02 k8s-master03; do
     scp /etc/kubernetes/kube-proxy.kubeconfig  $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done

for NODE in k8s-node01 k8s-node02; do
     scp /etc/kubernetes/kube-proxy.kubeconfig $NODE:/etc/kubernetes/kube-proxy.kubeconfig
 done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;添加 kube-proxy 的配置和 service 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /usr/lib/systemd/system/kube-proxy.service

[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --v=2

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果更改了集群 Pod 的网段，需要更改 kube-proxy.yaml 的 clusterCIDR 为自己的 Pod 网段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/kubernetes/kube-proxy.yaml

apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  acceptContentTypes: &amp;quot;&amp;quot;
  burst: 10
  contentType: application/vnd.kubernetes.protobuf
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
  qps: 5
clusterCIDR: 172.16.0.0/16 
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
hostnameOverride: &amp;quot;&amp;quot;
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
ipvs:
  masqueradeAll: true
  minSyncPeriod: 5s
  scheduler: &amp;quot;rr&amp;quot;
  syncPeriod: 30s
kind: KubeProxyConfiguration
metricsBindAddress: 127.0.0.1:10249
mode: &amp;quot;ipvs&amp;quot;
nodePortAddresses: null
oomScoreAdj: -999
portRange: &amp;quot;&amp;quot;
udpIdleTimeout: 250ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# systemctl daemon-reload
[root@k8s-master01 k8s-ha-install]# systemctl enable --now kube-proxy
Created symlink /etc/systemd/system/multi-user.target.wants/kube-proxy.service → /usr/lib/systemd/system/kube-proxy.service.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时系统日志 /var/log/messages**** 显示只有如下两种信息为正常 ****，安装 calico 后即可恢复&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Unable to update cni config: no networks found in /etc/cni/net.d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2ZkVK&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2ZkVK.png&#34; alt=&#34;pE2ZkVK.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;9-calico组件的安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-calico组件的安装&#34;&gt;#&lt;/a&gt; 9. Calico 组件的安装&lt;/h4&gt;
&lt;p&gt;以下步骤只在 master01 执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更改 calico 的网段，主要需要将红色部分的网段，改为自己的 Pod 网段&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &amp;quot;s#POD_CIDR#172.16.0.0/16#g&amp;quot; calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;检查网段是自己的 Pod 网段， grep &amp;quot;IPV4POOL_CIDR&amp;quot; calico.yaml  -A 1&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看容器和节点状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 calico]# kubectl get po -n kube-system
NAME                                       READY   STATUS    RESTARTS      AGE
calico-kube-controllers-66686fdb54-mk2g6   1/1     Running   1 (20s ago)   85s
calico-node-8fxqp                          1/1     Running   0             85s
calico-node-8nkfl                          1/1     Running   0             86s
calico-node-pmpf4                          1/1     Running   0             86s
calico-node-vnlk7                          1/1     Running   0             86s
calico-node-xpchb                          1/1     Running   0             85s
calico-typha-67c6dc57d6-259t8              1/1     Running   0             86s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;如果容器状态异常可以使用 kubectl describe 或者 kubectl logs 查看容器的日志&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -n kube-system&lt;/li&gt;
&lt;li&gt;Kubectl logs -f POD_NAME -c upgrade-ipam -n kube-system&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;10-安装coredns&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10-安装coredns&#34;&gt;#&lt;/a&gt; 10. 安装 CoreDNS&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果更改了 k8s service 的网段需要将 coredns 的 serviceIP 改成 k8s service 网段的第十个 IP&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk &#39;&amp;#123;print $3&amp;#125;&#39;`0
sed -i &amp;quot;s#KUBEDNS_SERVICE_IP#$&amp;#123;COREDNS_SERVICE_IP&amp;#125;#g&amp;quot; CoreDNS/coredns.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装 coredns&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 k8s-ha-install]# kubectl  create -f CoreDNS/coredns.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11-metrics部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-metrics部署&#34;&gt;#&lt;/a&gt; 11. Metrics 部署&lt;/h4&gt;
&lt;p&gt;在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。&lt;/p&gt;
&lt;p&gt;以下操作均在&lt;mark&gt; master01 节点&lt;/mark&gt;执行，安装 metrics server:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/metrics-server
kubectl  create -f . 

serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;等待 metrics server 启动然后查看状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl  top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master01   231m         5%     1620Mi          42%       
k8s-master02   274m         6%     1203Mi          31%       
k8s-master03   202m         5%     1251Mi          32%       
k8s-node01     69m          1%     667Mi           17%       
k8s-node02     73m          1%     650Mi           16%
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果有如下报错，可以等待 10 分钟后，再次查看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-dashboard部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-dashboard部署&#34;&gt;#&lt;/a&gt; 12. Dashboard 部署&lt;/h4&gt;
&lt;h5 id=&#34;121-安装dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#121-安装dashboard&#34;&gt;#&lt;/a&gt; 12.1 安装 Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;122-登录dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#122-登录dashboard&#34;&gt;#&lt;/a&gt; 12.2 登录 dashboard&lt;/h5&gt;
&lt;p&gt;在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更改 dashboard 的 svc 为 NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看端口号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：&lt;/p&gt;
&lt;p&gt;访问 Dashboard：&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建登录 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;14-containerd配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-containerd配置镜像加速&#34;&gt;#&lt;/a&gt; 14. Containerd 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Containerd：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;15-docker配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#15-docker配置镜像加速&#34;&gt;#&lt;/a&gt; 15. Docker 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-10T12:58:40.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/2628187572.html</id>
        <title>MySQL运维DBA应用与实践</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/2628187572.html"/>
        <content type="html">&lt;h3 id=&#34;mysql运维dba应用与实践&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#mysql运维dba应用与实践&#34;&gt;#&lt;/a&gt; MySQL 运维 DBA 应用与实践&lt;/h3&gt;
&lt;h4 id=&#34;1日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1日志&#34;&gt;#&lt;/a&gt; 1. 日志&lt;/h4&gt;
&lt;p&gt;在任何一种数据库中，都会有各种各样的日志，这些日志记录了数据库运行的各个方面。可以帮助数据库管理员追踪数据库曾经发生的一些事情。&lt;/p&gt;
&lt;p&gt;对于 MySQL 数据库，提供了四种不同的日志帮助我们追踪。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;错误日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;二进制日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查询日志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;慢查询日志&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;11-错误日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-错误日志&#34;&gt;#&lt;/a&gt; 1.1 错误日志&lt;/h5&gt;
&lt;p&gt;错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld (MySQL 服务) 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，建议首先查看此日志。&lt;/p&gt;
&lt;p&gt;该日志是默认开启的，默认存放目录 /var/log/，默认的日志文件名为 mysqld.log。查看日志位置；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &#39;%log_error%&#39;;
+---------------------+---------------------+
| Variable_name       | Value               |
+---------------------+---------------------+
| binlog_error_action | ABORT_SERVER        |
| log_error           | /var/log/mysqld.log |
| log_error_verbosity | 3                   |
+---------------------+---------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-二进制日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-二进制日志&#34;&gt;#&lt;/a&gt; 1.2 二进制日志&lt;/h5&gt;
&lt;p&gt;二进制日志 (BINLOG) 记录了所有的 DDL (数据定义语言) 语句和 DML (数据操纵语言) 语句，但不包括数据查询（SELECT、 SHOW）语句。&lt;/p&gt;
&lt;p&gt;作用:&lt;/p&gt;
&lt;p&gt;①. 灾难时的数据恢复；&lt;/p&gt;
&lt;p&gt;②. MySQL 的主从复制。&lt;/p&gt;
&lt;p&gt;在 MySQL5.7 版本中，默认二进制日志是关闭着的，涉及到的参数如下:&lt;/p&gt;
&lt;h6 id=&#34;121-开启-bin-log记录&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#121-开启-bin-log记录&#34;&gt;#&lt;/a&gt; 1.2.1 开启 bin-log 记录&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;1.1改修配置文件
[root@db01 ~]# vim /etc/my.cnf
server-id=1
log-bin=mysql-bin
max_binlog_size=500M
expire_logs_days=15

1.2查看是否开启binlog.
mysql&amp;gt; show variables like &#39;log_%&#39;;
+----------------------------------------+--------------------------------+
| Variable_name                          | Value                          |
+----------------------------------------+--------------------------------+
| log_bin                                | ON                             |
| log_bin_basename                       | /var/lib/mysql/mysql-bin       |
| log_bin_index                          | /var/lib/mysql/mysql-bin.index |
| log_bin_trust_function_creators        | OFF                            |
| log_bin_use_v1_row_events              | OFF                            |
| log_builtin_as_identified_by_password  | OFF                            |
| log_error                              | /var/log/mysqld.log            |
| log_error_verbosity                    | 3                              |
| log_output                             | FILE                           |
| log_queries_not_using_indexes          | OFF                            |
| log_slave_updates                      | OFF                            |
| log_slow_admin_statements              | OFF                            |
| log_slow_slave_statements              | OFF                            |
| log_statements_unsafe_for_binlog       | ON                             |
| log_syslog                             | OFF                            |
| log_syslog_facility                    | daemon                         |
| log_syslog_include_pid                 | ON                             |
| log_syslog_tag                         |                                |
| log_throttle_queries_not_using_indexes | 0                              |
| log_timestamps                         | UTC                            |
| log_warnings                           | 2                              |
+----------------------------------------+--------------------------------+

1.3查看binlog
mysql&amp;gt; show binary logs;
+------------------+-----------+
| Log_name         | File_size |
+------------------+-----------+
| mysql-bin.000001 |     36825 |
| mysql-bin.000002 |    200464 |
| mysql-bin.000003 |    419809 |
+------------------+-----------+

1.4查看binlog日志保存天数 
# 0表示永久保留，expire_logs_days：保留指定日期范围内的binlog历史日志，上示例设置的15天内
mysql&amp;gt; show variables like &#39;expire_logs_days&#39;;
+------------------+-------+
| Variable_name    | Value |
+------------------+-------+
| expire_logs_days | 15    |
+------------------+-------+
1 row in set (0.00 sec)

1.5查看binlog日志保存大小
#max_binlog_size：bin log日志每达到设定大小后，会使用新的bin log日志。如mysql-bin.000002达到500M后，创建并使用mysql-bin.000003文件作为日志记录。
mysql&amp;gt; show variables like &#39;max_binlog_size&#39;;
+-----------------+-----------+
| Variable_name   | Value     |
+-----------------+-----------+
| max_binlog_size | 524288000 |
+-----------------+-----------+

1.6手动执行flush logs
#将会new一个新文件用于记录binlog
mysql&amp;gt; flush logs;

1.7手动清理binlog
#将mysql-bin.000010之前的日志清理掉
mysql&amp;gt; purge binary logs to &#39;mysql-bin.000010&#39;;
Query OK, 0 rows affected (0.01 sec)

#删除2022-04-21 18:08:00之前的binlog日志
mysql&amp;gt; purge binary logs before &#39;2022-04-21 18:08:00&#39;;

#清除全部binlog
mysql&amp;gt; reset master;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;122-日志格式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#122-日志格式&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1.2.2 日志格式&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;MySQL 服务器中提供了多种格式来记录二进制日志，具体格式及特点如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;日志格式&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;含义&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;STATEMENT&lt;/td&gt;
&lt;td&gt;基于 SQL 语句的日志记录，记录的是 SQL 语句，对数据进行修改的 SQL 都会记录在日志文件中。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ROW&lt;/td&gt;
&lt;td&gt;基于行的日志记录，记录的是每一行的数据变更。(默认)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MIXED&lt;/td&gt;
&lt;td&gt;混合了 STATEMENT 和 ROW 两种格式，默认采用 STATEMENT, 在某些特殊情况下会自动切换为 ROW 进行记录。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &#39;binlog_format&#39;;
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| binlog_format | ROW   |
+---------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 &lt;code&gt;mysqlbinlog&lt;/code&gt;  来查看，具体语法:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysqlbinlog [ 参数选项] logfilename
参数选项:
	-d			指定数据库名称，只列出指定的数据库相关操作。
	-o			忽略掉日志中的前n行命令。
	-v			将行事件(数据变更)重构为SQL语句
	-vv			将行事件(数据变更)重构为SQL语句，并输出注释信息
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; use zh;
Database changed
mysql&amp;gt; show tables;
+----------------+
| Tables_in_zh   |
+----------------+
| account        |
| course         |
| dept           |
| emp            |
| score          |
| student        |
| student_course |
| tb_user        |
| tb_user_edu    |
| user           |
| user1          |
+----------------+
11 rows in set (0.00 sec)

mysql&amp;gt;  update tb_user_edu set university = &amp;quot;北京大学&amp;quot;;
Query OK, 4 rows affected (0.00 sec)
Rows matched: 4  Changed: 4  Warnings: 0

#二进制日志查看
[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000001 
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;123-修改binlog格式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#123-修改binlog格式&#34;&gt;#&lt;/a&gt; 1.2.3 修改 binlog 格式&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# vim /etc/my.cnf
...
binlog_format=STATEMENT
...
[root@db01 ~]# systemctl restart mysqld

mysql&amp;gt;  update tb_user_edu set university = &#39;清华大学&#39;;
[root@db01 ~]# mysqlbinlog -v /var/lib/mysql/mysql-bin.000002 
...
SET TIMESTAMP=1701440373/*!*/;
update tb_user_edu set university = &#39;清华大学&#39;
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-查询日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-查询日志&#34;&gt;#&lt;/a&gt; 1.3 查询日志&lt;/h5&gt;
&lt;p&gt;查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的 SQL 语句。默认情况下，&lt;strong&gt;查询日志是未开启的&lt;/strong&gt;。如果需要开启查询日志，可以设置以下配置︰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like &#39;%general%&#39;;
+------------------+-------------------------+
| Variable_name    | Value                   |
+------------------+-------------------------+
| general_log      | OFF                     |
| general_log_file | /var/lib/mysql/db01.log |
+------------------+-------------------------+
2 rows in set (0.00 sec)

#开启查询日志功能
[root@db01 ~]# cat /etc/my.cnf
general_log=1
general_log_file=/var/lib/mysql/mysql_query.log 
[root@db01 ~]# systemctl restart mysqld

[root@db01 ~]# tail -f /var/lib/mysql/mysql_query.log 
2023-12-01T14:31:28.554384Z	    2 Field List	student 
2023-12-01T14:31:28.554743Z	    2 Field List	student_course 
2023-12-01T14:31:35.737041Z	    2 Query	show variables like &#39;%general%&#39;
2023-12-01T14:31:37.345179Z	    2 Query	show variables like &#39;%general%&#39;
2023-12-01T14:32:17.593471Z	    2 Query	SELECT DATABASE()
2023-12-01T14:32:17.593651Z	    2 Init DB	zh
2023-12-01T14:32:25.249258Z	    2 Query	select * from emp
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;14-慢查询日志&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#14-慢查询日志&#34;&gt;#&lt;/a&gt; 1.4 慢查询日志&lt;/h5&gt;
&lt;p&gt;慢查询&lt;a href=&#34;https://so.csdn.net/so/search?q=%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95&amp;amp;spm=1001.2101.3001.7020&#34;&gt;日志记录&lt;/a&gt;了所有执行时间超过参数 &lt;code&gt;long_ query_time&lt;/code&gt;  设置值并且扫描记录数不小于 &lt;code&gt;min_examined_row_limit&lt;/code&gt;  的所有的 SQL 语句的日志，默认未开启。&lt;strong&gt; &lt;code&gt;long_query_time&lt;/code&gt;  默认为 10 秒，最小为 0，精度可以到微秒。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# vim /etc/my.cnf
#慢查询日志
slow_query_log=on
##执行时间参数
long_query_time=2
# 若没有指定，默认名字为hostname_slow.log
slow_query_log_file = /var/lib/mysql/slow-query.log
[root@db01 ~]# systemctl restart mysqld

#制造慢查询并执行
mysql&amp;gt; select sleep(3);
[root@db01 ~]# tail -f /var/lib/mysql/slow-query.log 
/usr/sbin/mysqld, Version: 5.7.43-log (MySQL Community Server (GPL)). started with:
Tcp port: 0  Unix socket: /var/lib/mysql/mysql.sock
Time                 Id Command    Argument
# Time: 2023-12-01T14:47:57.763735Z
# User@Host: root[root] @ localhost []  Id:     2
# Query_time: 3.001229  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0
use zh;
SET timestamp=1701442077;
select sleep(3);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;默认情况下，不会记录管理语句，也不会记录不使用索引进行查找的查询。可以使用 &lt;code&gt;log_slow_admin_statements&lt;/code&gt;  和更改此行为 &lt;code&gt;log_queries_not_using_indexes&lt;/code&gt; , 如下所述。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#记录执行较慢的管理语句
log_slow_admin_statements = 1
#记录执行较慢的未使用索引的语句
log_queries_not_using_indexes = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-主从复制&#34;&gt;#&lt;/a&gt; 2. 主从复制&lt;/h4&gt;
&lt;h5 id=&#34;21-主从复制的概述&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-主从复制的概述&#34;&gt;#&lt;/a&gt; 2.1 主从复制的概述&lt;/h5&gt;
&lt;p&gt;主从复制是指将&lt;strong&gt;主数据库的 DDL 和 DML 操作&lt;/strong&gt;通过&lt;strong&gt;二进制日志&lt;/strong&gt;传到&lt;strong&gt;从库服务器&lt;/strong&gt;中，然后在从库上对这些日志重新执行 (也叫重做) ，从而使得从库和主库的数据保持同步。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgO0Mj&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgO0Mj.png&#34; alt=&#34;pEgO0Mj.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MySQL 支持一台主库同时向多台从库进行复制，从库同时也可以作为其他从服务器的主库， 实现链状复制。&lt;/p&gt;
&lt;p&gt;MySQL 复制的有点主要包含以下三个方面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;主库出现问题，可以快速切换到从库提供服务；&lt;/li&gt;
&lt;li&gt;实现读写分离，降低主库的访问压力；（如果增删改对主库 查询对从库）&lt;/li&gt;
&lt;li&gt;可以在从库中执行备份，以避免备份期间影响主库服务。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;22-主从复制的原理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-主从复制的原理&#34;&gt;#&lt;/a&gt; 2.2 主从复制的原理&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgOdzQ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgOdzQ.png&#34; alt=&#34;pEgOdzQ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;从上图来看，复制分成三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Master 主库在事务提交时，会把数据变更记录在二进制日志文件 Binlog 中。&lt;/li&gt;
&lt;li&gt;从库 IO 线程读取主库的二进制日志文件 Binlog，写入到从库的中继日志 Relay Log。&lt;/li&gt;
&lt;li&gt;slave 重做中继日志中的事件，SQL 线程将改变反映它自己的数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;23-主从复制的搭建&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-主从复制的搭建&#34;&gt;#&lt;/a&gt; 2.3 主从复制的搭建&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;主从复制的搭建步骤&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;准备主从复制服务器环境&lt;/li&gt;
&lt;li&gt;完成主库配置&lt;/li&gt;
&lt;li&gt;完成从库配置&lt;/li&gt;
&lt;/ol&gt;
&lt;h6 id=&#34;231-服务器准备&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#231-服务器准备&#34;&gt;#&lt;/a&gt; 2.3.1 服务器准备&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgODLn&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgODLn.png&#34; alt=&#34;pEgODLn.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;232-主库配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#232-主库配置&#34;&gt;#&lt;/a&gt; 2.3.2 主库配置&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;#1. 安装 MySQL&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1、关闭防火墙、selinux、环境配置
[root@db01 ~]# hostnamectl set-hostname db01
[root@db01 ~]# systemctl stop firewalld
[root@db01 ~]# systemctl disable firewalld
[root@db01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@db01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@db01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@db01 ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@db01 ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@db01 ~]# ntpdate time2.aliyun.com
[root@db01 ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@db01 ~]# mkdir /soft /data /scripts /backup

#2、安装Mysql5.7
[root@db01 ~]# yum install -y mysql-community-server
[root@db01 ~]# systemctl start mysqld &amp;amp;&amp;amp; systemctl enable mysqld

[root@db01 ~]# mysql -uroot -p$(awk &#39;/temporary password/&amp;#123;print $NF&amp;#125;&#39; /var/log/mysqld.log)
mysql&amp;gt; ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;passwd&#39;;
mysql&amp;gt; grant all on *.* to &#39;root&#39;@&#39;192.168.1.%&#39; identified by &#39;passwd&#39;;

#3、允许root用户在任何地方进行远程登录，并具有所有库任何操作权限，具体操作如下：
mysql -u root -p&amp;quot;youpass&amp;quot;
mysql&amp;gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;passwd&#39; WITH GRANT OPTION;
FLUSH PRIVILEGES;

#4.配置主库
[root@db01 ~]# vim /etc/my.cnf
server-id=1                #mysql服务ID，保证整个集群环境中唯一， 取值范围: 1 - 2^&amp;#123;32&amp;#125;-1
log-bin=mysql-bin          #启动二进制日志
read-only=0                #是否只读,1代表只读, 0代表读写
#binlog-ignore-db=mysql    #忽略的数据，指不需要同步的数据库
#binlog-do-db=db01         #指定同步的数据库
[root@db01 ~]# systemctl restart mysqld

#5.创建repl用户，并设置密码，该用户可在任意主机连接该MySQL服务
mysql&amp;gt; grant replication slave on *.* to &#39;repl&#39;@&#39;%&#39; identified by &#39;passwd&#39;;

#6.查看master位置点
mysql&amp;gt; show master status;        
+------------------+----------+--------------+------------------+-------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+------------------+----------+--------------+------------------+-------------------+
| mysql-bin.000006 |      889 |              |                  |                   |
+------------------+----------+--------------+------------------+-------------------+
1 row in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;233-从库配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#233-从库配置&#34;&gt;#&lt;/a&gt; 2.3.3 从库配置&lt;/h6&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数名&lt;/th&gt;
&lt;th&gt;含义&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;8.0.23 之前&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_HOST&lt;/td&gt;
&lt;td&gt;主库 IP 地址&lt;/td&gt;
&lt;td&gt;MASTER_HOST&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_USER&lt;/td&gt;
&lt;td&gt;连接主库的用户名&lt;/td&gt;
&lt;td&gt;MASTER_USER&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_PASSWORD&lt;/td&gt;
&lt;td&gt;连接主库的密码&lt;/td&gt;
&lt;td&gt;MASTER_PASSWORD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_LOG FILE&lt;/td&gt;
&lt;td&gt;binlog 日志文件名&lt;/td&gt;
&lt;td&gt;MASTER LOG_FILE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SOURCE_LOG POS&lt;/td&gt;
&lt;td&gt;binlog 日志文件位置&lt;/td&gt;
&lt;td&gt;MASTER_LOG_POS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;#1.配置从库
[root@db02 ~]# vim /etc/my.cnf
server-id=2           #mysql服务ID
read-only=1           #是否只读,1代表只读, 0代表读写
[root@db02 ~]# systemctl restart mysqld

#2..配置从服务器，连接主服务器
mysql&amp;gt; change master to master_host=&#39;192.168.40.150&#39;,master_user=&#39;repl&#39;,master_password=&#39;passwd&#39;,master_log_file=&#39;mysql-bin.000006&#39;,master_log_pos=889;

#3.开启从库
mysql&amp;gt; start slave;
Query OK, 0 rows affected (0.00 sec)

#4.检查主从复制状态
mysql&amp;gt; show slave status\G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 192.168.40.150
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000006
          Read_Master_Log_Pos: 889
               Relay_Log_File: db02-relay-bin.000002
                Relay_Log_Pos: 320
        Relay_Master_Log_File: mysql-bin.000006
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 889
              Relay_Log_Space: 526
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 1
                  Master_UUID: 9b911bea-43e6-11ee-b239-000c29074f5d
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind: 
      Last_IO_Error_Timestamp: 
     Last_SQL_Error_Timestamp: 
               Master_SSL_Crl: 
           Master_SSL_Crlpath: 
           Retrieved_Gtid_Set: 
            Executed_Gtid_Set: 
                Auto_Position: 0
         Replicate_Rewrite_DB: 
                 Channel_Name: 
           Master_TLS_Version: 
1 row in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-分库分表&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-分库分表&#34;&gt;#&lt;/a&gt; 3. &lt;a href=&#34;https://so.csdn.net/so/search?q=%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8&amp;amp;spm=1001.2101.3001.7020&#34;&gt;分库分表&lt;/a&gt;&lt;/h4&gt;
&lt;h5 id=&#34;31-分库分表介绍&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-分库分表介绍&#34;&gt;#&lt;/a&gt; 3.1 分库分表介绍&lt;/h5&gt;
&lt;h6 id=&#34;311-现在的问题&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#311-现在的问题&#34;&gt;#&lt;/a&gt; 3.1.1 现在的问题&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;单数据库&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所有数据都是存放在一个&lt;a href=&#34;https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%BA%93%E6%96%87%E4%BB%B6&amp;amp;spm=1001.2101.3001.7020&#34;&gt;数据库文件&lt;/a&gt;里的，经过常年累月，内存不足了怎么办？&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgOyd0&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgOyd0.png&#34; alt=&#34;pEgOyd0.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;随着互联网及移动互联网的发展，应用系统的数据量也是成指数式增长，若采用单数据库进行数据存储，存在以下性能瓶颈：&lt;/p&gt;
&lt;p&gt;IO 瓶颈：热点数据太多，数据库缓存不足，产生大量磁盘 IO，效率较低。请求数据太多，带宽不够，网络 IO 瓶颈。&lt;br /&gt;
CPU 瓶颈： 排序、分组、连接查询、聚合统计等 SQL 会耗费大量的 CPU 资源，请求数太多，CPU 出现瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgO6oV&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgO6oV.png&#34; alt=&#34;pEgO6oV.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分库分表的中心思想：&lt;br /&gt;
将数据分散存储，使得单一数据库 / 表的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的。&lt;/strong&gt;&lt;/p&gt;
&lt;h6 id=&#34;312-拆分策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#312-拆分策略&#34;&gt;#&lt;/a&gt; 3.1.2 拆分策略&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2dAXt&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2dAXt.png&#34; alt=&#34;pE2dAXt.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;313-垂直拆分策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#313-垂直拆分策略&#34;&gt;#&lt;/a&gt; 3.1.3 垂直拆分策略&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2dnAS&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2dnAS.png&#34; alt=&#34;pE2dnAS.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;特点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个库的表结构都不一样。&lt;/li&gt;
&lt;li&gt;每个库的数据也不一样 。&lt;/li&gt;
&lt;li&gt;所有，库的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2dQpj&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2dQpj.png&#34; alt=&#34;pE2dQpj.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;特点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个表的结构都不一样。&lt;/li&gt;
&lt;li&gt;每个表的数据也术一样，一般通过一列 (主键 / 外键) 关联。&lt;/li&gt;
&lt;li&gt;所有表的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h6 id=&#34;314-水平拆分策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#314-水平拆分策略&#34;&gt;#&lt;/a&gt; 3.1.4 水平拆分策略&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pE2d3Xq&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/10/pE2d3Xq.png&#34; alt=&#34;pE2d3Xq.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;水平分库：以 “字段” 为依据，改为以 “行（记录）” 为依据。讲一个库的数据拆分到多个库&lt;/p&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个库的表结构都一样。&lt;/li&gt;
&lt;li&gt;每个库的数据都不一样。&lt;/li&gt;
&lt;li&gt;所有库的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/8Vp5L6j.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个表的表结构都一样 。&lt;/li&gt;
&lt;li&gt;每个表的数据都不一样 。&lt;/li&gt;
&lt;li&gt;所有表的并集是全量数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/2ctPFwi.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;shardingJDBC：基于 AOP 原理，在应用程序中对本地执行的 SQL 进行拦截，解析、改写、路由处理。需要自行编码配置实现，只支持 java 语言，性能较高。&lt;/li&gt;
&lt;li&gt;MyCat：数据库分库分表中间件，不用调整代码即可实现分库分表，支持多种语言，性能不及前者。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/lgs1r8g.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;h5 id=&#34;32-mycat概述&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-mycat概述&#34;&gt;#&lt;/a&gt; 3.2 Mycat 概述&lt;/h5&gt;
&lt;p&gt;Mycat 是开源的、活跃的、基于 Java 语言编写的&lt;strong&gt; MySQL 数据库中间件&lt;/strong&gt;。可以像使用 mysql 一样来使用 mycat，对于开发人员来说根本感觉不到 mycat 的存在。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/KFB4gQ8.png&#34; alt=&#34;5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性能可靠稳定&lt;/li&gt;
&lt;li&gt;强大的技术团队&lt;/li&gt;
&lt;li&gt;体系完善&lt;/li&gt;
&lt;li&gt;社区活跃&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mycat 是采用 java 语言开发的开源的数据库中间件，支持 Windows 和 Linux 运行环境，下面介绍 MyCat 的 Linux 中的环境搭建。 我们需要在准备好的服务器中安装如下软件。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;服务器&lt;/th&gt;
&lt;th&gt;安装软件&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.213&lt;/td&gt;
&lt;td&gt;JDK、Mycat&lt;/td&gt;
&lt;td&gt;MyCat 中间件服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.210&lt;/td&gt;
&lt;td&gt;MySQL&lt;/td&gt;
&lt;td&gt;分片服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.211&lt;/td&gt;
&lt;td&gt;MySQL&lt;/td&gt;
&lt;td&gt;分片服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;192.168.40.212&lt;/td&gt;
&lt;td&gt;MySQL&lt;/td&gt;
&lt;td&gt;分片服务器&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;JDK 安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#解压jdk
[root@mycat ~]# tar xf jdk-8u371-linux-x64.tar.gz -C /usr/local
[root@mycat ~]# ln -s /usr/local/jdk1.8.0_371/ /usr/local/jdk

# 添加环境变量
[root@mycat ~]# vim /etc/profile.d/jdk.sh 
export JAVA_HOME=/usr/local/jdk
export PATH=$PATH:$JAVA_HOME/bin
export JRE_HOME=$JAVA_HOME/jre 
export CLASSPATH=$JAVA_HOME/lib/:$JRE_HOME/lib/

[root@mycat ~]# source /etc/profile
[root@mycat ~]# java -version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mycat 安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# tar xf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz -C /usr/local/
[root@mycat ~]# ll /usr/local/mycat/
total 12
drwxr-xr-x 2 root root  190 Dec  2 22:15 bin
drwxrwxrwx 2 root root    6 Mar  1  2016 catlet
drwxrwxrwx 4 root root 4096 Dec  2 22:15 conf
drwxr-xr-x 2 root root 4096 Dec  2 22:15 lib
drwxrwxrwx 2 root root    6 Oct 28  2016 logs
-rwxrwxrwx 1 root root  217 Oct 28  2016 version.txt

#上传jar包
[root@mycat ~]# rz /usr/local/mycat/lib/mysql-connector-java-8.0.25.jar
[root@mycat lib]# chmod 777 mysql-connector-java-8.0.25.jar 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/n86yXtx.png&#34; alt=&#34;4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/U26clQE.png&#34; alt=&#34;6.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;321-mycat入门&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#321-mycat入门&#34;&gt;#&lt;/a&gt; 3.2.1 Mycat 入门&lt;/h6&gt;
&lt;p&gt;由于 tb_gorder 表中数据量很大，磁盘 IO 及容量都到达了瓶颈，现在需要对 tb_order 表进行数据分片，分为三个数据节点，每一个节点主机位于不同的服务器上，具体的结构，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/YjmWPQf.png&#34; alt=&#34;5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/PQqJdjJ.png&#34; alt=&#34;8.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;322-mycat配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#322-mycat配置&#34;&gt;#&lt;/a&gt; 3.2.2 Mycat 配置&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/iXUxPhi.png&#34; alt=&#34;9.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;DB01&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;TB_ORDER&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; rule=&amp;quot;auto-sharding-long&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;db01&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;db01&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;db01&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/KkUttwJ.png&#34; alt=&#34;10.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat mycat]# cat /usr/local/mycat/conf/server.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;!-- - - Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;); 
	- you may not use this file except in compliance with the License. - You 
	may obtain a copy of the License at - - http://www.apache.org/licenses/LICENSE-2.0 
	- - Unless required by applicable law or agreed to in writing, software - 
	distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS, - WITHOUT 
	WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. - See the 
	License for the specific language governing permissions and - limitations 
	under the License. --&amp;gt;
&amp;lt;!DOCTYPE mycat:server SYSTEM &amp;quot;server.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:server xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;system&amp;gt;
	&amp;lt;property name=&amp;quot;useSqlStat&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;  &amp;lt;!-- 1为开启实时统计、0为关闭 --&amp;gt;
	&amp;lt;property name=&amp;quot;useGlobleTableCheck&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;  &amp;lt;!-- 1为开启全加班一致性检测、0为关闭 --&amp;gt;

		&amp;lt;property name=&amp;quot;sequnceHandlerType&amp;quot;&amp;gt;2&amp;lt;/property&amp;gt;
      &amp;lt;!--  &amp;lt;property name=&amp;quot;useCompression&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;--&amp;gt; &amp;lt;!--1为开启mysql压缩协议--&amp;gt;
        &amp;lt;!--  &amp;lt;property name=&amp;quot;fakeMySQLVersion&amp;quot;&amp;gt;5.6.20&amp;lt;/property&amp;gt;--&amp;gt; &amp;lt;!--设置模拟的MySQL版本号--&amp;gt;
	&amp;lt;!-- &amp;lt;property name=&amp;quot;processorBufferChunk&amp;quot;&amp;gt;40960&amp;lt;/property&amp;gt; --&amp;gt;
	&amp;lt;!-- 
	&amp;lt;property name=&amp;quot;processors&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt; 
	&amp;lt;property name=&amp;quot;processorExecutor&amp;quot;&amp;gt;32&amp;lt;/property&amp;gt; 
	 --&amp;gt;
		&amp;lt;!--默认为type 0: DirectByteBufferPool | type 1 ByteBufferArena--&amp;gt;
		&amp;lt;property name=&amp;quot;processorBufferPoolType&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;
		&amp;lt;!--默认是65535 64K 用于sql解析时最大文本长度 --&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;maxStringLiteralLength&amp;quot;&amp;gt;65535&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;sequnceHandlerType&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;backSocketNoDelay&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;frontSocketNoDelay&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--&amp;lt;property name=&amp;quot;processorExecutor&amp;quot;&amp;gt;16&amp;lt;/property&amp;gt;--&amp;gt;
		&amp;lt;!--
			&amp;lt;property name=&amp;quot;serverPort&amp;quot;&amp;gt;8066&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;managerPort&amp;quot;&amp;gt;9066&amp;lt;/property&amp;gt; 
			&amp;lt;property name=&amp;quot;idleTimeout&amp;quot;&amp;gt;300000&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;bindIp&amp;quot;&amp;gt;0.0.0.0&amp;lt;/property&amp;gt; 
			&amp;lt;property name=&amp;quot;frontWriteQueueSize&amp;quot;&amp;gt;4096&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;processors&amp;quot;&amp;gt;32&amp;lt;/property&amp;gt; --&amp;gt;
		&amp;lt;!--分布式事务开关，0为不过滤分布式事务，1为过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤），2为不过滤分布式事务,但是记录分布式事务日志--&amp;gt;
		&amp;lt;property name=&amp;quot;handleDistributedTransactions&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;
		
			&amp;lt;!--
			off heap for merge/order/group/limit      1开启   0关闭
		--&amp;gt;
		&amp;lt;property name=&amp;quot;useOffHeapForMerge&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt;

		&amp;lt;!--
			单位为m
		--&amp;gt;
		&amp;lt;property name=&amp;quot;memoryPageSize&amp;quot;&amp;gt;1m&amp;lt;/property&amp;gt;

		&amp;lt;!--
			单位为k
		--&amp;gt;
		&amp;lt;property name=&amp;quot;spillsFileBufferSize&amp;quot;&amp;gt;1k&amp;lt;/property&amp;gt;

		&amp;lt;property name=&amp;quot;useStreamOutput&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;

		&amp;lt;!--
			单位为m
		--&amp;gt;
		&amp;lt;property name=&amp;quot;systemReserveMemorySize&amp;quot;&amp;gt;384m&amp;lt;/property&amp;gt;


		&amp;lt;!--是否采用zookeeper协调切换  --&amp;gt;
		&amp;lt;property name=&amp;quot;useZKSwitch&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;


	&amp;lt;/system&amp;gt;
	
	&amp;lt;!-- 全局SQL防火墙设置 --&amp;gt;
	&amp;lt;!-- 
	&amp;lt;firewall&amp;gt; 
	   &amp;lt;whitehost&amp;gt;
	      &amp;lt;host host=&amp;quot;127.0.0.1&amp;quot; user=&amp;quot;mycat&amp;quot;/&amp;gt;
	      &amp;lt;host host=&amp;quot;127.0.0.2&amp;quot; user=&amp;quot;mycat&amp;quot;/&amp;gt;
	   &amp;lt;/whitehost&amp;gt;
       &amp;lt;blacklist check=&amp;quot;false&amp;quot;&amp;gt;
       &amp;lt;/blacklist&amp;gt;
	&amp;lt;/firewall&amp;gt;
	--&amp;gt;
	
	&amp;lt;user name=&amp;quot;root&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;DB01&amp;lt;/property&amp;gt;
		
		&amp;lt;!-- 表级 DML 权限设置 --&amp;gt;
		&amp;lt;!-- 		
		&amp;lt;privileges check=&amp;quot;false&amp;quot;&amp;gt;
			&amp;lt;schema name=&amp;quot;TESTDB&amp;quot; dml=&amp;quot;0110&amp;quot; &amp;gt;
				&amp;lt;table name=&amp;quot;tb01&amp;quot; dml=&amp;quot;0000&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
				&amp;lt;table name=&amp;quot;tb02&amp;quot; dml=&amp;quot;1111&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
			&amp;lt;/schema&amp;gt;
		&amp;lt;/privileges&amp;gt;		
		 --&amp;gt;
	&amp;lt;/user&amp;gt;

	&amp;lt;user name=&amp;quot;user&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;DB01&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;readOnly&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;
	&amp;lt;/user&amp;gt;

&amp;lt;/mycat:server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;323-mycat启动&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#323-mycat启动&#34;&gt;#&lt;/a&gt; 3.2.3 Mycat 启动&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;#1.启动mycat
[root@mycat mycat]# ./bin/mycat restart

#2.wrapper.log日志中常见错误
ERROR | wrapper | 2021/1/10 13:31:05 | Startup failed: Timed out waiting for signal from JVM.
ERROR | wrapper | 2021/1/10 13:31:05 | JVM did not exit on request, terminated

#3.启动Mycat超时,前往wrapper.conf配置超时策略
[root@mycat mycat]# vim /usr/local/mycat/conf/wrapper.conf
...
wrapper.startup.timeout=300     //添加此行，超时时间300秒
wrapper.ping.timeout=120

#4.查看mycat是否启动
[root@mycat mycat]# tail -f logs/wrapper.log
...
INFO   | jvm 1    | 2023/12/02 22:53:44 | MyCAT Server startup successfully. see logs in logs/mycat.log
[root@mycat mycat]# netstat -lntp|grep 8066
tcp6       0      0 :::8066                 :::*                    LISTEN      18028/java
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;324-分片测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#324-分片测试&#34;&gt;#&lt;/a&gt; 3.2.4 分片测试&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@db3 ~]#  mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3
Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| DB01     |
+----------+
1 row in set (0.00 sec)

mysql&amp;gt; use DB01;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&amp;gt; show tables;
+----------------+
| Tables in DB01 |
+----------------+
| tb_order       |
+----------------+
1 row in set (0.00 sec)
mysql&amp;gt; CREATE TABLE TB_ORDER(
    -&amp;gt; id BIGINT(20) NOT NULL,
    -&amp;gt; title VARCHAR(100) NOT NULL,
    -&amp;gt; PRIMARY KEY (id)
    -&amp;gt; )ENGINE=INNODB DEFAULT CHARSET=utf8;
Query OK, 0 rows affected (0.04 sec)
 OK!
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(1,&#39;guods1&#39;);
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(2,&#39;guods2&#39;);
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(3,&#39;guods3&#39;);
mysql&amp;gt;INSERT INTO TB_ORDER(id,title) VALUES(4,&#39;guods4&#39;);
mysql&amp;gt; select * from TB_ORDER;
+------+--------+
| id   | title  |
+------+--------+
|    1 | guods1 |
|    2 | guods2 |
|    3 | guods3 |
|    4 | guods4 |
+------+--------+
4 rows in set (0.03 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;数据写入到 db1 中，因为 mycat 分片规则为 0-50000000 存入节点 1,5000001-10000000 存入节点 2,10000001-15000000 存入节点 3，15000001 以上无法插入数据，需要增加数据节点。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat mycat]# vim conf/rule.xml
...
        &amp;lt;tableRule name=&amp;quot;auto-sharding-long&amp;quot;&amp;gt;
                &amp;lt;rule&amp;gt;
                        &amp;lt;columns&amp;gt;id&amp;lt;/columns&amp;gt;
                        &amp;lt;algorithm&amp;gt;rang-long&amp;lt;/algorithm&amp;gt;
                &amp;lt;/rule&amp;gt;
        &amp;lt;/tableRule&amp;gt;

....
       &amp;lt;function name=&amp;quot;rang-long&amp;quot;
                class=&amp;quot;io.mycat.route.function.AutoPartitionByLong&amp;quot;&amp;gt;
                &amp;lt;property name=&amp;quot;mapFile&amp;quot;&amp;gt;autopartition-long.txt&amp;lt;/property&amp;gt;
        &amp;lt;/function&amp;gt;

...

[root@mycat mycat]# cat conf/autopartition-long.txt
# range start-end ,data node index
# K=1000,M=10000.
0-500M=0
500M-1000M=1

#5000001-10000000存入节点2 
mysql&amp;gt; INSERT INTO TB_ORDER(id,title) VALUES(5000001,&#39;guods5000001&#39;);
Query OK, 1 row affected (0.01 sec)
 OK!
 
#10000001-15000000存入节点3 
mysql&amp;gt; INSERT INTO TB_ORDER(id,title) VALUES(10000001,&#39;guods10000001&#39;);
Query OK, 1 row affected (0.00 sec)
 OK!

#15000001以上无法插入数据，需要增加数据节点
mysql&amp;gt; INSERT INTO TB_ORDER(id,title) VALUES(15000001,&#39;guods15000001&#39;);
ERROR 1064 (HY000): can&#39;t find any valid datanode :TB_ORDER -&amp;gt; ID -&amp;gt; 15000001
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-mycat配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-mycat配置&#34;&gt;#&lt;/a&gt; 3.3 Mycat 配置&lt;/h5&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/I9QLBBR.png&#34; alt=&#34;11.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;331-schema标签&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#331-schema标签&#34;&gt;#&lt;/a&gt; 3.3.1 Schema 标签&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/TmYK7fP.png&#34; alt=&#34;13.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;schema 标签用于定义 MyCat 实例中的逻辑库，一个 MyCat 实例中，可以有多个逻辑库，可以通过 schema 标签来划分不同的逻辑库。MyCat 中的逻辑库的概念，等同于 MySQL 中的 database 概念，需要操作某个逻辑库下的表时也需要切换逻辑库 (use xxx)。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/SGo0DCv.png&#34; alt=&#34;14.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/XtDxXWj.png&#34; alt=&#34;15.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;332-datanode标签&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#332-datanode标签&#34;&gt;#&lt;/a&gt; 3.3.2 Datanode 标签&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/phHZ48F.png&#34; alt=&#34;16.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;333-datahost标签&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#333-datahost标签&#34;&gt;#&lt;/a&gt; 3.3.3 Datahost 标签&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/fXBnwcS.png&#34; alt=&#34;17.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;334-rulexml&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#334-rulexml&#34;&gt;#&lt;/a&gt; 3.3.4 rule.xml&lt;/h6&gt;
&lt;p&gt;rule.xml 中定义所有拆分表的规则，在使用过程中可以灵活的使用分片算法，或者对同一个分片算法使用不同的参数，它让分片过程可配置化。主要包含两类标签： &lt;code&gt;tableRule&lt;/code&gt; 、 &lt;code&gt;Function&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/Ecm1Nvr.png&#34; alt=&#34;18.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;335-serverxml&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#335-serverxml&#34;&gt;#&lt;/a&gt; 3.3.5 server.xml&lt;/h6&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/xIDxYpu.png&#34; alt=&#34;19.png&#34; /&gt;&lt;/p&gt;
&lt;h5 id=&#34;34-mycat分片&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-mycat分片&#34;&gt;#&lt;/a&gt; 3.4 Mycat 分片&lt;/h5&gt;
&lt;h6 id=&#34;341-分库分表-mycat分片-垂直分库&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#341-分库分表-mycat分片-垂直分库&#34;&gt;#&lt;/a&gt; 3.4.1 分库分表 - MyCat 分片 - 垂直分库&lt;/h6&gt;
&lt;p&gt;场景：在业务系统中，涉及以下表结构，但是由于用户与订单每天都会产生大量的数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分，原有的数据库表如下。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/plSdAyY.png&#34; alt=&#34;20.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ps: 分库不需要指定 rule，涉及分表需要使用 rule；&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。&lt;br /&gt;
②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。&lt;br /&gt;
③三台服务器关闭防火墙或者开放对应的端口。&lt;br /&gt;
④分别在三台 MySQL 中创建数据库 shopping。&lt;br /&gt;
&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/uMZB18q.png&#34; alt=&#34;21.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;schema.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;server.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/server.xml 
...
	&amp;lt;user name=&amp;quot;root&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING&amp;lt;/property&amp;gt;
		
		&amp;lt;!-- 表级 DML 权限设置 --&amp;gt;
		&amp;lt;!-- 		
		&amp;lt;privileges check=&amp;quot;false&amp;quot;&amp;gt;
			&amp;lt;schema name=&amp;quot;TESTDB&amp;quot; dml=&amp;quot;0110&amp;quot; &amp;gt;
				&amp;lt;table name=&amp;quot;tb01&amp;quot; dml=&amp;quot;0000&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
				&amp;lt;table name=&amp;quot;tb02&amp;quot; dml=&amp;quot;1111&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
			&amp;lt;/schema&amp;gt;
		&amp;lt;/privileges&amp;gt;		
		 --&amp;gt;
	&amp;lt;/user&amp;gt;

	&amp;lt;user name=&amp;quot;user&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;readOnly&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;
	&amp;lt;/user&amp;gt;

&amp;lt;/mycat:server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;分库分表 - MyCat 分片 - 垂直分库 - 测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;垂直分库 - 测试&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.重启mycat
[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...
[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log 
...
INFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log
create database shopping default charset utf8mb4;

#2.在3台节点创建shopping数据库
mysql&amp;gt; create database shopping default charset utf8mb4;
mysql&amp;gt; create database shopping default charset utf8mb4;
mysql&amp;gt; create database shopping default charset utf8mb4;

#3.登入mycat
[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| SHOPPING |
+----------+
1 row in set (0.01 sec)

#4.查看逻辑库
mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| SHOPPING |
+----------+
1 row in set (0.01 sec)

#5.切换到SHOPPING数据库
mysql&amp;gt; use SHOPPING;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed

#6.查看逻辑表
mysql&amp;gt; show tables;
+--------------------+
| Tables in SHOPPING |
+--------------------+
| tb_areas_city      |
| tb_areas_provinces |
| tb_areas_region    |
| tb_goods_base      |
| tb_goods_brand     |
| tb_goods_cat       |
| tb_goods_desc      |
| tb_goods_item      |
| tb_order_item      |
| tb_order_master    |
| tb_order_pay_log   |
| tb_user            |
| tb_user_address    |
+--------------------+
13 rows in set (0.00 sec)

#7.上传shopping-table.sql表结构文件与shopping-insert.sql数据文件

#8.执行shopping-table.sql文件
mysql&amp;gt; source /root/shopping-table.sql

#9.执行shopping-insert.sql文件
mysql&amp;gt; source /root/shopping-insert.sql

#10.查看三个数据库可以发现（根据schema.xml配置文件的配置进行了实现）
①192.168.40.210的数据库中存放了 tb_goods_base、tb_goods_brand、tb_goods_cat、tb_goods_desc、tb_goods_item这五张表
②192.168.40.211的数据库中存放了 tb_order_item、tb_order_master、tb_order_pay_log这三张表；
③192.168.40.212的数据库中存放了 tb_user、tb_user_address、tb_areas_provinces、tb_areas_city、tb_areas_region这五张表
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;exam1: 查询用户的收件人及收件人地址信息 (包含省、市、区)。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;
+-----------+-----------+-----------+-----------+-----------+--------------------+
| user_id   | contact   | province  | city      | area      | address            |
+-----------+-----------+-----------+-----------+-----------+--------------------+
| deng      | 叶问      | 北京市    | 市辖区    | 西城区    | 咏春武馆总部       |
| java00001 | 李佳红    | 北京市    | 市辖区    | 崇文区    | 修正大厦           |
| deng      | 李小龙    | 北京市    | 市辖区    | 崇文区    | 永春武馆           |
| zhaoliu   | 赵三      | 北京市    | 市辖区    | 宣武区    | 西直门             |
| java00001 | 李嘉诚    | 北京市    | 市辖区    | 朝阳区    | 金燕龙办公楼       |
| java00001 | 李佳星    | 北京市    | 市辖区    | 朝阳区    | 中腾大厦           |
+-----------+-----------+-----------+-----------+-----------+--------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ps: 此查询语句只涉及了一个分片所以查询成功&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;exam2: 查询每一笔订单及订单的收件地址信息 (包含省、市、区)。&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;
ERROR 1064 (HY000): invalid route in sql, multi tables found but datanode has no intersection  sql:SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ps: 此查询语句涉及多个分片所以查询报错，为了解决这个问题需要进行全局表配置&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全局表配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于省、市、区 / 县表 tb_areas_provinces，tb_areas_city，tb_areas_region，是属于数据字典表，在多个业务模块中都可能会遇到，可以将其设置为全局表，利于业务操作。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/EqJJ3Yv.png&#34; alt=&#34;22.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 修改 MyCat—schema.xml 文件配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;schema.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;

		&amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot;/&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot;/&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. 全局表测试&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.删除3个节点上原有表

#2.重启mycat
[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...
[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log 
...
INFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log
create database shopping default charset utf8mb4;

#3.执行shopping-table.sql文件
[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql&amp;gt; source /root/shopping-table.sql

#4.执行shopping-insert.sql文件
mysql&amp;gt; source /root/shopping-insert.sql

#5 exam1:查询用户的收件人及收件人地址信息(包含省、市、区)。
mysql&amp;gt; select ua.user_id,ua.contact,p.province,c.city,r.area,ua.address from tb_user_address ua,tb_areas_city c,tb_areas_provinces p,tb_areas_region r where ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id = r.areaid;

#6 exam2:查询每一笔订单及订单的收件地址信息(包含省、市、区)
mysql&amp;gt; SELECT order_id,payment,receiver,province,city,area FROM tb_order_master o,tb_areas_provinces p,tb_areas_city c,tb_areas_region r WHERE o.receiver_province = p.provinceid AND o.receiver_city = c.cityid AND o.receiver_region = r.areaid;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;342-分库分表-mycat分片-水平分表&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#342-分库分表-mycat分片-水平分表&#34;&gt;#&lt;/a&gt; 3.4.2 分库分表 - MyCat 分片 - 水平分表&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;水平分表&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;场景&lt;/strong&gt;：在业务系统中，有一张表（日志表），业务系统每天都会产生大量的日志数据，单台服务器的数据存储及处理能力是有限的，可以对数据库表进行拆分。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/0kMP4Ru.png&#34; alt=&#34;23.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;准备环境：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;①如图所示准备三台 Linux 服务器（ip 为：192.168.40.210、192.168.40.211、192.168.40.212）可以根据自己的实际情况进行准备。&lt;br /&gt;
②三台服务器上都安装 MySQL，在 192.168.40.213 服务器上安装 MyCat。&lt;br /&gt;
③三台服务器关闭防火墙或者开放对应的端口。&lt;br /&gt;
④分别在三台 MySQL 中创建数据库 itcast。&lt;br /&gt;
&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/zTm8XwU.png&#34; alt=&#34;24.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 三台 MySQL 中创建数据库 itcast&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mysql&amp;gt; create database itcast default charset utf8mb4;
mysql&amp;gt; create database itcast default charset utf8mb4;
mysql&amp;gt; create database itcast default charset utf8mb4;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.MyCat—server.xml 文件配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;

                &amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;

        &amp;lt;schema name=&amp;quot;ITCAST&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
        	&amp;lt;table name=&amp;quot;tb_log&amp;quot; dataNode=&amp;quot;dn4,dn5,dn6&amp;quot; primaryKey=&amp;quot;id&amp;quot; rule=&amp;quot;mod-long&amp;quot; /&amp;gt;
        &amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;

	&amp;lt;dataNode name=&amp;quot;dn4&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn5&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn6&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3.MyCat—server.xml 文件配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.xml 文件配置如下：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/server.xml 
...
	&amp;lt;user name=&amp;quot;root&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING,ITCAST&amp;lt;/property&amp;gt;
		
		&amp;lt;!-- 表级 DML 权限设置 --&amp;gt;
		&amp;lt;!-- 		
		&amp;lt;privileges check=&amp;quot;false&amp;quot;&amp;gt;
			&amp;lt;schema name=&amp;quot;TESTDB&amp;quot; dml=&amp;quot;0110&amp;quot; &amp;gt;
				&amp;lt;table name=&amp;quot;tb01&amp;quot; dml=&amp;quot;0000&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
				&amp;lt;table name=&amp;quot;tb02&amp;quot; dml=&amp;quot;1111&amp;quot;&amp;gt;&amp;lt;/table&amp;gt;
			&amp;lt;/schema&amp;gt;
		&amp;lt;/privileges&amp;gt;		
		 --&amp;gt;
	&amp;lt;/user&amp;gt;

	&amp;lt;user name=&amp;quot;user&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;password&amp;quot;&amp;gt;Superman*2023&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;schemas&amp;quot;&amp;gt;SHOPPING,ITCAST&amp;lt;/property&amp;gt;
		&amp;lt;property name=&amp;quot;readOnly&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;
	&amp;lt;/user&amp;gt;

&amp;lt;/mycat:server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;4.MyCat 启动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#1.重启mycat
[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...
[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log 
...
INFO   | jvm 1    | 2023/12/03 15:29:02 | MyCAT Server startup successfully. see logs in logs/mycat.log
create database shopping default charset utf8mb4;

#2.登入mycat
[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| ITCAST   |
| SHOPPING |
+----------+
2 rows in set (0.00 sec)

mysql&amp;gt; use ITCAST;
mysql&amp;gt; show tables;
+------------------+
| Tables in ITCAST |
+------------------+
| tb_log           |
+------------------+

#3.创建表结构及数据导入
mysql&amp;gt; CREATE TABLE tb_log (
    -&amp;gt;   id bigint(20) NOT NULL COMMENT &#39;ID&#39;,
    -&amp;gt;   model_name varchar(200) DEFAULT NULL COMMENT &#39;模块名&#39;,
    -&amp;gt;   model_value varchar(200) DEFAULT NULL COMMENT &#39;模块值&#39;,
    -&amp;gt;   return_value varchar(200) DEFAULT NULL COMMENT &#39;返回值&#39;,
    -&amp;gt;   return_class varchar(200) DEFAULT NULL COMMENT &#39;返回值类型&#39;,
    -&amp;gt;   operate_user varchar(20) DEFAULT NULL COMMENT &#39;操作用户&#39;,
    -&amp;gt;   operate_time varchar(20) DEFAULT NULL COMMENT &#39;操作时间&#39;,
    -&amp;gt;   param_and_value varchar(500) DEFAULT NULL COMMENT &#39;请求参数名及参数值&#39;,
    -&amp;gt;   operate_class varchar(200) DEFAULT NULL COMMENT &#39;操作类&#39;,
    -&amp;gt;   operate_method varchar(200) DEFAULT NULL COMMENT &#39;操作方法&#39;,
    -&amp;gt;   cost_time bigint(20) DEFAULT NULL COMMENT &#39;执行方法耗时, 单位 ms&#39;,
    -&amp;gt;   source int(1) DEFAULT NULL COMMENT &#39;来源 : 1 PC , 2 Android , 3 IOS&#39;,
    -&amp;gt;   PRIMARY KEY (id)
    -&amp;gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
Query OK, 0 rows affected (0.09 sec)
 OK!
查看三个数据库可以发现表和表结构都有了

#4.添加数据
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;1&#39;,&#39;user&#39;,&#39;insert&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:12:28&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;insert&#39;,&#39;10&#39;,1);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;2&#39;,&#39;user&#39;,&#39;insert&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:12:27&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;insert&#39;,&#39;23&#39;,1);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;3&#39;,&#39;user&#39;,&#39;update&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:16:45&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;update&#39;,&#39;34&#39;,1);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;4&#39;,&#39;user&#39;,&#39;update&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:16:45&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;20\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;Tom\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;1\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;update&#39;,&#39;13&#39;,2);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;5&#39;,&#39;user&#39;,&#39;insert&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:30:31&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;200\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;TomCat\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;0\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;insert&#39;,&#39;29&#39;,3);
INSERT INTO tb_log (id, model_name, model_value, return_value, return_class, operate_user, operate_time, param_and_value, operate_class, operate_method, cost_time，source) VALUES(&#39;6&#39;,&#39;user&#39;,&#39;find&#39;,&#39;success&#39;,&#39;java.lang.String&#39;,&#39;10001&#39;,&#39;2022-01-06 18:30:31&#39;,&#39;&amp;#123;\&amp;quot;age\&amp;quot;:\&amp;quot;200\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;TomCat\&amp;quot;,\&amp;quot;gender\&amp;quot;:\&amp;quot;0\&amp;quot;&amp;#125;&#39;,&#39;cn.itcast.controller.UserController&#39;,&#39;find&#39;,&#39;29&#39;,2);

查看三个数据库内的tb_log表发现有数据了，数据的分布规则是 id模以3的结果为0的数据分布在第一个节点，id模以3的结果为1的数据分布在第二个节点，id模以3的结果为2的数据分布在第三个节点
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;33-分库分表-分片规则&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-分库分表-分片规则&#34;&gt;#&lt;/a&gt; 3.3 分库分表 - 分片规则&lt;/h5&gt;
&lt;h6 id=&#34;331-分库分表-分片规则-范围分片&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#331-分库分表-分片规则-范围分片&#34;&gt;#&lt;/a&gt; 3.3.1 分库分表 - 分片规则 - 范围分片&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;范围分片&lt;/strong&gt;：根据指定的字段及其配置的范围与数据节点的对应情况，来决定该数据属于哪一个分片。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/sdd8bvs.png&#34; alt=&#34;25.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/R3ecZ4k.png&#34; alt=&#34;26.png&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/autopartition-long.txt
# range start-end ,data node index
# K=1000,M=10000.
0-500M=0
500M-1000M=1
1000M-1500M=2
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;332-分库分表-分片规则-取模分片&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#332-分库分表-分片规则-取模分片&#34;&gt;#&lt;/a&gt; 3.3.2 分库分表 - 分片规则 - 取模分片&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;取模分片&lt;/strong&gt;：根据指定的字段值与节点数量进行求模运算，根据运算结果，来决定该数据属于哪一个分片。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/Xvn6sHi.png&#34; alt=&#34;1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/aaey4H2.png&#34; alt=&#34;2.png&#34; /&gt;&lt;/p&gt;
&lt;h6 id=&#34;333-分库分表-分片规则-一致性hash算法&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#333-分库分表-分片规则-一致性hash算法&#34;&gt;#&lt;/a&gt; 3.3.3 分库分表 - 分片规则 - 一致性 hash 算法&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;一致性 hash 算法&lt;/strong&gt;：所谓一致性哈希，相同的哈希因子计算值总是被划分到相同的分区表中，不会因为分区节点的增加而改变原来数据的分区位置。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/6ANYtsD.png&#34; alt=&#34;3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://wp-cdn.4ce.cn/v2/8i8c5Le.png&#34; alt=&#34;4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一致性 hash 测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;schema.xml 配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml 
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE mycat:schema SYSTEM &amp;quot;schema.dtd&amp;quot;&amp;gt;
&amp;lt;mycat:schema xmlns:mycat=&amp;quot;http://io.mycat/&amp;quot;&amp;gt;
	&amp;lt;schema name=&amp;quot;SHOPPING&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_base&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_brand&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_cat&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_desc&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_goods_item&amp;quot; dataNode=&amp;quot;dn1&amp;quot; primaryKey=&amp;quot;goods_id&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_order_item&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_master&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;order_id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_order_pay_log&amp;quot; dataNode=&amp;quot;dn2&amp;quot; primaryKey=&amp;quot;out_trade_no&amp;quot; /&amp;gt;
		
		&amp;lt;table name=&amp;quot;tb_user&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_user_address&amp;quot; dataNode=&amp;quot;dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; /&amp;gt;

                &amp;lt;table name=&amp;quot;tb_areas_provinces&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_city&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
		&amp;lt;table name=&amp;quot;tb_areas_region&amp;quot; dataNode=&amp;quot;dn1,dn2,dn3&amp;quot; primaryKey=&amp;quot;id&amp;quot; type=&amp;quot;global&amp;quot; /&amp;gt;
	&amp;lt;/schema&amp;gt;

        &amp;lt;schema name=&amp;quot;ITCAST&amp;quot; checkSQLschema=&amp;quot;true&amp;quot; sqlMaxLimit=&amp;quot;100&amp;quot;&amp;gt;
        	&amp;lt;table name=&amp;quot;tb_log&amp;quot; dataNode=&amp;quot;dn4,dn5,dn6&amp;quot; primaryKey=&amp;quot;id&amp;quot; rule=&amp;quot;mod-long&amp;quot; /&amp;gt;
        	&amp;lt;table name=&amp;quot;tb_order&amp;quot; dataNode=&amp;quot;dn4,dn5,dn6&amp;quot; primaryKey=&amp;quot;id&amp;quot; rule=&amp;quot;sharding-by-murmur&amp;quot; /&amp;gt;
        &amp;lt;/schema&amp;gt;
	
	&amp;lt;dataNode name=&amp;quot;dn1&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn2&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn3&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;shopping&amp;quot; /&amp;gt;

	&amp;lt;dataNode name=&amp;quot;dn4&amp;quot; dataHost=&amp;quot;dhost1&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn5&amp;quot; dataHost=&amp;quot;dhost2&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	&amp;lt;dataNode name=&amp;quot;dn6&amp;quot; dataHost=&amp;quot;dhost3&amp;quot; database=&amp;quot;itcast&amp;quot; /&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost1&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.210:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost2&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.211:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
	
	&amp;lt;dataHost name=&amp;quot;dhost3&amp;quot; maxCon=&amp;quot;1000&amp;quot; minCon=&amp;quot;10&amp;quot; balance=&amp;quot;0&amp;quot;
			  writeType=&amp;quot;0&amp;quot; dbType=&amp;quot;mysql&amp;quot; dbDriver=&amp;quot;jdbc&amp;quot; switchType=&amp;quot;1&amp;quot;  slaveThreshold=&amp;quot;100&amp;quot;&amp;gt;
		&amp;lt;heartbeat&amp;gt;select user()&amp;lt;/heartbeat&amp;gt;
		
		&amp;lt;writeHost host=&amp;quot;master&amp;quot; url=&amp;quot;jdbc:mysql://192.168.40.212:3306?useSSL=false&amp;amp;amp;serverTimezone=Asia/Shanghai&amp;amp;amp;characterEncoding=utf8&amp;quot; user=&amp;quot;root&amp;quot; password=&amp;quot;Superman*2023&amp;quot; /&amp;gt;
	&amp;lt;/dataHost&amp;gt;
&amp;lt;/mycat:schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;rule.xml 配置&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# cat /usr/local/mycat/conf/rule.xml 
...
	&amp;lt;function name=&amp;quot;murmur&amp;quot;
		class=&amp;quot;io.mycat.route.function.PartitionByMurmurHash&amp;quot;&amp;gt;
		&amp;lt;property name=&amp;quot;seed&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt;&amp;lt;!-- 默认是0 --&amp;gt;
		&amp;lt;property name=&amp;quot;count&amp;quot;&amp;gt;3&amp;lt;/property&amp;gt;&amp;lt;!-- 要分片的数据库节点数量，必须指定，否则没法分片 --&amp;gt;
		&amp;lt;property name=&amp;quot;virtualBucketTimes&amp;quot;&amp;gt;160&amp;lt;/property&amp;gt;&amp;lt;!-- 一个实际的数据库节点被映射为这么多虚拟节点，默认是160倍，也就是虚拟节点数是物理节点数的160倍 --&amp;gt;
		&amp;lt;!-- &amp;lt;property name=&amp;quot;weightMapFile&amp;quot;&amp;gt;weightMapFile&amp;lt;/property&amp;gt; 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写，以从0开始到count-1的整数值也就是节点索引为key，以节点权重值为值。所有权重值必须是正整数，否则以1代替 --&amp;gt;
		&amp;lt;!-- &amp;lt;property name=&amp;quot;bucketMapPath&amp;quot;&amp;gt;/etc/mycat/bucketMapPath&amp;lt;/property&amp;gt; 
			用于测试时观察各物理节点与虚拟节点的分布情况，如果指定了这个属性，会把虚拟节点的murmur hash值与物理节点的映射按行输出到这个文件，没有默认值，如果不指定，就不会输出任何东西 --&amp;gt;
	&amp;lt;/function&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;重启 mycat 并插入数据测试&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...

[root@mycat ~]# tail -f  /usr/local/mycat/logs/wrapper.log
...
INFO   | jvm 1    | 2023/12/03 22:17:47 | MyCAT Server startup successfully. see logs in logs/mycat.log

[root@db3 ~]# mysql -h 192.168.40.213 -P 8066 -uroot -p&#39;Superman*2023&#39;
Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)

mysql&amp;gt; show databases;
+----------+
| DATABASE |
+----------+
| ITCAST   |
| SHOPPING |
+----------+
2 rows in set (0.00 sec)

mysql&amp;gt; use ITCAST;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&amp;gt; show tables;
+------------------+
| Tables in ITCAST |
+------------------+
| tb_log           |
| tb_order         |
+------------------+
2 rows in set (0.00 sec)

#创建表结构
create table tb_order(
    id  varchar(100) not null primary key,
    money   int null,
    content varchar(200) null
);

#插入数据
INSERT INTO tb_order (id, money, content) VALUES (&#39;b92fdaaf-6fc4-11ec-b831-482ae33c4a2d&#39;, 10, &#39;b92fdaf8-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b93482b6-6fc4-11ec-b831-482ae33c4a2d&#39;, 20, &#39;b93482d5-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b937e246-6fc4-11ec-b831-482ae33c4a2d&#39;, 50, &#39;b937e25d-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b93be2dd-6fc4-11ec-b831-482ae33c4a2d&#39;, 100, &#39;b93be2f9-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b93f2d68-6fc4-11ec-b831-482ae33c4a2d&#39;, 130, &#39;b93f2d7d-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9451b98-6fc4-11ec-b831-482ae33c4a2d&#39;, 30, &#39;b9451bcc-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9488ec1-6fc4-11ec-b831-482ae33c4a2d&#39;, 560, &#39;b9488edb-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b94be6e6-6fc4-11ec-b831-482ae33c4a2d&#39;, 10, &#39;b94be6ff-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b94ee10d-6fc4-11ec-b831-482ae33c4a2d&#39;, 123, &#39;b94ee12c-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b952492a-6fc4-11ec-b831-482ae33c4a2d&#39;, 145, &#39;b9524945-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b95553ac-6fc4-11ec-b831-482ae33c4a2d&#39;, 543, &#39;b95553c8-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9581cdd-6fc4-11ec-b831-482ae33c4a2d&#39;, 17, &#39;b9581cfa-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b95afc0f-6fc4-11ec-b831-482ae33c4a2d&#39;, 18, &#39;b95afc2a-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b95daa99-6fc4-11ec-b831-482ae33c4a2d&#39;, 134, &#39;b95daab2-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b9667e3c-6fc4-11ec-b831-482ae33c4a2d&#39;, 156, &#39;b9667e60-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b96ab489-6fc4-11ec-b831-482ae33c4a2d&#39;, 175, &#39;b96ab4a5-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b96e2942-6fc4-11ec-b831-482ae33c4a2d&#39;, 180, &#39;b96e295b-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b97092ec-6fc4-11ec-b831-482ae33c4a2d&#39;, 123, &#39;b9709306-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b973727a-6fc4-11ec-b831-482ae33c4a2d&#39;, 230, &#39;b9737293-6fc4-11ec-b831-482ae33c4a2d&#39;);
INSERT INTO tb_order (id, money, content) VALUES (&#39;b978840f-6fc4-11ec-b831-482ae33c4a2d&#39;, 560, &#39;b978843c-6fc4-11ec-b831-482ae33c4a2d&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PS：数据按一致性 hash 分布在不同节点&lt;/p&gt;
</content>
        <category term="MySQL" />
        <updated>2025-04-09T14:02:40.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/2771271649.html</id>
        <title>云原生K8s安全专家CKS认证考题详解</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/2771271649.html"/>
        <content type="html">&lt;div class=&#34;hbe hbe-container&#34; id=&#34;hexo-blog-encrypt&#34; data-wpm=&#34;抱歉, 这个密码看着不太对, 请再试试。&#34; data-whm=&#34;抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。&#34;&gt;
  &lt;script id=&#34;hbeData&#34; type=&#34;hbeData&#34; data-hmacdigest=&#34;51b7696c170db1f393208c9728cf1b39666792a92daee416449ae392a4ae125d&#34;&gt;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&lt;/script&gt;
  &lt;div class=&#34;hbe hbe-content&#34;&gt;
    &lt;div class=&#34;hbe hbe-input hbe-input-xray&#34;&gt;
      &lt;input class=&#34;hbe hbe-input-field hbe-input-field-xray&#34; type=&#34;password&#34; id=&#34;hbePass&#34;&gt;
      &lt;label class=&#34;hbe hbe-input-label hbe-input-label-xray&#34; for=&#34;hbePass&#34;&gt;
        &lt;span class=&#34;hbe hbe-input-label-content hbe-input-label-content-xray&#34;&gt;您好, 这里需要输入密码。&lt;/span&gt;
      &lt;/label&gt;
      &lt;svg class=&#34;hbe hbe-graphic hbe-graphic-xray&#34; width=&#34;300%&#34; height=&#34;100%&#34; viewBox=&#34;0 0 1200 60&#34; preserveAspectRatio=&#34;none&#34;&gt;
        &lt;path d=&#34;M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
        &lt;path d=&#34;M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0&#34;&gt;&lt;/path&gt;
      &lt;/svg&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;script data-pjax src=&#34;/lib/hbe.js&#34;&gt;&lt;/script&gt;&lt;link href=&#34;/css/hbe.style.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt;</content>
        <category term="Kubernetes" />
        <updated>2025-04-09T13:38:39.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/1414180692.html</id>
        <title>Redis集群（主从+哨兵）模式</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/1414180692.html"/>
        <content type="html">&lt;h3 id=&#34;redis集群主从哨兵模式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#redis集群主从哨兵模式&#34;&gt;#&lt;/a&gt; Redis 集群（主从 + 哨兵）模式&lt;/h3&gt;
&lt;h3 id=&#34;一-什么是redis主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#一-什么是redis主从复制&#34;&gt;#&lt;/a&gt; 一、什么是 redis 主从复制？&lt;/h3&gt;
&lt;p&gt;主从复制，是指将一台 Redis 服务器的数据，复制到其他的 Redis 服务器。前者称为主节点 (master)，后者称为从节点 (slave), 数据的复制是单向的，只能由主节点到从节点。master 以写为主，slave 以读为主。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgTlKx&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgTlKx.png&#34; alt=&#34;pEgTlKx.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;二-主从复制的作用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#二-主从复制的作用&#34;&gt;#&lt;/a&gt; 二、主从复制的作用&lt;/h3&gt;
&lt;p&gt;数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。&lt;br /&gt;
故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。&lt;br /&gt;
负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。&lt;br /&gt;
读写分离：用于实现读写分离，主库写、从库读，读写分离不仅可以提高服务器的负载能力，同时可根据需求的变化，改变从库的数量；&lt;br /&gt;
高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是 Redis 高可用的基础。&lt;/p&gt;
&lt;h3 id=&#34;三-实现主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#三-实现主从复制&#34;&gt;#&lt;/a&gt; 三、实现主从复制&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;redis01&lt;/td&gt;
&lt;td&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;redis02&lt;/td&gt;
&lt;td&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;slave&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;redis03&lt;/td&gt;
&lt;td&gt;192.168.40.103&lt;/td&gt;
&lt;td&gt;slave&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;31-关闭防火墙-selinux&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-关闭防火墙-selinux&#34;&gt;#&lt;/a&gt; 3.1 关闭防火墙、selinux&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@master01 ~]# hostnamectl set-hostname redis01
[root@redis01 ~]# systemctl stop firewalld
[root@redis01 ~]# systemctl disable firewalld
[root@redis01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@redis01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@redis01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@redis01 ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@redis01 ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@redis01 ~]# ntpdate time2.aliyun.com
[root@redis01 ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/null
[root@redis01 ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;32-安装redis&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-安装redis&#34;&gt;#&lt;/a&gt; 3.2 安装 redis&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 ~]# yum install gcc-c++ -y
[root@redis01 soft]# wget https://download.redis.io/releases/redis-6.2.11.tar.gz
[root@redis01 soft]# tar xf redis-6.2.11.tar.gz 
[root@redis01 soft]# ln -s /soft/redis-6.2.11 /soft/redis
[root@redis01 soft]# cd /soft/redis
[root@redis01 redis]# make            #执行make编译
[root@redis01 redis]# make install    #将 src下的许多可执行文件复制到/usr/local/bin 目录下
[root@redis01 redis]# redis-server /soft/redis/redis.conf &amp;amp;
[root@redis01 redis]# netstat -lntp|grep redis
tcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      69686/redis-server  
tcp6       0      0 ::1:6379                :::*                    LISTEN      69686/redis-server     
[root@redis01 redis]# redis-cli shutdown      #关闭Redis服务
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;33-redis配置文件说明&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-redis配置文件说明&#34;&gt;#&lt;/a&gt; 3.3 redis 配置文件说明&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 redis]# vim redis.conf 
bind 127.0.0.1      		# 绑定的ip
protected-mode yes  		# 保护模式
port 6379           		# 端口设置
daemonize yes               # 后台启动
bind 127.0.0.1      		# 绑定的ip
protected-mode yes  		# 保护模式
port 6379           		# 端口设置
loglevel notice     		# 记录日志级别
logfile &amp;quot;redis.log&amp;quot;         # 日志的文件位置名
dir ./               		# 日志存储目录
databases 16        		# 数据库的数量，默认是 16 个数据库
always-show-logo yes 		# 是否总是显示LOGO

# 如果900s内，如果至少有一个1 key进行了修改，我们及进行持久化操作
save 900 1
# 如果300s内，如果至少10 key进行了修改，我们及进行持久化操作
save 300 10
# 如果60s内，如果至少10000 key进行了修改，我们及进行持久化操作
save 60 10000
# 我们之后学习持久化，会自己定义这个测试！
stop-writes-on-bgsave-error yes   # 持久化如果出错，是否还需要继续工作！
rdbcompression yes                # 是否压缩 rdb 文件，需要消耗一些cpu资源！
rdbchecksum yes                   # 保存rdb文件的时候，进行错误的检查校验！
dbfilename dump.rdb               # rdb 文件保存的名称！
dir ./                            # rdb 文件保存的目录！

slaveof 192.168.1.154 6379        # 配置主从复制
requirepass foobared              # 配置redis登录密码

appendonly no    # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！
appendfilename &amp;quot;appendonly.aof&amp;quot;   # 持久化的文件的名字
# appendfsync always        # 每次修改都会 sync。消耗性能
appendfsync everysec        # 每秒执行一次 sync，可能会丢失这1s的数据！
# appendfsync no            # 不执行 sync，这个时候操作系统自己同步数据，速度最快！
no-appendfsync-on-rewrite   #重写时是否可以运用appendsync，默认no，可以保证数据的安全性
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;34-redis环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-redis环境配置&#34;&gt;#&lt;/a&gt; 3.4 redis 环境配置&lt;/h4&gt;
&lt;p&gt;#修改 maser 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.101 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#修改 slave01 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.102 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
replicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#修改 slave02 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.103 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
replicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;35-启动3台redis服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#35-启动3台redis服务&#34;&gt;#&lt;/a&gt; 3.5 启动 3 台 redis 服务&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#启动redis01
[root@redis01 redis]# redis-server /soft/redis/redis.conf
[root@redis0[root@redis01 redis]# redis-server /soft/redis/redis.conf redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server 

#启动redis02
[root@redis02 redis]# redis-server /soft/redis/redis.conf
[root@redis02 redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server

启动redis03
[root@redis03 redis]# redis-server /soft/redis/redis.conf
[root@redis03 redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;36-查看主从状态&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#36-查看主从状态&#34;&gt;#&lt;/a&gt; 3.6 查看主从状态&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#主节点
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023
192.168.40.101:6379&amp;gt; info replication
# Replication
role:master
connected_slaves:2
slave0:ip=192.168.40.102,port=6379,state=online,offset=616,lag=0
slave1:ip=192.168.40.103,port=6379,state=online,offset=616,lag=0
master_failover_state:no-failover
master_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:616
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:616

#从节点
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.103:6379&amp;gt; info replication
# Replication
role:slave
master_host:192.168.40.101
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:812
slave_repl_offset:812
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:812
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:295
repl_backlog_histlen:518
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;37-测试主从&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#37-测试主从&#34;&gt;#&lt;/a&gt; 3.7 测试主从&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.101:6379&amp;gt; set k1 v1
OK
192.168.40.101:6379&amp;gt; set k2 v2
OK

[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.103:6379&amp;gt; get k1
&amp;quot;v1&amp;quot;
192.168.40.103:6379&amp;gt; get k2
&amp;quot;v2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;注意:&lt;/strong&gt;&lt;br /&gt;
1、主机可以写，从机不能写，只能读。主机中的所有数据都会保存到从机中去。&lt;br /&gt;
2、主机断开连接，从机依旧连接到主机的，但是没有写操作，这个时候，主机如果回来了，从机依旧可以直接获取到主机写的信息！&lt;br /&gt;
3、如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从主机中获取值！&lt;br /&gt;
4、主从复制原理&lt;br /&gt;
 Slave 启动成功连接到 master 后会发送一个 sync 同步命令&lt;br /&gt;
 Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，并完成一次完全同步。&lt;br /&gt;
全量复制：slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。&lt;br /&gt;
增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave，完成同步，但是只要是重新连接 master，一次完全同步（全量复制）将被自动执行！ 主机的数据一定可以在从机中看到。&lt;/p&gt;
&lt;h3 id=&#34;四-哨兵模式搭建&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#四-哨兵模式搭建&#34;&gt;#&lt;/a&gt; 四、哨兵模式搭建&lt;/h3&gt;
&lt;p&gt;1、什么是 redis 哨兵？&lt;br /&gt;
RedisSentinel 是 Redis 的高可用性解决方案，由一个或多个 Sentinel（哨兵）实例组成。它可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，它的主要功能如下：&lt;br /&gt;
监控 (Monitoring)：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。&lt;br /&gt;
通知 (Notification)：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。&lt;br /&gt;
故障迁移：当主服务器不能正常工作时，Sentinel 会自动进行故障迁移，也就是主从切换。&lt;br /&gt;
统一的配置：管理连接者询问 sentinel 取得主从的地址。&lt;/p&gt;
&lt;p&gt;2、哨兵原理是什么？&lt;br /&gt;
Sentinel 使用的算法核心是 Raft 算法，主要用途就是用于分布式系统，系统容错，以及 Leader 选举，每个 Sentinel 都需要定期的执行以下任务：&lt;br /&gt;
每个 Sentinel 会自动发现其他 Sentinel 和从服务器，它以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。&lt;br /&gt;
如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。&lt;br /&gt;
如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有 Sentinel 要以每秒一次的频率确认主服务器的确进入了主观下线状态。&lt;br /&gt;
如果一个主服务器被标记为主观下线， 并且有足够数量的 Sentinel（至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断，那么这个主服务器被标记为客观下线。&lt;br /&gt;
在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO 命令。当一个主服务器 Sentinel 标记为客观下线时，Sentinel 向下线主服务器的所有从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。&lt;br /&gt;
当没有足够数量的 Sentinel 同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向 Sentinel 的 PING 命令返回有效回复时， 主服务器的主管下线状态就会被移除。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgT1r6&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgT1r6.png&#34; alt=&#34;pEgT1r6.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;41-搭建哨兵&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#41-搭建哨兵&#34;&gt;#&lt;/a&gt; 4.1 搭建哨兵&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;在每台服务器上部署一个哨兵，配置方式如下:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# vim sentinel.conf
#端口默认为26379。
port 26379
#关闭保护模式，可以外部访问。
protected-mode no
#设置为后台启动。
daemonize yes
#日志文件。
logfile &amp;quot;/soft/redis/sentinel.log&amp;quot;
#指定服务器IP地址和端口，并且指定当有2台哨兵认为主机挂了，则对主机进行容灾切换。注意:三台哨兵这里的ip配置均为主节点ip 和端口
sentinel monitor mymaster 192.168.40.101 6379 2
#当在Redis实例中开启了requirepass，这里就需要提供密码。
sentinel auth-pass mymaster psw66
#这里设置了主机多少秒无响应，则认为挂了。
sentinel down-after-milliseconds mymaster 3000
#主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的
snetinel parallel-syncs mymaster 1
#故障转移的超时时间，这里设置为三分钟。
sentinel failover-timeout mymaster 180000
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;42-启动三台服务器上的哨兵&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#42-启动三台服务器上的哨兵&#34;&gt;#&lt;/a&gt; 4.2 启动三台服务器上的哨兵&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#启动redis01的sentine
[root@redis01 redis]# redis-sentinel /soft/redis/sentinel.conf
[root@redis01 redis]#  netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      33536/redis-sentine 
tcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server 
tcp6       0      0 :::26379                :::*                    LISTEN      33536/redis-sentine

#启动redis02的sentine
[root@redis02 redis]# redis-sentinel /soft/redis/sentinel.conf
[root@redis02 redis]#  netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      18757/redis-sentine 
tcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server  
tcp6       0      0 :::26379                :::*                    LISTEN      18757/redis-sentine

#启动redis03的sentine
[root@redis03 redis]# redis-sentinel /soft/redis/sentinel.conf                     
[root@redis03 redis]# netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      19745/redis-sentine 
tcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server  
tcp6       0      0 :::26379                :::*                    LISTEN      19745/redis-sentine
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;43-连接客户端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#43-连接客户端&#34;&gt;#&lt;/a&gt; 4.3 连接客户端&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-cli -p 26379
127.0.0.1:26379&amp;gt;  info sentinel
# Sentinel
sentinel_masters:1
sentinel_tilt:0
sentinel_running_scripts:0
sentinel_scripts_queue_length:0
sentinel_simulate_failure_flags:0
master0:name=mymaster,status=ok,address=192.168.40.101:6379,slaves=2,sentinels=3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;44-redis容灾切换&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#44-redis容灾切换&#34;&gt;#&lt;/a&gt; 4.4 redis 容灾切换&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#连接redis客户端
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 
#验证密码
192.168.40.101:6379&amp;gt; auth Superman*2023
OK
#关闭redis服务
192.168.40.101:6379&amp;gt; shutdown
not connected&amp;gt;
#退出客户端
not connected&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;关闭主节点之后，我们去查看哨兵日志:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 ~]# tail -f /soft/redis/sentinel.log 
91936:X 14 Apr 2023 23:26:23.838 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
91936:X 14 Apr 2023 23:26:23.838 # Redis version=6.2.11, bits=64, commit=00000000, modified=0, pid=91936, just started
91936:X 14 Apr 2023 23:26:23.838 # Configuration loaded
91936:X 14 Apr 2023 23:26:23.838 * monotonic clock: POSIX clock_gettime
91936:X 14 Apr 2023 23:26:23.839 * Running mode=sentinel, port=26379.
91936:X 14 Apr 2023 23:26:23.839 # Sentinel ID is 835b4c8544fb250af5fd479f834ee369cc4f388e
91936:X 14 Apr 2023 23:26:23.839 # +monitor master mymaster 192.168.40.101 6379 quorum 2



91936:X 14 Apr 2023 23:31:25.329 # +sdown master mymaster 192.168.40.101 6379   #这里应该是发现主节点宕机
91936:X 14 Apr 2023 23:31:25.359 # +new-epoch 5
91936:X 14 Apr 2023 23:31:25.360 # +vote-for-leader ab43979285cb47b1b459aeb0ab91b63fa9d1a989 5
91936:X 14 Apr 2023 23:31:25.401 # +odown master mymaster 192.168.40.101 6379 #quorum 3/2 两个哨兵都觉得主节点宕机了
91936:X 14 Apr 2023 23:31:25.401 # Next failover delay: I will not start a failover before Fri Apr 14 23:37:25 2023
91936:X 14 Apr 2023 23:31:26.468 # +config-update-from sentinel ab43979285cb47b1b459aeb0ab91b63fa9d1a989 192.168.40.102 26379 @ mymaster 192.168.40.101 6379
91936:X 14 Apr 2023 23:31:26.468 # +switch-master mymaster 192.168.40.101 6379 192.168.40.103 6379 #通过投票选举40.103为新的主节点
91936:X 14 Apr 2023 23:31:26.468 * +slave slave 192.168.40.102:6379 192.168.40.102 6379 @ mymaster 192.168.40.103 6379
91936:X 14 Apr 2023 23:31:26.469 * +slave slave 192.168.40.101:6379 192.168.40.101 6379 @ mymaster 192.168.40.103 6379
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面我们去 40.103 下查看哨兵主从切换是否成功&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103
192.168.40.103:6379&amp;gt; auth Superman*2023
OK
192.168.40.103:6379&amp;gt; info replication
# Replication
role:master   # 40.103变成主节点了
connected_slaves:1   # 下面的从机个数为1
slave0:ip=192.168.40.102,port=6379,state=online,offset=108708,lag=1
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935
master_repl_offset:108851
second_repl_offset:59887
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:108851
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重新连接挂掉的主节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-server redis.conf 
[root@redis01 redis]#  redis-cli -p 6379 -h 192.168.40.101
192.168.40.101:6379&amp;gt; auth Superman*2023
OK
192.168.40.101:6379&amp;gt; info replication
# Replication
role:slave          #主节点连接回来之后自动变成了从节点，并且成功连上了主机
master_host:192.168.40.103
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:130607
slave_repl_offset:130607
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:130607
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:126982
repl_backlog_histlen:3626
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再去主节点确认一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;192.168.40.103:6379&amp;gt; info replication
# Replication
role:master
connected_slaves:2   #两个从节点
slave0:ip=192.168.40.102,port=6379,state=online,offset=147879,lag=1
slave1:ip=192.168.40.101,port=6379,state=online,offset=147879,lag=1
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935
master_repl_offset:148165
second_repl_offset:59887
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:148165
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;五、哨兵模式的优缺点&lt;br /&gt;
 1. 优点&lt;/p&gt;
&lt;p&gt;哨兵集群，基于主从复制模式，所有的主从配置优点，它全有&lt;/p&gt;
&lt;p&gt;主从可以切换，故障可以转移，系统的可用性就会更好&lt;/p&gt;
&lt;p&gt;哨兵模式就是主从模式的升级，手动到自动，更加健壮！&lt;/p&gt;
&lt;p&gt;2. 缺点&lt;/p&gt;
&lt;p&gt;Redis 不好在线扩容，集群容量一旦到达上限，在线扩容就十分麻烦&lt;/p&gt;
&lt;p&gt;哨兵模式的配置繁琐&lt;/p&gt;
&lt;p&gt;3. 哨兵模式的配置文件详解&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Example sentinel.conf
# 哨兵sentinel实例运行的端口 默认26379
port 26379
 
# 哨兵sentinel的工作目录
dir /tmp
 
# 哨兵sentinel监控的redis主节点的 ip port
# master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&amp;quot;.-_&amp;quot;组成。
# quorum 配置多少个sentinel哨兵统一认为master主节点失联 那么这时客观上认为主节点失联了
# sentinel monitor &amp;lt;master-name&amp;gt; &amp;lt;ip&amp;gt; &amp;lt;redis-port&amp;gt; &amp;lt;quorum&amp;gt;
sentinel monitor mymaster 127.0.0.1 6379 2
  
# 当在Redis实例中开启了requirepass foobared 授权密码这样所有连接Redis实例的客户端都要提供 密码
# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码
# sentinel auth-pass &amp;lt;master-name&amp;gt; &amp;lt;password&amp;gt;
sentinel auth-pass mymaster MySUPER--secret-0123passw0rd
 
# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时哨兵主观上认为主节点下线 默认30秒
# sentinel down-after-milliseconds &amp;lt;master-name&amp;gt; &amp;lt;milliseconds&amp;gt;
sentinel down-after-milliseconds mymaster 30000
 
# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长， 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。
# sentinel parallel-syncs &amp;lt;master-name&amp;gt; &amp;lt;numslaves&amp;gt;
sentinel parallel-syncs mymaster 1
 
# 故障转移的超时时间 failover-timeout 可以用在以下这些方面：
#1. 同一个sentinel对同一个master两次failover之间的间隔时间。
#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那 里同步数据时。
#3.当想要取消一个正在进行的failover所需要的时间。
#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时， slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了 # 默认三分钟
# sentinel failover-timeout &amp;lt;master-name&amp;gt; &amp;lt;milliseconds&amp;gt; bilibili：
sentinel failover-timeout mymaster 180000
 
# SCRIPTS EXECUTION
#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知 相关人员。
#对于脚本的运行结果有以下规则：
#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10
#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。
#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。
#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。
#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等）， 将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信 息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配 置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无 法正常启动成功。
#通知脚本
# shell编程
# sentinel notification-script &amp;lt;master-name&amp;gt; &amp;lt;script-path&amp;gt; sentinel
notification-script mymaster /var/redis/notify.sh
 
# 客户端重新配置主节点参数脚本
# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已 经发生改变的信息。
# 以下参数将会在调用脚本时传给脚本:
# &amp;lt;master-name&amp;gt; &amp;lt;role&amp;gt; &amp;lt;state&amp;gt; &amp;lt;from-ip&amp;gt; &amp;lt;from-port&amp;gt; &amp;lt;to-ip&amp;gt; &amp;lt;to-port&amp;gt; # 目前&amp;lt;state&amp;gt;总是“failover”,
# &amp;lt;role&amp;gt;是“leader”或者“observer”中的一个。
# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通 信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。
# sentinel client-reconfig-script &amp;lt;master-name&amp;gt; &amp;lt;script-path&amp;gt; sentinel client-reconfig-
script mymaster /var/redis/reconfig.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;再去看一下 redis 的配置文件和哨兵的配置文件，你会惊讶的发现，里边的配置文件已经被改过来了。&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat redis.con
...
replicaof 192.168.40.103 6379
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Redis" />
        <updated>2025-04-09T11:50:06.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/3166738000.html</id>
        <title>Kubeadm高可用安装K8s集群</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/3166738000.html"/>
        <content type="html">&lt;h2 id=&#34;kubeadm高可用安装k8s集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#kubeadm高可用安装k8s集群&#34;&gt;#&lt;/a&gt; Kubeadm 高可用安装 K8s 集群&lt;/h2&gt;
&lt;h4 id=&#34;1-基本配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-基本配置&#34;&gt;#&lt;/a&gt; 1. 基本配置&lt;/h4&gt;
&lt;h5 id=&#34;11-基本环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-基本环境配置&#34;&gt;#&lt;/a&gt; 1.1 基本环境配置&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP 地址&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master 节点 * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived 虚拟 IP（不占用机器）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker 节点 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* 配置信息 *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;系统版本&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 网段&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service 网段&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改主机名（其它节点按需修改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 hosts，修改 /etc/hosts 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 yum 源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 配置基础源
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;必备工具安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭 swap 分区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ntpdate：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;同步时间并配置上海时区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 limit：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;升级系统：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;下载安装所有的源码文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-内核配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-内核配置&#34;&gt;#&lt;/a&gt; 1.2 内核配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ipvsadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 ipvs 模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 ipvs.conf，并配置开机自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# 加入以下内容
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;内核优化配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;应用配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置完内核后，重启机器，之后查看内核模块是否已自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-高可用组件安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-高可用组件安装&#34;&gt;#&lt;/a&gt; 2. 高可用组件安装&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;通过 yum 安装 HAProxy 和 KeepAlived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 HAProxy，需要注意黄色部分的 IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443       #HAProxy监听端口
  bind 127.0.0.1:16443     #HAProxy监听端口
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IP地址
  server k8s-master02	192.168.1.72:6443  check       #API Server IP地址
  server k8s-master03	192.168.1.73:6443  check       #API Server IP地址
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 KeepAlived，需要注意黄色部分的配置。&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #网卡名称
    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #网卡名称
    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #网卡名称
    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置 KeepAlived 健康检查文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置健康检查文件添加执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;启动 haproxy 和 keepalived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;所有节点测试VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld&lt;/li&gt;
&lt;li&gt;所有节点查看 selinux 状态，必须为 disable：getenforce&lt;/li&gt;
&lt;li&gt;master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master 节点查看监听端口：netstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果以上都没有问题，需要确认：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;是否是公有云机器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否是私有云机器（类似 OpenStack）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询&lt;/p&gt;
&lt;h4 id=&#34;3-runtime安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtime安装&#34;&gt;#&lt;/a&gt; 3. Runtime 安装&lt;/h4&gt;
&lt;p&gt;如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。&lt;/p&gt;
&lt;h5 id=&#34;31-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-安装containerd&#34;&gt;#&lt;/a&gt; 3.1 安装 Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置安装源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先配置 Containerd 所需的模块（&lt;mark&gt;所有节点&lt;/mark&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;，配置 Containerd 所需的内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;生成 Containerd 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改 Containerd 的 Cgroup 和 Pause 镜像配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 Containerd，并配置开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 crictl 客户端连接的运行时位置（可选）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-安装kubernetes组件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-安装kubernetes组件&#34;&gt;#&lt;/a&gt; 4 . 安装 Kubernetes 组件&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置源（注意更改版本号）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先在&lt;mark&gt; Master01 节点&lt;/mark&gt;查看最新的 Kubernetes 版本是多少：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum list kubeadm.x86_64 --showduplicates | sort -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;此时 kubelet 是起不来的，日志会有报错不影响！&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-集群初始化&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-集群初始化&#34;&gt;#&lt;/a&gt; 5 . 集群初始化&lt;/h4&gt;
&lt;p&gt;以下操作在&lt;mark&gt; master01&lt;/mark&gt;（注意黄色部分）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.71
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  name: k8s-master01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  certSANs:
  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443
controllerManager: &amp;#123;&amp;#125;
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致
networking:
  dnsDomain: cluster.local
  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突
  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突
scheduler: &amp;#123;&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;master01 节点&lt;/mark&gt;更新 kubeadm 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 new.yaml 文件复制到&lt;mark&gt;其他 master 节点&lt;/mark&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后&lt;mark&gt;所有 Master 节点&lt;/mark&gt;提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config images pull --config /root/new.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正确的反馈信息如下（&lt;em&gt;&lt;strong&gt;* 版本可能不一样 *&lt;/strong&gt;&lt;/em&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml 
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init --config /root/new.yaml  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

# 不要复制文档当中的，要去使用节点生成的
  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&amp;quot;kubeadm init phase upload-certs --upload-certs&amp;quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;配置环境变量，用于访问 Kubernetes 集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF
source /root/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;查看节点状态：（显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE   VERSION
k8s-master01   NotReady   control-plane   24s   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;# kubectl get pods -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-初始化失败排查&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-初始化失败排查&#34;&gt;#&lt;/a&gt; 5.1 初始化失败排查&lt;/h5&gt;
&lt;p&gt;如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/messages | grep -v &amp;quot;not found&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经常出错的原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突&lt;/li&gt;
&lt;li&gt;VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;52-高可用master&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-高可用master&#34;&gt;#&lt;/a&gt; 5.2 高可用 Master&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;其他 master&lt;/strong&gt; 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看当前状态：（如果显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;53-token过期处理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#53-token过期处理&#34;&gt;#&lt;/a&gt; 5.3 Token 过期处理&lt;/h5&gt;
&lt;p&gt;注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。&lt;/p&gt;
&lt;p&gt;Token 过期后生成新的 token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm token create --print-join-command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Master 需要生成 --certificate-key：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase upload-certs  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-node节点的配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-node节点的配置&#34;&gt;#&lt;/a&gt; 6. Node 节点的配置&lt;/h4&gt;
&lt;p&gt;Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点初始化完成后，查看集群状态（NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
k8s-node01     NotReady   &amp;lt;none&amp;gt;          13s     v1.32.3
k8s-node02     NotReady   &amp;lt;none&amp;gt;          10s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-calico组件的安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-calico组件的安装&#34;&gt;#&lt;/a&gt; 7. Calico 组件的安装&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt;/etc/NetworkManager/conf.d/calico.conf&amp;lt;&amp;lt;EOF
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
EOF
systemctl daemon-reload
systemctl restart NetworkManager
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下步骤只在&lt;mark&gt; master01&lt;/mark&gt; 执行（.x 不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x &amp;amp;&amp;amp; cd calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改 Pod 网段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= &#39;&amp;#123;print $NF&amp;#125;&#39;`

sed -i &amp;quot;s#POD_CIDR#$&amp;#123;POD_SUBNET&amp;#125;#g&amp;quot; calico.yaml
kubectl apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看容器和节点状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h
calico-node-7mzmb                          1/1     Running   0          24h
calico-node-ljqnl                          1/1     Running   0          24h
calico-node-njqlb                          1/1     Running   0          24h
calico-node-ph4m4                          1/1     Running   0          24h
calico-node-rx8rl                          1/1     Running   0          24h
coredns-76fccbbb6b-76559                   1/1     Running   0          24h
coredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h
etcd-k8s-master01                          1/1     Running   0          24h
etcd-k8s-master02                          1/1     Running   0          24h
etcd-k8s-master03                          1/1     Running   0          24h
kube-apiserver-k8s-master01                1/1     Running   0          24h
kube-apiserver-k8s-master02                1/1     Running   0          24h
kube-apiserver-k8s-master03                1/1     Running   0          24h
kube-controller-manager-k8s-master01       1/1     Running   0          24h
kube-controller-manager-k8s-master02       1/1     Running   0          24h
kube-controller-manager-k8s-master03       1/1     Running   0          24h
kube-proxy-9dtz4                           1/1     Running   0          24h
kube-proxy-jh7rl                           1/1     Running   0          24h
kube-proxy-jvvwt                           1/1     Running   0          24h
kube-proxy-sh89l                           1/1     Running   0          24h
kube-proxy-t2j49                           1/1     Running   0          24h
kube-scheduler-k8s-master01                1/1     Running   0          24h
kube-scheduler-k8s-master02                1/1     Running   0          24h
kube-scheduler-k8s-master03                1/1     Running   0          24h
metrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时节点全部变为 Ready 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   24h   v1.32.3
k8s-master02   Ready    control-plane   24h   v1.32.3
k8s-master03   Ready    control-plane   24h   v1.32.3
k8s-node01     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
k8s-node02     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-metrics部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-metrics部署&#34;&gt;#&lt;/a&gt; 8. Metrics 部署&lt;/h4&gt;
&lt;p&gt;在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。&lt;/p&gt;
&lt;p&gt;将&lt;mark&gt; Master01 节点&lt;/mark&gt;的 front-proxy-ca.crt 复制到所有 Node 节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt

scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下操作均在&lt;mark&gt; master01 节点&lt;/mark&gt;执行:&lt;/p&gt;
&lt;p&gt;安装 metrics server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/kubeadm-metrics-server

# kubectl  create -f comp.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl top node
NAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
k8s-master01   132m         3%       932Mi           5%          
k8s-master02   131m         3%       845Mi           5%          
k8s-master03   148m         3%       912Mi           5%          
k8s-node01     54m          1%       600Mi           3%          
k8s-node02     49m          1%       602Mi           3%          
[root@k8s-master01 ~]#  kubectl top po -A
NAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   
ingress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            
ingress-nginx          ingress-nginx-controller-r978m               1m           104Mi           
krm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            
krm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             
krm                    nginx-574cf48959-vcfjs                       0m           2Mi             
kube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            
kube-system            calico-node-7mzmb                            16m          176Mi           
kube-system            calico-node-ljqnl                            15m          182Mi           
kube-system            calico-node-njqlb                            19m          180Mi           
kube-system            calico-node-ph4m4                            15m          178Mi           
kube-system            calico-node-rx8rl                            17m          180Mi           
kube-system            coredns-76fccbbb6b-76559                     2m           16Mi            
kube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            
kube-system            etcd-k8s-master01                            22m          86Mi            
kube-system            etcd-k8s-master02                            27m          84Mi            
kube-system            etcd-k8s-master03                            22m          84Mi            
kube-system            kube-apiserver-k8s-master01                  22m          267Mi           
kube-system            kube-apiserver-k8s-master02                  20m          242Mi           
kube-system            kube-apiserver-k8s-master03                  18m          241Mi           
kube-system            kube-controller-manager-k8s-master01         6m           69Mi            
kube-system            kube-controller-manager-k8s-master02         2m           21Mi            
kube-system            kube-controller-manager-k8s-master03         1m           19Mi            
kube-system            kube-proxy-9dtz4                             11m          30Mi            
kube-system            kube-proxy-jh7rl                             1m           27Mi            
kube-system            kube-proxy-jvvwt                             17m          29Mi            
kube-system            kube-proxy-sh89l                             1m           29Mi            
kube-system            kube-proxy-t2j49                             16m          29Mi            
kube-system            kube-scheduler-k8s-master01                  6m           25Mi            
kube-system            kube-scheduler-k8s-master02                  6m           25Mi            
kube-system            kube-scheduler-k8s-master03                  6m           25Mi            
kube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            
kubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            
kubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9-dashboard部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-dashboard部署&#34;&gt;#&lt;/a&gt; 9. Dashboard 部署&lt;/h4&gt;
&lt;h5 id=&#34;91-安装dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#91-安装dashboard&#34;&gt;#&lt;/a&gt; 9.1 安装 Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;92-登录dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#92-登录dashboard&#34;&gt;#&lt;/a&gt; 9.2 登录 dashboard&lt;/h5&gt;
&lt;p&gt;在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更改 dashboard 的 svc 为 NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看端口号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：&lt;/p&gt;
&lt;p&gt;访问 Dashboard：&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建登录 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;10必看一些必须的配置更改&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10必看一些必须的配置更改&#34;&gt;#&lt;/a&gt; 10.【必看】一些必须的配置更改&lt;/h4&gt;
&lt;p&gt;将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：&lt;/p&gt;
&lt;p&gt;在 master01 节点执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
mode: ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新 Kube-Proxy 的 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;验证 Kube-Proxy 模式:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode
ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11必看注意事项&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11必看注意事项&#34;&gt;#&lt;/a&gt; 11.【必看】注意事项&lt;/h4&gt;
&lt;p&gt;注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。&lt;/p&gt;
&lt;p&gt;启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。&lt;/p&gt;
&lt;p&gt;其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。&lt;/p&gt;
&lt;p&gt;kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行更改，更改完成后，可以通过 patch 重启 kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-containerd配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-containerd配置镜像加速&#34;&gt;#&lt;/a&gt; 12. Containerd 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Containerd：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;13-docker配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-docker配置镜像加速&#34;&gt;#&lt;/a&gt; 13. Docker 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-09T10:28:34.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/1922841233.html</id>
        <title>Rsync服务实践</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/1922841233.html"/>
        <content type="html">&lt;h3 id=&#34;ursync服务实践u&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ursync服务实践u&#34;&gt;#&lt;/a&gt; &lt;u&gt;Rsync 服务实践&lt;/u&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;1rsync服务端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1rsync服务端&#34;&gt;#&lt;/a&gt; 1.rsync 服务端&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-rsync客户端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-rsync客户端&#34;&gt;#&lt;/a&gt; 2. rsync 客户端&lt;/h4&gt;
&lt;h5 id=&#34;21-安装rsync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-安装rsync&#34;&gt;#&lt;/a&gt; 2.1 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-配置传输密码&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-配置传输密码&#34;&gt;#&lt;/a&gt; 2.2 配置传输密码&lt;/h5&gt;
&lt;p&gt;方法 1：将密码写入文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]#  echo &#39;your passwd&#39; &amp;gt; /etc/rsync.pass
[root@db01 ~]# cat /etc/rsync.pass 
your passwd
[root@db01 ~]# chmod 600 /etc/rsync.pass
--测试收发数据：
[root@db01 ~]# rsync -avz --password-file=/etc/rsync.pass /root/test rsync_backup@192.168.40.101::backup_file
sending incremental file list

sent 47 bytes  received 20 bytes  134.00 bytes/sec
total size is 0  speedup is 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;方法 2：使用密码环境变量 RSYNC_PASSWORD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# export RSYNC_PASSWORD=&#39;your passwd&#39;
--测试收发数据：
[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file
sending incremental file list

sent 47 bytes  received 20 bytes  134.00 bytes/sec
total size is 0  speedup is 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ursync企业级备份案例u&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ursync企业级备份案例u&#34;&gt;#&lt;/a&gt; &lt;u&gt;Rsync 企业级备份案例&lt;/u&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;客户端需求&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端每天凌晨 3 点备份 MySQL 至 /backup 下以 &amp;quot;主机名_IP 地址_当前时间命名&amp;quot; 的目录中&lt;/li&gt;
&lt;li&gt;客户端推送 /backup 目录下数据备份目录至 Rsync 备份服务器&lt;/li&gt;
&lt;li&gt;客户端只保留最近七天的备份数据，避免浪费磁盘空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;服务端需求&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务端部署 rsync 服务，用于接收用户的备份数据&lt;/li&gt;
&lt;li&gt;服务端每天校验客户端推送过来的数据是否完整，并将结果以邮件的方式发送给管理员&lt;/li&gt;
&lt;li&gt;服务端仅保留 6 个月的备份数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：所有服务器的备份目录均为 /backup，所有脚本存放目录均为 /scripts。&lt;/p&gt;
&lt;h4 id=&#34;1-服务端部署rsync服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-服务端部署rsync服务&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1. 服务端部署 rsync 服务&lt;/strong&gt;&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux-2&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync-2&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件-2&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限-2&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启-2&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-rsync客户端-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-rsync客户端-2&#34;&gt;#&lt;/a&gt; 2. rsync 客户端&lt;/h4&gt;
&lt;h5 id=&#34;21-安装rsync-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-安装rsync-2&#34;&gt;#&lt;/a&gt; 2.1 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-测试客户端备份数据并推送至rsync服务器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-测试客户端备份数据并推送至rsync服务器&#34;&gt;#&lt;/a&gt; 2.2 测试客户端备份数据并推送至 rsync 服务器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# export RSYNC_PASSWORD=&#39;your passwd&#39;
[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-客户端备份数据并推送至rsync服务器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-客户端备份数据并推送至rsync服务器&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.3 客户端备份数据并推送至 rsync 服务器&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# mkdir /scripts
[root@db01 ~]# cat /scripts/mysql_backup.sh 
#!/bin/bash
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin

#1、定义变量
Host=$(hostname)
Ip=$(ifconfig ens192 | awk &#39;NR==2&amp;#123;print $2&amp;#125;&#39;)
Date=$(date +%F)
BackupDir=/backup/mysql
Dest=$&amp;#123;BackupDir&amp;#125;/$&amp;#123;Host&amp;#125;_$&amp;#123;Ip&amp;#125;_$&amp;#123;Date&amp;#125;
FILE_NAME=mysql_backup_`date &#39;+%Y%m%d%H%M%S&#39;`;
OLDBINLOG=/var/lib/mysql/oldbinlog

#2、创建备份目录
if [ ! -d $Dest ];then
  mkdir -p $Dest
fi

#3、备份目录
/usr/bin/mysqldump -u&#39;root&#39; -p&#39;your passwd&#39; nf_flms &amp;gt; $Dest/nf-flms_$&amp;#123;FILE_NAME&amp;#125;.sql
tar -czvf $Dest/$&amp;#123;FILE_NAME&amp;#125;.tar.gz $Dest/nf-flms_$&amp;#123;FILE_NAME&amp;#125;.sql
rm -rf $Dest/*$&amp;#123;FILE_NAME&amp;#125;.sql
echo &amp;quot;Your database backup successfully&amp;quot;

#4、校验
md5sum $Dest/* &amp;gt;$Dest/backup_check_$Date

#5、将备份目录推动到rsync服务端
Rsync_Ip=192.168.1.145
Rsync_user=rsync_backup
Rsync_Module=backup_mysql
export RSYNC_PASSWORD=your passwd
rsync -avz $Dest $Rsync_user@$Rsync_Ip::$Rsync_Module

#6、删除15天备份目录
find $Dest -type d -mtime +15 | xargs rm -rf
echo &amp;quot;remove file  successfully&amp;quot;

[root@db01 ~]# chmod +x /scripts/etc_backup.sh
[root@db01 ~]# crontab -e
00 03 * * * /bin/bash /scripts/mysql_backup.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-服务端校验数据并将结果以邮件发送给管理员&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-服务端校验数据并将结果以邮件发送给管理员&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.4 服务端校验数据并将结果以邮件发送给管理员&lt;/strong&gt;&lt;/h5&gt;
&lt;h6 id=&#34;241-配置邮件服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#241-配置邮件服务&#34;&gt;#&lt;/a&gt; 2.4.1 配置邮件服务&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum -y install mailx
[root@backup ~]# cat /etc/mail.rc      #最后一行插入
set from=373370405@qq.com
set smtp=smtps://smtp.qq.com:465
set smtp-auth-user=373370405@qq.com
set smtp-auth-password=**********   # 发件邮箱的授权码
set smtp-auth=login
set ssl-verify=ignore
set nss-config-dir=/etc/pki/nssdb
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;242-发送邮件测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#242-发送邮件测试&#34;&gt;#&lt;/a&gt; 2.4.2 发送邮件测试&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]#  echo Hello World | mail -s test 373370405@qq.com &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;243-配置脚本校验数据并将结果发送给管理员&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#243-配置脚本校验数据并将结果发送给管理员&#34;&gt;#&lt;/a&gt; 2.4.3 配置脚本校验数据并将结果发送给管理员&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup mysql]# cat /scripts/check_backup.sh 
#!/bin/bash
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin

#1、定义变量
Path=/backup/mysql
Date=$(date +%F)

#2、查看flag文件，并对对文件进行校验,然后将校验的结果保存至result_时间
find $Path -type f -name &amp;quot;backup_check_$&amp;#123;Date&amp;#125;*&amp;quot;|xargs md5sum -c &amp;gt;$Path/result_$&amp;#123;Date&amp;#125;

#3、将校验结果发送邮件给管理员
mail -s &amp;quot;Mysql Backup&amp;quot; 373370405@qq.com &amp;lt;$Path/result_$&amp;#123;Date&amp;#125; &amp;amp;&amp;gt; /dev/null

#4、删除超过7天的校验结果文件，删除超过180天的备份数据文件
find $Path -type f -name &amp;quot;result*&amp;quot; -mtime +7 | xargs rm -rf
find $Path -type f -mtime +180 | xargs rm -rf
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;244-写计划任务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#244-写计划任务&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.4.4 写计划任务&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# chmod +x /scripts/check_backup.sh 
[root@db01 ~]# crontab -e
00 06 * * * /bin/bash /scripts/mysql_backup.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;rsyncsersync实现数据实时同步&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#rsyncsersync实现数据实时同步&#34;&gt;#&lt;/a&gt; Rsync+sersync 实现数据实时同步&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;1rsync服务端-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1rsync服务端-2&#34;&gt;#&lt;/a&gt; 1.rsync 服务端&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux-3&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync-3&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件-3&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限-3&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启-3&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-客户端安装sersync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-客户端安装sersync&#34;&gt;#&lt;/a&gt; 2. 客户端安装 sersync&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;2.1 安装 sercync 依赖&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs ~]# yum install -y inotify-tools rsync
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.2 安装 sercync&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs ~]# mkdir -p /soft
[root@nfs ~]# cd /soft/
[root@nfs ~]# wget https://down.whsir.com/downloads/sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs soft]# tar -xf sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs soft]# mv GNU-Linux-x86 /usr/local/sersync
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-修改配置文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-修改配置文件&#34;&gt;#&lt;/a&gt; 2.3 &lt;strong&gt;修改配置文件&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs soft]# cd /usr/local/sersync/
[root@nfs sersync]# cp confxml.xml confxml.xml.bak
[root@nfs sersync]# vim confxml.xml
...
5    &amp;lt;fileSystem xfs=&amp;quot;true&amp;quot;/&amp;gt;    #第5行 false改为true
13          &amp;lt;delete start=&amp;quot;true&amp;quot;/&amp;gt; #第13-20行 false改为true,#说明：监控以上变化推送
14        &amp;lt;createFolder start=&amp;quot;true&amp;quot;/&amp;gt;
15        &amp;lt;createFile start=&amp;quot;false&amp;quot;/&amp;gt;
16        &amp;lt;closeWrite start=&amp;quot;true&amp;quot;/&amp;gt;
17        &amp;lt;moveFrom start=&amp;quot;true&amp;quot;/&amp;gt;
18        &amp;lt;moveTo start=&amp;quot;true&amp;quot;/&amp;gt;
19        &amp;lt;attrib start=&amp;quot;true&amp;quot;/&amp;gt;
20        &amp;lt;modify start=&amp;quot;true&amp;quot;/&amp;gt;
24        &amp;lt;localpath watch=&amp;quot;/data&amp;quot;&amp;gt;      #监控的本地目录
25             &amp;lt;remote ip=&amp;quot;192.168.1.145&amp;quot; name=&amp;quot;backup_file&amp;quot;/&amp;gt;  #rsync服务端IP和模块名backup_file
30      &amp;lt;commonParams params=&amp;quot;-avz&amp;quot;/&amp;gt;  #rsync命令选项
31      &amp;lt;auth start=&amp;quot;true&amp;quot; users=&amp;quot;rsync_backup&amp;quot; passwordfile=&amp;quot;/etc/rsync.passwd&amp;quot;/&amp;gt; #rsync认证信息
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-生成密码文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-生成密码文件&#34;&gt;#&lt;/a&gt; 2.4 生成密码文件&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# echo &#39;your passwd&#39; &amp;gt; /etc/rsync.passwd
[root@nfs sersync]# chmod 600 /etc/rsync.passwd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-启动sersync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-启动sersync&#34;&gt;#&lt;/a&gt; 2.5 启动 sersync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# ln -s /usr/local/sersync/sersync2 /usr/bin/
[root@nfs sersync]# sersync2 -dro /usr/local/sersync/confxml.xml     #针对配置文件confxml.xml启动sersync
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.5 设置 sersync 开机自启&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@qzj_nfs sersync]# vim /etc/rc.d/rc.local   
/usr/local/sersync/sersync2 -d -r -o  /usr/local/sersync/confxml.xml  #在最后添加一行
[root@qzj_nfs sersync]# chmod +x /etc/rc.d/rc.local
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.6 测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在客户端 /data 目录增删改目录文件，rsync 服务端数据存放目录变化&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@backup backup]# watch ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.7 添加脚本监控 sersync 是否正常运行&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# cat /scripts/check_sersync.sh 
#!/bin/sh
sersync=&amp;quot;/usr/local/sersync/sersync2&amp;quot;
confxml=&amp;quot;/usr/local/sersync/confxml.xml&amp;quot;
status=$(ps aux |grep &#39;sersync2&#39;|grep -v &#39;grep&#39;|wc -l)
if [ $status -eq 0 ];
then
$sersync -d -r -o $confxml &amp;amp;
else
exit 0;
fi

[root@nfs sersync]# chmod +x /scripts/check_sersync.sh
[root@nfs sersync]# crontab -l
*/5 * * * * /usr/bin/sh /scripts/check_sersync.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;补充： 多实例情况&lt;/strong&gt;&lt;/em&gt;&lt;br /&gt;
 1、配置多个 confxml.xml 文件（比如：www、bbs、blog.... 等等）&lt;br /&gt;
2、修改端口、同步路径、模块名称&lt;br /&gt;
 3、根据不同的需求同步对应的实例文件&lt;br /&gt;
 /usr/local/sersync/sersync2 -dro /usr/local/sersync/www_confxml.xml&lt;br /&gt;
/usr/local/sersync/sersync2 -dro /usr/local/sersync/bbs_confxml.xml&lt;/p&gt;
</content>
        <category term="rsync" />
        <updated>2025-03-30T12:45:48.000Z</updated>
    </entry>
    <entry>
        <id>http://imxuyong.cn/posts/3071070978.html</id>
        <title>企业级私有仓库Harbor搭建</title>
        <link rel="alternate" href="http://imxuyong.cn/posts/3071070978.html"/>
        <content type="html">&lt;h3 id=&#34;企业级私有仓库harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#企业级私有仓库harbor&#34;&gt;#&lt;/a&gt; 企业级私有仓库 Harbor&lt;/h3&gt;
&lt;p&gt;企业部署 Kuberetes 集群环境之后，我们就可以将原来在传统虚拟机上运行的业务，迁移到 kubernetes 上，让 Kubernetes 通过容器的方式来管理。而一旦我们需要将传统业务使用容器的方式运行起来，就需要构建很多镜像，那么这些镜像就需要有一个专门的位置存储起来，为我们提供镜像上传和镜像下载等功能。但我们不能使用阿里云或者 Dockerhub 等仓库，首先拉取速度比较慢，其次镜像的安全性无法保证，所以就需要部署一个私有的镜像仓库来管理这些容器镜像。同时该仓库还需要提供高可用功能，确保随时都能上传和下载可用的容器镜像。&lt;/p&gt;
&lt;h4 id=&#34;1-关闭防火墙-selinux-环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-关闭防火墙-selinux-环境配置&#34;&gt;#&lt;/a&gt; 1、关闭防火墙、Selinux、环境配置&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# sudo mkdir -p /etc/docker
[root@harbor ~]# hostnamectl set-hostname harbor
[root@harbor ~]# systemctl stop firewalld
[root@harbor ~]# systemctl disable firewalld
[root@harbor ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@harbor ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate -y
[root@harbor ~]# yum update -y
[root@harbor ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-docker安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-docker安装&#34;&gt;#&lt;/a&gt; 2、Docker 安装&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# yum install -y yum-utils device-mapper-persistent-data lvm2
[root@harbor ~]# curl -o /etc/yum.repos.d/docker-ce.repo  https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@harbor ~]# yum list docker-ce --showduplicates |sort -r 
[root@harbor ~]# yum install docker-ce docker-compose -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-配置docker加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-配置docker加速&#34;&gt;#&lt;/a&gt; 3、配置 Docker 加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# sudo mkdir -p /etc/docker
[root@harbor ~]# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
[root@harbor ~]# systemctl enable docker --now
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-安装harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-安装harbor&#34;&gt;#&lt;/a&gt; 4、安装 Harbor&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# cd /soft/
[root@harbor ~]# wget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz
[root@harbor soft]# tar xf harbor-offline-installer-v2.6.1.tgz
[root@harbor soft]# cd harbor
[root@harbor harbor]# vim harbor.yml
hostname: 192.168.1.134
...
#https:
#  # https port for harbor, default is 443
#  port: 443
#  # The path of cert and key files for nginx
#  certificate: /your/certificate/path
#  private_key: /your/private/key/path
...
harbor_admin_password: Harbor12345
[root@harbor harbor]#  ./install.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-配置nginx负载均衡调度&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-配置nginx负载均衡调度&#34;&gt;#&lt;/a&gt; 5、配置 Nginx 负载均衡调度&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@lb ~]# vim s.hmallleasing.com.conf
server &amp;#123;
    listen 443 ssl;
    server_name harbor.hmallleasing.com;
    client_max_body_size 1G; 
    ssl_prefer_server_ciphers on;
    ssl_certificate  /etc/nginx/sslkey/_.hmallleasing.com_chain.crt;
    ssl_certificate_key  /etc/nginx/sslkey/_.hmallleasing.com_key.key;
    location / &amp;#123;
        proxy_pass http://192.168.1.134;
#      include proxy_params;
#        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        proxy_connect_timeout 30;
        proxy_send_timeout 60;
        proxy_read_timeout 60;
        
        proxy_buffering on;
        proxy_buffer_size 32k;
        proxy_buffers 4 128k;
        proxy_temp_file_write_size 10240k;		
        proxy_max_temp_file_size 10240k;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-推送镜像至harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-推送镜像至harbor&#34;&gt;#&lt;/a&gt; 6、推送镜像至 Harbor&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor harbor]# docker tag beae173ccac6 harbor.hmallleasing.com/ops/busybox.v1
[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1
[root@harbor harbor]# docker login harbor.hmallleasing.com
[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-harbor停止与启动&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-harbor停止与启动&#34;&gt;#&lt;/a&gt; 7、Harbor 停止与启动&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#停用Harbor
[root@harbor harbor]# pwd
/soft/harbor
[root@harbor harbor]# docker-compose stop
 #启动Harbor
[root@harbor harbor]# docker-compose up -d
[root@harbor harbor]# docker-compose start
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Harbor" />
        <updated>2025-03-30T08:17:00.000Z</updated>
    </entry>
</feed>
