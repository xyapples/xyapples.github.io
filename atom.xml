<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://xuyong.cn</id>
    <title>LinuxSre云原生</title>
    <link href="http://xuyong.cn" />
    <updated>2025-04-09T11:50:06.000Z</updated>
    <category term="Kubernetes" />
    <category term="Harbor" />
    <category term="rsync" />
    <category term="Redis" />
    <entry>
        <id>http://xuyong.cn/posts/1414180692.html</id>
        <title>Redis集群（主从+哨兵）模式</title>
        <link rel="alternate" href="http://xuyong.cn/posts/1414180692.html"/>
        <content type="html">&lt;h3 id=&#34;一-什么是redis主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#一-什么是redis主从复制&#34;&gt;#&lt;/a&gt; 一、什么是 redis 主从复制？&lt;/h3&gt;
&lt;p&gt;主从复制，是指将一台 Redis 服务器的数据，复制到其他的 Redis 服务器。前者称为主节点 (master)，后者称为从节点 (slave), 数据的复制是单向的，只能由主节点到从节点。master 以写为主，slave 以读为主。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgTlKx&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgTlKx.png&#34; alt=&#34;pEgTlKx.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;二-主从复制的作用&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#二-主从复制的作用&#34;&gt;#&lt;/a&gt; 二、主从复制的作用&lt;/h3&gt;
&lt;p&gt;数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。&lt;br /&gt;
故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。&lt;br /&gt;
负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。&lt;br /&gt;
读写分离：用于实现读写分离，主库写、从库读，读写分离不仅可以提高服务器的负载能力，同时可根据需求的变化，改变从库的数量；&lt;br /&gt;
高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是 Redis 高可用的基础。&lt;/p&gt;
&lt;h3 id=&#34;三-实现主从复制&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#三-实现主从复制&#34;&gt;#&lt;/a&gt; 三、实现主从复制&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;redis01&lt;/td&gt;
&lt;td&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;redis02&lt;/td&gt;
&lt;td&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;slave&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;redis03&lt;/td&gt;
&lt;td&gt;192.168.40.103&lt;/td&gt;
&lt;td&gt;slave&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;31-关闭防火墙-selinux&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-关闭防火墙-selinux&#34;&gt;#&lt;/a&gt; 3.1 关闭防火墙、selinux&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@master01 ~]# hostnamectl set-hostname redis01
[root@redis01 ~]# systemctl stop firewalld
[root@redis01 ~]# systemctl disable firewalld
[root@redis01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@redis01 ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@redis01 ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@redis01 ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@redis01 ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@redis01 ~]# ntpdate time2.aliyun.com
[root@redis01 ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/null
[root@redis01 ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;32-安装redis&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#32-安装redis&#34;&gt;#&lt;/a&gt; 3.2 安装 redis&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 ~]# yum install gcc-c++ -y
[root@redis01 soft]# wget https://download.redis.io/releases/redis-6.2.11.tar.gz
[root@redis01 soft]# tar xf redis-6.2.11.tar.gz 
[root@redis01 soft]# ln -s /soft/redis-6.2.11 /soft/redis
[root@redis01 soft]# cd /soft/redis
[root@redis01 redis]# make            #执行make编译
[root@redis01 redis]# make install    #将 src下的许多可执行文件复制到/usr/local/bin 目录下
[root@redis01 redis]# redis-server /soft/redis/redis.conf &amp;amp;
[root@redis01 redis]# netstat -lntp|grep redis
tcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      69686/redis-server  
tcp6       0      0 ::1:6379                :::*                    LISTEN      69686/redis-server     
[root@redis01 redis]# redis-cli shutdown      #关闭Redis服务
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;33-redis配置文件说明&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#33-redis配置文件说明&#34;&gt;#&lt;/a&gt; 3.3 redis 配置文件说明&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 redis]# vim redis.conf 
bind 127.0.0.1      		# 绑定的ip
protected-mode yes  		# 保护模式
port 6379           		# 端口设置
daemonize yes               # 后台启动
bind 127.0.0.1      		# 绑定的ip
protected-mode yes  		# 保护模式
port 6379           		# 端口设置
loglevel notice     		# 记录日志级别
logfile &amp;quot;redis.log&amp;quot;         # 日志的文件位置名
dir ./               		# 日志存储目录
databases 16        		# 数据库的数量，默认是 16 个数据库
always-show-logo yes 		# 是否总是显示LOGO

# 如果900s内，如果至少有一个1 key进行了修改，我们及进行持久化操作
save 900 1
# 如果300s内，如果至少10 key进行了修改，我们及进行持久化操作
save 300 10
# 如果60s内，如果至少10000 key进行了修改，我们及进行持久化操作
save 60 10000
# 我们之后学习持久化，会自己定义这个测试！
stop-writes-on-bgsave-error yes   # 持久化如果出错，是否还需要继续工作！
rdbcompression yes                # 是否压缩 rdb 文件，需要消耗一些cpu资源！
rdbchecksum yes                   # 保存rdb文件的时候，进行错误的检查校验！
dbfilename dump.rdb               # rdb 文件保存的名称！
dir ./                            # rdb 文件保存的目录！

slaveof 192.168.1.154 6379        # 配置主从复制
requirepass foobared              # 配置redis登录密码

appendonly no    # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！
appendfilename &amp;quot;appendonly.aof&amp;quot;   # 持久化的文件的名字
# appendfsync always        # 每次修改都会 sync。消耗性能
appendfsync everysec        # 每秒执行一次 sync，可能会丢失这1s的数据！
# appendfsync no            # 不执行 sync，这个时候操作系统自己同步数据，速度最快！
no-appendfsync-on-rewrite   #重写时是否可以运用appendsync，默认no，可以保证数据的安全性
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;34-redis环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#34-redis环境配置&#34;&gt;#&lt;/a&gt; 3.4 redis 环境配置&lt;/h4&gt;
&lt;p&gt;#修改 maser 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.101 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#修改 slave01 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.102 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
replicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#修改 slave02 配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim redis.conf
bind 192.168.40.103 #绑定本机ip地址
port 6739          #绑定端口号
daemonize yes      #用来指定redis是否要用守护进程的方式启动，默认为no
pidfile /var/run/redis_6379.pid
logfile &amp;quot;redis.log&amp;quot;   #redis日志文件
replicaof  192.168.40.101 6379 #配置文件中设置主节点，redis主从复制这个地方只配置从库，注意:主库不需要这个配置
requirepass Superman*2023  #本地redis密码
masterauth Superman*2023   #主节点redis密码 注意:从节点也要配置，后边哨兵容灾切换用到
protected-mode yes    #保护模式
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;35-启动3台redis服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#35-启动3台redis服务&#34;&gt;#&lt;/a&gt; 3.5 启动 3 台 redis 服务&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#启动redis01
[root@redis01 redis]# redis-server /soft/redis/redis.conf
[root@redis0[root@redis01 redis]# redis-server /soft/redis/redis.conf redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server 

#启动redis02
[root@redis02 redis]# redis-server /soft/redis/redis.conf
[root@redis02 redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server

启动redis03
[root@redis03 redis]# redis-server /soft/redis/redis.conf
[root@redis03 redis]# netstat -lntp|grep redis
tcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;36-查看主从状态&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#36-查看主从状态&#34;&gt;#&lt;/a&gt; 3.6 查看主从状态&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#主节点
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023
192.168.40.101:6379&amp;gt; info replication
# Replication
role:master
connected_slaves:2
slave0:ip=192.168.40.102,port=6379,state=online,offset=616,lag=0
slave1:ip=192.168.40.103,port=6379,state=online,offset=616,lag=0
master_failover_state:no-failover
master_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:616
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:616

#从节点
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.103:6379&amp;gt; info replication
# Replication
role:slave
master_host:192.168.40.101
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:812
slave_repl_offset:812
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:93df7cd5095dcccdbf8266787031b17cf638a2ad
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:812
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:295
repl_backlog_histlen:518
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;37-测试主从&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#37-测试主从&#34;&gt;#&lt;/a&gt; 3.7 测试主从&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.101:6379&amp;gt; set k1 v1
OK
192.168.40.101:6379&amp;gt; set k2 v2
OK

[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103 -a Superman*2023
Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.40.103:6379&amp;gt; get k1
&amp;quot;v1&amp;quot;
192.168.40.103:6379&amp;gt; get k2
&amp;quot;v2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;注意:&lt;/strong&gt;&lt;br /&gt;
1、主机可以写，从机不能写，只能读。主机中的所有数据都会保存到从机中去。&lt;br /&gt;
2、主机断开连接，从机依旧连接到主机的，但是没有写操作，这个时候，主机如果回来了，从机依旧可以直接获取到主机写的信息！&lt;br /&gt;
3、如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从主机中获取值！&lt;br /&gt;
4、主从复制原理&lt;br /&gt;
 Slave 启动成功连接到 master 后会发送一个 sync 同步命令&lt;br /&gt;
 Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，并完成一次完全同步。&lt;br /&gt;
全量复制：slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。&lt;br /&gt;
增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave，完成同步，但是只要是重新连接 master，一次完全同步（全量复制）将被自动执行！ 主机的数据一定可以在从机中看到。&lt;/p&gt;
&lt;h3 id=&#34;四-哨兵模式搭建&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#四-哨兵模式搭建&#34;&gt;#&lt;/a&gt; 四、哨兵模式搭建&lt;/h3&gt;
&lt;p&gt;1、什么是 redis 哨兵？&lt;br /&gt;
RedisSentinel 是 Redis 的高可用性解决方案，由一个或多个 Sentinel（哨兵）实例组成。它可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，它的主要功能如下：&lt;br /&gt;
监控 (Monitoring)：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。&lt;br /&gt;
通知 (Notification)：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。&lt;br /&gt;
故障迁移：当主服务器不能正常工作时，Sentinel 会自动进行故障迁移，也就是主从切换。&lt;br /&gt;
统一的配置：管理连接者询问 sentinel 取得主从的地址。&lt;/p&gt;
&lt;p&gt;2、哨兵原理是什么？&lt;br /&gt;
Sentinel 使用的算法核心是 Raft 算法，主要用途就是用于分布式系统，系统容错，以及 Leader 选举，每个 Sentinel 都需要定期的执行以下任务：&lt;br /&gt;
每个 Sentinel 会自动发现其他 Sentinel 和从服务器，它以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。&lt;br /&gt;
如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。&lt;br /&gt;
如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有 Sentinel 要以每秒一次的频率确认主服务器的确进入了主观下线状态。&lt;br /&gt;
如果一个主服务器被标记为主观下线， 并且有足够数量的 Sentinel（至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断，那么这个主服务器被标记为客观下线。&lt;br /&gt;
在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO 命令。当一个主服务器 Sentinel 标记为客观下线时，Sentinel 向下线主服务器的所有从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。&lt;br /&gt;
当没有足够数量的 Sentinel 同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向 Sentinel 的 PING 命令返回有效回复时， 主服务器的主管下线状态就会被移除。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgT1r6&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgT1r6.png&#34; alt=&#34;pEgT1r6.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;41-搭建哨兵&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#41-搭建哨兵&#34;&gt;#&lt;/a&gt; 4.1 搭建哨兵&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;在每台服务器上部署一个哨兵，配置方式如下:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# vim sentinel.conf
#端口默认为26379。
port 26379
#关闭保护模式，可以外部访问。
protected-mode no
#设置为后台启动。
daemonize yes
#日志文件。
logfile &amp;quot;/soft/redis/sentinel.log&amp;quot;
#指定服务器IP地址和端口，并且指定当有2台哨兵认为主机挂了，则对主机进行容灾切换。注意:三台哨兵这里的ip配置均为主节点ip 和端口
sentinel monitor mymaster 192.168.40.101 6379 2
#当在Redis实例中开启了requirepass，这里就需要提供密码。
sentinel auth-pass mymaster psw66
#这里设置了主机多少秒无响应，则认为挂了。
sentinel down-after-milliseconds mymaster 3000
#主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的
snetinel parallel-syncs mymaster 1
#故障转移的超时时间，这里设置为三分钟。
sentinel failover-timeout mymaster 180000
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;42-启动三台服务器上的哨兵&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#42-启动三台服务器上的哨兵&#34;&gt;#&lt;/a&gt; 4.2 启动三台服务器上的哨兵&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#启动redis01的sentine
[root@redis01 redis]# redis-sentinel /soft/redis/sentinel.conf
[root@redis01 redis]#  netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      33536/redis-sentine 
tcp        0      0 192.168.40.101:6379     0.0.0.0:*               LISTEN      117358/redis-server 
tcp6       0      0 :::26379                :::*                    LISTEN      33536/redis-sentine

#启动redis02的sentine
[root@redis02 redis]# redis-sentinel /soft/redis/sentinel.conf
[root@redis02 redis]#  netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      18757/redis-sentine 
tcp        0      0 192.168.40.102:6379     0.0.0.0:*               LISTEN      18210/redis-server  
tcp6       0      0 :::26379                :::*                    LISTEN      18757/redis-sentine

#启动redis03的sentine
[root@redis03 redis]# redis-sentinel /soft/redis/sentinel.conf                     
[root@redis03 redis]# netstat -lntp|grep redis
tcp        0      0 0.0.0.0:26379           0.0.0.0:*               LISTEN      19745/redis-sentine 
tcp        0      0 192.168.40.103:6379     0.0.0.0:*               LISTEN      19186/redis-server  
tcp6       0      0 :::26379                :::*                    LISTEN      19745/redis-sentine
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;43-连接客户端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#43-连接客户端&#34;&gt;#&lt;/a&gt; 4.3 连接客户端&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-cli -p 26379
127.0.0.1:26379&amp;gt;  info sentinel
# Sentinel
sentinel_masters:1
sentinel_tilt:0
sentinel_running_scripts:0
sentinel_scripts_queue_length:0
sentinel_simulate_failure_flags:0
master0:name=mymaster,status=ok,address=192.168.40.101:6379,slaves=2,sentinels=3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;44-redis容灾切换&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#44-redis容灾切换&#34;&gt;#&lt;/a&gt; 4.4 redis 容灾切换&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#连接redis客户端
[root@redis01 redis]# redis-cli -p 6379 -h 192.168.40.101 
#验证密码
192.168.40.101:6379&amp;gt; auth Superman*2023
OK
#关闭redis服务
192.168.40.101:6379&amp;gt; shutdown
not connected&amp;gt;
#退出客户端
not connected&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;关闭主节点之后，我们去查看哨兵日志:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 ~]# tail -f /soft/redis/sentinel.log 
91936:X 14 Apr 2023 23:26:23.838 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
91936:X 14 Apr 2023 23:26:23.838 # Redis version=6.2.11, bits=64, commit=00000000, modified=0, pid=91936, just started
91936:X 14 Apr 2023 23:26:23.838 # Configuration loaded
91936:X 14 Apr 2023 23:26:23.838 * monotonic clock: POSIX clock_gettime
91936:X 14 Apr 2023 23:26:23.839 * Running mode=sentinel, port=26379.
91936:X 14 Apr 2023 23:26:23.839 # Sentinel ID is 835b4c8544fb250af5fd479f834ee369cc4f388e
91936:X 14 Apr 2023 23:26:23.839 # +monitor master mymaster 192.168.40.101 6379 quorum 2



91936:X 14 Apr 2023 23:31:25.329 # +sdown master mymaster 192.168.40.101 6379   #这里应该是发现主节点宕机
91936:X 14 Apr 2023 23:31:25.359 # +new-epoch 5
91936:X 14 Apr 2023 23:31:25.360 # +vote-for-leader ab43979285cb47b1b459aeb0ab91b63fa9d1a989 5
91936:X 14 Apr 2023 23:31:25.401 # +odown master mymaster 192.168.40.101 6379 #quorum 3/2 两个哨兵都觉得主节点宕机了
91936:X 14 Apr 2023 23:31:25.401 # Next failover delay: I will not start a failover before Fri Apr 14 23:37:25 2023
91936:X 14 Apr 2023 23:31:26.468 # +config-update-from sentinel ab43979285cb47b1b459aeb0ab91b63fa9d1a989 192.168.40.102 26379 @ mymaster 192.168.40.101 6379
91936:X 14 Apr 2023 23:31:26.468 # +switch-master mymaster 192.168.40.101 6379 192.168.40.103 6379 #通过投票选举40.103为新的主节点
91936:X 14 Apr 2023 23:31:26.468 * +slave slave 192.168.40.102:6379 192.168.40.102 6379 @ mymaster 192.168.40.103 6379
91936:X 14 Apr 2023 23:31:26.469 * +slave slave 192.168.40.101:6379 192.168.40.101 6379 @ mymaster 192.168.40.103 6379
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面我们去 40.103 下查看哨兵主从切换是否成功&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis03 redis]# redis-cli -p 6379 -h 192.168.40.103
192.168.40.103:6379&amp;gt; auth Superman*2023
OK
192.168.40.103:6379&amp;gt; info replication
# Replication
role:master   # 40.103变成主节点了
connected_slaves:1   # 下面的从机个数为1
slave0:ip=192.168.40.102,port=6379,state=online,offset=108708,lag=1
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935
master_repl_offset:108851
second_repl_offset:59887
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:108851
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重新连接挂掉的主节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@redis01 redis]# redis-server redis.conf 
[root@redis01 redis]#  redis-cli -p 6379 -h 192.168.40.101
192.168.40.101:6379&amp;gt; auth Superman*2023
OK
192.168.40.101:6379&amp;gt; info replication
# Replication
role:slave          #主节点连接回来之后自动变成了从节点，并且成功连上了主机
master_host:192.168.40.103
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_read_repl_offset:130607
slave_repl_offset:130607
slave_priority:100
slave_read_only:1
replica_announced:1
connected_slaves:0
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:130607
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:126982
repl_backlog_histlen:3626
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再去主节点确认一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;192.168.40.103:6379&amp;gt; info replication
# Replication
role:master
connected_slaves:2   #两个从节点
slave0:ip=192.168.40.102,port=6379,state=online,offset=147879,lag=1
slave1:ip=192.168.40.101,port=6379,state=online,offset=147879,lag=1
master_failover_state:no-failover
master_replid:cf36f762dcae0c07b54f7287dc19d7ecc0d50dd3
master_replid2:a7de32d10b2d31f8886c84ca91dc7f055439c935
master_repl_offset:148165
second_repl_offset:59887
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:148165
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;五、哨兵模式的优缺点&lt;br /&gt;
 1. 优点&lt;/p&gt;
&lt;p&gt;哨兵集群，基于主从复制模式，所有的主从配置优点，它全有&lt;/p&gt;
&lt;p&gt;主从可以切换，故障可以转移，系统的可用性就会更好&lt;/p&gt;
&lt;p&gt;哨兵模式就是主从模式的升级，手动到自动，更加健壮！&lt;/p&gt;
&lt;p&gt;2. 缺点&lt;/p&gt;
&lt;p&gt;Redis 不好在线扩容，集群容量一旦到达上限，在线扩容就十分麻烦&lt;/p&gt;
&lt;p&gt;哨兵模式的配置繁琐&lt;/p&gt;
&lt;p&gt;3. 哨兵模式的配置文件详解&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Example sentinel.conf
# 哨兵sentinel实例运行的端口 默认26379
port 26379
 
# 哨兵sentinel的工作目录
dir /tmp
 
# 哨兵sentinel监控的redis主节点的 ip port
# master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&amp;quot;.-_&amp;quot;组成。
# quorum 配置多少个sentinel哨兵统一认为master主节点失联 那么这时客观上认为主节点失联了
# sentinel monitor &amp;lt;master-name&amp;gt; &amp;lt;ip&amp;gt; &amp;lt;redis-port&amp;gt; &amp;lt;quorum&amp;gt;
sentinel monitor mymaster 127.0.0.1 6379 2
  
# 当在Redis实例中开启了requirepass foobared 授权密码这样所有连接Redis实例的客户端都要提供 密码
# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码
# sentinel auth-pass &amp;lt;master-name&amp;gt; &amp;lt;password&amp;gt;
sentinel auth-pass mymaster MySUPER--secret-0123passw0rd
 
# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时哨兵主观上认为主节点下线 默认30秒
# sentinel down-after-milliseconds &amp;lt;master-name&amp;gt; &amp;lt;milliseconds&amp;gt;
sentinel down-after-milliseconds mymaster 30000
 
# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长， 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。
# sentinel parallel-syncs &amp;lt;master-name&amp;gt; &amp;lt;numslaves&amp;gt;
sentinel parallel-syncs mymaster 1
 
# 故障转移的超时时间 failover-timeout 可以用在以下这些方面：
#1. 同一个sentinel对同一个master两次failover之间的间隔时间。
#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那 里同步数据时。
#3.当想要取消一个正在进行的failover所需要的时间。
#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时， slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了 # 默认三分钟
# sentinel failover-timeout &amp;lt;master-name&amp;gt; &amp;lt;milliseconds&amp;gt; bilibili：
sentinel failover-timeout mymaster 180000
 
# SCRIPTS EXECUTION
#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知 相关人员。
#对于脚本的运行结果有以下规则：
#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10
#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。
#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。
#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。
#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等）， 将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信 息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配 置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无 法正常启动成功。
#通知脚本
# shell编程
# sentinel notification-script &amp;lt;master-name&amp;gt; &amp;lt;script-path&amp;gt; sentinel
notification-script mymaster /var/redis/notify.sh
 
# 客户端重新配置主节点参数脚本
# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已 经发生改变的信息。
# 以下参数将会在调用脚本时传给脚本:
# &amp;lt;master-name&amp;gt; &amp;lt;role&amp;gt; &amp;lt;state&amp;gt; &amp;lt;from-ip&amp;gt; &amp;lt;from-port&amp;gt; &amp;lt;to-ip&amp;gt; &amp;lt;to-port&amp;gt; # 目前&amp;lt;state&amp;gt;总是“failover”,
# &amp;lt;role&amp;gt;是“leader”或者“observer”中的一个。
# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通 信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。
# sentinel client-reconfig-script &amp;lt;master-name&amp;gt; &amp;lt;script-path&amp;gt; sentinel client-reconfig-
script mymaster /var/redis/reconfig.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;再去看一下 redis 的配置文件和哨兵的配置文件，你会惊讶的发现，里边的配置文件已经被改过来了。&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat redis.con
...
replicaof 192.168.40.103 6379
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Redis" />
        <updated>2025-04-09T11:50:06.000Z</updated>
    </entry>
    <entry>
        <id>http://xuyong.cn/posts/3166738000.html</id>
        <title>Kubeadm高可用安装K8s集群</title>
        <link rel="alternate" href="http://xuyong.cn/posts/3166738000.html"/>
        <content type="html">&lt;h2 id=&#34;kubeadm高可用安装k8s集群&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#kubeadm高可用安装k8s集群&#34;&gt;#&lt;/a&gt; Kubeadm 高可用安装 K8s 集群&lt;/h2&gt;
&lt;h4 id=&#34;1-基本配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-基本配置&#34;&gt;#&lt;/a&gt; 1. 基本配置&lt;/h4&gt;
&lt;h5 id=&#34;11-基本环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-基本环境配置&#34;&gt;#&lt;/a&gt; 1.1 基本环境配置&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP 地址&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master01 ~ 03&lt;/td&gt;
&lt;td&gt;192.168.1.71 ~ 73&lt;/td&gt;
&lt;td&gt;master 节点 * 3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;192.168.1.70&lt;/td&gt;
&lt;td&gt;keepalived 虚拟 IP（不占用机器）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-node01 ~ 02&lt;/td&gt;
&lt;td&gt;192.168.1.74/75&lt;/td&gt;
&lt;td&gt;worker 节点 * 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;请统一替换这些网段，Pod 网段和 service 和宿主机网段不要重复！！！&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;&lt;strong&gt;* 配置信息 *&lt;/strong&gt;&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;系统版本&lt;/td&gt;
&lt;td&gt;Rocky Linux 8/9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containerd&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 网段&lt;/td&gt;
&lt;td&gt;172.16.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service 网段&lt;/td&gt;
&lt;td&gt;10.96.0.0/16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改主机名（其它节点按需修改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hostnamectl set-hostname k8s-master01 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 hosts，修改 /etc/hosts 如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.71 k8s-master01
192.168.1.72 k8s-master02
192.168.1.73 k8s-master03
192.168.1.74 k8s-node01
192.168.1.75 k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 yum 源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 配置基础源
sed -e &#39;s|^mirrorlist=|#mirrorlist=|g&#39; \
    -e &#39;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#39; \
    -i.bak \
    /etc/yum.repos.d/*.repo

yum makecache
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;必备工具安装：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git rsyslog -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭防火墙、selinux、dnsmasq、swap、开启 rsyslog。服务器配置如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld 
systemctl disable --now dnsmasq
setenforce 0
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/sysconfig/selinux
sed -i &#39;s#SELINUX=enforcing#SELINUX=disabled#g&#39; /etc/selinux/config
systemctl enable --now rsyslog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;关闭 swap 分区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a &amp;amp;&amp;amp; sysctl -w vm.swappiness=0
sed -ri &#39;/^[^#]*swap/s@^@#@&#39; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ntpdate：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dnf install epel-release -y
sudo dnf config-manager --set-enabled epel
sudo dnf install ntpsec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;同步时间并配置上海时区：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
ntpdate time2.aliyun.com
# 加入到crontab
crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 limit：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ulimit -SHn 65535
vim /etc/security/limits.conf
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;升级系统：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum update -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;免密钥登录其他节点，安装过程中生成配置文件和证书均在 Master01 上操作，集群管理也在 Master01 上操作：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;注意：公有云环境，可能需要把 kubectl 放在一个非 Master 节点上&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;下载安装所有的源码文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/ ; git clone https://gitee.com/chinagei/k8s-ha-install
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-内核配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-内核配置&#34;&gt;#&lt;/a&gt; 1.2 内核配置&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 ipvsadm：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install ipvsadm ipset sysstat conntrack libseccomp -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 ipvs 模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;创建 ipvs.conf，并配置开机自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/modules-load.d/ipvs.conf 
# 加入以下内容
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;然后执行 systemctl enable --now systemd-modules-load.service 即可（报错不用管）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable --now systemd-modules-load.service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;内核优化配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;应用配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置完内核后，重启机器，之后查看内核模块是否已自动加载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reboot
lsmod | grep --color=auto -e ip_vs -e nf_conntrack
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-高可用组件安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-高可用组件安装&#34;&gt;#&lt;/a&gt; 2. 高可用组件安装&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;注意：如果安装的不是高可用集群，haproxy 和 keepalived 无需安装&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注意：公有云要用公有云自带的负载均衡，比如阿里云的 SLB、NLB，腾讯云的 ELB，用来替代 haproxy 和 keepalived，因为公有云大部分都是不支持 keepalived 的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;通过 yum 安装 HAProxy 和 KeepAlived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install keepalived haproxy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 HAProxy，需要注意黄色部分的 IP：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/haproxy
[root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg 
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443       #HAProxy监听端口
  bind 127.0.0.1:16443     #HAProxy监听端口
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master01	192.168.1.71:6443  check       #API Server IP地址
  server k8s-master02	192.168.1.72:6443  check       #API Server IP地址
  server k8s-master03	192.168.1.73:6443  check       #API Server IP地址
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 Master 节点&lt;/mark&gt;配置 KeepAlived，需要注意黄色部分的配置。&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 etc]# mkdir /etc/keepalived

[root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
    interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state MASTER
    interface ens160               #网卡名称
    mcast_src_ip 192.168.1.71      #K8s-master01 IP地址
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70        #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master02 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
   interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                #网卡名称
    mcast_src_ip 192.168.1.72       #K8s-master02 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70              #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master03 节点&lt;/mark&gt;的配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs &amp;#123;
    router_id LVS_DEVEL
script_user root
    enable_script_security
&amp;#125;
vrrp_script chk_apiserver &amp;#123;
    script &amp;quot;/etc/keepalived/check_apiserver.sh&amp;quot;
 interval 5
    weight -5
    fall 2  
rise 1
&amp;#125;
vrrp_instance VI_1 &amp;#123;
    state BACKUP
    interface ens160                 #网卡名称
    mcast_src_ip 192.168.1.73        #K8s-master03 IP地址
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication &amp;#123;
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    &amp;#125;
    virtual_ipaddress &amp;#123;
        192.168.1.70          #VIP地址
    &amp;#125;
    track_script &amp;#123;
       chk_apiserver
    &amp;#125;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置 KeepAlived 健康检查文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# vim /etc/keepalived/check_apiserver.sh 
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == &amp;quot;&amp;quot; ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &amp;quot;0&amp;quot; ]]; then
    echo &amp;quot;systemctl stop keepalived&amp;quot;
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;配置健康检查文件添加执行权限：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chmod +x /etc/keepalived/check_apiserver.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有 master 节点&lt;/mark&gt;启动 haproxy 和 keepalived：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 keepalived]# systemctl daemon-reload
[root@k8s-master01 keepalived]# systemctl enable --now haproxy
[root@k8s-master01 keepalived]# systemctl enable --now keepalived
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;重要：如果安装了 keepalived 和 haproxy，需要测试 keepalived 是否是正常的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;所有节点测试VIP
[root@k8s-master01 ~]# ping 192.168.1.70 -c 4
PING 192.168.1.70 (192.168.1.70) 56(84) bytes of data.
64 bytes from 192.168.1.70: icmp_seq=1 ttl=64 time=0.464 ms
64 bytes from 192.168.1.70: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 192.168.1.70: icmp_seq=3 ttl=64 time=0.062 ms
64 bytes from 192.168.1.70: icmp_seq=4 ttl=64 time=0.063 ms

[root@k8s-master01 ~]# telnet 192.168.1.70 16443
Trying 192.168.1.70...
Connected to 192.168.1.70.
Escape character is &#39;^]&#39;.
Connection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果 ping 不通且 telnet 没有出现 ] ，则认为 VIP 不可以，不可在继续往下执行，需要排查 keepalived 的问题，比如防火墙和 selinux，haproxy 和 keepalived 的状态，监听端口等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有节点查看防火墙状态必须为 disable 和 inactive：systemctl status firewalld&lt;/li&gt;
&lt;li&gt;所有节点查看 selinux 状态，必须为 disable：getenforce&lt;/li&gt;
&lt;li&gt;master 节点查看 haproxy 和 keepalived 状态：systemctl status keepalived haproxy&lt;/li&gt;
&lt;li&gt;master 节点查看监听端口：netstat -lntp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果以上都没有问题，需要确认：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;是否是公有云机器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;是否是私有云机器（类似 OpenStack）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述公有云一般都是不支持 keepalived，私有云可能也有限制，需要和自己的私有云管理员咨询&lt;/p&gt;
&lt;h4 id=&#34;3-runtime安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-runtime安装&#34;&gt;#&lt;/a&gt; 3. Runtime 安装&lt;/h4&gt;
&lt;p&gt;如果安装的版本低于 1.24，选择 Docker 和 Containerd 均可，高于 1.24 建议选择 Containerd 作为 Runtime，不再推荐使用 Docker 作为 Runtime。&lt;/p&gt;
&lt;h5 id=&#34;31-安装containerd&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#31-安装containerd&#34;&gt;#&lt;/a&gt; 3.1 安装 Containerd&lt;/h5&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置安装源：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -y
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 docker-ce（如果在以前已经安装过，需要重新安装更新一下）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install docker-ce containerd -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;可以无需启动 Docker，只需要配置和启动 Containerd 即可。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先配置 Containerd 所需的模块（&lt;mark&gt;所有节点&lt;/mark&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载模块：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# modprobe -- overlay
# modprobe -- br_netfilter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;，配置 Containerd 所需的内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;加载内核：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sysctl --system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;生成 Containerd 的配置文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;更改 Containerd 的 Cgroup 和 Pause 镜像配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s#SystemdCgroup = false#SystemdCgroup = true#g&#39; /etc/containerd/config.toml
sed -i &#39;s#k8s.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.gcr.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io/pause#registry.cn-hangzhou.aliyuncs.com/google_containers/pause#g&#39;  /etc/containerd/config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;启动 Containerd，并配置开机自启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置 crictl 客户端连接的运行时位置（可选）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/crictl.yaml &amp;lt;&amp;lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-安装kubernetes组件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-安装kubernetes组件&#34;&gt;#&lt;/a&gt; 4 . 安装 Kubernetes 组件&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;配置源（注意更改版本号）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先在&lt;mark&gt; Master01 节点&lt;/mark&gt;查看最新的 Kubernetes 版本是多少：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum list kubeadm.x86_64 --showduplicates | sort -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;安装 1.32 最新版本 kubeadm、kubelet 和 kubectl：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# yum install kubeadm-1.32* kubelet-1.32* kubectl-1.32* -y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;设置 Kubelet 开机自启动（由于还未初始化，没有 kubelet 的配置文件，此时 kubelet 无法启动，无需关心）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;此时 kubelet 是起不来的，日志会有报错不影响！&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;5-集群初始化&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-集群初始化&#34;&gt;#&lt;/a&gt; 5 . 集群初始化&lt;/h4&gt;
&lt;p&gt;以下操作在&lt;mark&gt; master01&lt;/mark&gt;（注意黄色部分）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.71
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  name: k8s-master01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  certSANs:
  - 192.168.1.70               # 如果搭建的不是高可用集群，把此处改为master的IP
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.1.70:16443 # 如果搭建的不是高可用集群，把此处IP改为master的IP，端口改成6443
controllerManager: &amp;#123;&amp;#125;
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.32.3    # 更改此处的版本号和kubeadm version一致
networking:
  dnsDomain: cluster.local
  podSubnet: 172.16.0.0/16    # 注意此处的网段，不要与service和节点网段冲突
  serviceSubnet: 10.96.0.0/16 # 注意此处的网段，不要与pod和节点网段冲突
scheduler: &amp;#123;&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;master01 节点&lt;/mark&gt;更新 kubeadm 文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 new.yaml 文件复制到&lt;mark&gt;其他 master 节点&lt;/mark&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之后&lt;mark&gt;所有 Master 节点&lt;/mark&gt;提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括 IP 地址也不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm config images pull --config /root/new.yaml 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正确的反馈信息如下（&lt;em&gt;&lt;strong&gt;* 版本可能不一样 *&lt;/strong&gt;&lt;/em&gt;）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master02 ~]# kubeadm config images pull --config /root/new.yaml 
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.32.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.3
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.10
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.16-0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;初始化，初始化以后会在 /etc/kubernetes 目录下生成对应的证书和配置文件，之后其他 Master 节点加入 Master01 即可：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init --config /root/new.yaml  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初始化成功以后，会产生 Token 值，用于其他节点加入时使用，因此要记录下初始化成功生成的 token 值（令牌值）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

# 不要复制文档当中的，要去使用节点生成的
  kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&amp;quot;kubeadm init phase upload-certs --upload-certs&amp;quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;配置环境变量，用于访问 Kubernetes 集群：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF
source /root/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;mark&gt;Master01 节点&lt;/mark&gt;查看节点状态：（显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE   VERSION
k8s-master01   NotReady   control-plane   24s   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;采用初始化安装方式，所有的系统组件均以容器的方式运行并且在 kube-system 命名空间内，此时可以查看 Pod 状态（显示 pending 不影响）：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-\&#34;&gt;# kubectl get pods -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;51-初始化失败排查&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#51-初始化失败排查&#34;&gt;#&lt;/a&gt; 5.1 初始化失败排查&lt;/h5&gt;
&lt;p&gt;如果初始化失败，重置后再次初始化，命令如下（没有失败不要执行）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果多次尝试都是初始化失败，需要看系统日志，CentOS/RockyLinux 日志路径:/var/log/messages，Ubuntu 系列日志路径:/var/log/syslog：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/messages | grep -v &amp;quot;not found&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经常出错的原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Containerd 的配置文件修改的不对，自行参考《安装 containerd》小节核对&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，比如非高可用集群忘记修改 16443 端口为 6443&lt;/li&gt;
&lt;li&gt;new.yaml 配置问题，三个网段有交叉，出现 IP 地址冲突&lt;/li&gt;
&lt;li&gt;VIP 不通导致无法初始化成功，此时 messages 日志会有 VIP 超时的报错&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;52-高可用master&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#52-高可用master&#34;&gt;#&lt;/a&gt; 5.2 高可用 Master&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;其他 master&lt;/strong&gt; 加入集群，master02 和 master03 分别执行 (千万不要在 master01 再次执行，不能直接复制文档当中的命令，而是你自己刚才 master01 初始化之后产生的命令)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:df72788de04bbc2e8fca70becb8a9e8503a962b5d7cd9b1842a0c39930d08c94 \
	--control-plane --certificate-key c595f7f4a7a3beb0d5bdb75d9e4eff0a60b977447e76c1d6885e82c3aa43c94c
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看当前状态：（如果显示 NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;53-token过期处理&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#53-token过期处理&#34;&gt;#&lt;/a&gt; 5.3 Token 过期处理&lt;/h5&gt;
&lt;p&gt;注意：以下步骤是上述 init 命令产生的 Token 过期了才需要执行以下步骤，如果没有过期不需要执行，直接 join 即可。&lt;/p&gt;
&lt;p&gt;Token 过期后生成新的 token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm token create --print-join-command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Master 需要生成 --certificate-key：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase upload-certs  --upload-certs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-node节点的配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-node节点的配置&#34;&gt;#&lt;/a&gt; 6. Node 节点的配置&lt;/h4&gt;
&lt;p&gt;Node 节点上主要部署公司的一些业务应用，生产环境中不建议 Master 节点部署系统组件之外的其他 Pod，测试环境可以允许 Master 节点部署 Pod 以节省系统资源。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm join 192.168.1.70:16443 --token 7t2weq.bjbawausm0jaxury \
	--discovery-token-ca-cert-hash sha256:377702f508fe70b9d8ab68beccaa9af1b4609b754e4cc2fcc6185974e1d620b5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点初始化完成后，查看集群状态（NotReady 不影响）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   4m23s   v1.32.3
k8s-master02   NotReady   control-plane   66s     v1.32.3
k8s-master03   NotReady   control-plane   14s     v1.32.3
k8s-node01     NotReady   &amp;lt;none&amp;gt;          13s     v1.32.3
k8s-node02     NotReady   &amp;lt;none&amp;gt;          10s     v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-calico组件的安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-calico组件的安装&#34;&gt;#&lt;/a&gt; 7. Calico 组件的安装&lt;/h4&gt;
&lt;p&gt;&lt;mark&gt;所有节点&lt;/mark&gt;禁止 NetworkManager 管理 Calico 的网络接口，防止有冲突或干扰：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt;/etc/NetworkManager/conf.d/calico.conf&amp;lt;&amp;lt;EOF
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
EOF
systemctl daemon-reload
systemctl restart NetworkManager
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下步骤只在&lt;mark&gt; master01&lt;/mark&gt; 执行（.x 不需要更改）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.32.x &amp;amp;&amp;amp; cd calico/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改 Pod 网段：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= &#39;&amp;#123;print $NF&amp;#125;&#39;`

sed -i &amp;quot;s#POD_CIDR#$&amp;#123;POD_SUBNET&amp;#125;#g&amp;quot; calico.yaml
kubectl apply -f calico.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看容器和节点状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6f497d8478-v2q8c   1/1     Running   0          24h
calico-node-7mzmb                          1/1     Running   0          24h
calico-node-ljqnl                          1/1     Running   0          24h
calico-node-njqlb                          1/1     Running   0          24h
calico-node-ph4m4                          1/1     Running   0          24h
calico-node-rx8rl                          1/1     Running   0          24h
coredns-76fccbbb6b-76559                   1/1     Running   0          24h
coredns-76fccbbb6b-hkvn7                   1/1     Running   0          24h
etcd-k8s-master01                          1/1     Running   0          24h
etcd-k8s-master02                          1/1     Running   0          24h
etcd-k8s-master03                          1/1     Running   0          24h
kube-apiserver-k8s-master01                1/1     Running   0          24h
kube-apiserver-k8s-master02                1/1     Running   0          24h
kube-apiserver-k8s-master03                1/1     Running   0          24h
kube-controller-manager-k8s-master01       1/1     Running   0          24h
kube-controller-manager-k8s-master02       1/1     Running   0          24h
kube-controller-manager-k8s-master03       1/1     Running   0          24h
kube-proxy-9dtz4                           1/1     Running   0          24h
kube-proxy-jh7rl                           1/1     Running   0          24h
kube-proxy-jvvwt                           1/1     Running   0          24h
kube-proxy-sh89l                           1/1     Running   0          24h
kube-proxy-t2j49                           1/1     Running   0          24h
kube-scheduler-k8s-master01                1/1     Running   0          24h
kube-scheduler-k8s-master02                1/1     Running   0          24h
kube-scheduler-k8s-master03                1/1     Running   0          24h
metrics-server-7d9d8df576-jgnp2            1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时节点全部变为 Ready 状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   24h   v1.32.3
k8s-master02   Ready    control-plane   24h   v1.32.3
k8s-master03   Ready    control-plane   24h   v1.32.3
k8s-node01     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
k8s-node02     Ready    &amp;lt;none&amp;gt;          24h   v1.32.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;8-metrics部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#8-metrics部署&#34;&gt;#&lt;/a&gt; 8. Metrics 部署&lt;/h4&gt;
&lt;p&gt;在新版的 Kubernetes 中系统资源的采集均使用 Metrics-server，可以通过 Metrics 采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率。&lt;/p&gt;
&lt;p&gt;将&lt;mark&gt; Master01 节点&lt;/mark&gt;的 front-proxy-ca.crt 复制到所有 Node 节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt

scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下操作均在&lt;mark&gt; master01 节点&lt;/mark&gt;执行:&lt;/p&gt;
&lt;p&gt;安装 metrics server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/kubeadm-metrics-server

# kubectl  create -f comp.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看状态：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get po -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-7d9d8df576-jgnp2   1/1     Running   0          24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;等 Pod 变成 1/1   Running 后，查看节点和 Pod 资源使用率：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]#  kubectl top node
NAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
k8s-master01   132m         3%       932Mi           5%          
k8s-master02   131m         3%       845Mi           5%          
k8s-master03   148m         3%       912Mi           5%          
k8s-node01     54m          1%       600Mi           3%          
k8s-node02     49m          1%       602Mi           3%          
[root@k8s-master01 ~]#  kubectl top po -A
NAMESPACE              NAME                                         CPU(cores)   MEMORY(bytes)   
ingress-nginx          ingress-nginx-controller-5v9gl               2m           98Mi            
ingress-nginx          ingress-nginx-controller-r978m               1m           104Mi           
krm                    krm-backend-d7ff675d8-vmt9z                  1m           21Mi            
krm                    krm-frontend-588ffd677b-c2pgj                1m           4Mi             
krm                    nginx-574cf48959-vcfjs                       0m           2Mi             
kube-system            calico-kube-controllers-6f497d8478-v2q8c     6m           17Mi            
kube-system            calico-node-7mzmb                            16m          176Mi           
kube-system            calico-node-ljqnl                            15m          182Mi           
kube-system            calico-node-njqlb                            19m          180Mi           
kube-system            calico-node-ph4m4                            15m          178Mi           
kube-system            calico-node-rx8rl                            17m          180Mi           
kube-system            coredns-76fccbbb6b-76559                     2m           16Mi            
kube-system            coredns-76fccbbb6b-hkvn7                     2m           16Mi            
kube-system            etcd-k8s-master01                            22m          86Mi            
kube-system            etcd-k8s-master02                            27m          84Mi            
kube-system            etcd-k8s-master03                            22m          84Mi            
kube-system            kube-apiserver-k8s-master01                  22m          267Mi           
kube-system            kube-apiserver-k8s-master02                  20m          242Mi           
kube-system            kube-apiserver-k8s-master03                  18m          241Mi           
kube-system            kube-controller-manager-k8s-master01         6m           69Mi            
kube-system            kube-controller-manager-k8s-master02         2m           21Mi            
kube-system            kube-controller-manager-k8s-master03         1m           19Mi            
kube-system            kube-proxy-9dtz4                             11m          30Mi            
kube-system            kube-proxy-jh7rl                             1m           27Mi            
kube-system            kube-proxy-jvvwt                             17m          29Mi            
kube-system            kube-proxy-sh89l                             1m           29Mi            
kube-system            kube-proxy-t2j49                             16m          29Mi            
kube-system            kube-scheduler-k8s-master01                  6m           25Mi            
kube-system            kube-scheduler-k8s-master02                  6m           25Mi            
kube-system            kube-scheduler-k8s-master03                  6m           25Mi            
kube-system            metrics-server-7d9d8df576-jgnp2              2m           26Mi            
kubernetes-dashboard   dashboard-metrics-scraper-69b4796d9b-klnwr   1m           19Mi            
kubernetes-dashboard   kubernetes-dashboard-778584b9dd-pd5ln        1m           31Mi  
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;9-dashboard部署&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#9-dashboard部署&#34;&gt;#&lt;/a&gt; 9. Dashboard 部署&lt;/h4&gt;
&lt;h5 id=&#34;91-安装dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#91-安装dashboard&#34;&gt;#&lt;/a&gt; 9.1 安装 Dashboard&lt;/h5&gt;
&lt;p&gt;Dashboard 用于展示集群中的各类资源，同时也可以通过 Dashboard 实时查看 Pod 的日志和在容器中执行一些命令等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /root/k8s-ha-install/dashboard/

[root@k8s-master01 dashboard]# kubectl  create -f .
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;92-登录dashboard&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#92-登录dashboard&#34;&gt;#&lt;/a&gt; 9.2 登录 dashboard&lt;/h5&gt;
&lt;p&gt;在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问 Dashboard 的问题，参考下图：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--test-type --ignore-certificate-errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgWfHJ&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgWfHJ.png&#34; alt=&#34;pEgWfHJ.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更改 dashboard 的 svc 为 NodePort:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW5NR&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW5NR.png&#34; alt=&#34;pEgW5NR.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;将 ClusterIP 更改为 NodePort（如果已经为 NodePort 忽略此步骤）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;查看端口号：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.139.11   &amp;lt;none&amp;gt;        443:32409/TCP   24h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据自己的实例端口号，通过任意安装了 kube-proxy 的宿主机的 IP + 端口即可访问到 dashboard：&lt;/p&gt;
&lt;p&gt;访问 Dashboard：&lt;a href=&#34;https://192.168.181.129:31106&#34;&gt;https://192.168.1.71:32409&lt;/a&gt; （把 IP 地址和端口改成你自己的）选择登录方式为令牌（即 token 方式），参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgW736&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgW736.png&#34; alt=&#34;pEgW736.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;创建登录 Token：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create token admin-user -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将 token 值输入到令牌后，单击登录即可访问 Dashboard，参考下图：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://imgse.com/i/pEgfPv8&#34;&gt;&lt;img loading=&#34;lazy&#34; data-src=&#34;https://s21.ax1x.com/2025/04/09/pEgfPv8.png&#34; alt=&#34;pEgfPv8.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;10必看一些必须的配置更改&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#10必看一些必须的配置更改&#34;&gt;#&lt;/a&gt; 10.【必看】一些必须的配置更改&lt;/h4&gt;
&lt;p&gt;将 Kube-proxy 改为 ipvs 模式，因为在初始化集群的时候注释了 ipvs 配置，所以需要自行修改一下：&lt;/p&gt;
&lt;p&gt;在 master01 节点执行：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
mode: ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;更新 Kube-Proxy 的 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;验证 Kube-Proxy 模式:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01]# curl 127.0.0.1:10249/proxyMode
ipvs
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;11必看注意事项&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11必看注意事项&#34;&gt;#&lt;/a&gt; 11.【必看】注意事项&lt;/h4&gt;
&lt;p&gt;注意：kubeadm 安装的集群，证书有效期默认是一年。master 节点的 kube-apiserver、kube-scheduler、kube-controller-manager、etcd 都是以容器运行的。可以通过 kubectl get po -n kube-system 查看。&lt;/p&gt;
&lt;p&gt;启动和二进制不同的是，kubelet 的配置文件在 /etc/sysconfig/kubelet 和 /var/lib/kubelet/config.yaml，修改后需要重启 kubelet 进程。&lt;/p&gt;
&lt;p&gt;其他组件的配置文件在 /etc/kubernetes/manifests 目录下，比如 kube-apiserver.yaml，该 yaml 文件更改后，kubelet 会自动刷新配置，也就是会重启 pod。不能再次创建该文件。&lt;/p&gt;
&lt;p&gt;kube-proxy 的配置在 kube-system 命名空间下的 configmap 中，可以通过&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进行更改，更改完成后，可以通过 patch 重启 kube-proxy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch daemonset kube-proxy -p &amp;quot;&amp;#123;\&amp;quot;spec\&amp;quot;:&amp;#123;\&amp;quot;template\&amp;quot;:&amp;#123;\&amp;quot;metadata\&amp;quot;:&amp;#123;\&amp;quot;annotations\&amp;quot;:&amp;#123;\&amp;quot;date\&amp;quot;:\&amp;quot;`date +&#39;%s&#39;`\&amp;quot;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;#125;&amp;quot; -n kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Kubeadm 安装后，master 节点默认不允许部署 pod，可以通过以下方式删除 Taint，即可部署 Pod：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@k8s-master01 ~]# kubectl  taint node  -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;12-containerd配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-containerd配置镜像加速&#34;&gt;#&lt;/a&gt; 12. Containerd 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# vim /etc/containerd/config.toml
#添加以下配置镜像加速服务
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;docker.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://registry-1.docker.io&amp;quot;, &amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;]
       [plugins.&amp;quot;io.containerd.grpc.v1.cri&amp;quot;.registry.mirrors.&amp;quot;registry.k8s.io&amp;quot;]
        endpoint=[&amp;quot;https://dockerproxy.com&amp;quot;, &amp;quot;https://mirror.baidubce.com&amp;quot;,&amp;quot;https://ccr.ccs.tencentyun.com&amp;quot;,&amp;quot;https://docker.m.daocloud.io&amp;quot;,&amp;quot;https://docker.nju.edu.cn&amp;quot;,&amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hbv0b596.mirror.aliyuncs.com&amp;quot;, &amp;quot;https://k8s.m.daocloud.io&amp;quot;, &amp;quot;https://docker.mirrors.ustc.edu.cn&amp;quot;,&amp;quot;https://hub-mirror.c.163.com&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Containerd：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl restart containerd
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;13-docker配置镜像加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-docker配置镜像加速&#34;&gt;#&lt;/a&gt; 13. Docker 配置镜像加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;# sudo mkdir -p /etc/docker
# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;所有节点重新启动 Docker：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# systemctl daemon-reload
# systemctl enable --now docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;本文出自于：&lt;a href=&#34;https://edu.51cto.com/course/23845.html&#34;&gt;https://edu.51cto.com/course/23845.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
        <category term="Kubernetes" />
        <updated>2025-04-09T10:28:34.000Z</updated>
    </entry>
    <entry>
        <id>http://xuyong.cn/posts/1922841233.html</id>
        <title>Rsync服务实践</title>
        <link rel="alternate" href="http://xuyong.cn/posts/1922841233.html"/>
        <content type="html">&lt;h3 id=&#34;ursync服务实践u&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ursync服务实践u&#34;&gt;#&lt;/a&gt; &lt;u&gt;Rsync 服务实践&lt;/u&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;1rsync服务端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1rsync服务端&#34;&gt;#&lt;/a&gt; 1.rsync 服务端&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-rsync客户端&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-rsync客户端&#34;&gt;#&lt;/a&gt; 2. rsync 客户端&lt;/h4&gt;
&lt;h5 id=&#34;21-安装rsync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-安装rsync&#34;&gt;#&lt;/a&gt; 2.1 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-配置传输密码&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-配置传输密码&#34;&gt;#&lt;/a&gt; 2.2 配置传输密码&lt;/h5&gt;
&lt;p&gt;方法 1：将密码写入文件&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]#  echo &#39;your passwd&#39; &amp;gt; /etc/rsync.pass
[root@db01 ~]# cat /etc/rsync.pass 
your passwd
[root@db01 ~]# chmod 600 /etc/rsync.pass
--测试收发数据：
[root@db01 ~]# rsync -avz --password-file=/etc/rsync.pass /root/test rsync_backup@192.168.40.101::backup_file
sending incremental file list

sent 47 bytes  received 20 bytes  134.00 bytes/sec
total size is 0  speedup is 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;方法 2：使用密码环境变量 RSYNC_PASSWORD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# export RSYNC_PASSWORD=&#39;your passwd&#39;
--测试收发数据：
[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file
sending incremental file list

sent 47 bytes  received 20 bytes  134.00 bytes/sec
total size is 0  speedup is 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;ursync企业级备份案例u&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ursync企业级备份案例u&#34;&gt;#&lt;/a&gt; &lt;u&gt;Rsync 企业级备份案例&lt;/u&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;客户端需求&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端每天凌晨 3 点备份 MySQL 至 /backup 下以 &amp;quot;主机名_IP 地址_当前时间命名&amp;quot; 的目录中&lt;/li&gt;
&lt;li&gt;客户端推送 /backup 目录下数据备份目录至 Rsync 备份服务器&lt;/li&gt;
&lt;li&gt;客户端只保留最近七天的备份数据，避免浪费磁盘空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;服务端需求&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务端部署 rsync 服务，用于接收用户的备份数据&lt;/li&gt;
&lt;li&gt;服务端每天校验客户端推送过来的数据是否完整，并将结果以邮件的方式发送给管理员&lt;/li&gt;
&lt;li&gt;服务端仅保留 6 个月的备份数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：所有服务器的备份目录均为 /backup，所有脚本存放目录均为 /scripts。&lt;/p&gt;
&lt;h4 id=&#34;1-服务端部署rsync服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-服务端部署rsync服务&#34;&gt;#&lt;/a&gt; &lt;strong&gt;1. 服务端部署 rsync 服务&lt;/strong&gt;&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux-2&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync-2&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件-2&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限-2&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启-2&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-rsync客户端-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-rsync客户端-2&#34;&gt;#&lt;/a&gt; 2. rsync 客户端&lt;/h4&gt;
&lt;h5 id=&#34;21-安装rsync-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#21-安装rsync-2&#34;&gt;#&lt;/a&gt; 2.1 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# yum install nfs-utils -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;22-测试客户端备份数据并推送至rsync服务器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#22-测试客户端备份数据并推送至rsync服务器&#34;&gt;#&lt;/a&gt; 2.2 测试客户端备份数据并推送至 rsync 服务器&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# export RSYNC_PASSWORD=&#39;your passwd&#39;
[root@db01 ~]# rsync -avz /root/test rsync_backup@192.168.40.101::backup_file
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-客户端备份数据并推送至rsync服务器&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-客户端备份数据并推送至rsync服务器&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.3 客户端备份数据并推送至 rsync 服务器&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@db01 ~]# mkdir /scripts
[root@db01 ~]# cat /scripts/mysql_backup.sh 
#!/bin/bash
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin

#1、定义变量
Host=$(hostname)
Ip=$(ifconfig ens192 | awk &#39;NR==2&amp;#123;print $2&amp;#125;&#39;)
Date=$(date +%F)
BackupDir=/backup/mysql
Dest=$&amp;#123;BackupDir&amp;#125;/$&amp;#123;Host&amp;#125;_$&amp;#123;Ip&amp;#125;_$&amp;#123;Date&amp;#125;
FILE_NAME=mysql_backup_`date &#39;+%Y%m%d%H%M%S&#39;`;
OLDBINLOG=/var/lib/mysql/oldbinlog

#2、创建备份目录
if [ ! -d $Dest ];then
  mkdir -p $Dest
fi

#3、备份目录
/usr/bin/mysqldump -u&#39;root&#39; -p&#39;your passwd&#39; nf_flms &amp;gt; $Dest/nf-flms_$&amp;#123;FILE_NAME&amp;#125;.sql
tar -czvf $Dest/$&amp;#123;FILE_NAME&amp;#125;.tar.gz $Dest/nf-flms_$&amp;#123;FILE_NAME&amp;#125;.sql
rm -rf $Dest/*$&amp;#123;FILE_NAME&amp;#125;.sql
echo &amp;quot;Your database backup successfully&amp;quot;

#4、校验
md5sum $Dest/* &amp;gt;$Dest/backup_check_$Date

#5、将备份目录推动到rsync服务端
Rsync_Ip=192.168.1.145
Rsync_user=rsync_backup
Rsync_Module=backup_mysql
export RSYNC_PASSWORD=your passwd
rsync -avz $Dest $Rsync_user@$Rsync_Ip::$Rsync_Module

#6、删除15天备份目录
find $Dest -type d -mtime +15 | xargs rm -rf
echo &amp;quot;remove file  successfully&amp;quot;

[root@db01 ~]# chmod +x /scripts/etc_backup.sh
[root@db01 ~]# crontab -e
00 03 * * * /bin/bash /scripts/mysql_backup.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-服务端校验数据并将结果以邮件发送给管理员&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-服务端校验数据并将结果以邮件发送给管理员&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.4 服务端校验数据并将结果以邮件发送给管理员&lt;/strong&gt;&lt;/h5&gt;
&lt;h6 id=&#34;241-配置邮件服务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#241-配置邮件服务&#34;&gt;#&lt;/a&gt; 2.4.1 配置邮件服务&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum -y install mailx
[root@backup ~]# cat /etc/mail.rc      #最后一行插入
set from=373370405@qq.com
set smtp=smtps://smtp.qq.com:465
set smtp-auth-user=373370405@qq.com
set smtp-auth-password=**********   # 发件邮箱的授权码
set smtp-auth=login
set ssl-verify=ignore
set nss-config-dir=/etc/pki/nssdb
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;242-发送邮件测试&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#242-发送邮件测试&#34;&gt;#&lt;/a&gt; 2.4.2 发送邮件测试&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]#  echo Hello World | mail -s test 373370405@qq.com &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;243-配置脚本校验数据并将结果发送给管理员&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#243-配置脚本校验数据并将结果发送给管理员&#34;&gt;#&lt;/a&gt; 2.4.3 配置脚本校验数据并将结果发送给管理员&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup mysql]# cat /scripts/check_backup.sh 
#!/bin/bash
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin

#1、定义变量
Path=/backup/mysql
Date=$(date +%F)

#2、查看flag文件，并对对文件进行校验,然后将校验的结果保存至result_时间
find $Path -type f -name &amp;quot;backup_check_$&amp;#123;Date&amp;#125;*&amp;quot;|xargs md5sum -c &amp;gt;$Path/result_$&amp;#123;Date&amp;#125;

#3、将校验结果发送邮件给管理员
mail -s &amp;quot;Mysql Backup&amp;quot; 373370405@qq.com &amp;lt;$Path/result_$&amp;#123;Date&amp;#125; &amp;amp;&amp;gt; /dev/null

#4、删除超过7天的校验结果文件，删除超过180天的备份数据文件
find $Path -type f -name &amp;quot;result*&amp;quot; -mtime +7 | xargs rm -rf
find $Path -type f -mtime +180 | xargs rm -rf
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;244-写计划任务&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#244-写计划任务&#34;&gt;#&lt;/a&gt; &lt;strong&gt;2.4.4 写计划任务&lt;/strong&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# chmod +x /scripts/check_backup.sh 
[root@db01 ~]# crontab -e
00 06 * * * /bin/bash /scripts/mysql_backup.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;rsyncsersync实现数据实时同步&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#rsyncsersync实现数据实时同步&#34;&gt;#&lt;/a&gt; Rsync+sersync 实现数据实时同步&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;主机名&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;角色&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;server&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.101&lt;/td&gt;
&lt;td&gt;rsync 服务端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;client&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.40.102&lt;/td&gt;
&lt;td&gt;rsync 客户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;1rsync服务端-2&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1rsync服务端-2&#34;&gt;#&lt;/a&gt; 1.rsync 服务端&lt;/h4&gt;
&lt;h5 id=&#34;11-关闭防火墙-selinux-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#11-关闭防火墙-selinux-3&#34;&gt;#&lt;/a&gt; 1.1 关闭防火墙、selinux&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@localhost ~]# hostnamectl set-hostname backup
[root@localhost ~]# bash
[root@backup ~]# hostnamectl set-hostname aizj_lb01
[root@backup ~]# systemctl stop firewalld
[root@backup ~]# systemctl disable firewalld
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@backup ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config
[root@backup ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate git -y
[root@backup ~]# yum update -y --exclude=kernel* &amp;amp;&amp;amp; reboot
[root@backup ~]# echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone
[root@backup ~]# ntpdate time2.aliyun.com
[root@backup ~]# crontab -e
*/5 * * * * /usr/sbin/ntpdate time2.aliyun.com &amp;amp;&amp;gt; /dev/nul
[root@backup ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;12-安装rsync-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#12-安装rsync-3&#34;&gt;#&lt;/a&gt; 1.2 安装 rsync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# yum install -y rsync
[root@server ~]# systemctl start rsyncd
[root@server ~]# systemctl enable rsyncd
[root@backup ~]# useradd -M -s /sbin/nologin rsync
[root@backup ~]# mkdir -p /backup/mysql  /backup/file
[root@backup ~]# chown -R rsync.rsync /backup/mysql /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;13-修改配置文件-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#13-修改配置文件-3&#34;&gt;#&lt;/a&gt; 1.3 修改配置文件&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;&lt;mark&gt;#生产环境中取消注释，导致备份数据报错&lt;/mark&gt;&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#带注释配置文件
[root@backup ~]# vim /etc/rsyncd.conf
uid = rsync             #运行服务的用户
gid = rsync             #运行服务的组
port = 873              #服务监听端口
fake super = yes        #服务无需使用root用户身份，即可接收文件的完整属性
use chroot = no         #禁锢目录,不允许获取root权限
max connections = 200   #最大连接数,最多能有多少个客户端跟服务端的873端口建立连接
timeout = 600           #超时时间
ignore errors          #忽略错误
read only = false      #客户是否只读
list = false           #不允许查看模块信息
auth users = rsync_backup         #定义虚拟用户，用户数据传输
secrets file = /etc/rsync.passwd  #定义虚拟用户密码认证文件
log file = /var/log/rsyncd.log    #日志文件存放的位置
[backup_mysql]         #模块名
comment = welcome to rsync_backup
path = /backup/mysql   #数据存放目录
[backup_file]          #模块名
comment = welcome to rsync_backup
path = /backup/file    #数据存放目录 

#不带注释配置文件
[root@backup ~]# cat /etc/rsyncd.conf
uid = rsync        
gid = rsync         
port = 873     
fake super = yes     
use chroot = no        
max connections = 200  
timeout = 600         
ignore errors       
read only = false    
list = false          
auth users = rsync_backup        
secrets file = /etc/rsync.passwd
log file = /var/log/rsyncd.log    
[backup_mysql]       
comment = welcome to rsync_backup
path = /backup/mysql  
[backup_file]         
comment = welcome to rsync_backup
path = /backup/file 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;4-创建虚拟用户密码文件并设置权限-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-创建虚拟用户密码文件并设置权限-3&#34;&gt;#&lt;/a&gt; 4. 创建虚拟用户密码文件并设置权限&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# cat /etc/rsync.passwd
rsync_backup:your passwd
[root@backup ~]# chmod 600 /etc/rsync.passwd
[root@backup ~]# systemctl restart rsyncd &amp;amp;&amp;amp; systemctl status rsyncd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;5-检查服务端口是否开启-3&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-检查服务端口是否开启-3&#34;&gt;#&lt;/a&gt; 5. 检查服务端口是否开启&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@backup ~]# netstat -lntp | grep &amp;quot;rsync&amp;quot;
tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN      20357/rsync         
tcp6       0      0 :::873                  :::*                    LISTEN      20357/rsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-客户端安装sersync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-客户端安装sersync&#34;&gt;#&lt;/a&gt; 2. 客户端安装 sersync&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;2.1 安装 sercync 依赖&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs ~]# yum install -y inotify-tools rsync
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.2 安装 sercync&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs ~]# mkdir -p /soft
[root@nfs ~]# cd /soft/
[root@nfs ~]# wget https://down.whsir.com/downloads/sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs soft]# tar -xf sersync2.5.4_64bit_binary_stable_final.tar.gz
[root@nfs soft]# mv GNU-Linux-x86 /usr/local/sersync
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;23-修改配置文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#23-修改配置文件&#34;&gt;#&lt;/a&gt; 2.3 &lt;strong&gt;修改配置文件&lt;/strong&gt;&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs soft]# cd /usr/local/sersync/
[root@nfs sersync]# cp confxml.xml confxml.xml.bak
[root@nfs sersync]# vim confxml.xml
...
5    &amp;lt;fileSystem xfs=&amp;quot;true&amp;quot;/&amp;gt;    #第5行 false改为true
13          &amp;lt;delete start=&amp;quot;true&amp;quot;/&amp;gt; #第13-20行 false改为true,#说明：监控以上变化推送
14        &amp;lt;createFolder start=&amp;quot;true&amp;quot;/&amp;gt;
15        &amp;lt;createFile start=&amp;quot;false&amp;quot;/&amp;gt;
16        &amp;lt;closeWrite start=&amp;quot;true&amp;quot;/&amp;gt;
17        &amp;lt;moveFrom start=&amp;quot;true&amp;quot;/&amp;gt;
18        &amp;lt;moveTo start=&amp;quot;true&amp;quot;/&amp;gt;
19        &amp;lt;attrib start=&amp;quot;true&amp;quot;/&amp;gt;
20        &amp;lt;modify start=&amp;quot;true&amp;quot;/&amp;gt;
24        &amp;lt;localpath watch=&amp;quot;/data&amp;quot;&amp;gt;      #监控的本地目录
25             &amp;lt;remote ip=&amp;quot;192.168.1.145&amp;quot; name=&amp;quot;backup_file&amp;quot;/&amp;gt;  #rsync服务端IP和模块名backup_file
30      &amp;lt;commonParams params=&amp;quot;-avz&amp;quot;/&amp;gt;  #rsync命令选项
31      &amp;lt;auth start=&amp;quot;true&amp;quot; users=&amp;quot;rsync_backup&amp;quot; passwordfile=&amp;quot;/etc/rsync.passwd&amp;quot;/&amp;gt; #rsync认证信息
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;24-生成密码文件&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#24-生成密码文件&#34;&gt;#&lt;/a&gt; 2.4 生成密码文件&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# echo &#39;your passwd&#39; &amp;gt; /etc/rsync.passwd
[root@nfs sersync]# chmod 600 /etc/rsync.passwd
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;25-启动sersync&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#25-启动sersync&#34;&gt;#&lt;/a&gt; 2.5 启动 sersync&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# ln -s /usr/local/sersync/sersync2 /usr/bin/
[root@nfs sersync]# sersync2 -dro /usr/local/sersync/confxml.xml     #针对配置文件confxml.xml启动sersync
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.5 设置 sersync 开机自启&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@qzj_nfs sersync]# vim /etc/rc.d/rc.local   
/usr/local/sersync/sersync2 -d -r -o  /usr/local/sersync/confxml.xml  #在最后添加一行
[root@qzj_nfs sersync]# chmod +x /etc/rc.d/rc.local
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.6 测试&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在客户端 /data 目录增删改目录文件，rsync 服务端数据存放目录变化&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@backup backup]# watch ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2.7 添加脚本监控 sersync 是否正常运行&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@nfs sersync]# cat /scripts/check_sersync.sh 
#!/bin/sh
sersync=&amp;quot;/usr/local/sersync/sersync2&amp;quot;
confxml=&amp;quot;/usr/local/sersync/confxml.xml&amp;quot;
status=$(ps aux |grep &#39;sersync2&#39;|grep -v &#39;grep&#39;|wc -l)
if [ $status -eq 0 ];
then
$sersync -d -r -o $confxml &amp;amp;
else
exit 0;
fi

[root@nfs sersync]# chmod +x /scripts/check_sersync.sh
[root@nfs sersync]# crontab -l
*/5 * * * * /usr/bin/sh /scripts/check_sersync.sh &amp;amp;&amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;补充： 多实例情况&lt;/strong&gt;&lt;/em&gt;&lt;br /&gt;
 1、配置多个 confxml.xml 文件（比如：www、bbs、blog.... 等等）&lt;br /&gt;
2、修改端口、同步路径、模块名称&lt;br /&gt;
 3、根据不同的需求同步对应的实例文件&lt;br /&gt;
 /usr/local/sersync/sersync2 -dro /usr/local/sersync/www_confxml.xml&lt;br /&gt;
/usr/local/sersync/sersync2 -dro /usr/local/sersync/bbs_confxml.xml&lt;/p&gt;
</content>
        <category term="rsync" />
        <updated>2025-03-30T12:45:48.000Z</updated>
    </entry>
    <entry>
        <id>http://xuyong.cn/posts/3071070978.html</id>
        <title>企业级私有仓库Harbor搭建</title>
        <link rel="alternate" href="http://xuyong.cn/posts/3071070978.html"/>
        <content type="html">&lt;h3 id=&#34;企业级私有仓库harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#企业级私有仓库harbor&#34;&gt;#&lt;/a&gt; 企业级私有仓库 Harbor&lt;/h3&gt;
&lt;p&gt;企业部署 Kuberetes 集群环境之后，我们就可以将原来在传统虚拟机上运行的业务，迁移到 kubernetes 上，让 Kubernetes 通过容器的方式来管理。而一旦我们需要将传统业务使用容器的方式运行起来，就需要构建很多镜像，那么这些镜像就需要有一个专门的位置存储起来，为我们提供镜像上传和镜像下载等功能。但我们不能使用阿里云或者 Dockerhub 等仓库，首先拉取速度比较慢，其次镜像的安全性无法保证，所以就需要部署一个私有的镜像仓库来管理这些容器镜像。同时该仓库还需要提供高可用功能，确保随时都能上传和下载可用的容器镜像。&lt;/p&gt;
&lt;h4 id=&#34;1-关闭防火墙-selinux-环境配置&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1-关闭防火墙-selinux-环境配置&#34;&gt;#&lt;/a&gt; 1、关闭防火墙、Selinux、环境配置&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# sudo mkdir -p /etc/docker
[root@harbor ~]# hostnamectl set-hostname harbor
[root@harbor ~]# systemctl stop firewalld
[root@harbor ~]# systemctl disable firewalld
[root@harbor ~]# sed -i &#39;s/^SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/sysconfig/selinux
[root@harbor ~]# yum install net-tools vim tree lrzsz wget unzip dos2unix bash-completion  lsof ntp ntpdate -y
[root@harbor ~]# yum update -y
[root@harbor ~]# mkdir /soft /data /scripts /backup
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;2-docker安装&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2-docker安装&#34;&gt;#&lt;/a&gt; 2、Docker 安装&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# yum install -y yum-utils device-mapper-persistent-data lvm2
[root@harbor ~]# curl -o /etc/yum.repos.d/docker-ce.repo  https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@harbor ~]# yum list docker-ce --showduplicates |sort -r 
[root@harbor ~]# yum install docker-ce docker-compose -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-配置docker加速&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3-配置docker加速&#34;&gt;#&lt;/a&gt; 3、配置 Docker 加速&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# sudo mkdir -p /etc/docker
[root@harbor ~]# sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;
&amp;#123;
  &amp;quot;registry-mirrors&amp;quot;: [
	  &amp;quot;https://docker.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s.credclouds.com&amp;quot;,
	  &amp;quot;https://quay.credclouds.com&amp;quot;,
	  &amp;quot;https://gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://k8s-gcr.credclouds.com&amp;quot;,
	  &amp;quot;https://ghcr.credclouds.com&amp;quot;,
	  &amp;quot;https://do.nark.eu.org&amp;quot;,
	  &amp;quot;https://docker.m.daocloud.io&amp;quot;,
	  &amp;quot;https://docker.nju.edu.cn&amp;quot;,
	  &amp;quot;https://docker.mirrors.sjtug.sjtu.edu.cn&amp;quot;,
	  &amp;quot;https://docker.1panel.live&amp;quot;,
	  &amp;quot;https://docker.rainbond.cc&amp;quot;
  ], 
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;] 
&amp;#125;
EOF
[root@harbor ~]# systemctl enable docker --now
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;4-安装harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#4-安装harbor&#34;&gt;#&lt;/a&gt; 4、安装 Harbor&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor ~]# cd /soft/
[root@harbor ~]# wget https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz
[root@harbor soft]# tar xf harbor-offline-installer-v2.6.1.tgz
[root@harbor soft]# cd harbor
[root@harbor harbor]# vim harbor.yml
hostname: 192.168.1.134
...
#https:
#  # https port for harbor, default is 443
#  port: 443
#  # The path of cert and key files for nginx
#  certificate: /your/certificate/path
#  private_key: /your/private/key/path
...
harbor_admin_password: Harbor12345
[root@harbor harbor]#  ./install.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;5-配置nginx负载均衡调度&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#5-配置nginx负载均衡调度&#34;&gt;#&lt;/a&gt; 5、配置 Nginx 负载均衡调度&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@lb ~]# vim s.hmallleasing.com.conf
server &amp;#123;
    listen 443 ssl;
    server_name harbor.hmallleasing.com;
    client_max_body_size 1G; 
    ssl_prefer_server_ciphers on;
    ssl_certificate  /etc/nginx/sslkey/_.hmallleasing.com_chain.crt;
    ssl_certificate_key  /etc/nginx/sslkey/_.hmallleasing.com_key.key;
    location / &amp;#123;
        proxy_pass http://192.168.1.134;
#      include proxy_params;
#        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        proxy_connect_timeout 30;
        proxy_send_timeout 60;
        proxy_read_timeout 60;
        
        proxy_buffering on;
        proxy_buffer_size 32k;
        proxy_buffers 4 128k;
        proxy_temp_file_write_size 10240k;		
        proxy_max_temp_file_size 10240k;
    &amp;#125;
&amp;#125;

server &amp;#123;
    listen 80;
    server_name s.hmallleasing.com;
    return 302 https://$server_name$request_uri;
&amp;#125;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;6-推送镜像至harbor&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#6-推送镜像至harbor&#34;&gt;#&lt;/a&gt; 6、推送镜像至 Harbor&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;[root@harbor harbor]# docker tag beae173ccac6 harbor.hmallleasing.com/ops/busybox.v1
[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1
[root@harbor harbor]# docker login harbor.hmallleasing.com
[root@harbor harbor]# docker push harbor.hmallleasing.com/ops/busybox.v1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;7-harbor停止与启动&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#7-harbor停止与启动&#34;&gt;#&lt;/a&gt; 7、Harbor 停止与启动&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;#停用Harbor
[root@harbor harbor]# pwd
/soft/harbor
[root@harbor harbor]# docker-compose stop
 #启动Harbor
[root@harbor harbor]# docker-compose up -d
[root@harbor harbor]# docker-compose start
&lt;/code&gt;&lt;/pre&gt;
</content>
        <category term="Harbor" />
        <updated>2025-03-30T08:17:00.000Z</updated>
    </entry>
</feed>
